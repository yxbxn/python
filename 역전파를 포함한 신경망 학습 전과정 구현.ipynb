{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d18d01",
   "metadata": {},
   "source": [
    "# 역전파를 포함한 신경망 학습 및 추론 구현\n",
    "- Softmax-with-Loss 계층\n",
    "- 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e564e3",
   "metadata": {},
   "source": [
    "### Softmax-with-loss 계층\n",
    "- Softmax와 Cross Entropy Error가 같이 조합된 경우, 역전파가 단순히 y - t의 값 즉, 오차를 전달한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89292b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x-c)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    return exp_x / sum_exp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05682243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*(np.log(y+1e-7))) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e9569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None # 정답 레이블(원-핫 벡터)\n",
    "        \n",
    "    def forward(self,x,t): # Softmax의 출력값 y와 기존 정답라벨 t 비교\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        return loss\n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # 배치의 수(Batch size)로 나눠서 데이터 1개당 오차를 전달\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58617d",
   "metadata": {},
   "source": [
    "- Q. 배치의 수(Batch size)로 나눠서 데이터 1개당 오차를 전달하는 이유는 무엇일까?\n",
    "- CEE를 계산할 때 데이터 개수(batch_size)로 나눠 정규화하여 데이터 1개당 오차를 계산해 합했기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ad6bb",
   "metadata": {},
   "source": [
    "### 오차역전파를 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33d96c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65bc9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3ddab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None # 생성자에서 초기화해야 나중에 불러올 수 있음\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout) \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f38a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d11a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet: # 해당 TwoLayerNet은 수치미분 방식이 아닌 해석학적 방법(역전파)으로 기울기를 구한다\n",
    "    def __init__(self,input_size,hidden_size,output_size,\n",
    "                weight_init_std = 0.01):\n",
    "        # 가중치 초기화 - params 라는 딕셔너리에 보관되어 있다\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "        \n",
    "        # 신경망의 각 기능을 계층으로 구현할 것이다.\n",
    "        # 행렬곱 계층 2개 / 활성화함수(ReLU) 계층 1개 / SoftmaxwithLoss 계층 1개\n",
    "        \n",
    "        self.layers = OrderedDict() # 순서가 있는 딕셔너리\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W1\"],self.params[\"b1\"]) # Affine 클래스의 인스턴스를 보관\n",
    "        self.layers[\"ReLU\"] = ReLU()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W2\"],self.params[\"b2\"])\n",
    "        \n",
    "        self.lastlayer = SoftmaxWithLoss() # 마지막 계층\n",
    "        \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values(): # layers.values에는 Affine1 계층, ReLU계층, Affine2 계층이 순차적으로\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastlayer.forward(y,t) # CEE 산출\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1) # 행 별로 최댓값의 인덱스 추출\n",
    "        if t.ndim != 1: # 원-핫 인코디여부 판단\n",
    "            t = np.argmax(t,axis=1)\n",
    "        else:\n",
    "            pass # 원-핫 인코딩이 되지 않은 경우는 t 자체가 정답 레이블 정보\n",
    "        \n",
    "        return np.sum(y == t) / float(x.shape[0])\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self,x,t): # 역전파 해석학적 미분\n",
    "        # 순전파 예측하고 loss 계산까지 선행되어야 W 가 할당된다\n",
    "        self.loss(x,t)\n",
    "            \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout) # self.loss(x,t)를 시행하면 self.predict(x) 하게 되니까 y,t가 할당됨\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse() # 순전파 시행했던 것 순서 뒤집어서 역전파 시행\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장 / 기울기 값 저장\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine2\"].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2de88",
   "metadata": {},
   "source": [
    "- 신경망의 각 기능을 계층으로 구현할 것이다.\n",
    "- 행렬곱 계층 2개 / 활성화함수(ReLU) 계층 1개 / SoftmaxwithLoss 계층 1개\n",
    "- 행렬곱1 -> ReLU -> 행렬곱2 -> SoftmaxwithLoss  \n",
    "- 역전파를 시행하기 위해서는 predict와 loss 계산이 순차적으로 되어 있어야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517c4d0",
   "metadata": {},
   "source": [
    "  ### 오차역전파로 구한 기울기 검증하기\n",
    "  - 해석학적 미분과 수치학적 미분의 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2e3e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽어오기\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "batch_size = 3\n",
    "x_batch = x_train[:batch_size] # batch_size 3개\n",
    "t_batch = t_train[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b725f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매개변수 W1 의 차이  4.302967606482729e-10\n",
      "매개변수 b1 의 차이  2.6441861126786507e-09\n",
      "매개변수 W2 의 차이  5.019841105248321e-09\n",
      "매개변수 b2 의 차이  1.3966660540459807e-07\n"
     ]
    }
   ],
   "source": [
    "grd_backprop = network.gradient(x_batch,t_batch) # 역전파 방식으로 기울기 계산\n",
    "grd_numerical = network.numerical_gradient(x_batch,t_batch) # 수치미분 방식으로 기울기 계산\n",
    "\n",
    "# 기울기 차이의 절댓값의 평균\n",
    "\n",
    "for key in grd_numerical.keys():\n",
    "    diff = np.average(np.abs(grd_numerical[key] - grd_backprop[key]))\n",
    "    print(\"매개변수\",key,\"의 차이 \", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64997079",
   "metadata": {},
   "source": [
    "### 오차역전파를 적용한 신경망 구현하기\n",
    "- 실제 학습 진행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6de9e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 반복 0.09 0.0756\n",
      "\n",
      "600 번째 반복 0.09 0.0756\n",
      "\n",
      "1200 번째 반복 0.07 0.0756\n",
      "\n",
      "1800 번째 반복 0.06 0.0756\n",
      "\n",
      "2400 번째 반복 0.12 0.0756\n",
      "\n",
      "3000 번째 반복 0.05 0.0756\n",
      "\n",
      "3600 번째 반복 0.06 0.0756\n",
      "\n",
      "4200 번째 반복 0.06 0.0756\n",
      "\n",
      "4800 번째 반복 0.08 0.0756\n",
      "\n",
      "5400 번째 반복 0.08 0.0756\n",
      "\n",
      "6000 번째 반복 0.08 0.0756\n",
      "\n",
      "6600 번째 반복 0.06 0.0756\n",
      "\n",
      "7200 번째 반복 0.13 0.0756\n",
      "\n",
      "7800 번째 반복 0.05 0.0756\n",
      "\n",
      "8400 번째 반복 0.06 0.0756\n",
      "\n",
      "9000 번째 반복 0.07 0.0756\n",
      "\n",
      "9600 번째 반복 0.06 0.0756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽어온 뒤에 훈련, 평가데이터로 나누기\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "# 신경망 구현\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "loss_lst = [] ; train_acc_lst = [] ; test_acc_lst = []\n",
    "\n",
    "# 학습진행\n",
    "iters_num = 10000 ; batch_size = 100 ; lrn_rate = 0.1 ; train_size = x_train.shape[0]\n",
    "iter_per_epoch = max(train_size/batch_size,1) # 1epoch당 반복횟수\n",
    "\n",
    "for i in range(iters_num): # 학습자체는 10,000회 = iters_num 만큼 진행\n",
    "    mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[mask]\n",
    "    t_batch = t_train[mask]\n",
    "    \n",
    "    # 오차 역전파로 기울기 구하기\n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    # 경사하강법으로 기울기 갱신\n",
    "    for key in [\"W1\",\"W2\",\"b1\",\"b2\"]:\n",
    "        grad[key] -= lrn_rate * grad[key]\n",
    "        \n",
    "        loss = network.loss(x_batch,t_batch) # 손실함수 기록하기\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "    if i%iter_per_epoch == 0: # 1epoch 마다 정확도 기록하기\n",
    "        train_acc = network.accuracy(x_batch,t_batch)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        \n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        test_acc_lst.append(test_acc)\n",
    "        print(i,\"번째 반복\",train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dabda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
