{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e48f49",
   "metadata": {},
   "source": [
    "## 바른 학습을 위해\n",
    "- 오버피팅 방지하는 다양한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bb94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eaef7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6743c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad2a4b",
   "metadata": {},
   "source": [
    "epoch마다 train acc와 test acc 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5402ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.09, test acc:0.1032\n",
      "epoch:1, train acc:0.09666666666666666, test acc:0.1038\n",
      "epoch:2, train acc:0.09333333333333334, test acc:0.1052\n",
      "epoch:3, train acc:0.10333333333333333, test acc:0.11\n",
      "epoch:4, train acc:0.10333333333333333, test acc:0.1136\n",
      "epoch:5, train acc:0.11, test acc:0.1154\n",
      "epoch:6, train acc:0.11666666666666667, test acc:0.121\n",
      "epoch:7, train acc:0.15333333333333332, test acc:0.1285\n",
      "epoch:8, train acc:0.15666666666666668, test acc:0.1361\n",
      "epoch:9, train acc:0.17, test acc:0.1402\n",
      "epoch:10, train acc:0.2, test acc:0.1572\n",
      "epoch:11, train acc:0.21333333333333335, test acc:0.165\n",
      "epoch:12, train acc:0.25666666666666665, test acc:0.1903\n",
      "epoch:13, train acc:0.2866666666666667, test acc:0.2089\n",
      "epoch:14, train acc:0.31666666666666665, test acc:0.2305\n",
      "epoch:15, train acc:0.33666666666666667, test acc:0.2481\n",
      "epoch:16, train acc:0.3433333333333333, test acc:0.2492\n",
      "epoch:17, train acc:0.36666666666666664, test acc:0.2649\n",
      "epoch:18, train acc:0.3933333333333333, test acc:0.29\n",
      "epoch:19, train acc:0.41, test acc:0.2936\n",
      "epoch:20, train acc:0.42, test acc:0.3055\n",
      "epoch:21, train acc:0.45, test acc:0.3339\n",
      "epoch:22, train acc:0.47, test acc:0.3494\n",
      "epoch:23, train acc:0.49, test acc:0.3659\n",
      "epoch:24, train acc:0.5033333333333333, test acc:0.381\n",
      "epoch:25, train acc:0.5133333333333333, test acc:0.3917\n",
      "epoch:26, train acc:0.51, test acc:0.3864\n",
      "epoch:27, train acc:0.5333333333333333, test acc:0.4087\n",
      "epoch:28, train acc:0.54, test acc:0.4313\n",
      "epoch:29, train acc:0.58, test acc:0.4578\n",
      "epoch:30, train acc:0.6033333333333334, test acc:0.4705\n",
      "epoch:31, train acc:0.63, test acc:0.49\n",
      "epoch:32, train acc:0.61, test acc:0.4857\n",
      "epoch:33, train acc:0.6333333333333333, test acc:0.5029\n",
      "epoch:34, train acc:0.6533333333333333, test acc:0.5119\n",
      "epoch:35, train acc:0.6533333333333333, test acc:0.5203\n",
      "epoch:36, train acc:0.64, test acc:0.5153\n",
      "epoch:37, train acc:0.6466666666666666, test acc:0.5246\n",
      "epoch:38, train acc:0.6366666666666667, test acc:0.524\n",
      "epoch:39, train acc:0.64, test acc:0.5231\n",
      "epoch:40, train acc:0.6533333333333333, test acc:0.5281\n",
      "epoch:41, train acc:0.65, test acc:0.521\n",
      "epoch:42, train acc:0.66, test acc:0.5451\n",
      "epoch:43, train acc:0.6733333333333333, test acc:0.5437\n",
      "epoch:44, train acc:0.68, test acc:0.5538\n",
      "epoch:45, train acc:0.6966666666666667, test acc:0.5625\n",
      "epoch:46, train acc:0.7, test acc:0.5566\n",
      "epoch:47, train acc:0.67, test acc:0.5598\n",
      "epoch:48, train acc:0.6833333333333333, test acc:0.5588\n",
      "epoch:49, train acc:0.7133333333333334, test acc:0.5658\n",
      "epoch:50, train acc:0.7, test acc:0.5683\n",
      "epoch:51, train acc:0.7233333333333334, test acc:0.5884\n",
      "epoch:52, train acc:0.7366666666666667, test acc:0.5923\n",
      "epoch:53, train acc:0.7333333333333333, test acc:0.5907\n",
      "epoch:54, train acc:0.7333333333333333, test acc:0.5887\n",
      "epoch:55, train acc:0.7433333333333333, test acc:0.5882\n",
      "epoch:56, train acc:0.75, test acc:0.5955\n",
      "epoch:57, train acc:0.7533333333333333, test acc:0.5918\n",
      "epoch:58, train acc:0.7533333333333333, test acc:0.6037\n",
      "epoch:59, train acc:0.76, test acc:0.598\n",
      "epoch:60, train acc:0.7633333333333333, test acc:0.6029\n",
      "epoch:61, train acc:0.7633333333333333, test acc:0.6028\n",
      "epoch:62, train acc:0.7666666666666667, test acc:0.6113\n",
      "epoch:63, train acc:0.77, test acc:0.6057\n",
      "epoch:64, train acc:0.76, test acc:0.6063\n",
      "epoch:65, train acc:0.7633333333333333, test acc:0.6145\n",
      "epoch:66, train acc:0.7666666666666667, test acc:0.6256\n",
      "epoch:67, train acc:0.7566666666666667, test acc:0.6171\n",
      "epoch:68, train acc:0.7733333333333333, test acc:0.6226\n",
      "epoch:69, train acc:0.7633333333333333, test acc:0.6221\n",
      "epoch:70, train acc:0.7633333333333333, test acc:0.607\n",
      "epoch:71, train acc:0.7766666666666666, test acc:0.6215\n",
      "epoch:72, train acc:0.7366666666666667, test acc:0.5866\n",
      "epoch:73, train acc:0.7633333333333333, test acc:0.6115\n",
      "epoch:74, train acc:0.77, test acc:0.6154\n",
      "epoch:75, train acc:0.7766666666666666, test acc:0.6166\n",
      "epoch:76, train acc:0.76, test acc:0.6041\n",
      "epoch:77, train acc:0.7566666666666667, test acc:0.6088\n",
      "epoch:78, train acc:0.7533333333333333, test acc:0.6057\n",
      "epoch:79, train acc:0.7366666666666667, test acc:0.5942\n",
      "epoch:80, train acc:0.78, test acc:0.6295\n",
      "epoch:81, train acc:0.7566666666666667, test acc:0.6334\n",
      "epoch:82, train acc:0.7533333333333333, test acc:0.6104\n",
      "epoch:83, train acc:0.7633333333333333, test acc:0.6198\n",
      "epoch:84, train acc:0.78, test acc:0.6275\n",
      "epoch:85, train acc:0.77, test acc:0.6434\n",
      "epoch:86, train acc:0.7666666666666667, test acc:0.6388\n",
      "epoch:87, train acc:0.7766666666666666, test acc:0.63\n",
      "epoch:88, train acc:0.8, test acc:0.6362\n",
      "epoch:89, train acc:0.8033333333333333, test acc:0.6375\n",
      "epoch:90, train acc:0.8, test acc:0.6257\n",
      "epoch:91, train acc:0.8, test acc:0.65\n",
      "epoch:92, train acc:0.8233333333333334, test acc:0.6515\n",
      "epoch:93, train acc:0.7933333333333333, test acc:0.6451\n",
      "epoch:94, train acc:0.8033333333333333, test acc:0.6619\n",
      "epoch:95, train acc:0.82, test acc:0.6536\n",
      "epoch:96, train acc:0.8066666666666666, test acc:0.6611\n",
      "epoch:97, train acc:0.8266666666666667, test acc:0.659\n",
      "epoch:98, train acc:0.8333333333333334, test acc:0.6707\n",
      "epoch:99, train acc:0.8333333333333334, test acc:0.668\n",
      "epoch:100, train acc:0.8066666666666666, test acc:0.6427\n",
      "epoch:101, train acc:0.8266666666666667, test acc:0.6593\n",
      "epoch:102, train acc:0.8166666666666667, test acc:0.6568\n",
      "epoch:103, train acc:0.8233333333333334, test acc:0.6626\n",
      "epoch:104, train acc:0.83, test acc:0.6713\n",
      "epoch:105, train acc:0.82, test acc:0.6539\n",
      "epoch:106, train acc:0.84, test acc:0.6664\n",
      "epoch:107, train acc:0.84, test acc:0.6651\n",
      "epoch:108, train acc:0.83, test acc:0.6728\n",
      "epoch:109, train acc:0.8233333333333334, test acc:0.6611\n",
      "epoch:110, train acc:0.8166666666666667, test acc:0.6614\n",
      "epoch:111, train acc:0.8266666666666667, test acc:0.661\n",
      "epoch:112, train acc:0.8433333333333334, test acc:0.6724\n",
      "epoch:113, train acc:0.8233333333333334, test acc:0.6681\n",
      "epoch:114, train acc:0.8533333333333334, test acc:0.6844\n",
      "epoch:115, train acc:0.85, test acc:0.6868\n",
      "epoch:116, train acc:0.8533333333333334, test acc:0.6767\n",
      "epoch:117, train acc:0.8466666666666667, test acc:0.6758\n",
      "epoch:118, train acc:0.8366666666666667, test acc:0.6752\n",
      "epoch:119, train acc:0.8466666666666667, test acc:0.6855\n",
      "epoch:120, train acc:0.8333333333333334, test acc:0.6906\n",
      "epoch:121, train acc:0.85, test acc:0.6885\n",
      "epoch:122, train acc:0.8, test acc:0.6599\n",
      "epoch:123, train acc:0.84, test acc:0.6825\n",
      "epoch:124, train acc:0.8233333333333334, test acc:0.6722\n",
      "epoch:125, train acc:0.83, test acc:0.6813\n",
      "epoch:126, train acc:0.85, test acc:0.6846\n",
      "epoch:127, train acc:0.8533333333333334, test acc:0.6916\n",
      "epoch:128, train acc:0.8466666666666667, test acc:0.6972\n",
      "epoch:129, train acc:0.85, test acc:0.6963\n",
      "epoch:130, train acc:0.86, test acc:0.6975\n",
      "epoch:131, train acc:0.8466666666666667, test acc:0.6941\n",
      "epoch:132, train acc:0.86, test acc:0.6985\n",
      "epoch:133, train acc:0.85, test acc:0.6934\n",
      "epoch:134, train acc:0.8366666666666667, test acc:0.7008\n",
      "epoch:135, train acc:0.8433333333333334, test acc:0.6916\n",
      "epoch:136, train acc:0.8566666666666667, test acc:0.6962\n",
      "epoch:137, train acc:0.8633333333333333, test acc:0.7053\n",
      "epoch:138, train acc:0.8433333333333334, test acc:0.7001\n",
      "epoch:139, train acc:0.8533333333333334, test acc:0.701\n",
      "epoch:140, train acc:0.8566666666666667, test acc:0.7044\n",
      "epoch:141, train acc:0.8633333333333333, test acc:0.7082\n",
      "epoch:142, train acc:0.8566666666666667, test acc:0.7009\n",
      "epoch:143, train acc:0.84, test acc:0.6959\n",
      "epoch:144, train acc:0.8533333333333334, test acc:0.7009\n",
      "epoch:145, train acc:0.85, test acc:0.6982\n",
      "epoch:146, train acc:0.85, test acc:0.7013\n",
      "epoch:147, train acc:0.8533333333333334, test acc:0.6952\n",
      "epoch:148, train acc:0.8566666666666667, test acc:0.7073\n",
      "epoch:149, train acc:0.8433333333333334, test acc:0.7042\n",
      "epoch:150, train acc:0.8566666666666667, test acc:0.7012\n",
      "epoch:151, train acc:0.8633333333333333, test acc:0.7064\n",
      "epoch:152, train acc:0.8466666666666667, test acc:0.7064\n",
      "epoch:153, train acc:0.85, test acc:0.7067\n",
      "epoch:154, train acc:0.8433333333333334, test acc:0.7016\n",
      "epoch:155, train acc:0.85, test acc:0.7091\n",
      "epoch:156, train acc:0.8566666666666667, test acc:0.6986\n",
      "epoch:157, train acc:0.85, test acc:0.6979\n",
      "epoch:158, train acc:0.8633333333333333, test acc:0.7031\n",
      "epoch:159, train acc:0.8666666666666667, test acc:0.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:160, train acc:0.86, test acc:0.7175\n",
      "epoch:161, train acc:0.86, test acc:0.7119\n",
      "epoch:162, train acc:0.85, test acc:0.7002\n",
      "epoch:163, train acc:0.8433333333333334, test acc:0.6957\n",
      "epoch:164, train acc:0.8533333333333334, test acc:0.7033\n",
      "epoch:165, train acc:0.8566666666666667, test acc:0.7083\n",
      "epoch:166, train acc:0.8466666666666667, test acc:0.7058\n",
      "epoch:167, train acc:0.8566666666666667, test acc:0.7111\n",
      "epoch:168, train acc:0.8533333333333334, test acc:0.7062\n",
      "epoch:169, train acc:0.85, test acc:0.7024\n",
      "epoch:170, train acc:0.8533333333333334, test acc:0.7052\n",
      "epoch:171, train acc:0.84, test acc:0.7006\n",
      "epoch:172, train acc:0.85, test acc:0.708\n",
      "epoch:173, train acc:0.8466666666666667, test acc:0.7057\n",
      "epoch:174, train acc:0.8533333333333334, test acc:0.7133\n",
      "epoch:175, train acc:0.8433333333333334, test acc:0.702\n",
      "epoch:176, train acc:0.8533333333333334, test acc:0.7088\n",
      "epoch:177, train acc:0.8533333333333334, test acc:0.7119\n",
      "epoch:178, train acc:0.8566666666666667, test acc:0.7089\n",
      "epoch:179, train acc:0.86, test acc:0.7106\n",
      "epoch:180, train acc:0.8466666666666667, test acc:0.7037\n",
      "epoch:181, train acc:0.84, test acc:0.7016\n",
      "epoch:182, train acc:0.8633333333333333, test acc:0.708\n",
      "epoch:183, train acc:0.8566666666666667, test acc:0.7131\n",
      "epoch:184, train acc:0.8566666666666667, test acc:0.7079\n",
      "epoch:185, train acc:0.87, test acc:0.7209\n",
      "epoch:186, train acc:0.8566666666666667, test acc:0.7096\n",
      "epoch:187, train acc:0.85, test acc:0.7054\n",
      "epoch:188, train acc:0.8466666666666667, test acc:0.7036\n",
      "epoch:189, train acc:0.85, test acc:0.706\n",
      "epoch:190, train acc:0.8566666666666667, test acc:0.7041\n",
      "epoch:191, train acc:0.8466666666666667, test acc:0.7059\n",
      "epoch:192, train acc:0.8566666666666667, test acc:0.7055\n",
      "epoch:193, train acc:0.8533333333333334, test acc:0.7004\n",
      "epoch:194, train acc:0.85, test acc:0.704\n",
      "epoch:195, train acc:0.8666666666666667, test acc:0.7041\n",
      "epoch:196, train acc:0.8666666666666667, test acc:0.7093\n",
      "epoch:197, train acc:0.87, test acc:0.7136\n",
      "epoch:198, train acc:0.86, test acc:0.7095\n",
      "epoch:199, train acc:0.8433333333333334, test acc:0.6981\n",
      "epoch:200, train acc:0.8533333333333334, test acc:0.7117\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75feb17",
   "metadata": {},
   "source": [
    "그래프로 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103ae786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59009d1",
   "metadata": {},
   "source": [
    "### 가중치 감소로 오버피팅 억제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db74104c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5419a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c0401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bee16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c9a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7657ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.10333333333333333, test acc:0.0976\n",
      "epoch:1, train acc:0.11, test acc:0.1004\n",
      "epoch:2, train acc:0.12666666666666668, test acc:0.1083\n",
      "epoch:3, train acc:0.14333333333333334, test acc:0.1145\n",
      "epoch:4, train acc:0.14333333333333334, test acc:0.1208\n",
      "epoch:5, train acc:0.16333333333333333, test acc:0.1288\n",
      "epoch:6, train acc:0.18666666666666668, test acc:0.1333\n",
      "epoch:7, train acc:0.2, test acc:0.1484\n",
      "epoch:8, train acc:0.22, test acc:0.1549\n",
      "epoch:9, train acc:0.21666666666666667, test acc:0.1745\n",
      "epoch:10, train acc:0.23333333333333334, test acc:0.1817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-df2a3bf0be5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtest_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\multi_layer_net.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\multi_layer_net.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5333c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1aaef8",
   "metadata": {},
   "source": [
    "- 가중치 감쇠 0.1을 주지 않으면 train acc가 0.99 넘게 오버피팅이 발생\n",
    "- 가중치 감쇠 0.2 적용 시, 학습이 제대로 되지 않는 모습 적절한 가중치 감쇠 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb4e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "weight_decay_lambda = 0.2 # weight decay를 사용하지 않을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20623aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6f7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c56402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.11666666666666667, test acc:0.1014\n",
      "epoch:1, train acc:0.12333333333333334, test acc:0.1047\n",
      "epoch:2, train acc:0.13666666666666666, test acc:0.1114\n",
      "epoch:3, train acc:0.14666666666666667, test acc:0.1208\n",
      "epoch:4, train acc:0.17666666666666667, test acc:0.1482\n",
      "epoch:5, train acc:0.2, test acc:0.1734\n",
      "epoch:6, train acc:0.23, test acc:0.1849\n",
      "epoch:7, train acc:0.25, test acc:0.193\n",
      "epoch:8, train acc:0.27, test acc:0.2128\n",
      "epoch:9, train acc:0.31333333333333335, test acc:0.2432\n",
      "epoch:10, train acc:0.3566666666666667, test acc:0.2786\n",
      "epoch:11, train acc:0.36666666666666664, test acc:0.2865\n",
      "epoch:12, train acc:0.42333333333333334, test acc:0.3194\n",
      "epoch:13, train acc:0.4166666666666667, test acc:0.3182\n",
      "epoch:14, train acc:0.42333333333333334, test acc:0.3364\n",
      "epoch:15, train acc:0.46, test acc:0.3526\n",
      "epoch:16, train acc:0.4766666666666667, test acc:0.3656\n",
      "epoch:17, train acc:0.4866666666666667, test acc:0.3824\n",
      "epoch:18, train acc:0.49333333333333335, test acc:0.3924\n",
      "epoch:19, train acc:0.47333333333333333, test acc:0.3843\n",
      "epoch:20, train acc:0.48333333333333334, test acc:0.3907\n",
      "epoch:21, train acc:0.49, test acc:0.397\n",
      "epoch:22, train acc:0.48, test acc:0.3956\n",
      "epoch:23, train acc:0.5, test acc:0.3947\n",
      "epoch:24, train acc:0.5033333333333333, test acc:0.404\n",
      "epoch:25, train acc:0.51, test acc:0.4001\n",
      "epoch:26, train acc:0.5166666666666667, test acc:0.4135\n",
      "epoch:27, train acc:0.52, test acc:0.4218\n",
      "epoch:28, train acc:0.5233333333333333, test acc:0.4293\n",
      "epoch:29, train acc:0.5233333333333333, test acc:0.4183\n",
      "epoch:30, train acc:0.52, test acc:0.4335\n",
      "epoch:31, train acc:0.5266666666666666, test acc:0.4187\n",
      "epoch:32, train acc:0.5166666666666667, test acc:0.4138\n",
      "epoch:33, train acc:0.53, test acc:0.4326\n",
      "epoch:34, train acc:0.5233333333333333, test acc:0.4227\n",
      "epoch:35, train acc:0.54, test acc:0.4365\n",
      "epoch:36, train acc:0.53, test acc:0.4301\n",
      "epoch:37, train acc:0.5333333333333333, test acc:0.4318\n",
      "epoch:38, train acc:0.5466666666666666, test acc:0.4429\n",
      "epoch:39, train acc:0.55, test acc:0.4466\n",
      "epoch:40, train acc:0.54, test acc:0.4494\n",
      "epoch:41, train acc:0.5533333333333333, test acc:0.4517\n",
      "epoch:42, train acc:0.5366666666666666, test acc:0.4376\n",
      "epoch:43, train acc:0.5366666666666666, test acc:0.4398\n",
      "epoch:44, train acc:0.5333333333333333, test acc:0.4353\n",
      "epoch:45, train acc:0.5433333333333333, test acc:0.4387\n",
      "epoch:46, train acc:0.5366666666666666, test acc:0.4399\n",
      "epoch:47, train acc:0.5366666666666666, test acc:0.435\n",
      "epoch:48, train acc:0.5433333333333333, test acc:0.4298\n",
      "epoch:49, train acc:0.5366666666666666, test acc:0.4307\n",
      "epoch:50, train acc:0.54, test acc:0.4341\n",
      "epoch:51, train acc:0.5433333333333333, test acc:0.445\n",
      "epoch:52, train acc:0.55, test acc:0.4397\n",
      "epoch:53, train acc:0.5533333333333333, test acc:0.4564\n",
      "epoch:54, train acc:0.55, test acc:0.4507\n",
      "epoch:55, train acc:0.5466666666666666, test acc:0.4527\n",
      "epoch:56, train acc:0.5466666666666666, test acc:0.4525\n",
      "epoch:57, train acc:0.5466666666666666, test acc:0.4605\n",
      "epoch:58, train acc:0.55, test acc:0.4588\n",
      "epoch:59, train acc:0.5533333333333333, test acc:0.4637\n",
      "epoch:60, train acc:0.5533333333333333, test acc:0.4624\n",
      "epoch:61, train acc:0.56, test acc:0.456\n",
      "epoch:62, train acc:0.5466666666666666, test acc:0.4524\n",
      "epoch:63, train acc:0.54, test acc:0.442\n",
      "epoch:64, train acc:0.5333333333333333, test acc:0.4359\n",
      "epoch:65, train acc:0.5366666666666666, test acc:0.4421\n",
      "epoch:66, train acc:0.5333333333333333, test acc:0.44\n",
      "epoch:67, train acc:0.5266666666666666, test acc:0.4394\n",
      "epoch:68, train acc:0.5233333333333333, test acc:0.4329\n",
      "epoch:69, train acc:0.5366666666666666, test acc:0.4358\n",
      "epoch:70, train acc:0.5333333333333333, test acc:0.4379\n",
      "epoch:71, train acc:0.5266666666666666, test acc:0.431\n",
      "epoch:72, train acc:0.5266666666666666, test acc:0.4308\n",
      "epoch:73, train acc:0.5266666666666666, test acc:0.4261\n",
      "epoch:74, train acc:0.53, test acc:0.4302\n",
      "epoch:75, train acc:0.5266666666666666, test acc:0.4309\n",
      "epoch:76, train acc:0.5166666666666667, test acc:0.425\n",
      "epoch:77, train acc:0.52, test acc:0.4311\n",
      "epoch:78, train acc:0.52, test acc:0.4319\n",
      "epoch:79, train acc:0.52, test acc:0.4292\n",
      "epoch:80, train acc:0.52, test acc:0.4334\n",
      "epoch:81, train acc:0.52, test acc:0.4311\n",
      "epoch:82, train acc:0.5166666666666667, test acc:0.4292\n",
      "epoch:83, train acc:0.52, test acc:0.4266\n",
      "epoch:84, train acc:0.5133333333333333, test acc:0.4152\n",
      "epoch:85, train acc:0.5033333333333333, test acc:0.4192\n",
      "epoch:86, train acc:0.5166666666666667, test acc:0.421\n",
      "epoch:87, train acc:0.5133333333333333, test acc:0.4163\n",
      "epoch:88, train acc:0.5133333333333333, test acc:0.4214\n",
      "epoch:89, train acc:0.5133333333333333, test acc:0.4136\n",
      "epoch:90, train acc:0.5, test acc:0.4047\n",
      "epoch:91, train acc:0.5, test acc:0.4103\n",
      "epoch:92, train acc:0.49333333333333335, test acc:0.4065\n",
      "epoch:93, train acc:0.49333333333333335, test acc:0.4047\n",
      "epoch:94, train acc:0.4866666666666667, test acc:0.4014\n",
      "epoch:95, train acc:0.48, test acc:0.3975\n",
      "epoch:96, train acc:0.48333333333333334, test acc:0.3985\n",
      "epoch:97, train acc:0.4766666666666667, test acc:0.3931\n",
      "epoch:98, train acc:0.47333333333333333, test acc:0.3929\n",
      "epoch:99, train acc:0.4666666666666667, test acc:0.3896\n",
      "epoch:100, train acc:0.4633333333333333, test acc:0.3881\n",
      "epoch:101, train acc:0.4666666666666667, test acc:0.3886\n",
      "epoch:102, train acc:0.47333333333333333, test acc:0.3912\n",
      "epoch:103, train acc:0.4666666666666667, test acc:0.389\n",
      "epoch:104, train acc:0.4633333333333333, test acc:0.3893\n",
      "epoch:105, train acc:0.4633333333333333, test acc:0.3826\n",
      "epoch:106, train acc:0.4633333333333333, test acc:0.3895\n",
      "epoch:107, train acc:0.4533333333333333, test acc:0.3782\n",
      "epoch:108, train acc:0.44333333333333336, test acc:0.375\n",
      "epoch:109, train acc:0.45, test acc:0.3752\n",
      "epoch:110, train acc:0.43666666666666665, test acc:0.3736\n",
      "epoch:111, train acc:0.44, test acc:0.3781\n",
      "epoch:112, train acc:0.44, test acc:0.3784\n",
      "epoch:113, train acc:0.44, test acc:0.3753\n",
      "epoch:114, train acc:0.44, test acc:0.3736\n",
      "epoch:115, train acc:0.43666666666666665, test acc:0.3666\n",
      "epoch:116, train acc:0.43, test acc:0.368\n",
      "epoch:117, train acc:0.44333333333333336, test acc:0.3705\n",
      "epoch:118, train acc:0.4266666666666667, test acc:0.3661\n",
      "epoch:119, train acc:0.42333333333333334, test acc:0.3613\n",
      "epoch:120, train acc:0.42333333333333334, test acc:0.3626\n",
      "epoch:121, train acc:0.42, test acc:0.3615\n",
      "epoch:122, train acc:0.42333333333333334, test acc:0.3625\n",
      "epoch:123, train acc:0.43666666666666665, test acc:0.3673\n",
      "epoch:124, train acc:0.43666666666666665, test acc:0.3673\n",
      "epoch:125, train acc:0.44333333333333336, test acc:0.3696\n",
      "epoch:126, train acc:0.44666666666666666, test acc:0.3727\n",
      "epoch:127, train acc:0.44333333333333336, test acc:0.3779\n",
      "epoch:128, train acc:0.42, test acc:0.3641\n",
      "epoch:129, train acc:0.4066666666666667, test acc:0.3587\n",
      "epoch:130, train acc:0.4166666666666667, test acc:0.3618\n",
      "epoch:131, train acc:0.42333333333333334, test acc:0.3624\n",
      "epoch:132, train acc:0.4066666666666667, test acc:0.3514\n",
      "epoch:133, train acc:0.4033333333333333, test acc:0.3468\n",
      "epoch:134, train acc:0.4033333333333333, test acc:0.3464\n",
      "epoch:135, train acc:0.4066666666666667, test acc:0.351\n",
      "epoch:136, train acc:0.41, test acc:0.3502\n",
      "epoch:137, train acc:0.4066666666666667, test acc:0.3484\n",
      "epoch:138, train acc:0.41333333333333333, test acc:0.3509\n",
      "epoch:139, train acc:0.4, test acc:0.3484\n",
      "epoch:140, train acc:0.41333333333333333, test acc:0.3511\n",
      "epoch:141, train acc:0.42, test acc:0.3536\n",
      "epoch:142, train acc:0.4033333333333333, test acc:0.3452\n",
      "epoch:143, train acc:0.41, test acc:0.3468\n",
      "epoch:144, train acc:0.3933333333333333, test acc:0.3378\n",
      "epoch:145, train acc:0.39, test acc:0.3329\n",
      "epoch:146, train acc:0.38666666666666666, test acc:0.3349\n",
      "epoch:147, train acc:0.38666666666666666, test acc:0.3387\n",
      "epoch:148, train acc:0.38666666666666666, test acc:0.3411\n",
      "epoch:149, train acc:0.3933333333333333, test acc:0.339\n",
      "epoch:150, train acc:0.3933333333333333, test acc:0.3382\n",
      "epoch:151, train acc:0.38666666666666666, test acc:0.3376\n",
      "epoch:152, train acc:0.39666666666666667, test acc:0.3377\n",
      "epoch:153, train acc:0.4033333333333333, test acc:0.3374\n",
      "epoch:154, train acc:0.4, test acc:0.3362\n",
      "epoch:155, train acc:0.39, test acc:0.3328\n",
      "epoch:156, train acc:0.38666666666666666, test acc:0.3309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:157, train acc:0.3933333333333333, test acc:0.3313\n",
      "epoch:158, train acc:0.3933333333333333, test acc:0.3265\n",
      "epoch:159, train acc:0.38666666666666666, test acc:0.3222\n",
      "epoch:160, train acc:0.37666666666666665, test acc:0.3194\n",
      "epoch:161, train acc:0.37666666666666665, test acc:0.3168\n",
      "epoch:162, train acc:0.38, test acc:0.3179\n",
      "epoch:163, train acc:0.37666666666666665, test acc:0.3168\n",
      "epoch:164, train acc:0.37666666666666665, test acc:0.3149\n",
      "epoch:165, train acc:0.36666666666666664, test acc:0.3121\n",
      "epoch:166, train acc:0.36, test acc:0.3056\n",
      "epoch:167, train acc:0.35333333333333333, test acc:0.2993\n",
      "epoch:168, train acc:0.3433333333333333, test acc:0.2942\n",
      "epoch:169, train acc:0.33666666666666667, test acc:0.2907\n",
      "epoch:170, train acc:0.32666666666666666, test acc:0.2834\n",
      "epoch:171, train acc:0.33, test acc:0.2809\n",
      "epoch:172, train acc:0.32666666666666666, test acc:0.28\n",
      "epoch:173, train acc:0.32666666666666666, test acc:0.2793\n",
      "epoch:174, train acc:0.32666666666666666, test acc:0.2771\n",
      "epoch:175, train acc:0.32666666666666666, test acc:0.2743\n",
      "epoch:176, train acc:0.31666666666666665, test acc:0.2751\n",
      "epoch:177, train acc:0.31, test acc:0.2691\n",
      "epoch:178, train acc:0.31, test acc:0.2667\n",
      "epoch:179, train acc:0.30333333333333334, test acc:0.2647\n",
      "epoch:180, train acc:0.30333333333333334, test acc:0.2637\n",
      "epoch:181, train acc:0.3, test acc:0.2629\n",
      "epoch:182, train acc:0.3, test acc:0.2628\n",
      "epoch:183, train acc:0.3, test acc:0.2608\n",
      "epoch:184, train acc:0.3, test acc:0.2606\n",
      "epoch:185, train acc:0.3, test acc:0.26\n",
      "epoch:186, train acc:0.3, test acc:0.2628\n",
      "epoch:187, train acc:0.3, test acc:0.2618\n",
      "epoch:188, train acc:0.3, test acc:0.2599\n",
      "epoch:189, train acc:0.3, test acc:0.2584\n",
      "epoch:190, train acc:0.3, test acc:0.2584\n",
      "epoch:191, train acc:0.3, test acc:0.2581\n",
      "epoch:192, train acc:0.29, test acc:0.2532\n",
      "epoch:193, train acc:0.28, test acc:0.2487\n",
      "epoch:194, train acc:0.28, test acc:0.2475\n",
      "epoch:195, train acc:0.27666666666666667, test acc:0.2448\n",
      "epoch:196, train acc:0.2733333333333333, test acc:0.2428\n",
      "epoch:197, train acc:0.27, test acc:0.2415\n",
      "epoch:198, train acc:0.27, test acc:0.2402\n",
      "epoch:199, train acc:0.27, test acc:0.2408\n",
      "epoch:200, train acc:0.27, test acc:0.2415\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1e3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49d370",
   "metadata": {},
   "source": [
    "### 드롭아웃으로 오버피팅 억제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddff94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6586c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 줄이기\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76b0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306466211116495\n",
      "=== epoch:1, train acc:0.08333333333333333, test acc:0.0935 ===\n",
      "train loss:2.3233742007586047\n",
      "train loss:2.316996688370058\n",
      "train loss:2.3128754499657806\n",
      "=== epoch:2, train acc:0.1, test acc:0.101 ===\n",
      "train loss:2.305261699158031\n",
      "train loss:2.323233781629496\n",
      "train loss:2.322380959380987\n",
      "=== epoch:3, train acc:0.10666666666666667, test acc:0.1037 ===\n",
      "train loss:2.3299841311425937\n",
      "train loss:2.2944871777306837\n",
      "train loss:2.313756615900901\n",
      "=== epoch:4, train acc:0.10333333333333333, test acc:0.1087 ===\n",
      "train loss:2.3115197446062776\n",
      "train loss:2.3146947128784494\n",
      "train loss:2.2915479274684913\n",
      "=== epoch:5, train acc:0.12, test acc:0.1127 ===\n",
      "train loss:2.3060189595239247\n",
      "train loss:2.3060533806881014\n",
      "train loss:2.3087166566969675\n",
      "=== epoch:6, train acc:0.11, test acc:0.1165 ===\n",
      "train loss:2.300769070790303\n",
      "train loss:2.307209865920115\n",
      "train loss:2.290991967723541\n",
      "=== epoch:7, train acc:0.11666666666666667, test acc:0.1241 ===\n",
      "train loss:2.3042449149116804\n",
      "train loss:2.2975776964477137\n",
      "train loss:2.2885401175155495\n",
      "=== epoch:8, train acc:0.13, test acc:0.1316 ===\n",
      "train loss:2.290009505624647\n",
      "train loss:2.3114718620468526\n",
      "train loss:2.314299966573357\n",
      "=== epoch:9, train acc:0.12333333333333334, test acc:0.1372 ===\n",
      "train loss:2.300997411163083\n",
      "train loss:2.292089951101843\n",
      "train loss:2.2936505064186385\n",
      "=== epoch:10, train acc:0.13333333333333333, test acc:0.1406 ===\n",
      "train loss:2.3057349982420634\n",
      "train loss:2.294437959087648\n",
      "train loss:2.2972333948046346\n",
      "=== epoch:11, train acc:0.14, test acc:0.146 ===\n",
      "train loss:2.2966845916733263\n",
      "train loss:2.302595062438771\n",
      "train loss:2.2936272236303665\n",
      "=== epoch:12, train acc:0.15, test acc:0.1514 ===\n",
      "train loss:2.287574277852396\n",
      "train loss:2.2908719939764004\n",
      "train loss:2.2864183601058214\n",
      "=== epoch:13, train acc:0.16, test acc:0.1565 ===\n",
      "train loss:2.2900563264228655\n",
      "train loss:2.279374210136909\n",
      "train loss:2.2798698285287617\n",
      "=== epoch:14, train acc:0.15666666666666668, test acc:0.1615 ===\n",
      "train loss:2.2927065558190676\n",
      "train loss:2.2836143283649046\n",
      "train loss:2.2726964474231117\n",
      "=== epoch:15, train acc:0.17, test acc:0.17 ===\n",
      "train loss:2.2960013969785003\n",
      "train loss:2.2689028758724774\n",
      "train loss:2.3006900325557735\n",
      "=== epoch:16, train acc:0.17, test acc:0.1749 ===\n",
      "train loss:2.275545627425918\n",
      "train loss:2.300626775246902\n",
      "train loss:2.2834397872269903\n",
      "=== epoch:17, train acc:0.18333333333333332, test acc:0.1792 ===\n",
      "train loss:2.2854016707761486\n",
      "train loss:2.275003031339694\n",
      "train loss:2.264771086151391\n",
      "=== epoch:18, train acc:0.19333333333333333, test acc:0.188 ===\n",
      "train loss:2.2621317308720457\n",
      "train loss:2.269633891726682\n",
      "train loss:2.2817180115804274\n",
      "=== epoch:19, train acc:0.19333333333333333, test acc:0.1881 ===\n",
      "train loss:2.2827966059700158\n",
      "train loss:2.277116157370144\n",
      "train loss:2.2743481096972133\n",
      "=== epoch:20, train acc:0.19666666666666666, test acc:0.1909 ===\n",
      "train loss:2.271351861406633\n",
      "train loss:2.274971025352863\n",
      "train loss:2.282493765244773\n",
      "=== epoch:21, train acc:0.19666666666666666, test acc:0.1911 ===\n",
      "train loss:2.2847516496402545\n",
      "train loss:2.281258419481394\n",
      "train loss:2.2808844123978633\n",
      "=== epoch:22, train acc:0.20333333333333334, test acc:0.1955 ===\n",
      "train loss:2.291271840559267\n",
      "train loss:2.2796015335111512\n",
      "train loss:2.2634768516493193\n",
      "=== epoch:23, train acc:0.21333333333333335, test acc:0.2009 ===\n",
      "train loss:2.2665197128286847\n",
      "train loss:2.2768140916321418\n",
      "train loss:2.2785730658557726\n",
      "=== epoch:24, train acc:0.22333333333333333, test acc:0.2104 ===\n",
      "train loss:2.2663376392810712\n",
      "train loss:2.274713623840891\n",
      "train loss:2.2633045335824757\n",
      "=== epoch:25, train acc:0.23333333333333334, test acc:0.2194 ===\n",
      "train loss:2.258350522315239\n",
      "train loss:2.263467401701561\n",
      "train loss:2.263810696225091\n",
      "=== epoch:26, train acc:0.24333333333333335, test acc:0.2248 ===\n",
      "train loss:2.2719670864073733\n",
      "train loss:2.258505320526888\n",
      "train loss:2.2721435310239184\n",
      "=== epoch:27, train acc:0.25, test acc:0.2254 ===\n",
      "train loss:2.2667403604002296\n",
      "train loss:2.255703762010085\n",
      "train loss:2.2550799902834817\n",
      "=== epoch:28, train acc:0.25333333333333335, test acc:0.2319 ===\n",
      "train loss:2.2628350101872896\n",
      "train loss:2.2541026848274903\n",
      "train loss:2.2513549666058568\n",
      "=== epoch:29, train acc:0.26666666666666666, test acc:0.2345 ===\n",
      "train loss:2.27104335020848\n",
      "train loss:2.266373564037674\n",
      "train loss:2.2463008280965533\n",
      "=== epoch:30, train acc:0.2733333333333333, test acc:0.2378 ===\n",
      "train loss:2.234608790240035\n",
      "train loss:2.257359700564871\n",
      "train loss:2.2516675582899257\n",
      "=== epoch:31, train acc:0.27, test acc:0.2433 ===\n",
      "train loss:2.2460809665510375\n",
      "train loss:2.2692647383740514\n",
      "train loss:2.2580551559271207\n",
      "=== epoch:32, train acc:0.2833333333333333, test acc:0.2473 ===\n",
      "train loss:2.2514674039957336\n",
      "train loss:2.259232688923554\n",
      "train loss:2.254856259730866\n",
      "=== epoch:33, train acc:0.2833333333333333, test acc:0.2519 ===\n",
      "train loss:2.2359951467459744\n",
      "train loss:2.2411223544755603\n",
      "train loss:2.246183040606921\n",
      "=== epoch:34, train acc:0.2966666666666667, test acc:0.2551 ===\n",
      "train loss:2.2355342739651434\n",
      "train loss:2.2583691397117778\n",
      "train loss:2.2462930817236413\n",
      "=== epoch:35, train acc:0.3, test acc:0.2542 ===\n",
      "train loss:2.250624086878304\n",
      "train loss:2.2509002890509837\n",
      "train loss:2.238299275075596\n",
      "=== epoch:36, train acc:0.30666666666666664, test acc:0.2541 ===\n",
      "train loss:2.2469496470793904\n",
      "train loss:2.2270729149146855\n",
      "train loss:2.249170334568686\n",
      "=== epoch:37, train acc:0.31666666666666665, test acc:0.2617 ===\n",
      "train loss:2.2151523810020137\n",
      "train loss:2.2349687067928143\n",
      "train loss:2.222176044617786\n",
      "=== epoch:38, train acc:0.31666666666666665, test acc:0.2601 ===\n",
      "train loss:2.2353180610683046\n",
      "train loss:2.2392839645559044\n",
      "train loss:2.2554847349947846\n",
      "=== epoch:39, train acc:0.32666666666666666, test acc:0.2612 ===\n",
      "train loss:2.2522851644798494\n",
      "train loss:2.2691303186269622\n",
      "train loss:2.2120275118514803\n",
      "=== epoch:40, train acc:0.32666666666666666, test acc:0.2614 ===\n",
      "train loss:2.233681913580767\n",
      "train loss:2.2379408299392383\n",
      "train loss:2.237635800680383\n",
      "=== epoch:41, train acc:0.3233333333333333, test acc:0.2604 ===\n",
      "train loss:2.2311098030616296\n",
      "train loss:2.204024102053522\n",
      "train loss:2.229517581285359\n",
      "=== epoch:42, train acc:0.31666666666666665, test acc:0.2592 ===\n",
      "train loss:2.2480636673351673\n",
      "train loss:2.2324528887559136\n",
      "train loss:2.2086472348960715\n",
      "=== epoch:43, train acc:0.31666666666666665, test acc:0.2613 ===\n",
      "train loss:2.2221036284932363\n",
      "train loss:2.2226573237758105\n",
      "train loss:2.228207205098986\n",
      "=== epoch:44, train acc:0.31333333333333335, test acc:0.2637 ===\n",
      "train loss:2.222286467570423\n",
      "train loss:2.228216764631159\n",
      "train loss:2.2027883032968463\n",
      "=== epoch:45, train acc:0.31333333333333335, test acc:0.2593 ===\n",
      "train loss:2.175825519634632\n",
      "train loss:2.2138095574150434\n",
      "train loss:2.224458255014681\n",
      "=== epoch:46, train acc:0.31333333333333335, test acc:0.2562 ===\n",
      "train loss:2.20016953170136\n",
      "train loss:2.2305340690599884\n",
      "train loss:2.2050653626787793\n",
      "=== epoch:47, train acc:0.30666666666666664, test acc:0.2535 ===\n",
      "train loss:2.204181707905084\n",
      "train loss:2.200520706809454\n",
      "train loss:2.2086306634284476\n",
      "=== epoch:48, train acc:0.30666666666666664, test acc:0.2517 ===\n",
      "train loss:2.2189813096507076\n",
      "train loss:2.207840655563053\n",
      "train loss:2.1910574040759934\n",
      "=== epoch:49, train acc:0.30666666666666664, test acc:0.248 ===\n",
      "train loss:2.2289168518644864\n",
      "train loss:2.194211712716603\n",
      "train loss:2.2117735733704755\n",
      "=== epoch:50, train acc:0.29333333333333333, test acc:0.2455 ===\n",
      "train loss:2.2140305935525157\n",
      "train loss:2.231060525386386\n",
      "train loss:2.1683812080913203\n",
      "=== epoch:51, train acc:0.2866666666666667, test acc:0.2424 ===\n",
      "train loss:2.1715043750530034\n",
      "train loss:2.2179230949048576\n",
      "train loss:2.223687022218727\n",
      "=== epoch:52, train acc:0.2866666666666667, test acc:0.2369 ===\n",
      "train loss:2.1866634306045283\n",
      "train loss:2.1606003196243586\n",
      "train loss:2.216471425981635\n",
      "=== epoch:53, train acc:0.2866666666666667, test acc:0.2339 ===\n",
      "train loss:2.17530738761391\n",
      "train loss:2.163543264998564\n",
      "train loss:2.1955676651925735\n",
      "=== epoch:54, train acc:0.29, test acc:0.2362 ===\n",
      "train loss:2.2113123911587937\n",
      "train loss:2.1864020673574216\n",
      "train loss:2.1699184315435724\n",
      "=== epoch:55, train acc:0.29333333333333333, test acc:0.2363 ===\n",
      "train loss:2.1638485031866854\n",
      "train loss:2.1991380107183787\n",
      "train loss:2.1637002537235914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:56, train acc:0.29333333333333333, test acc:0.2372 ===\n",
      "train loss:2.2043457542969236\n",
      "train loss:2.190724808082194\n",
      "train loss:2.12176917575993\n",
      "=== epoch:57, train acc:0.29, test acc:0.235 ===\n",
      "train loss:2.201563117509279\n",
      "train loss:2.21408780497264\n",
      "train loss:2.156440709260063\n",
      "=== epoch:58, train acc:0.2966666666666667, test acc:0.236 ===\n",
      "train loss:2.185494282222665\n",
      "train loss:2.179938471891265\n",
      "train loss:2.1669958492015713\n",
      "=== epoch:59, train acc:0.30333333333333334, test acc:0.2393 ===\n",
      "train loss:2.155080301619888\n",
      "train loss:2.214972568723867\n",
      "train loss:2.1924286573298772\n",
      "=== epoch:60, train acc:0.30333333333333334, test acc:0.2384 ===\n",
      "train loss:2.1954490380953584\n",
      "train loss:2.1917199165158703\n",
      "train loss:2.135285917841273\n",
      "=== epoch:61, train acc:0.3, test acc:0.2344 ===\n",
      "train loss:2.1374873671940895\n",
      "train loss:2.1712117660836183\n",
      "train loss:2.157337407301946\n",
      "=== epoch:62, train acc:0.30333333333333334, test acc:0.2358 ===\n",
      "train loss:2.197214408911791\n",
      "train loss:2.1340088662450527\n",
      "train loss:2.1963347648446616\n",
      "=== epoch:63, train acc:0.31, test acc:0.239 ===\n",
      "train loss:2.0961473835332782\n",
      "train loss:2.1556404134038156\n",
      "train loss:2.145186422866775\n",
      "=== epoch:64, train acc:0.30333333333333334, test acc:0.2352 ===\n",
      "train loss:2.1047583066942135\n",
      "train loss:2.17416321067493\n",
      "train loss:2.136464455604462\n",
      "=== epoch:65, train acc:0.3, test acc:0.2329 ===\n",
      "train loss:2.095199340756926\n",
      "train loss:2.1280060004313297\n",
      "train loss:2.1508010494399574\n",
      "=== epoch:66, train acc:0.3, test acc:0.2286 ===\n",
      "train loss:2.1491211067169003\n",
      "train loss:2.188961098552264\n",
      "train loss:2.1609381744049823\n",
      "=== epoch:67, train acc:0.30333333333333334, test acc:0.2331 ===\n",
      "train loss:2.1405163138459025\n",
      "train loss:2.1293217140319594\n",
      "train loss:2.159784360535679\n",
      "=== epoch:68, train acc:0.30333333333333334, test acc:0.2295 ===\n",
      "train loss:2.1665215432176983\n",
      "train loss:2.1190703258067587\n",
      "train loss:2.1568460648894527\n",
      "=== epoch:69, train acc:0.3, test acc:0.2273 ===\n",
      "train loss:2.170126839609187\n",
      "train loss:2.1709183254058817\n",
      "train loss:2.1750460640745963\n",
      "=== epoch:70, train acc:0.30333333333333334, test acc:0.2313 ===\n",
      "train loss:2.1516825187335904\n",
      "train loss:2.175807002980665\n",
      "train loss:2.1746239355833823\n",
      "=== epoch:71, train acc:0.31, test acc:0.2362 ===\n",
      "train loss:2.077103026731032\n",
      "train loss:2.1143349405387286\n",
      "train loss:2.1176134605231867\n",
      "=== epoch:72, train acc:0.30333333333333334, test acc:0.2345 ===\n",
      "train loss:2.169600097198854\n",
      "train loss:2.0909819749760112\n",
      "train loss:2.1027689477333427\n",
      "=== epoch:73, train acc:0.30666666666666664, test acc:0.2368 ===\n",
      "train loss:2.1212013433462173\n",
      "train loss:2.109703888278283\n",
      "train loss:2.1571999518097926\n",
      "=== epoch:74, train acc:0.30666666666666664, test acc:0.2378 ===\n",
      "train loss:2.109575903841557\n",
      "train loss:2.091084240127404\n",
      "train loss:2.1726588023881166\n",
      "=== epoch:75, train acc:0.30666666666666664, test acc:0.2383 ===\n",
      "train loss:2.1165126104410485\n",
      "train loss:2.1022525852333307\n",
      "train loss:2.0140884658957856\n",
      "=== epoch:76, train acc:0.30333333333333334, test acc:0.2353 ===\n",
      "train loss:2.120465052456174\n",
      "train loss:2.1057812096584296\n",
      "train loss:2.073566312069658\n",
      "=== epoch:77, train acc:0.30666666666666664, test acc:0.2387 ===\n",
      "train loss:2.0063600918365263\n",
      "train loss:2.0455780146046676\n",
      "train loss:2.0772572522998973\n",
      "=== epoch:78, train acc:0.30333333333333334, test acc:0.2325 ===\n",
      "train loss:2.143493080130799\n",
      "train loss:2.068131661541089\n",
      "train loss:2.082874160344243\n",
      "=== epoch:79, train acc:0.30333333333333334, test acc:0.2348 ===\n",
      "train loss:2.044735241403992\n",
      "train loss:2.0835912799630263\n",
      "train loss:2.0685884237235816\n",
      "=== epoch:80, train acc:0.3, test acc:0.2346 ===\n",
      "train loss:2.140197848367549\n",
      "train loss:2.0900989694716725\n",
      "train loss:2.079859770449529\n",
      "=== epoch:81, train acc:0.30333333333333334, test acc:0.237 ===\n",
      "train loss:2.07826003382277\n",
      "train loss:2.1109006022836727\n",
      "train loss:2.085782782184177\n",
      "=== epoch:82, train acc:0.30333333333333334, test acc:0.2385 ===\n",
      "train loss:2.050714121979553\n",
      "train loss:2.017120493526961\n",
      "train loss:2.068569892746531\n",
      "=== epoch:83, train acc:0.30666666666666664, test acc:0.2391 ===\n",
      "train loss:2.154741115688295\n",
      "train loss:2.102921123146962\n",
      "train loss:2.127026817473995\n",
      "=== epoch:84, train acc:0.31333333333333335, test acc:0.2468 ===\n",
      "train loss:2.1174355933169964\n",
      "train loss:2.058312749914067\n",
      "train loss:2.0838782531105857\n",
      "=== epoch:85, train acc:0.31333333333333335, test acc:0.2508 ===\n",
      "train loss:1.9999144715841848\n",
      "train loss:2.094235247661783\n",
      "train loss:2.0710289838161295\n",
      "=== epoch:86, train acc:0.31666666666666665, test acc:0.2508 ===\n",
      "train loss:1.962951165302697\n",
      "train loss:2.1029572331143274\n",
      "train loss:2.100354090592684\n",
      "=== epoch:87, train acc:0.31333333333333335, test acc:0.25 ===\n",
      "train loss:2.0949216334010505\n",
      "train loss:2.001015997987105\n",
      "train loss:2.066105628699365\n",
      "=== epoch:88, train acc:0.31666666666666665, test acc:0.2509 ===\n",
      "train loss:2.0981840242166054\n",
      "train loss:2.053092753418431\n",
      "train loss:2.0666996677843845\n",
      "=== epoch:89, train acc:0.32, test acc:0.2545 ===\n",
      "train loss:2.123406989792541\n",
      "train loss:2.087528664847112\n",
      "train loss:2.108928030580263\n",
      "=== epoch:90, train acc:0.3333333333333333, test acc:0.2613 ===\n",
      "train loss:2.080345918222377\n",
      "train loss:2.0285288320436736\n",
      "train loss:1.9975793759875915\n",
      "=== epoch:91, train acc:0.3333333333333333, test acc:0.2633 ===\n",
      "train loss:2.0010705444051404\n",
      "train loss:1.9825810895798133\n",
      "train loss:2.0740051988891444\n",
      "=== epoch:92, train acc:0.33666666666666667, test acc:0.2616 ===\n",
      "train loss:2.0799947947013546\n",
      "train loss:1.9554917851969507\n",
      "train loss:1.9620829559855135\n",
      "=== epoch:93, train acc:0.3333333333333333, test acc:0.257 ===\n",
      "train loss:2.045447353978871\n",
      "train loss:2.0266142699301266\n",
      "train loss:2.013073431991896\n",
      "=== epoch:94, train acc:0.33666666666666667, test acc:0.2589 ===\n",
      "train loss:1.9700995200417593\n",
      "train loss:2.0597891491057245\n",
      "train loss:1.924463444088442\n",
      "=== epoch:95, train acc:0.33666666666666667, test acc:0.2597 ===\n",
      "train loss:2.0126721395479774\n",
      "train loss:1.9742468024440378\n",
      "train loss:2.0370932172848124\n",
      "=== epoch:96, train acc:0.3333333333333333, test acc:0.2637 ===\n",
      "train loss:2.047839689931791\n",
      "train loss:2.055010692565265\n",
      "train loss:2.031704936162639\n",
      "=== epoch:97, train acc:0.32666666666666666, test acc:0.2689 ===\n",
      "train loss:2.086728812914129\n",
      "train loss:2.025600659233238\n",
      "train loss:1.9228989213061882\n",
      "=== epoch:98, train acc:0.32666666666666666, test acc:0.272 ===\n",
      "train loss:1.980111611418458\n",
      "train loss:2.0427352839722053\n",
      "train loss:2.0122394285697367\n",
      "=== epoch:99, train acc:0.3466666666666667, test acc:0.2774 ===\n",
      "train loss:1.9710721370176754\n",
      "train loss:2.0520410722583526\n",
      "train loss:1.9252601091984871\n",
      "=== epoch:100, train acc:0.3466666666666667, test acc:0.2771 ===\n",
      "train loss:2.096549575065842\n",
      "train loss:2.058671294047095\n",
      "train loss:2.009575307889618\n",
      "=== epoch:101, train acc:0.35333333333333333, test acc:0.2788 ===\n",
      "train loss:2.0216313617497037\n",
      "train loss:2.069756165033512\n",
      "train loss:2.0958592559973273\n",
      "=== epoch:102, train acc:0.35333333333333333, test acc:0.2832 ===\n",
      "train loss:1.9759118986026982\n",
      "train loss:1.928232361361231\n",
      "train loss:1.9306104123263919\n",
      "=== epoch:103, train acc:0.3566666666666667, test acc:0.2815 ===\n",
      "train loss:1.9732486877805235\n",
      "train loss:2.026683973009633\n",
      "train loss:1.994379439942019\n",
      "=== epoch:104, train acc:0.3566666666666667, test acc:0.2834 ===\n",
      "train loss:1.9596710308120233\n",
      "train loss:2.0712337546525355\n",
      "train loss:2.048992533824373\n",
      "=== epoch:105, train acc:0.36, test acc:0.2878 ===\n",
      "train loss:2.0596536875919393\n",
      "train loss:1.9104685928129987\n",
      "train loss:2.066678671525226\n",
      "=== epoch:106, train acc:0.36, test acc:0.2918 ===\n",
      "train loss:1.9754689404095338\n",
      "train loss:1.9205523961415818\n",
      "train loss:1.9298552677457301\n",
      "=== epoch:107, train acc:0.36333333333333334, test acc:0.2899 ===\n",
      "train loss:1.9511642140415142\n",
      "train loss:1.8787648961853531\n",
      "train loss:1.9350564932672016\n",
      "=== epoch:108, train acc:0.36, test acc:0.2905 ===\n",
      "train loss:2.0457107021910885\n",
      "train loss:1.9253613193892403\n",
      "train loss:2.044716693778173\n",
      "=== epoch:109, train acc:0.36666666666666664, test acc:0.2977 ===\n",
      "train loss:2.001470676211955\n",
      "train loss:1.992995753346786\n",
      "train loss:2.0207822040190675\n",
      "=== epoch:110, train acc:0.37, test acc:0.2997 ===\n",
      "train loss:2.013799685900152\n",
      "train loss:1.9469390074474737\n",
      "train loss:1.8848027541557408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:111, train acc:0.37333333333333335, test acc:0.303 ===\n",
      "train loss:1.863452083852164\n",
      "train loss:1.937497807885072\n",
      "train loss:1.969950842592458\n",
      "=== epoch:112, train acc:0.37, test acc:0.3027 ===\n",
      "train loss:1.9049242890656959\n",
      "train loss:2.017450237110186\n",
      "train loss:1.835137259118409\n",
      "=== epoch:113, train acc:0.37333333333333335, test acc:0.306 ===\n",
      "train loss:1.9738056651486022\n",
      "train loss:1.9669826390060792\n",
      "train loss:1.9638809475322234\n",
      "=== epoch:114, train acc:0.37333333333333335, test acc:0.3074 ===\n",
      "train loss:1.947992724339209\n",
      "train loss:1.9648766440084484\n",
      "train loss:1.9905953751590344\n",
      "=== epoch:115, train acc:0.38666666666666666, test acc:0.3137 ===\n",
      "train loss:2.073229472587781\n",
      "train loss:2.001055140157013\n",
      "train loss:1.883459914052046\n",
      "=== epoch:116, train acc:0.4, test acc:0.3175 ===\n",
      "train loss:1.9715764168907208\n",
      "train loss:1.9532246361397336\n",
      "train loss:2.0137251617989946\n",
      "=== epoch:117, train acc:0.4033333333333333, test acc:0.3217 ===\n",
      "train loss:2.0072060775902303\n",
      "train loss:1.9603636658612413\n",
      "train loss:1.9550114175511046\n",
      "=== epoch:118, train acc:0.4066666666666667, test acc:0.3219 ===\n",
      "train loss:1.885017062953431\n",
      "train loss:1.8895416826579001\n",
      "train loss:1.8947465154142729\n",
      "=== epoch:119, train acc:0.4033333333333333, test acc:0.3201 ===\n",
      "train loss:1.8552356140450899\n",
      "train loss:1.9365921897213685\n",
      "train loss:1.8968552182221823\n",
      "=== epoch:120, train acc:0.4, test acc:0.3223 ===\n",
      "train loss:1.9660937032613084\n",
      "train loss:1.9179496054361116\n",
      "train loss:1.8679196626831647\n",
      "=== epoch:121, train acc:0.4033333333333333, test acc:0.3222 ===\n",
      "train loss:1.9181336613500322\n",
      "train loss:1.9588848723547656\n",
      "train loss:1.9112120510320258\n",
      "=== epoch:122, train acc:0.4066666666666667, test acc:0.3279 ===\n",
      "train loss:1.9428524536329028\n",
      "train loss:1.9360273467166624\n",
      "train loss:1.8310146635423836\n",
      "=== epoch:123, train acc:0.4066666666666667, test acc:0.3302 ===\n",
      "train loss:1.8845972365136319\n",
      "train loss:1.9136446423616598\n",
      "train loss:1.9513628266037728\n",
      "=== epoch:124, train acc:0.4066666666666667, test acc:0.3289 ===\n",
      "train loss:1.8999139258945368\n",
      "train loss:1.7436191233528284\n",
      "train loss:1.9633620079528364\n",
      "=== epoch:125, train acc:0.4, test acc:0.3241 ===\n",
      "train loss:1.8078206443892393\n",
      "train loss:1.9204161737519514\n",
      "train loss:1.9692494248375538\n",
      "=== epoch:126, train acc:0.41, test acc:0.3274 ===\n",
      "train loss:1.8495257007477273\n",
      "train loss:1.8848171274290086\n",
      "train loss:1.896807106938679\n",
      "=== epoch:127, train acc:0.4166666666666667, test acc:0.3328 ===\n",
      "train loss:1.9463139324575893\n",
      "train loss:1.9467062871570346\n",
      "train loss:1.9488738095153086\n",
      "=== epoch:128, train acc:0.42, test acc:0.3368 ===\n",
      "train loss:1.8875726606137162\n",
      "train loss:1.882605702478759\n",
      "train loss:1.9098870956035958\n",
      "=== epoch:129, train acc:0.42333333333333334, test acc:0.338 ===\n",
      "train loss:1.9276721882692482\n",
      "train loss:1.8804383636124242\n",
      "train loss:1.877575203711257\n",
      "=== epoch:130, train acc:0.42333333333333334, test acc:0.3379 ===\n",
      "train loss:1.874869470499332\n",
      "train loss:1.8347752151398244\n",
      "train loss:1.9046259965815606\n",
      "=== epoch:131, train acc:0.43, test acc:0.3438 ===\n",
      "train loss:1.9562914691765356\n",
      "train loss:1.7830735662921955\n",
      "train loss:1.8085722992757807\n",
      "=== epoch:132, train acc:0.43, test acc:0.3433 ===\n",
      "train loss:1.8431579923732528\n",
      "train loss:1.820205641233764\n",
      "train loss:1.8824685962172387\n",
      "=== epoch:133, train acc:0.44, test acc:0.3501 ===\n",
      "train loss:1.9471758210465586\n",
      "train loss:1.9387082708468153\n",
      "train loss:1.8295844230772655\n",
      "=== epoch:134, train acc:0.43666666666666665, test acc:0.352 ===\n",
      "train loss:1.8863842683703818\n",
      "train loss:1.8979563979446197\n",
      "train loss:1.8874920252782115\n",
      "=== epoch:135, train acc:0.43666666666666665, test acc:0.3539 ===\n",
      "train loss:1.766198065415861\n",
      "train loss:1.790087729946218\n",
      "train loss:1.8359418005865962\n",
      "=== epoch:136, train acc:0.43666666666666665, test acc:0.3508 ===\n",
      "train loss:1.8247117369754406\n",
      "train loss:1.8385637125601533\n",
      "train loss:1.9072738092688164\n",
      "=== epoch:137, train acc:0.44, test acc:0.3547 ===\n",
      "train loss:1.9173286519184223\n",
      "train loss:1.8598588161658984\n",
      "train loss:1.6957436621442066\n",
      "=== epoch:138, train acc:0.44, test acc:0.3578 ===\n",
      "train loss:1.8419253994828595\n",
      "train loss:1.8563795237200855\n",
      "train loss:1.8685046302136465\n",
      "=== epoch:139, train acc:0.43666666666666665, test acc:0.3596 ===\n",
      "train loss:1.9092898101685765\n",
      "train loss:1.8378940258212757\n",
      "train loss:1.7307121659583504\n",
      "=== epoch:140, train acc:0.45, test acc:0.362 ===\n",
      "train loss:1.921209739286273\n",
      "train loss:1.9430290350368924\n",
      "train loss:1.9762455307716535\n",
      "=== epoch:141, train acc:0.4633333333333333, test acc:0.3679 ===\n",
      "train loss:1.7770123139522243\n",
      "train loss:1.818934408089961\n",
      "train loss:1.8679358866506193\n",
      "=== epoch:142, train acc:0.45666666666666667, test acc:0.3659 ===\n",
      "train loss:1.8589132858490516\n",
      "train loss:1.8521994076542447\n",
      "train loss:1.8526514095098443\n",
      "=== epoch:143, train acc:0.4533333333333333, test acc:0.3654 ===\n",
      "train loss:1.809842574348896\n",
      "train loss:1.828976228192824\n",
      "train loss:1.8875240369376975\n",
      "=== epoch:144, train acc:0.45666666666666667, test acc:0.3689 ===\n",
      "train loss:1.8277376801492873\n",
      "train loss:1.7947614781598957\n",
      "train loss:1.8396395809696298\n",
      "=== epoch:145, train acc:0.4533333333333333, test acc:0.3716 ===\n",
      "train loss:1.7804115223304375\n",
      "train loss:1.7556824123598886\n",
      "train loss:1.7931502216594848\n",
      "=== epoch:146, train acc:0.4533333333333333, test acc:0.3727 ===\n",
      "train loss:1.6574393178431628\n",
      "train loss:1.7929381225118413\n",
      "train loss:1.7315551723651157\n",
      "=== epoch:147, train acc:0.4533333333333333, test acc:0.3705 ===\n",
      "train loss:1.8290707046627848\n",
      "train loss:1.8197239568243662\n",
      "train loss:1.8403142870625413\n",
      "=== epoch:148, train acc:0.47333333333333333, test acc:0.3795 ===\n",
      "train loss:1.9267036905764983\n",
      "train loss:1.8015016369917185\n",
      "train loss:1.7847627306552893\n",
      "=== epoch:149, train acc:0.49, test acc:0.3848 ===\n",
      "train loss:1.7521190638221582\n",
      "train loss:1.8567626135548054\n",
      "train loss:1.9187238467023007\n",
      "=== epoch:150, train acc:0.48, test acc:0.3838 ===\n",
      "train loss:1.7634433806132994\n",
      "train loss:1.7059626972168829\n",
      "train loss:2.0104631015999987\n",
      "=== epoch:151, train acc:0.4866666666666667, test acc:0.3825 ===\n",
      "train loss:1.7830327053299195\n",
      "train loss:1.8379623375874352\n",
      "train loss:1.8028815046991016\n",
      "=== epoch:152, train acc:0.48333333333333334, test acc:0.3855 ===\n",
      "train loss:1.8307731171941615\n",
      "train loss:1.7890442329647467\n",
      "train loss:1.7971813290147596\n",
      "=== epoch:153, train acc:0.49333333333333335, test acc:0.3912 ===\n",
      "train loss:1.9061444168470063\n",
      "train loss:1.854118395072082\n",
      "train loss:1.8826491128521545\n",
      "=== epoch:154, train acc:0.5033333333333333, test acc:0.3989 ===\n",
      "train loss:1.7834515315176218\n",
      "train loss:1.7522203012017477\n",
      "train loss:1.6638197349589163\n",
      "=== epoch:155, train acc:0.5066666666666667, test acc:0.3955 ===\n",
      "train loss:1.7276052791568994\n",
      "train loss:1.7313195758271933\n",
      "train loss:1.642404802938783\n",
      "=== epoch:156, train acc:0.4866666666666667, test acc:0.3898 ===\n",
      "train loss:1.7132303900702632\n",
      "train loss:1.7194705578706464\n",
      "train loss:1.7743966388969232\n",
      "=== epoch:157, train acc:0.49333333333333335, test acc:0.3885 ===\n",
      "train loss:1.774510671833936\n",
      "train loss:1.7432010411035117\n",
      "train loss:1.8711055844826194\n",
      "=== epoch:158, train acc:0.49666666666666665, test acc:0.3916 ===\n",
      "train loss:1.7005424321709806\n",
      "train loss:1.569724105866936\n",
      "train loss:1.7408298147956731\n",
      "=== epoch:159, train acc:0.49333333333333335, test acc:0.3893 ===\n",
      "train loss:1.660870391042236\n",
      "train loss:1.8229695404279482\n",
      "train loss:1.7283972647915748\n",
      "=== epoch:160, train acc:0.49333333333333335, test acc:0.3905 ===\n",
      "train loss:1.726467484046831\n",
      "train loss:1.8038296539317051\n",
      "train loss:1.687605147499169\n",
      "=== epoch:161, train acc:0.49333333333333335, test acc:0.3912 ===\n",
      "train loss:1.8259191479980126\n",
      "train loss:1.7226172390316723\n",
      "train loss:1.6660484387227086\n",
      "=== epoch:162, train acc:0.49333333333333335, test acc:0.3938 ===\n",
      "train loss:1.7125585680419582\n",
      "train loss:1.8129335080652489\n",
      "train loss:1.8801586078450847\n",
      "=== epoch:163, train acc:0.49666666666666665, test acc:0.396 ===\n",
      "train loss:1.7921393489950452\n",
      "train loss:1.7789450955522776\n",
      "train loss:1.7122385311070107\n",
      "=== epoch:164, train acc:0.49666666666666665, test acc:0.3963 ===\n",
      "train loss:1.8206450777678997\n",
      "train loss:1.811054917373853\n",
      "train loss:1.8078561997829983\n",
      "=== epoch:165, train acc:0.5033333333333333, test acc:0.4006 ===\n",
      "train loss:1.7246025573148578\n",
      "train loss:1.691201594482689\n",
      "train loss:1.7323912438207782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:166, train acc:0.49666666666666665, test acc:0.4022 ===\n",
      "train loss:1.601684135433723\n",
      "train loss:1.7534902978161329\n",
      "train loss:1.6689916438129972\n",
      "=== epoch:167, train acc:0.49666666666666665, test acc:0.4021 ===\n",
      "train loss:1.767247388982888\n",
      "train loss:1.7652262820605011\n",
      "train loss:1.7727762141998011\n",
      "=== epoch:168, train acc:0.51, test acc:0.4072 ===\n",
      "train loss:1.6497015887287703\n",
      "train loss:1.8147428397102647\n",
      "train loss:1.7310005145524383\n",
      "=== epoch:169, train acc:0.51, test acc:0.4066 ===\n",
      "train loss:1.7079196307432756\n",
      "train loss:1.7113215901791798\n",
      "train loss:1.7317891399700982\n",
      "=== epoch:170, train acc:0.5133333333333333, test acc:0.4084 ===\n",
      "train loss:1.790786745746264\n",
      "train loss:1.8151818308201715\n",
      "train loss:1.7186703009842716\n",
      "=== epoch:171, train acc:0.5233333333333333, test acc:0.4118 ===\n",
      "train loss:1.6778716516976533\n",
      "train loss:1.768381609365936\n",
      "train loss:1.6891317476807217\n",
      "=== epoch:172, train acc:0.53, test acc:0.4156 ===\n",
      "train loss:1.6391650514771285\n",
      "train loss:1.6751290743088896\n",
      "train loss:1.7667181471304871\n",
      "=== epoch:173, train acc:0.5366666666666666, test acc:0.4183 ===\n",
      "train loss:1.6907334101748888\n",
      "train loss:1.7454665775065237\n",
      "train loss:1.7378241656069577\n",
      "=== epoch:174, train acc:0.5366666666666666, test acc:0.4185 ===\n",
      "train loss:1.6731617141719428\n",
      "train loss:1.7197805229276355\n",
      "train loss:1.7751625898201149\n",
      "=== epoch:175, train acc:0.54, test acc:0.4192 ===\n",
      "train loss:1.741912228683132\n",
      "train loss:1.6844273424469034\n",
      "train loss:1.6351793154968979\n",
      "=== epoch:176, train acc:0.5366666666666666, test acc:0.4218 ===\n",
      "train loss:1.6902212963028616\n",
      "train loss:1.57468276361576\n",
      "train loss:1.707432025341697\n",
      "=== epoch:177, train acc:0.54, test acc:0.421 ===\n",
      "train loss:1.6783240534001047\n",
      "train loss:1.7392929856980783\n",
      "train loss:1.5724173987671386\n",
      "=== epoch:178, train acc:0.55, test acc:0.4223 ===\n",
      "train loss:1.7247349605023148\n",
      "train loss:1.6294788574945491\n",
      "train loss:1.5912112384159725\n",
      "=== epoch:179, train acc:0.55, test acc:0.4232 ===\n",
      "train loss:1.7310804614135156\n",
      "train loss:1.627958516241037\n",
      "train loss:1.7001622392434694\n",
      "=== epoch:180, train acc:0.54, test acc:0.4225 ===\n",
      "train loss:1.6144641944228466\n",
      "train loss:1.7285393986956397\n",
      "train loss:1.7787172363641854\n",
      "=== epoch:181, train acc:0.55, test acc:0.4255 ===\n",
      "train loss:1.6105142779313482\n",
      "train loss:1.6235072318149764\n",
      "train loss:1.7167728002644445\n",
      "=== epoch:182, train acc:0.5566666666666666, test acc:0.4295 ===\n",
      "train loss:1.5179784772185088\n",
      "train loss:1.5677420088595801\n",
      "train loss:1.7249818738592742\n",
      "=== epoch:183, train acc:0.5566666666666666, test acc:0.4296 ===\n",
      "train loss:1.7401551689627706\n",
      "train loss:1.6368073193421253\n",
      "train loss:1.5484931385768923\n",
      "=== epoch:184, train acc:0.5466666666666666, test acc:0.4311 ===\n",
      "train loss:1.5974244107887225\n",
      "train loss:1.695956168950966\n",
      "train loss:1.6288922333343112\n",
      "=== epoch:185, train acc:0.5533333333333333, test acc:0.4291 ===\n",
      "train loss:1.6604919701883278\n",
      "train loss:1.6273283864287966\n",
      "train loss:1.7403766261338998\n",
      "=== epoch:186, train acc:0.55, test acc:0.4328 ===\n",
      "train loss:1.54806900916699\n",
      "train loss:1.5925704868591493\n",
      "train loss:1.6814110990387412\n",
      "=== epoch:187, train acc:0.5466666666666666, test acc:0.4285 ===\n",
      "train loss:1.5863674724521801\n",
      "train loss:1.6666648080751756\n",
      "train loss:1.6084668661606312\n",
      "=== epoch:188, train acc:0.54, test acc:0.4298 ===\n",
      "train loss:1.7393499320306212\n",
      "train loss:1.5584883253748152\n",
      "train loss:1.5069289880666847\n",
      "=== epoch:189, train acc:0.55, test acc:0.4347 ===\n",
      "train loss:1.5427575357779955\n",
      "train loss:1.5336068870055564\n",
      "train loss:1.6504807799034722\n",
      "=== epoch:190, train acc:0.54, test acc:0.4346 ===\n",
      "train loss:1.540493425598388\n",
      "train loss:1.7589093052630633\n",
      "train loss:1.6502430734953624\n",
      "=== epoch:191, train acc:0.5433333333333333, test acc:0.4381 ===\n",
      "train loss:1.59962158800939\n",
      "train loss:1.601770200825981\n",
      "train loss:1.616589464461352\n",
      "=== epoch:192, train acc:0.56, test acc:0.441 ===\n",
      "train loss:1.5612813083260255\n",
      "train loss:1.51013095548976\n",
      "train loss:1.575434308080463\n",
      "=== epoch:193, train acc:0.56, test acc:0.4398 ===\n",
      "train loss:1.7201677629291967\n",
      "train loss:1.477165709742033\n",
      "train loss:1.6411379247501543\n",
      "=== epoch:194, train acc:0.5633333333333334, test acc:0.4414 ===\n",
      "train loss:1.618538203603042\n",
      "train loss:1.4916871314289881\n",
      "train loss:1.4881884191481232\n",
      "=== epoch:195, train acc:0.5566666666666666, test acc:0.4412 ===\n",
      "train loss:1.6018101084317837\n",
      "train loss:1.5023306979212088\n",
      "train loss:1.6234176269373717\n",
      "=== epoch:196, train acc:0.5666666666666667, test acc:0.4415 ===\n",
      "train loss:1.5761382275843647\n",
      "train loss:1.5327480943091152\n",
      "train loss:1.7252882538933654\n",
      "=== epoch:197, train acc:0.56, test acc:0.4439 ===\n",
      "train loss:1.5448585464902522\n",
      "train loss:1.5806991800504733\n",
      "train loss:1.4633821282826411\n",
      "=== epoch:198, train acc:0.57, test acc:0.4453 ===\n",
      "train loss:1.6032439963682368\n",
      "train loss:1.54545251606266\n",
      "train loss:1.5548369940912679\n",
      "=== epoch:199, train acc:0.5566666666666666, test acc:0.4434 ===\n",
      "train loss:1.6002185774943707\n",
      "train loss:1.5735079616510104\n",
      "train loss:1.607459703283637\n",
      "=== epoch:200, train acc:0.5533333333333333, test acc:0.4457 ===\n",
      "train loss:1.61106208651671\n",
      "train loss:1.5676701173058332\n",
      "train loss:1.495056265865563\n",
      "=== epoch:201, train acc:0.56, test acc:0.4477 ===\n",
      "train loss:1.4766437532505103\n",
      "train loss:1.5143720234868252\n",
      "train loss:1.4613072581129987\n",
      "=== epoch:202, train acc:0.56, test acc:0.4503 ===\n",
      "train loss:1.640829854478072\n",
      "train loss:1.60761518209723\n",
      "train loss:1.6524062699151592\n",
      "=== epoch:203, train acc:0.56, test acc:0.4527 ===\n",
      "train loss:1.6205358394578697\n",
      "train loss:1.4416077848682733\n",
      "train loss:1.6150049258460817\n",
      "=== epoch:204, train acc:0.5666666666666667, test acc:0.4526 ===\n",
      "train loss:1.501044427598424\n",
      "train loss:1.5741689822923988\n",
      "train loss:1.462139781751987\n",
      "=== epoch:205, train acc:0.5633333333333334, test acc:0.4533 ===\n",
      "train loss:1.5829506523174184\n",
      "train loss:1.5161850357891382\n",
      "train loss:1.4555729439479712\n",
      "=== epoch:206, train acc:0.5666666666666667, test acc:0.4552 ===\n",
      "train loss:1.6241504189463427\n",
      "train loss:1.4581638622630078\n",
      "train loss:1.4668230059280984\n",
      "=== epoch:207, train acc:0.5666666666666667, test acc:0.4572 ===\n",
      "train loss:1.5859037484907652\n",
      "train loss:1.6446975778374386\n",
      "train loss:1.6452889564113904\n",
      "=== epoch:208, train acc:0.58, test acc:0.4626 ===\n",
      "train loss:1.55723459005815\n",
      "train loss:1.5002854717867962\n",
      "train loss:1.5743642967363067\n",
      "=== epoch:209, train acc:0.5733333333333334, test acc:0.463 ===\n",
      "train loss:1.3581567274249275\n",
      "train loss:1.4593024313284875\n",
      "train loss:1.5232856839435454\n",
      "=== epoch:210, train acc:0.5733333333333334, test acc:0.4638 ===\n",
      "train loss:1.3810507484558212\n",
      "train loss:1.3882511512599387\n",
      "train loss:1.4423707856799155\n",
      "=== epoch:211, train acc:0.5733333333333334, test acc:0.4631 ===\n",
      "train loss:1.4943386591016838\n",
      "train loss:1.3958618833646446\n",
      "train loss:1.395149494937089\n",
      "=== epoch:212, train acc:0.5766666666666667, test acc:0.4642 ===\n",
      "train loss:1.4833030088495844\n",
      "train loss:1.6151506146045662\n",
      "train loss:1.6864757179365384\n",
      "=== epoch:213, train acc:0.58, test acc:0.4701 ===\n",
      "train loss:1.4668678557910595\n",
      "train loss:1.3823398601130026\n",
      "train loss:1.3480259270761374\n",
      "=== epoch:214, train acc:0.5866666666666667, test acc:0.4714 ===\n",
      "train loss:1.448968584271033\n",
      "train loss:1.46559123977792\n",
      "train loss:1.4623504609023597\n",
      "=== epoch:215, train acc:0.5966666666666667, test acc:0.4737 ===\n",
      "train loss:1.6216038829220363\n",
      "train loss:1.4391676413420635\n",
      "train loss:1.6171858656024087\n",
      "=== epoch:216, train acc:0.6033333333333334, test acc:0.4741 ===\n",
      "train loss:1.6002258135861576\n",
      "train loss:1.3762247272586554\n",
      "train loss:1.4383409130870746\n",
      "=== epoch:217, train acc:0.5966666666666667, test acc:0.4757 ===\n",
      "train loss:1.3947300279667334\n",
      "train loss:1.3757619387047257\n",
      "train loss:1.375314752198522\n",
      "=== epoch:218, train acc:0.6, test acc:0.4753 ===\n",
      "train loss:1.4759933774602099\n",
      "train loss:1.5022995666229428\n",
      "train loss:1.4622276942701817\n",
      "=== epoch:219, train acc:0.61, test acc:0.4794 ===\n",
      "train loss:1.3669341814983447\n",
      "train loss:1.3843379040169455\n",
      "train loss:1.4609514141464073\n",
      "=== epoch:220, train acc:0.61, test acc:0.4807 ===\n",
      "train loss:1.4469235535656446\n",
      "train loss:1.445915669811161\n",
      "train loss:1.5604840487444522\n",
      "=== epoch:221, train acc:0.6066666666666667, test acc:0.4837 ===\n",
      "train loss:1.5471660017280462\n",
      "train loss:1.467757238039431\n",
      "train loss:1.4438819131921738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.62, test acc:0.4851 ===\n",
      "train loss:1.4183465324123992\n",
      "train loss:1.4545543974882411\n",
      "train loss:1.4115579966448637\n",
      "=== epoch:223, train acc:0.6233333333333333, test acc:0.4878 ===\n",
      "train loss:1.4117753910437292\n",
      "train loss:1.3873947336072212\n",
      "train loss:1.3675283135712375\n",
      "=== epoch:224, train acc:0.6233333333333333, test acc:0.488 ===\n",
      "train loss:1.5479985881799125\n",
      "train loss:1.3541978959952385\n",
      "train loss:1.4034139705617479\n",
      "=== epoch:225, train acc:0.6233333333333333, test acc:0.4891 ===\n",
      "train loss:1.440403018738745\n",
      "train loss:1.4764444387355564\n",
      "train loss:1.4887438322602093\n",
      "=== epoch:226, train acc:0.62, test acc:0.4899 ===\n",
      "train loss:1.4090230323535555\n",
      "train loss:1.562307415010366\n",
      "train loss:1.4496912061393403\n",
      "=== epoch:227, train acc:0.6166666666666667, test acc:0.4912 ===\n",
      "train loss:1.4117036891269574\n",
      "train loss:1.4017382236684954\n",
      "train loss:1.4830368223933297\n",
      "=== epoch:228, train acc:0.62, test acc:0.4902 ===\n",
      "train loss:1.3148115400546394\n",
      "train loss:1.2912160687574583\n",
      "train loss:1.581487201004996\n",
      "=== epoch:229, train acc:0.6133333333333333, test acc:0.4869 ===\n",
      "train loss:1.296942387228282\n",
      "train loss:1.1682508715651234\n",
      "train loss:1.3931844566748233\n",
      "=== epoch:230, train acc:0.6066666666666667, test acc:0.4891 ===\n",
      "train loss:1.3935527557303922\n",
      "train loss:1.3715418668011181\n",
      "train loss:1.4232189725231696\n",
      "=== epoch:231, train acc:0.61, test acc:0.4901 ===\n",
      "train loss:1.2617164697526213\n",
      "train loss:1.3311602078963423\n",
      "train loss:1.275518506189129\n",
      "=== epoch:232, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.434589733930982\n",
      "train loss:1.4096389035882564\n",
      "train loss:1.3917692124003511\n",
      "=== epoch:233, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.2818695302209142\n",
      "train loss:1.4026666009448507\n",
      "train loss:1.43017668191691\n",
      "=== epoch:234, train acc:0.62, test acc:0.4928 ===\n",
      "train loss:1.1710561071587469\n",
      "train loss:1.4822128479521124\n",
      "train loss:1.1993306322183\n",
      "=== epoch:235, train acc:0.61, test acc:0.4966 ===\n",
      "train loss:1.2656868294621497\n",
      "train loss:1.355177579547156\n",
      "train loss:1.1916068126361834\n",
      "=== epoch:236, train acc:0.6166666666666667, test acc:0.4939 ===\n",
      "train loss:1.3140271438601119\n",
      "train loss:1.3003115742701947\n",
      "train loss:1.32219638625986\n",
      "=== epoch:237, train acc:0.6166666666666667, test acc:0.4943 ===\n",
      "train loss:1.1644347289183075\n",
      "train loss:1.4207487909877816\n",
      "train loss:1.3074685913952524\n",
      "=== epoch:238, train acc:0.6166666666666667, test acc:0.4957 ===\n",
      "train loss:1.228960293123101\n",
      "train loss:1.4195609153070061\n",
      "train loss:1.40238838294953\n",
      "=== epoch:239, train acc:0.62, test acc:0.4973 ===\n",
      "train loss:1.3808227177664696\n",
      "train loss:1.3341466980320278\n",
      "train loss:1.4339088323483165\n",
      "=== epoch:240, train acc:0.6233333333333333, test acc:0.5005 ===\n",
      "train loss:1.3125003242997466\n",
      "train loss:1.3263370542515907\n",
      "train loss:1.4818384355857797\n",
      "=== epoch:241, train acc:0.6233333333333333, test acc:0.5043 ===\n",
      "train loss:1.3287529531646056\n",
      "train loss:1.3937851045421972\n",
      "train loss:1.2150911221815253\n",
      "=== epoch:242, train acc:0.6266666666666667, test acc:0.5054 ===\n",
      "train loss:1.3427367787132964\n",
      "train loss:1.3137481052621167\n",
      "train loss:1.4476330692196522\n",
      "=== epoch:243, train acc:0.6333333333333333, test acc:0.5105 ===\n",
      "train loss:1.308608411332156\n",
      "train loss:1.4778569546594766\n",
      "train loss:1.1400750300715272\n",
      "=== epoch:244, train acc:0.6266666666666667, test acc:0.5113 ===\n",
      "train loss:1.3619732985344646\n",
      "train loss:1.2083959315807078\n",
      "train loss:1.288040631833285\n",
      "=== epoch:245, train acc:0.63, test acc:0.5113 ===\n",
      "train loss:1.365655138211914\n",
      "train loss:1.3640579582060959\n",
      "train loss:1.2200629382725374\n",
      "=== epoch:246, train acc:0.6333333333333333, test acc:0.5107 ===\n",
      "train loss:1.3011220142843616\n",
      "train loss:1.310209685042117\n",
      "train loss:1.3672493602550668\n",
      "=== epoch:247, train acc:0.63, test acc:0.5128 ===\n",
      "train loss:1.351409787100981\n",
      "train loss:1.1595067385526727\n",
      "train loss:1.1410123166524082\n",
      "=== epoch:248, train acc:0.6333333333333333, test acc:0.5127 ===\n",
      "train loss:1.275452742568816\n",
      "train loss:1.396810384830021\n",
      "train loss:1.2612874106618572\n",
      "=== epoch:249, train acc:0.63, test acc:0.5145 ===\n",
      "train loss:1.21477018962594\n",
      "train loss:1.1997441465647338\n",
      "train loss:1.1639943701986015\n",
      "=== epoch:250, train acc:0.63, test acc:0.513 ===\n",
      "train loss:1.2743407539486264\n",
      "train loss:1.2497999396875503\n",
      "train loss:1.2040936949321057\n",
      "=== epoch:251, train acc:0.6333333333333333, test acc:0.5139 ===\n",
      "train loss:1.16388560137538\n",
      "train loss:1.219159498633611\n",
      "train loss:1.1364667686978864\n",
      "=== epoch:252, train acc:0.6366666666666667, test acc:0.5157 ===\n",
      "train loss:1.1921830934806392\n",
      "train loss:1.1298404912913282\n",
      "train loss:1.399244499397906\n",
      "=== epoch:253, train acc:0.6366666666666667, test acc:0.5178 ===\n",
      "train loss:1.1775865934841214\n",
      "train loss:1.2647793402124439\n",
      "train loss:1.2155369465695003\n",
      "=== epoch:254, train acc:0.64, test acc:0.518 ===\n",
      "train loss:1.2293331351631485\n",
      "train loss:1.1527264144535403\n",
      "train loss:1.3675531767986207\n",
      "=== epoch:255, train acc:0.64, test acc:0.5169 ===\n",
      "train loss:1.3133567579860903\n",
      "train loss:1.1467386893884572\n",
      "train loss:1.1356097313405282\n",
      "=== epoch:256, train acc:0.6433333333333333, test acc:0.5206 ===\n",
      "train loss:1.1908281649187138\n",
      "train loss:1.1513170784093583\n",
      "train loss:1.3369344993405123\n",
      "=== epoch:257, train acc:0.6466666666666666, test acc:0.5211 ===\n",
      "train loss:1.1115841186825335\n",
      "train loss:1.1910838726466628\n",
      "train loss:1.0521012179608429\n",
      "=== epoch:258, train acc:0.64, test acc:0.5175 ===\n",
      "train loss:1.17451663437281\n",
      "train loss:1.2180602075075082\n",
      "train loss:1.074061808073714\n",
      "=== epoch:259, train acc:0.6366666666666667, test acc:0.519 ===\n",
      "train loss:1.1520708482786135\n",
      "train loss:1.1476955321561184\n",
      "train loss:1.2157996911996296\n",
      "=== epoch:260, train acc:0.64, test acc:0.5217 ===\n",
      "train loss:1.1384795232510094\n",
      "train loss:1.0569084305750323\n",
      "train loss:1.1917819772760796\n",
      "=== epoch:261, train acc:0.65, test acc:0.5244 ===\n",
      "train loss:1.2550672082374927\n",
      "train loss:1.1750098295736886\n",
      "train loss:1.298699452657462\n",
      "=== epoch:262, train acc:0.6566666666666666, test acc:0.526 ===\n",
      "train loss:1.266089851659973\n",
      "train loss:1.1897316838105962\n",
      "train loss:1.241490250420644\n",
      "=== epoch:263, train acc:0.6566666666666666, test acc:0.527 ===\n",
      "train loss:1.1156105448835623\n",
      "train loss:1.1449100297684571\n",
      "train loss:1.2283657088276438\n",
      "=== epoch:264, train acc:0.66, test acc:0.5306 ===\n",
      "train loss:1.127053958045407\n",
      "train loss:1.1464714686856554\n",
      "train loss:1.2510267756488334\n",
      "=== epoch:265, train acc:0.6533333333333333, test acc:0.5313 ===\n",
      "train loss:1.1189348908774295\n",
      "train loss:1.0428306145948951\n",
      "train loss:1.100043088613373\n",
      "=== epoch:266, train acc:0.6566666666666666, test acc:0.5316 ===\n",
      "train loss:1.2035948815665156\n",
      "train loss:1.1714703350430284\n",
      "train loss:1.0719876856855255\n",
      "=== epoch:267, train acc:0.6566666666666666, test acc:0.5308 ===\n",
      "train loss:1.1297963247809804\n",
      "train loss:1.2411578860272308\n",
      "train loss:1.1340313077458084\n",
      "=== epoch:268, train acc:0.66, test acc:0.5345 ===\n",
      "train loss:1.1044595151182683\n",
      "train loss:1.1736621975423216\n",
      "train loss:1.1192580038802544\n",
      "=== epoch:269, train acc:0.6566666666666666, test acc:0.5347 ===\n",
      "train loss:1.134031774340811\n",
      "train loss:1.3073268938631466\n",
      "train loss:1.2590638206633664\n",
      "=== epoch:270, train acc:0.6566666666666666, test acc:0.5388 ===\n",
      "train loss:1.1740987646918501\n",
      "train loss:1.1560711788312081\n",
      "train loss:1.1788198372567735\n",
      "=== epoch:271, train acc:0.66, test acc:0.5398 ===\n",
      "train loss:1.2131893288602051\n",
      "train loss:1.160347351323801\n",
      "train loss:1.1173030820922358\n",
      "=== epoch:272, train acc:0.6633333333333333, test acc:0.542 ===\n",
      "train loss:1.2665271704856738\n",
      "train loss:1.0690772826265005\n",
      "train loss:1.1037228366337366\n",
      "=== epoch:273, train acc:0.6566666666666666, test acc:0.5425 ===\n",
      "train loss:1.0908080048774154\n",
      "train loss:1.05256551064852\n",
      "train loss:1.0974251514359603\n",
      "=== epoch:274, train acc:0.66, test acc:0.5465 ===\n",
      "train loss:1.0753521369933337\n",
      "train loss:1.0542607106786632\n",
      "train loss:1.0924397058510498\n",
      "=== epoch:275, train acc:0.6666666666666666, test acc:0.5498 ===\n",
      "train loss:1.1127191810877772\n",
      "train loss:1.1134829227528247\n",
      "train loss:1.1965425636670135\n",
      "=== epoch:276, train acc:0.66, test acc:0.5462 ===\n",
      "train loss:1.0092791055355754\n",
      "train loss:1.1412261092739895\n",
      "train loss:1.2213100351916082\n",
      "=== epoch:277, train acc:0.67, test acc:0.5462 ===\n",
      "train loss:1.136137435754212\n",
      "train loss:1.0566513610714923\n",
      "train loss:1.018401930575842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.6666666666666666, test acc:0.5486 ===\n",
      "train loss:1.0874193582387206\n",
      "train loss:1.0418094114709664\n",
      "train loss:1.0120216389474441\n",
      "=== epoch:279, train acc:0.6733333333333333, test acc:0.5512 ===\n",
      "train loss:1.0568060210001897\n",
      "train loss:1.1224952015772531\n",
      "train loss:1.1263569077099056\n",
      "=== epoch:280, train acc:0.67, test acc:0.5542 ===\n",
      "train loss:0.98461136874101\n",
      "train loss:1.1920972859060037\n",
      "train loss:1.1785947116287852\n",
      "=== epoch:281, train acc:0.6733333333333333, test acc:0.5558 ===\n",
      "train loss:1.084859618396277\n",
      "train loss:0.9360729176722776\n",
      "train loss:1.181873792830452\n",
      "=== epoch:282, train acc:0.6733333333333333, test acc:0.5555 ===\n",
      "train loss:0.9529161696389034\n",
      "train loss:1.076630310748851\n",
      "train loss:1.08050008463593\n",
      "=== epoch:283, train acc:0.67, test acc:0.5559 ===\n",
      "train loss:1.0963873432347955\n",
      "train loss:1.2305915423633498\n",
      "train loss:1.0654327781361803\n",
      "=== epoch:284, train acc:0.6733333333333333, test acc:0.5549 ===\n",
      "train loss:1.17976571224707\n",
      "train loss:1.0651040003310612\n",
      "train loss:1.07647080193693\n",
      "=== epoch:285, train acc:0.6766666666666666, test acc:0.5545 ===\n",
      "train loss:1.0434379198301322\n",
      "train loss:1.1946393542185163\n",
      "train loss:0.9248372179225274\n",
      "=== epoch:286, train acc:0.6766666666666666, test acc:0.5561 ===\n",
      "train loss:0.9760895067864449\n",
      "train loss:0.9953704347960022\n",
      "train loss:0.9419919315740007\n",
      "=== epoch:287, train acc:0.68, test acc:0.5572 ===\n",
      "train loss:0.9866982249887059\n",
      "train loss:0.9395837930402189\n",
      "train loss:0.9135119066975034\n",
      "=== epoch:288, train acc:0.68, test acc:0.5563 ===\n",
      "train loss:1.0480032276619893\n",
      "train loss:1.1123146916797213\n",
      "train loss:1.019298976649288\n",
      "=== epoch:289, train acc:0.6833333333333333, test acc:0.5581 ===\n",
      "train loss:1.0533770968204135\n",
      "train loss:1.0138834481951786\n",
      "train loss:1.034457727162962\n",
      "=== epoch:290, train acc:0.68, test acc:0.5572 ===\n",
      "train loss:1.1042829130158793\n",
      "train loss:1.0274356821157085\n",
      "train loss:1.0282452686175676\n",
      "=== epoch:291, train acc:0.6833333333333333, test acc:0.5619 ===\n",
      "train loss:1.0538385452053824\n",
      "train loss:0.9440829396652138\n",
      "train loss:1.028482267387899\n",
      "=== epoch:292, train acc:0.6866666666666666, test acc:0.5659 ===\n",
      "train loss:0.9323650884273765\n",
      "train loss:0.9043018828264329\n",
      "train loss:1.1737123619599352\n",
      "=== epoch:293, train acc:0.6966666666666667, test acc:0.5667 ===\n",
      "train loss:1.0046762831328353\n",
      "train loss:1.0105227361607048\n",
      "train loss:0.859192961577333\n",
      "=== epoch:294, train acc:0.6866666666666666, test acc:0.5664 ===\n",
      "train loss:0.9301344261153623\n",
      "train loss:1.0291152875650837\n",
      "train loss:1.0379859787976857\n",
      "=== epoch:295, train acc:0.6966666666666667, test acc:0.5685 ===\n",
      "train loss:0.8921797040558829\n",
      "train loss:0.856398456927686\n",
      "train loss:1.0352314036163812\n",
      "=== epoch:296, train acc:0.6966666666666667, test acc:0.5693 ===\n",
      "train loss:1.0942260367010597\n",
      "train loss:0.9750134344716517\n",
      "train loss:1.0634005020293436\n",
      "=== epoch:297, train acc:0.69, test acc:0.5719 ===\n",
      "train loss:0.9115175532700575\n",
      "train loss:0.9311663700583519\n",
      "train loss:0.9693783969338614\n",
      "=== epoch:298, train acc:0.69, test acc:0.5743 ===\n",
      "train loss:1.0597827325185911\n",
      "train loss:1.0504322188185946\n",
      "train loss:1.0189951686192409\n",
      "=== epoch:299, train acc:0.6966666666666667, test acc:0.5762 ===\n",
      "train loss:0.9447887138246603\n",
      "train loss:1.1564443552982862\n",
      "train loss:0.9787197033465947\n",
      "=== epoch:300, train acc:0.7033333333333334, test acc:0.5766 ===\n",
      "train loss:0.9791444733549995\n",
      "train loss:0.9787897074256374\n",
      "train loss:0.9087327081812776\n",
      "=== epoch:301, train acc:0.71, test acc:0.5751 ===\n",
      "train loss:0.9203411457996056\n",
      "train loss:1.0053928187220311\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5789\n"
     ]
    }
   ],
   "source": [
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ad3f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dccfd",
   "metadata": {},
   "source": [
    "드롭아웃 비율 조정해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a7a3b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3314751181089512\n",
      "=== epoch:1, train acc:0.08666666666666667, test acc:0.1034 ===\n",
      "train loss:2.3393209350684536\n",
      "train loss:2.316973392017008\n",
      "train loss:2.320330647975749\n",
      "=== epoch:2, train acc:0.09, test acc:0.1054 ===\n",
      "train loss:2.3096688153665212\n",
      "train loss:2.315644790181693\n",
      "train loss:2.3051668297623507\n",
      "=== epoch:3, train acc:0.07666666666666666, test acc:0.1104 ===\n",
      "train loss:2.3121373355348935\n",
      "train loss:2.3000943990132647\n",
      "train loss:2.2679346128534514\n",
      "=== epoch:4, train acc:0.08666666666666667, test acc:0.1112 ===\n",
      "train loss:2.2790382893603276\n",
      "train loss:2.2800470053793895\n",
      "train loss:2.3197652621345344\n",
      "=== epoch:5, train acc:0.09666666666666666, test acc:0.1157 ===\n",
      "train loss:2.2760483068444675\n",
      "train loss:2.3182318850256776\n",
      "train loss:2.3084666431057097\n",
      "=== epoch:6, train acc:0.10333333333333333, test acc:0.118 ===\n",
      "train loss:2.2794577794697\n",
      "train loss:2.294928824587181\n",
      "train loss:2.2701767258192604\n",
      "=== epoch:7, train acc:0.10333333333333333, test acc:0.1219 ===\n",
      "train loss:2.2787233305391514\n",
      "train loss:2.275167847259375\n",
      "train loss:2.2492261400537923\n",
      "=== epoch:8, train acc:0.11, test acc:0.1227 ===\n",
      "train loss:2.277677463851423\n",
      "train loss:2.2833027555985894\n",
      "train loss:2.2887755450892313\n",
      "=== epoch:9, train acc:0.12, test acc:0.1281 ===\n",
      "train loss:2.2603139608853904\n",
      "train loss:2.261413481444062\n",
      "train loss:2.2672874530879663\n",
      "=== epoch:10, train acc:0.12666666666666668, test acc:0.1301 ===\n",
      "train loss:2.280975767426994\n",
      "train loss:2.2533497105312517\n",
      "train loss:2.2388683325196403\n",
      "=== epoch:11, train acc:0.13, test acc:0.1341 ===\n",
      "train loss:2.268293509020627\n",
      "train loss:2.270409009696171\n",
      "train loss:2.246787803878088\n",
      "=== epoch:12, train acc:0.13666666666666666, test acc:0.1369 ===\n",
      "train loss:2.2363150216505976\n",
      "train loss:2.261381894991155\n",
      "train loss:2.2690194038559937\n",
      "=== epoch:13, train acc:0.14, test acc:0.1394 ===\n",
      "train loss:2.252182668023545\n",
      "train loss:2.2553891149993\n",
      "train loss:2.247501984269832\n",
      "=== epoch:14, train acc:0.14666666666666667, test acc:0.1431 ===\n",
      "train loss:2.2438956839722706\n",
      "train loss:2.2653975249232023\n",
      "train loss:2.229117508431663\n",
      "=== epoch:15, train acc:0.16333333333333333, test acc:0.1492 ===\n",
      "train loss:2.2395315616997906\n",
      "train loss:2.2371708792981146\n",
      "train loss:2.2347003811846777\n",
      "=== epoch:16, train acc:0.16666666666666666, test acc:0.1571 ===\n",
      "train loss:2.2562851910474335\n",
      "train loss:2.2396253390588696\n",
      "train loss:2.2458841572448267\n",
      "=== epoch:17, train acc:0.17333333333333334, test acc:0.1661 ===\n",
      "train loss:2.246802576636897\n",
      "train loss:2.2317929128132645\n",
      "train loss:2.206902036471575\n",
      "=== epoch:18, train acc:0.18333333333333332, test acc:0.1754 ===\n",
      "train loss:2.221058581037594\n",
      "train loss:2.223989798713319\n",
      "train loss:2.2455424940453867\n",
      "=== epoch:19, train acc:0.20333333333333334, test acc:0.1835 ===\n",
      "train loss:2.2085512827387452\n",
      "train loss:2.2256773192976893\n",
      "train loss:2.219811375773403\n",
      "=== epoch:20, train acc:0.21666666666666667, test acc:0.1886 ===\n",
      "train loss:2.195888435767331\n",
      "train loss:2.199453457946164\n",
      "train loss:2.218299751710297\n",
      "=== epoch:21, train acc:0.23333333333333334, test acc:0.1962 ===\n",
      "train loss:2.228064617897319\n",
      "train loss:2.199407575057315\n",
      "train loss:2.1884990424537647\n",
      "=== epoch:22, train acc:0.23666666666666666, test acc:0.1974 ===\n",
      "train loss:2.211859114420577\n",
      "train loss:2.2065132471685893\n",
      "train loss:2.2085999038001045\n",
      "=== epoch:23, train acc:0.24, test acc:0.2002 ===\n",
      "train loss:2.184738845797837\n",
      "train loss:2.1876451618721613\n",
      "train loss:2.2068063486140925\n",
      "=== epoch:24, train acc:0.26666666666666666, test acc:0.2087 ===\n",
      "train loss:2.221595304021757\n",
      "train loss:2.2100650715804315\n",
      "train loss:2.1752261951599663\n",
      "=== epoch:25, train acc:0.2733333333333333, test acc:0.2149 ===\n",
      "train loss:2.1734652086590063\n",
      "train loss:2.1464117947157875\n",
      "train loss:2.1708208232401667\n",
      "=== epoch:26, train acc:0.28, test acc:0.2187 ===\n",
      "train loss:2.193653062638936\n",
      "train loss:2.168038391438246\n",
      "train loss:2.1663721089979706\n",
      "=== epoch:27, train acc:0.2866666666666667, test acc:0.2221 ===\n",
      "train loss:2.2250697998705022\n",
      "train loss:2.1712327288357742\n",
      "train loss:2.146426847541231\n",
      "=== epoch:28, train acc:0.30333333333333334, test acc:0.2333 ===\n",
      "train loss:2.1945349573367143\n",
      "train loss:2.122321132553878\n",
      "train loss:2.174403691401599\n",
      "=== epoch:29, train acc:0.30666666666666664, test acc:0.2353 ===\n",
      "train loss:2.135877083362867\n",
      "train loss:2.178073662055428\n",
      "train loss:2.218221544813119\n",
      "=== epoch:30, train acc:0.31333333333333335, test acc:0.2481 ===\n",
      "train loss:2.150380079151246\n",
      "train loss:2.1649648168103854\n",
      "train loss:2.1769385106620134\n",
      "=== epoch:31, train acc:0.32, test acc:0.251 ===\n",
      "train loss:2.1495172025238296\n",
      "train loss:2.1722576954473922\n",
      "train loss:2.174520756220784\n",
      "=== epoch:32, train acc:0.3233333333333333, test acc:0.2599 ===\n",
      "train loss:2.1391107017658073\n",
      "train loss:2.160747122613695\n",
      "train loss:2.123087070196292\n",
      "=== epoch:33, train acc:0.3333333333333333, test acc:0.2697 ===\n",
      "train loss:2.060234799256805\n",
      "train loss:2.121065542866403\n",
      "train loss:2.1618339473568837\n",
      "=== epoch:34, train acc:0.33, test acc:0.2723 ===\n",
      "train loss:2.1555695860448956\n",
      "train loss:2.1095991432557732\n",
      "train loss:2.21852315413534\n",
      "=== epoch:35, train acc:0.3433333333333333, test acc:0.2785 ===\n",
      "train loss:2.1248873808312574\n",
      "train loss:2.151492618777807\n",
      "train loss:2.153594834473054\n",
      "=== epoch:36, train acc:0.36, test acc:0.292 ===\n",
      "train loss:2.1137145937162356\n",
      "train loss:2.1496351354735648\n",
      "train loss:2.1219488128280624\n",
      "=== epoch:37, train acc:0.36333333333333334, test acc:0.2972 ===\n",
      "train loss:2.1700131198432375\n",
      "train loss:2.0696233938403408\n",
      "train loss:2.106695903597171\n",
      "=== epoch:38, train acc:0.36333333333333334, test acc:0.2998 ===\n",
      "train loss:2.1397263894684966\n",
      "train loss:2.1065762631417\n",
      "train loss:2.105102455002205\n",
      "=== epoch:39, train acc:0.36666666666666664, test acc:0.3092 ===\n",
      "train loss:2.081202220631357\n",
      "train loss:2.1416842601049604\n",
      "train loss:2.0649587674815\n",
      "=== epoch:40, train acc:0.39, test acc:0.3156 ===\n",
      "train loss:2.102533139653063\n",
      "train loss:2.0798136325846106\n",
      "train loss:2.137988683819812\n",
      "=== epoch:41, train acc:0.39666666666666667, test acc:0.3209 ===\n",
      "train loss:2.120071543900722\n",
      "train loss:2.1069852521884234\n",
      "train loss:2.1204466201187\n",
      "=== epoch:42, train acc:0.41, test acc:0.3281 ===\n",
      "train loss:2.0856271410891734\n",
      "train loss:2.0700122187639582\n",
      "train loss:2.1136004404013047\n",
      "=== epoch:43, train acc:0.41333333333333333, test acc:0.3415 ===\n",
      "train loss:2.0533532237708885\n",
      "train loss:2.086742259944952\n",
      "train loss:2.1344584588534414\n",
      "=== epoch:44, train acc:0.4533333333333333, test acc:0.3478 ===\n",
      "train loss:2.0512321151466897\n",
      "train loss:2.037815799621777\n",
      "train loss:2.049596410459815\n",
      "=== epoch:45, train acc:0.44, test acc:0.3447 ===\n",
      "train loss:2.105469909625949\n",
      "train loss:2.0524171526964223\n",
      "train loss:2.0601417597630385\n",
      "=== epoch:46, train acc:0.43666666666666665, test acc:0.3439 ===\n",
      "train loss:2.0981503857460364\n",
      "train loss:2.06237092889476\n",
      "train loss:2.0598684064174\n",
      "=== epoch:47, train acc:0.4633333333333333, test acc:0.351 ===\n",
      "train loss:2.027399715363854\n",
      "train loss:2.04160080389209\n",
      "train loss:2.01416795277841\n",
      "=== epoch:48, train acc:0.4533333333333333, test acc:0.3458 ===\n",
      "train loss:2.0283666549146924\n",
      "train loss:2.0467030577807934\n",
      "train loss:1.9973881002958476\n",
      "=== epoch:49, train acc:0.46, test acc:0.3523 ===\n",
      "train loss:2.0039743403424044\n",
      "train loss:2.066295705278144\n",
      "train loss:1.991652387384154\n",
      "=== epoch:50, train acc:0.4633333333333333, test acc:0.3658 ===\n",
      "train loss:2.0283537709628963\n",
      "train loss:2.0586031740830815\n",
      "train loss:1.9911143401824245\n",
      "=== epoch:51, train acc:0.4866666666666667, test acc:0.3789 ===\n",
      "train loss:1.9767693710032939\n",
      "train loss:2.0026433829797456\n",
      "train loss:2.0341105874606606\n",
      "=== epoch:52, train acc:0.5, test acc:0.3842 ===\n",
      "train loss:1.9778097228859353\n",
      "train loss:2.0434224045743967\n",
      "train loss:1.9628631575653828\n",
      "=== epoch:53, train acc:0.51, test acc:0.3911 ===\n",
      "train loss:1.9223105633308373\n",
      "train loss:2.017177047819587\n",
      "train loss:2.018043646922051\n",
      "=== epoch:54, train acc:0.5133333333333333, test acc:0.3993 ===\n",
      "train loss:1.9500777434306327\n",
      "train loss:2.006454204994922\n",
      "train loss:1.994693738127192\n",
      "=== epoch:55, train acc:0.53, test acc:0.4055 ===\n",
      "train loss:1.9607747816432832\n",
      "train loss:1.9704989768692176\n",
      "train loss:2.0194607794402977\n",
      "=== epoch:56, train acc:0.5533333333333333, test acc:0.4144 ===\n",
      "train loss:1.9168497112643124\n",
      "train loss:1.9566571207526704\n",
      "train loss:2.023822897081169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.5566666666666666, test acc:0.4157 ===\n",
      "train loss:1.9721562038951685\n",
      "train loss:1.9831237895919558\n",
      "train loss:1.9249488649550446\n",
      "=== epoch:58, train acc:0.5666666666666667, test acc:0.4169 ===\n",
      "train loss:1.9987866740320683\n",
      "train loss:1.929361675205635\n",
      "train loss:2.000839817672491\n",
      "=== epoch:59, train acc:0.5833333333333334, test acc:0.4346 ===\n",
      "train loss:1.973932005282387\n",
      "train loss:1.9374767791678238\n",
      "train loss:1.8406987348124373\n",
      "=== epoch:60, train acc:0.58, test acc:0.439 ===\n",
      "train loss:2.000701086821687\n",
      "train loss:1.8417411838556945\n",
      "train loss:1.8541279760966933\n",
      "=== epoch:61, train acc:0.5833333333333334, test acc:0.4464 ===\n",
      "train loss:1.9125640679186355\n",
      "train loss:1.950086794273136\n",
      "train loss:1.9126243624838575\n",
      "=== epoch:62, train acc:0.5933333333333334, test acc:0.4487 ===\n",
      "train loss:1.9855500172449785\n",
      "train loss:1.8776971627603078\n",
      "train loss:2.0099015276676626\n",
      "=== epoch:63, train acc:0.5966666666666667, test acc:0.4552 ===\n",
      "train loss:1.8876429114634632\n",
      "train loss:1.9001630210457259\n",
      "train loss:1.825306074543248\n",
      "=== epoch:64, train acc:0.6, test acc:0.4582 ===\n",
      "train loss:1.8609838352319994\n",
      "train loss:1.8965170921872896\n",
      "train loss:1.8332325460134584\n",
      "=== epoch:65, train acc:0.6133333333333333, test acc:0.4636 ===\n",
      "train loss:1.7812601088404358\n",
      "train loss:1.8974534385724366\n",
      "train loss:1.8250341488140238\n",
      "=== epoch:66, train acc:0.6133333333333333, test acc:0.4694 ===\n",
      "train loss:1.802202690342329\n",
      "train loss:1.808501577507559\n",
      "train loss:1.878816483128099\n",
      "=== epoch:67, train acc:0.6166666666666667, test acc:0.474 ===\n",
      "train loss:1.821700787325427\n",
      "train loss:1.8692206553303057\n",
      "train loss:1.8018492087461437\n",
      "=== epoch:68, train acc:0.6266666666666667, test acc:0.4753 ===\n",
      "train loss:1.8099422673718566\n",
      "train loss:1.8287160064209425\n",
      "train loss:1.8534570907437875\n",
      "=== epoch:69, train acc:0.6333333333333333, test acc:0.481 ===\n",
      "train loss:1.9452779008658876\n",
      "train loss:1.846584673256376\n",
      "train loss:1.7781442631176925\n",
      "=== epoch:70, train acc:0.6366666666666667, test acc:0.4911 ===\n",
      "train loss:1.749266573857681\n",
      "train loss:1.7657503888548953\n",
      "train loss:1.8162667885491928\n",
      "=== epoch:71, train acc:0.6466666666666666, test acc:0.4915 ===\n",
      "train loss:1.8771804540176507\n",
      "train loss:1.879317527286778\n",
      "train loss:1.8091056653249669\n",
      "=== epoch:72, train acc:0.6433333333333333, test acc:0.4964 ===\n",
      "train loss:1.8065523010572355\n",
      "train loss:1.7321570476260575\n",
      "train loss:1.7861011387392494\n",
      "=== epoch:73, train acc:0.6333333333333333, test acc:0.5047 ===\n",
      "train loss:1.7239072431208913\n",
      "train loss:1.7696910892494098\n",
      "train loss:1.7254181727443705\n",
      "=== epoch:74, train acc:0.6433333333333333, test acc:0.5038 ===\n",
      "train loss:1.6777487734147636\n",
      "train loss:1.7440732669206278\n",
      "train loss:1.753136771168056\n",
      "=== epoch:75, train acc:0.6533333333333333, test acc:0.5086 ===\n",
      "train loss:1.7251700424586658\n",
      "train loss:1.7155686987129248\n",
      "train loss:1.7498576292376302\n",
      "=== epoch:76, train acc:0.6566666666666666, test acc:0.5094 ===\n",
      "train loss:1.6150389995385888\n",
      "train loss:1.604624815695182\n",
      "train loss:1.7372650692117326\n",
      "=== epoch:77, train acc:0.6633333333333333, test acc:0.5102 ===\n",
      "train loss:1.7205789924865034\n",
      "train loss:1.6387920989117066\n",
      "train loss:1.6844148802388539\n",
      "=== epoch:78, train acc:0.6566666666666666, test acc:0.5067 ===\n",
      "train loss:1.7151046456104957\n",
      "train loss:1.5927156125740212\n",
      "train loss:1.6504976501065576\n",
      "=== epoch:79, train acc:0.66, test acc:0.5017 ===\n",
      "train loss:1.5541408636552114\n",
      "train loss:1.693129377879037\n",
      "train loss:1.6471899989793817\n",
      "=== epoch:80, train acc:0.66, test acc:0.5046 ===\n",
      "train loss:1.6782397422711173\n",
      "train loss:1.5655953744846618\n",
      "train loss:1.6412818251659607\n",
      "=== epoch:81, train acc:0.67, test acc:0.5144 ===\n",
      "train loss:1.583055976589565\n",
      "train loss:1.64280547010722\n",
      "train loss:1.582238768312077\n",
      "=== epoch:82, train acc:0.69, test acc:0.5221 ===\n",
      "train loss:1.6145857918527633\n",
      "train loss:1.4797368681641156\n",
      "train loss:1.598329006650105\n",
      "=== epoch:83, train acc:0.6966666666666667, test acc:0.5224 ===\n",
      "train loss:1.531476424333644\n",
      "train loss:1.5890966968520714\n",
      "train loss:1.5083759332574502\n",
      "=== epoch:84, train acc:0.7033333333333334, test acc:0.5188 ===\n",
      "train loss:1.646554812269387\n",
      "train loss:1.573197344738831\n",
      "train loss:1.4946957631631221\n",
      "=== epoch:85, train acc:0.7, test acc:0.5269 ===\n",
      "train loss:1.5390340248408036\n",
      "train loss:1.5975901788257278\n",
      "train loss:1.577103010274842\n",
      "=== epoch:86, train acc:0.6933333333333334, test acc:0.5342 ===\n",
      "train loss:1.5876816173756771\n",
      "train loss:1.4823276214057461\n",
      "train loss:1.5725928988773952\n",
      "=== epoch:87, train acc:0.6966666666666667, test acc:0.5433 ===\n",
      "train loss:1.583194925371978\n",
      "train loss:1.5453211314276305\n",
      "train loss:1.5407893214676998\n",
      "=== epoch:88, train acc:0.69, test acc:0.5462 ===\n",
      "train loss:1.5034946228593393\n",
      "train loss:1.4332620021675857\n",
      "train loss:1.4513092916073738\n",
      "=== epoch:89, train acc:0.68, test acc:0.544 ===\n",
      "train loss:1.5671411136595352\n",
      "train loss:1.4484973370031438\n",
      "train loss:1.531887920799126\n",
      "=== epoch:90, train acc:0.7033333333333334, test acc:0.5516 ===\n",
      "train loss:1.4598795104106914\n",
      "train loss:1.4698309582108129\n",
      "train loss:1.4752815401698731\n",
      "=== epoch:91, train acc:0.7066666666666667, test acc:0.5565 ===\n",
      "train loss:1.5236771420230675\n",
      "train loss:1.3904486915764378\n",
      "train loss:1.387796259429332\n",
      "=== epoch:92, train acc:0.71, test acc:0.5571 ===\n",
      "train loss:1.49151893352701\n",
      "train loss:1.48398431809724\n",
      "train loss:1.396134445444086\n",
      "=== epoch:93, train acc:0.7233333333333334, test acc:0.5674 ===\n",
      "train loss:1.4479403405714448\n",
      "train loss:1.421130218512689\n",
      "train loss:1.2977363382824163\n",
      "=== epoch:94, train acc:0.7166666666666667, test acc:0.5619 ===\n",
      "train loss:1.3541961071159379\n",
      "train loss:1.3084477059209112\n",
      "train loss:1.347213247832475\n",
      "=== epoch:95, train acc:0.7133333333333334, test acc:0.566 ===\n",
      "train loss:1.3757414574287565\n",
      "train loss:1.3353537141323886\n",
      "train loss:1.206984232514094\n",
      "=== epoch:96, train acc:0.73, test acc:0.5717 ===\n",
      "train loss:1.3934018217166064\n",
      "train loss:1.3141243791063182\n",
      "train loss:1.3269712153445077\n",
      "=== epoch:97, train acc:0.7333333333333333, test acc:0.5741 ===\n",
      "train loss:1.2986069588839633\n",
      "train loss:1.3039707982939424\n",
      "train loss:1.2857458088642089\n",
      "=== epoch:98, train acc:0.7366666666666667, test acc:0.5791 ===\n",
      "train loss:1.302838899210085\n",
      "train loss:1.2726860944843055\n",
      "train loss:1.2943622514922333\n",
      "=== epoch:99, train acc:0.74, test acc:0.5816 ===\n",
      "train loss:1.2538384468722823\n",
      "train loss:1.2884114175105827\n",
      "train loss:1.1058264463759209\n",
      "=== epoch:100, train acc:0.74, test acc:0.5808 ===\n",
      "train loss:1.3356074708317138\n",
      "train loss:1.319102653776705\n",
      "train loss:1.2123044335483557\n",
      "=== epoch:101, train acc:0.7466666666666667, test acc:0.5873 ===\n",
      "train loss:1.2101517596473959\n",
      "train loss:1.1305438836691706\n",
      "train loss:1.2671950535205283\n",
      "=== epoch:102, train acc:0.7533333333333333, test acc:0.5877 ===\n",
      "train loss:1.198045376898906\n",
      "train loss:1.1266987941813036\n",
      "train loss:1.1631002218963364\n",
      "=== epoch:103, train acc:0.7533333333333333, test acc:0.5977 ===\n",
      "train loss:1.2670228899092737\n",
      "train loss:1.1490931822932366\n",
      "train loss:1.1578501862072605\n",
      "=== epoch:104, train acc:0.76, test acc:0.6057 ===\n",
      "train loss:1.1396031173192664\n",
      "train loss:1.1901952140212353\n",
      "train loss:1.180812465046181\n",
      "=== epoch:105, train acc:0.7533333333333333, test acc:0.6073 ===\n",
      "train loss:1.038353966018775\n",
      "train loss:1.1298728882292783\n",
      "train loss:1.2219888326418122\n",
      "=== epoch:106, train acc:0.7533333333333333, test acc:0.6075 ===\n",
      "train loss:1.1552436330160267\n",
      "train loss:1.1991368834335177\n",
      "train loss:1.0667593034527518\n",
      "=== epoch:107, train acc:0.7666666666666667, test acc:0.6127 ===\n",
      "train loss:1.1886763077418436\n",
      "train loss:1.1872951322136354\n",
      "train loss:1.123853673580222\n",
      "=== epoch:108, train acc:0.7766666666666666, test acc:0.6172 ===\n",
      "train loss:0.9606848924379296\n",
      "train loss:1.1338854753675953\n",
      "train loss:1.0860508163563356\n",
      "=== epoch:109, train acc:0.7766666666666666, test acc:0.6171 ===\n",
      "train loss:0.9855647560741662\n",
      "train loss:1.0083980332611022\n",
      "train loss:1.1431922653109021\n",
      "=== epoch:110, train acc:0.7766666666666666, test acc:0.6161 ===\n",
      "train loss:1.1642094676987111\n",
      "train loss:1.0589737822781573\n",
      "train loss:0.9355160218592412\n",
      "=== epoch:111, train acc:0.7766666666666666, test acc:0.6184 ===\n",
      "train loss:0.9345701572644071\n",
      "train loss:1.029973054700705\n",
      "train loss:0.9501567832203287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.7766666666666666, test acc:0.6251 ===\n",
      "train loss:1.0890030922829073\n",
      "train loss:1.0035228391521682\n",
      "train loss:0.9110133998514475\n",
      "=== epoch:113, train acc:0.7833333333333333, test acc:0.6251 ===\n",
      "train loss:1.0500320898645155\n",
      "train loss:0.9052904373140686\n",
      "train loss:0.9918238062995709\n",
      "=== epoch:114, train acc:0.7866666666666666, test acc:0.6286 ===\n",
      "train loss:1.0058597085337555\n",
      "train loss:0.9719382211748507\n",
      "train loss:0.8695512975452963\n",
      "=== epoch:115, train acc:0.7933333333333333, test acc:0.6323 ===\n",
      "train loss:0.937137262392272\n",
      "train loss:0.962628665529667\n",
      "train loss:0.9420410394762888\n",
      "=== epoch:116, train acc:0.7933333333333333, test acc:0.6335 ===\n",
      "train loss:0.9139496799634164\n",
      "train loss:0.9841397565505181\n",
      "train loss:0.893294135343492\n",
      "=== epoch:117, train acc:0.8, test acc:0.6352 ===\n",
      "train loss:0.8902889073956183\n",
      "train loss:0.8583793642647641\n",
      "train loss:1.0255823885991873\n",
      "=== epoch:118, train acc:0.7966666666666666, test acc:0.6363 ===\n",
      "train loss:0.8937293306467883\n",
      "train loss:0.8589326076024073\n",
      "train loss:0.9251781822417804\n",
      "=== epoch:119, train acc:0.8, test acc:0.6403 ===\n",
      "train loss:0.9868012514650517\n",
      "train loss:0.9089774311551853\n",
      "train loss:0.8546447973401047\n",
      "=== epoch:120, train acc:0.8, test acc:0.6446 ===\n",
      "train loss:0.7677750521869819\n",
      "train loss:0.9516587560243375\n",
      "train loss:0.9515917566933865\n",
      "=== epoch:121, train acc:0.8, test acc:0.6438 ===\n",
      "train loss:0.8385862322444037\n",
      "train loss:0.8540074776984733\n",
      "train loss:0.8137802976853908\n",
      "=== epoch:122, train acc:0.8033333333333333, test acc:0.6436 ===\n",
      "train loss:0.664438476755531\n",
      "train loss:0.8406219383120593\n",
      "train loss:0.9233091317988462\n",
      "=== epoch:123, train acc:0.8133333333333334, test acc:0.6502 ===\n",
      "train loss:0.8653301131958413\n",
      "train loss:0.7398981747088684\n",
      "train loss:0.7939363333237166\n",
      "=== epoch:124, train acc:0.8133333333333334, test acc:0.6591 ===\n",
      "train loss:0.8329577236590306\n",
      "train loss:0.7460256538505083\n",
      "train loss:0.7750664346852829\n",
      "=== epoch:125, train acc:0.8233333333333334, test acc:0.6555 ===\n",
      "train loss:0.9066516390334103\n",
      "train loss:0.9103161976241505\n",
      "train loss:0.7242966934798888\n",
      "=== epoch:126, train acc:0.8166666666666667, test acc:0.6533 ===\n",
      "train loss:0.655183298740838\n",
      "train loss:0.8244573848848965\n",
      "train loss:0.7658673419762623\n",
      "=== epoch:127, train acc:0.8233333333333334, test acc:0.653 ===\n",
      "train loss:0.7253315240310415\n",
      "train loss:0.7643891388564934\n",
      "train loss:0.7186021468235502\n",
      "=== epoch:128, train acc:0.8233333333333334, test acc:0.651 ===\n",
      "train loss:0.9353225703701961\n",
      "train loss:0.7643161858716813\n",
      "train loss:0.7647478214836607\n",
      "=== epoch:129, train acc:0.8233333333333334, test acc:0.6534 ===\n",
      "train loss:0.8593101863481446\n",
      "train loss:0.8220071228285398\n",
      "train loss:0.7612616348479111\n",
      "=== epoch:130, train acc:0.8233333333333334, test acc:0.6528 ===\n",
      "train loss:0.6731284618116179\n",
      "train loss:0.75747621066513\n",
      "train loss:0.7865882748190804\n",
      "=== epoch:131, train acc:0.8333333333333334, test acc:0.6605 ===\n",
      "train loss:0.8804815892565513\n",
      "train loss:0.6582376854619554\n",
      "train loss:0.6146876938119017\n",
      "=== epoch:132, train acc:0.8266666666666667, test acc:0.6593 ===\n",
      "train loss:0.6428759458350143\n",
      "train loss:0.5389811181601682\n",
      "train loss:0.6710441111262675\n",
      "=== epoch:133, train acc:0.82, test acc:0.6616 ===\n",
      "train loss:0.7753589775185248\n",
      "train loss:0.5540974585985282\n",
      "train loss:0.6100905606442167\n",
      "=== epoch:134, train acc:0.83, test acc:0.6624 ===\n",
      "train loss:0.6568837215973028\n",
      "train loss:0.5749503545177888\n",
      "train loss:0.7711802999364592\n",
      "=== epoch:135, train acc:0.83, test acc:0.6627 ===\n",
      "train loss:0.631701328544024\n",
      "train loss:0.5920103660687862\n",
      "train loss:0.6210008446815514\n",
      "=== epoch:136, train acc:0.8366666666666667, test acc:0.6678 ===\n",
      "train loss:0.6096156673984635\n",
      "train loss:0.5553082004129845\n",
      "train loss:0.6954423651747015\n",
      "=== epoch:137, train acc:0.8333333333333334, test acc:0.6701 ===\n",
      "train loss:0.5866523937480126\n",
      "train loss:0.5545559129211205\n",
      "train loss:0.6213208065731999\n",
      "=== epoch:138, train acc:0.8366666666666667, test acc:0.6689 ===\n",
      "train loss:0.597370173637325\n",
      "train loss:0.6190745369584103\n",
      "train loss:0.6465800224720817\n",
      "=== epoch:139, train acc:0.8333333333333334, test acc:0.6683 ===\n",
      "train loss:0.7405140609090358\n",
      "train loss:0.7002878241326697\n",
      "train loss:0.6139495347868494\n",
      "=== epoch:140, train acc:0.8366666666666667, test acc:0.667 ===\n",
      "train loss:0.583687183445412\n",
      "train loss:0.696203098758028\n",
      "train loss:0.6533735658807979\n",
      "=== epoch:141, train acc:0.8433333333333334, test acc:0.6747 ===\n",
      "train loss:0.5318753995265396\n",
      "train loss:0.5952990020928511\n",
      "train loss:0.6003486752333967\n",
      "=== epoch:142, train acc:0.84, test acc:0.6685 ===\n",
      "train loss:0.7453471611696995\n",
      "train loss:0.5792504629688068\n",
      "train loss:0.4815265194754246\n",
      "=== epoch:143, train acc:0.8433333333333334, test acc:0.669 ===\n",
      "train loss:0.4960983991837597\n",
      "train loss:0.6450299008726295\n",
      "train loss:0.6120536830960532\n",
      "=== epoch:144, train acc:0.85, test acc:0.671 ===\n",
      "train loss:0.635511188230613\n",
      "train loss:0.612550868692479\n",
      "train loss:0.654827725577217\n",
      "=== epoch:145, train acc:0.86, test acc:0.6799 ===\n",
      "train loss:0.5734227550321803\n",
      "train loss:0.5566536525775291\n",
      "train loss:0.6195058031748345\n",
      "=== epoch:146, train acc:0.8533333333333334, test acc:0.6856 ===\n",
      "train loss:0.5795636417577549\n",
      "train loss:0.5491847474410405\n",
      "train loss:0.5006060272592049\n",
      "=== epoch:147, train acc:0.85, test acc:0.6822 ===\n",
      "train loss:0.4670273381804847\n",
      "train loss:0.6964649196917789\n",
      "train loss:0.49819429707268037\n",
      "=== epoch:148, train acc:0.8566666666666667, test acc:0.6857 ===\n",
      "train loss:0.5736392583160608\n",
      "train loss:0.5980144968702749\n",
      "train loss:0.5089115768588964\n",
      "=== epoch:149, train acc:0.86, test acc:0.687 ===\n",
      "train loss:0.5380381073006709\n",
      "train loss:0.5468153296164381\n",
      "train loss:0.5775739038443566\n",
      "=== epoch:150, train acc:0.86, test acc:0.6845 ===\n",
      "train loss:0.3820256749942546\n",
      "train loss:0.5488408458906374\n",
      "train loss:0.7430034375276868\n",
      "=== epoch:151, train acc:0.8633333333333333, test acc:0.6884 ===\n",
      "train loss:0.5253371841253066\n",
      "train loss:0.5025632579771115\n",
      "train loss:0.5921659592700902\n",
      "=== epoch:152, train acc:0.8666666666666667, test acc:0.6885 ===\n",
      "train loss:0.4448185315367615\n",
      "train loss:0.5383812319134497\n",
      "train loss:0.5271138227082361\n",
      "=== epoch:153, train acc:0.8666666666666667, test acc:0.6895 ===\n",
      "train loss:0.5412203450578992\n",
      "train loss:0.5043897266764403\n",
      "train loss:0.5591647827647142\n",
      "=== epoch:154, train acc:0.87, test acc:0.6919 ===\n",
      "train loss:0.5904391798997429\n",
      "train loss:0.482437019664887\n",
      "train loss:0.6700995120264486\n",
      "=== epoch:155, train acc:0.8666666666666667, test acc:0.6944 ===\n",
      "train loss:0.5622785055213826\n",
      "train loss:0.49969750614510017\n",
      "train loss:0.3518296434964632\n",
      "=== epoch:156, train acc:0.8733333333333333, test acc:0.6921 ===\n",
      "train loss:0.37189347743569895\n",
      "train loss:0.5281726486165771\n",
      "train loss:0.4096759554695582\n",
      "=== epoch:157, train acc:0.8766666666666667, test acc:0.6953 ===\n",
      "train loss:0.45112682209069194\n",
      "train loss:0.5003373416232721\n",
      "train loss:0.6269931735539052\n",
      "=== epoch:158, train acc:0.88, test acc:0.7018 ===\n",
      "train loss:0.5093421234381308\n",
      "train loss:0.548252797165669\n",
      "train loss:0.4341344538601853\n",
      "=== epoch:159, train acc:0.8766666666666667, test acc:0.6987 ===\n",
      "train loss:0.5454577473751214\n",
      "train loss:0.47379563701633737\n",
      "train loss:0.38165244526468206\n",
      "=== epoch:160, train acc:0.8766666666666667, test acc:0.6979 ===\n",
      "train loss:0.5106936284105201\n",
      "train loss:0.43318584995531567\n",
      "train loss:0.5089290547455244\n",
      "=== epoch:161, train acc:0.88, test acc:0.6965 ===\n",
      "train loss:0.5852294045164999\n",
      "train loss:0.4019460569380597\n",
      "train loss:0.38220339619535326\n",
      "=== epoch:162, train acc:0.8766666666666667, test acc:0.6969 ===\n",
      "train loss:0.5345931830975238\n",
      "train loss:0.5512533475452516\n",
      "train loss:0.6280666833984223\n",
      "=== epoch:163, train acc:0.8766666666666667, test acc:0.6975 ===\n",
      "train loss:0.6115921319402531\n",
      "train loss:0.5336315865818667\n",
      "train loss:0.41677699563521986\n",
      "=== epoch:164, train acc:0.8766666666666667, test acc:0.7003 ===\n",
      "train loss:0.4615033160516171\n",
      "train loss:0.3995064152900841\n",
      "train loss:0.477573832060774\n",
      "=== epoch:165, train acc:0.8866666666666667, test acc:0.7059 ===\n",
      "train loss:0.4747476521331556\n",
      "train loss:0.398344483766911\n",
      "train loss:0.44723107207283946\n",
      "=== epoch:166, train acc:0.8866666666666667, test acc:0.7075 ===\n",
      "train loss:0.600023633833381\n",
      "train loss:0.3878932894337983\n",
      "train loss:0.3782841041207945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.8766666666666667, test acc:0.7051 ===\n",
      "train loss:0.3562913190380809\n",
      "train loss:0.44151522590917547\n",
      "train loss:0.524248096218429\n",
      "=== epoch:168, train acc:0.8766666666666667, test acc:0.7032 ===\n",
      "train loss:0.4566745931973897\n",
      "train loss:0.5046605454591169\n",
      "train loss:0.327084520129558\n",
      "=== epoch:169, train acc:0.8833333333333333, test acc:0.7055 ===\n",
      "train loss:0.4514508852399375\n",
      "train loss:0.42001646791621206\n",
      "train loss:0.3251089408047702\n",
      "=== epoch:170, train acc:0.88, test acc:0.7028 ===\n",
      "train loss:0.4210808354792249\n",
      "train loss:0.30821991259147974\n",
      "train loss:0.37709539835732925\n",
      "=== epoch:171, train acc:0.8833333333333333, test acc:0.707 ===\n",
      "train loss:0.32386604128962626\n",
      "train loss:0.4318487394349312\n",
      "train loss:0.3518430207159878\n",
      "=== epoch:172, train acc:0.8933333333333333, test acc:0.707 ===\n",
      "train loss:0.448123243288776\n",
      "train loss:0.4003471315834391\n",
      "train loss:0.45217807954375333\n",
      "=== epoch:173, train acc:0.89, test acc:0.7118 ===\n",
      "train loss:0.43224531604201916\n",
      "train loss:0.4203703924971432\n",
      "train loss:0.3860015932392718\n",
      "=== epoch:174, train acc:0.8966666666666666, test acc:0.7153 ===\n",
      "train loss:0.5459943975873413\n",
      "train loss:0.3543154889251571\n",
      "train loss:0.3831171722121461\n",
      "=== epoch:175, train acc:0.89, test acc:0.7156 ===\n",
      "train loss:0.3126486192968705\n",
      "train loss:0.4603282534406752\n",
      "train loss:0.4064853629391348\n",
      "=== epoch:176, train acc:0.8933333333333333, test acc:0.7134 ===\n",
      "train loss:0.40383576518974423\n",
      "train loss:0.4380860464293766\n",
      "train loss:0.37008923644619707\n",
      "=== epoch:177, train acc:0.8966666666666666, test acc:0.7116 ===\n",
      "train loss:0.3878436467348211\n",
      "train loss:0.4664181026233777\n",
      "train loss:0.37726925245063214\n",
      "=== epoch:178, train acc:0.8966666666666666, test acc:0.7096 ===\n",
      "train loss:0.42968407112419854\n",
      "train loss:0.4576789197918136\n",
      "train loss:0.4227078404043052\n",
      "=== epoch:179, train acc:0.89, test acc:0.7109 ===\n",
      "train loss:0.45345822883900977\n",
      "train loss:0.32685542755570596\n",
      "train loss:0.3565319861742166\n",
      "=== epoch:180, train acc:0.8933333333333333, test acc:0.7116 ===\n",
      "train loss:0.3022540159932102\n",
      "train loss:0.41610581616194686\n",
      "train loss:0.40162879181312283\n",
      "=== epoch:181, train acc:0.8866666666666667, test acc:0.7105 ===\n",
      "train loss:0.39894187101326006\n",
      "train loss:0.33697901500890765\n",
      "train loss:0.4030901068489303\n",
      "=== epoch:182, train acc:0.8966666666666666, test acc:0.707 ===\n",
      "train loss:0.35564394814814887\n",
      "train loss:0.3778917596830248\n",
      "train loss:0.30273475090910507\n",
      "=== epoch:183, train acc:0.8933333333333333, test acc:0.7091 ===\n",
      "train loss:0.3548134478595079\n",
      "train loss:0.4113196886649989\n",
      "train loss:0.28332287525691413\n",
      "=== epoch:184, train acc:0.8966666666666666, test acc:0.7176 ===\n",
      "train loss:0.28450973847032957\n",
      "train loss:0.3724887118876268\n",
      "train loss:0.31955864149904006\n",
      "=== epoch:185, train acc:0.8966666666666666, test acc:0.7193 ===\n",
      "train loss:0.41313058614600157\n",
      "train loss:0.2716309309366766\n",
      "train loss:0.3040519905116495\n",
      "=== epoch:186, train acc:0.9066666666666666, test acc:0.7191 ===\n",
      "train loss:0.3121712405732873\n",
      "train loss:0.36398450042886277\n",
      "train loss:0.2888472534179611\n",
      "=== epoch:187, train acc:0.91, test acc:0.7217 ===\n",
      "train loss:0.33943488234734503\n",
      "train loss:0.3744811626080731\n",
      "train loss:0.3367118626215916\n",
      "=== epoch:188, train acc:0.9033333333333333, test acc:0.7172 ===\n",
      "train loss:0.4697912029471013\n",
      "train loss:0.3193703832913469\n",
      "train loss:0.27139530727604844\n",
      "=== epoch:189, train acc:0.9, test acc:0.7217 ===\n",
      "train loss:0.3366488714660835\n",
      "train loss:0.29327754812999607\n",
      "train loss:0.3595112172885447\n",
      "=== epoch:190, train acc:0.9033333333333333, test acc:0.7185 ===\n",
      "train loss:0.2670021693569596\n",
      "train loss:0.31170942372893684\n",
      "train loss:0.39045899921288557\n",
      "=== epoch:191, train acc:0.91, test acc:0.7127 ===\n",
      "train loss:0.2852228240771037\n",
      "train loss:0.35623830718200866\n",
      "train loss:0.36830007965936284\n",
      "=== epoch:192, train acc:0.9133333333333333, test acc:0.7187 ===\n",
      "train loss:0.2692127867165659\n",
      "train loss:0.3574315980656331\n",
      "train loss:0.35017905201351823\n",
      "=== epoch:193, train acc:0.92, test acc:0.7221 ===\n",
      "train loss:0.44907040146009486\n",
      "train loss:0.2961521847204532\n",
      "train loss:0.35125074319213373\n",
      "=== epoch:194, train acc:0.9166666666666666, test acc:0.721 ===\n",
      "train loss:0.26832654241651804\n",
      "train loss:0.3378583089535842\n",
      "train loss:0.30384089927529007\n",
      "=== epoch:195, train acc:0.9166666666666666, test acc:0.7266 ===\n",
      "train loss:0.4334022014571122\n",
      "train loss:0.24117012008338087\n",
      "train loss:0.2904049329039599\n",
      "=== epoch:196, train acc:0.9233333333333333, test acc:0.7321 ===\n",
      "train loss:0.3403738667982195\n",
      "train loss:0.3169310497740884\n",
      "train loss:0.28725919298168173\n",
      "=== epoch:197, train acc:0.9266666666666666, test acc:0.7287 ===\n",
      "train loss:0.26060709053186815\n",
      "train loss:0.22489165512223125\n",
      "train loss:0.21934678396559343\n",
      "=== epoch:198, train acc:0.9133333333333333, test acc:0.7289 ===\n",
      "train loss:0.321155716381516\n",
      "train loss:0.3090857202472624\n",
      "train loss:0.3743670958713718\n",
      "=== epoch:199, train acc:0.92, test acc:0.7293 ===\n",
      "train loss:0.26305019432654314\n",
      "train loss:0.28634888516836626\n",
      "train loss:0.21336471897740933\n",
      "=== epoch:200, train acc:0.92, test acc:0.7266 ===\n",
      "train loss:0.3414233600342316\n",
      "train loss:0.2684553279726481\n",
      "train loss:0.2335385329030648\n",
      "=== epoch:201, train acc:0.9166666666666666, test acc:0.7283 ===\n",
      "train loss:0.23732587505868175\n",
      "train loss:0.31384190946903756\n",
      "train loss:0.331068479748481\n",
      "=== epoch:202, train acc:0.9266666666666666, test acc:0.7281 ===\n",
      "train loss:0.30570878183014033\n",
      "train loss:0.2998062077032113\n",
      "train loss:0.2920046286928063\n",
      "=== epoch:203, train acc:0.9266666666666666, test acc:0.731 ===\n",
      "train loss:0.186265941968274\n",
      "train loss:0.38078322999520803\n",
      "train loss:0.30439914370602916\n",
      "=== epoch:204, train acc:0.9266666666666666, test acc:0.7295 ===\n",
      "train loss:0.26487269570264216\n",
      "train loss:0.21588075236037582\n",
      "train loss:0.3298007696503245\n",
      "=== epoch:205, train acc:0.93, test acc:0.7309 ===\n",
      "train loss:0.341585672000251\n",
      "train loss:0.3460265683327349\n",
      "train loss:0.3245520174746007\n",
      "=== epoch:206, train acc:0.9233333333333333, test acc:0.7291 ===\n",
      "train loss:0.2698935624328511\n",
      "train loss:0.18956865379966475\n",
      "train loss:0.2888209216143328\n",
      "=== epoch:207, train acc:0.9333333333333333, test acc:0.7324 ===\n",
      "train loss:0.2667065591479961\n",
      "train loss:0.32870745878462837\n",
      "train loss:0.28548986665206033\n",
      "=== epoch:208, train acc:0.93, test acc:0.7339 ===\n",
      "train loss:0.27115194681761184\n",
      "train loss:0.2809987303990017\n",
      "train loss:0.25055336639135645\n",
      "=== epoch:209, train acc:0.9333333333333333, test acc:0.7332 ===\n",
      "train loss:0.2934456991058031\n",
      "train loss:0.2521022984076552\n",
      "train loss:0.21353937691636266\n",
      "=== epoch:210, train acc:0.9366666666666666, test acc:0.7336 ===\n",
      "train loss:0.3094416072540507\n",
      "train loss:0.2849192862566676\n",
      "train loss:0.2837667380224702\n",
      "=== epoch:211, train acc:0.94, test acc:0.7347 ===\n",
      "train loss:0.215857022780644\n",
      "train loss:0.16717098096069902\n",
      "train loss:0.2122250497529906\n",
      "=== epoch:212, train acc:0.9366666666666666, test acc:0.7386 ===\n",
      "train loss:0.2119072459095473\n",
      "train loss:0.24393206350913196\n",
      "train loss:0.2649277242861019\n",
      "=== epoch:213, train acc:0.9466666666666667, test acc:0.7395 ===\n",
      "train loss:0.22720114247761602\n",
      "train loss:0.16999372421897926\n",
      "train loss:0.24670954576016935\n",
      "=== epoch:214, train acc:0.9433333333333334, test acc:0.7385 ===\n",
      "train loss:0.3270696266799664\n",
      "train loss:0.19782575450544435\n",
      "train loss:0.2828579825069614\n",
      "=== epoch:215, train acc:0.9433333333333334, test acc:0.7379 ===\n",
      "train loss:0.22077795145132856\n",
      "train loss:0.1630071123983612\n",
      "train loss:0.19849498053099934\n",
      "=== epoch:216, train acc:0.95, test acc:0.7411 ===\n",
      "train loss:0.2866566580119868\n",
      "train loss:0.17916903599450273\n",
      "train loss:0.2086510414270055\n",
      "=== epoch:217, train acc:0.9466666666666667, test acc:0.7391 ===\n",
      "train loss:0.18544509717659952\n",
      "train loss:0.2585688978919371\n",
      "train loss:0.23504890037760426\n",
      "=== epoch:218, train acc:0.9466666666666667, test acc:0.7412 ===\n",
      "train loss:0.24861829868286375\n",
      "train loss:0.29921749962679384\n",
      "train loss:0.19142413552905047\n",
      "=== epoch:219, train acc:0.9433333333333334, test acc:0.7425 ===\n",
      "train loss:0.21463677891002572\n",
      "train loss:0.24167063393019472\n",
      "train loss:0.22609892142928206\n",
      "=== epoch:220, train acc:0.9433333333333334, test acc:0.7433 ===\n",
      "train loss:0.207197912146232\n",
      "train loss:0.2605493077328009\n",
      "train loss:0.27990574356914766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:221, train acc:0.95, test acc:0.7436 ===\n",
      "train loss:0.2271276964277159\n",
      "train loss:0.18655909850646868\n",
      "train loss:0.23835464321562724\n",
      "=== epoch:222, train acc:0.9533333333333334, test acc:0.7459 ===\n",
      "train loss:0.17816765786780828\n",
      "train loss:0.1947044701511259\n",
      "train loss:0.19624099282561439\n",
      "=== epoch:223, train acc:0.96, test acc:0.7466 ===\n",
      "train loss:0.21858868642913012\n",
      "train loss:0.15983837995678646\n",
      "train loss:0.17408571338961681\n",
      "=== epoch:224, train acc:0.96, test acc:0.7467 ===\n",
      "train loss:0.13903908868145978\n",
      "train loss:0.24917512807371453\n",
      "train loss:0.14552743883821867\n",
      "=== epoch:225, train acc:0.96, test acc:0.7426 ===\n",
      "train loss:0.20348761741265448\n",
      "train loss:0.24980619701078915\n",
      "train loss:0.15982454720589184\n",
      "=== epoch:226, train acc:0.95, test acc:0.7421 ===\n",
      "train loss:0.2426850875297005\n",
      "train loss:0.20134773976706338\n",
      "train loss:0.21605980760554658\n",
      "=== epoch:227, train acc:0.95, test acc:0.7449 ===\n",
      "train loss:0.14406161120697403\n",
      "train loss:0.2260123021769342\n",
      "train loss:0.16040793887406374\n",
      "=== epoch:228, train acc:0.9566666666666667, test acc:0.7419 ===\n",
      "train loss:0.20404795969117945\n",
      "train loss:0.2211347277432554\n",
      "train loss:0.2283407767755891\n",
      "=== epoch:229, train acc:0.9533333333333334, test acc:0.7426 ===\n",
      "train loss:0.12842225592803824\n",
      "train loss:0.1384336740668977\n",
      "train loss:0.17997003901065742\n",
      "=== epoch:230, train acc:0.9533333333333334, test acc:0.7454 ===\n",
      "train loss:0.17280480554280683\n",
      "train loss:0.1797225850385656\n",
      "train loss:0.1881366694849984\n",
      "=== epoch:231, train acc:0.9566666666666667, test acc:0.7438 ===\n",
      "train loss:0.17596390026038866\n",
      "train loss:0.1572932564718338\n",
      "train loss:0.13129735060416478\n",
      "=== epoch:232, train acc:0.9566666666666667, test acc:0.7461 ===\n",
      "train loss:0.1649365353816248\n",
      "train loss:0.2519495536288616\n",
      "train loss:0.20561249379468244\n",
      "=== epoch:233, train acc:0.9566666666666667, test acc:0.7433 ===\n",
      "train loss:0.1348551660861858\n",
      "train loss:0.12078777109577464\n",
      "train loss:0.18740363444422797\n",
      "=== epoch:234, train acc:0.9566666666666667, test acc:0.7441 ===\n",
      "train loss:0.16245391011395685\n",
      "train loss:0.17337329368736001\n",
      "train loss:0.1492043987438627\n",
      "=== epoch:235, train acc:0.9566666666666667, test acc:0.7431 ===\n",
      "train loss:0.1598406594548851\n",
      "train loss:0.1949773084017518\n",
      "train loss:0.2074233029899124\n",
      "=== epoch:236, train acc:0.9566666666666667, test acc:0.7452 ===\n",
      "train loss:0.16478627852518385\n",
      "train loss:0.22270913432404138\n",
      "train loss:0.1326949823286314\n",
      "=== epoch:237, train acc:0.9566666666666667, test acc:0.7438 ===\n",
      "train loss:0.11879926820649736\n",
      "train loss:0.2266651256485227\n",
      "train loss:0.18478820218386902\n",
      "=== epoch:238, train acc:0.9666666666666667, test acc:0.7503 ===\n",
      "train loss:0.14092263572917454\n",
      "train loss:0.14055909865203467\n",
      "train loss:0.12868168345487357\n",
      "=== epoch:239, train acc:0.9633333333333334, test acc:0.7493 ===\n",
      "train loss:0.14543491662033345\n",
      "train loss:0.1245118252256099\n",
      "train loss:0.13099742504679512\n",
      "=== epoch:240, train acc:0.9666666666666667, test acc:0.7485 ===\n",
      "train loss:0.22507858713665765\n",
      "train loss:0.11262372498444345\n",
      "train loss:0.19686982393374589\n",
      "=== epoch:241, train acc:0.9666666666666667, test acc:0.7497 ===\n",
      "train loss:0.15912651614139717\n",
      "train loss:0.1931271551304609\n",
      "train loss:0.11830671150812928\n",
      "=== epoch:242, train acc:0.97, test acc:0.7502 ===\n",
      "train loss:0.12460813925039017\n",
      "train loss:0.14293650384844483\n",
      "train loss:0.20496144758756066\n",
      "=== epoch:243, train acc:0.97, test acc:0.7537 ===\n",
      "train loss:0.12480236643254877\n",
      "train loss:0.15920219496364077\n",
      "train loss:0.14402096743764092\n",
      "=== epoch:244, train acc:0.97, test acc:0.7543 ===\n",
      "train loss:0.12690602305992627\n",
      "train loss:0.11127734101412465\n",
      "train loss:0.19674215710914317\n",
      "=== epoch:245, train acc:0.97, test acc:0.7565 ===\n",
      "train loss:0.14945542872186926\n",
      "train loss:0.10953191191498649\n",
      "train loss:0.15705586222046386\n",
      "=== epoch:246, train acc:0.9733333333333334, test acc:0.7535 ===\n",
      "train loss:0.17291065525631816\n",
      "train loss:0.17242549726247142\n",
      "train loss:0.15083641047624063\n",
      "=== epoch:247, train acc:0.9733333333333334, test acc:0.755 ===\n",
      "train loss:0.13778516144522748\n",
      "train loss:0.17427405256833522\n",
      "train loss:0.12345732847027742\n",
      "=== epoch:248, train acc:0.9733333333333334, test acc:0.7552 ===\n",
      "train loss:0.13635269046185683\n",
      "train loss:0.11510984628203862\n",
      "train loss:0.11010371205449537\n",
      "=== epoch:249, train acc:0.97, test acc:0.7551 ===\n",
      "train loss:0.15791512006331263\n",
      "train loss:0.10292316909595246\n",
      "train loss:0.1700790560723622\n",
      "=== epoch:250, train acc:0.97, test acc:0.7532 ===\n",
      "train loss:0.1782573651135888\n",
      "train loss:0.12462168927270399\n",
      "train loss:0.06892099215425056\n",
      "=== epoch:251, train acc:0.97, test acc:0.7534 ===\n",
      "train loss:0.15432864128443544\n",
      "train loss:0.1346954267990259\n",
      "train loss:0.16210396305293778\n",
      "=== epoch:252, train acc:0.9766666666666667, test acc:0.7579 ===\n",
      "train loss:0.11862870585477925\n",
      "train loss:0.1468038854488039\n",
      "train loss:0.12380475263700479\n",
      "=== epoch:253, train acc:0.9766666666666667, test acc:0.7568 ===\n",
      "train loss:0.08002957021298356\n",
      "train loss:0.13825520811445183\n",
      "train loss:0.08213348294838044\n",
      "=== epoch:254, train acc:0.9766666666666667, test acc:0.7579 ===\n",
      "train loss:0.10793703147071698\n",
      "train loss:0.09988498730389353\n",
      "train loss:0.1645720634845032\n",
      "=== epoch:255, train acc:0.9766666666666667, test acc:0.7557 ===\n",
      "train loss:0.09206592108435047\n",
      "train loss:0.11640372286416766\n",
      "train loss:0.08656317687498101\n",
      "=== epoch:256, train acc:0.9733333333333334, test acc:0.7534 ===\n",
      "train loss:0.16658340548616213\n",
      "train loss:0.08684851521658697\n",
      "train loss:0.08923912404838248\n",
      "=== epoch:257, train acc:0.9733333333333334, test acc:0.7536 ===\n",
      "train loss:0.131273893564967\n",
      "train loss:0.12943599311273254\n",
      "train loss:0.08554953130817677\n",
      "=== epoch:258, train acc:0.9766666666666667, test acc:0.7567 ===\n",
      "train loss:0.10879640831780674\n",
      "train loss:0.17626735517307537\n",
      "train loss:0.09640760936562312\n",
      "=== epoch:259, train acc:0.9766666666666667, test acc:0.7576 ===\n",
      "train loss:0.10342089796707159\n",
      "train loss:0.15176265671102765\n",
      "train loss:0.06902548129614045\n",
      "=== epoch:260, train acc:0.9766666666666667, test acc:0.7595 ===\n",
      "train loss:0.11874499020033062\n",
      "train loss:0.1235664252403224\n",
      "train loss:0.08168558061636658\n",
      "=== epoch:261, train acc:0.9766666666666667, test acc:0.7603 ===\n",
      "train loss:0.09124745857012354\n",
      "train loss:0.09966552730040629\n",
      "train loss:0.13329013191781874\n",
      "=== epoch:262, train acc:0.9766666666666667, test acc:0.7585 ===\n",
      "train loss:0.1455517400446086\n",
      "train loss:0.1207856503805544\n",
      "train loss:0.15135454997446832\n",
      "=== epoch:263, train acc:0.9766666666666667, test acc:0.7598 ===\n",
      "train loss:0.1520612810677011\n",
      "train loss:0.10975815964429779\n",
      "train loss:0.1944851611590102\n",
      "=== epoch:264, train acc:0.9766666666666667, test acc:0.7644 ===\n",
      "train loss:0.10016488663849575\n",
      "train loss:0.07586616803597088\n",
      "train loss:0.12738932523647228\n",
      "=== epoch:265, train acc:0.9766666666666667, test acc:0.7649 ===\n",
      "train loss:0.10901311463810304\n",
      "train loss:0.15522507332994395\n",
      "train loss:0.11855235822156113\n",
      "=== epoch:266, train acc:0.9766666666666667, test acc:0.7627 ===\n",
      "train loss:0.13509554050636505\n",
      "train loss:0.07628274437364618\n",
      "train loss:0.12041324627927195\n",
      "=== epoch:267, train acc:0.9766666666666667, test acc:0.7617 ===\n",
      "train loss:0.0685259095360872\n",
      "train loss:0.11263301699009393\n",
      "train loss:0.10333573237607804\n",
      "=== epoch:268, train acc:0.9766666666666667, test acc:0.7609 ===\n",
      "train loss:0.13822977036676407\n",
      "train loss:0.07093680633171029\n",
      "train loss:0.1032184930180701\n",
      "=== epoch:269, train acc:0.9766666666666667, test acc:0.7637 ===\n",
      "train loss:0.08359807002462331\n",
      "train loss:0.07910561430955201\n",
      "train loss:0.12487605410987589\n",
      "=== epoch:270, train acc:0.9766666666666667, test acc:0.7613 ===\n",
      "train loss:0.057277458758252235\n",
      "train loss:0.09741414025397457\n",
      "train loss:0.10448245673023913\n",
      "=== epoch:271, train acc:0.98, test acc:0.7642 ===\n",
      "train loss:0.059630311042018776\n",
      "train loss:0.0980496264157331\n",
      "train loss:0.08981113257648021\n",
      "=== epoch:272, train acc:0.9766666666666667, test acc:0.762 ===\n",
      "train loss:0.09422291623157733\n",
      "train loss:0.10239041534737849\n",
      "train loss:0.09527250939271738\n",
      "=== epoch:273, train acc:0.98, test acc:0.7632 ===\n",
      "train loss:0.10263603634933129\n",
      "train loss:0.05622545055781213\n",
      "train loss:0.09169707841857759\n",
      "=== epoch:274, train acc:0.9766666666666667, test acc:0.7614 ===\n",
      "train loss:0.09144234831367486\n",
      "train loss:0.06575422366214177\n",
      "train loss:0.09836021828767731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:275, train acc:0.9766666666666667, test acc:0.7672 ===\n",
      "train loss:0.06369305613702442\n",
      "train loss:0.05001647875081544\n",
      "train loss:0.11458420227072402\n",
      "=== epoch:276, train acc:0.9766666666666667, test acc:0.7669 ===\n",
      "train loss:0.07236331187954331\n",
      "train loss:0.13737500289632398\n",
      "train loss:0.11724276418711643\n",
      "=== epoch:277, train acc:0.9833333333333333, test acc:0.769 ===\n",
      "train loss:0.09473087589056853\n",
      "train loss:0.09456787329578048\n",
      "train loss:0.09085356604693154\n",
      "=== epoch:278, train acc:0.98, test acc:0.7664 ===\n",
      "train loss:0.08779360451016645\n",
      "train loss:0.058643266613138255\n",
      "train loss:0.13507327073429543\n",
      "=== epoch:279, train acc:0.9833333333333333, test acc:0.7676 ===\n",
      "train loss:0.10608131389104056\n",
      "train loss:0.11740214211224907\n",
      "train loss:0.07551924787853667\n",
      "=== epoch:280, train acc:0.9833333333333333, test acc:0.7666 ===\n",
      "train loss:0.06539642731794604\n",
      "train loss:0.07028589425900271\n",
      "train loss:0.06739836360786086\n",
      "=== epoch:281, train acc:0.9833333333333333, test acc:0.7691 ===\n",
      "train loss:0.1479921432537477\n",
      "train loss:0.10183846721057735\n",
      "train loss:0.0689563993921679\n",
      "=== epoch:282, train acc:0.9833333333333333, test acc:0.7703 ===\n",
      "train loss:0.1139740116313339\n",
      "train loss:0.07102635274196317\n",
      "train loss:0.08850732253045085\n",
      "=== epoch:283, train acc:0.98, test acc:0.768 ===\n",
      "train loss:0.061526133593022996\n",
      "train loss:0.07619890549392742\n",
      "train loss:0.06539386649157233\n",
      "=== epoch:284, train acc:0.9766666666666667, test acc:0.7648 ===\n",
      "train loss:0.06851425170584965\n",
      "train loss:0.07248978989147514\n",
      "train loss:0.08597386254608345\n",
      "=== epoch:285, train acc:0.9833333333333333, test acc:0.7681 ===\n",
      "train loss:0.08164988726462384\n",
      "train loss:0.04439732628230582\n",
      "train loss:0.06687785926796262\n",
      "=== epoch:286, train acc:0.98, test acc:0.7656 ===\n",
      "train loss:0.08412839804033849\n",
      "train loss:0.06532930956179776\n",
      "train loss:0.048708241267070226\n",
      "=== epoch:287, train acc:0.9833333333333333, test acc:0.7664 ===\n",
      "train loss:0.07079366803990389\n",
      "train loss:0.07529241455438564\n",
      "train loss:0.09028574643672442\n",
      "=== epoch:288, train acc:0.9833333333333333, test acc:0.7674 ===\n",
      "train loss:0.07328159022682405\n",
      "train loss:0.06391708914408842\n",
      "train loss:0.10160064482236013\n",
      "=== epoch:289, train acc:0.9833333333333333, test acc:0.767 ===\n",
      "train loss:0.05035522526795684\n",
      "train loss:0.09210709036814192\n",
      "train loss:0.08434478121071884\n",
      "=== epoch:290, train acc:0.99, test acc:0.7676 ===\n",
      "train loss:0.06868255158283809\n",
      "train loss:0.0749864629778736\n",
      "train loss:0.08200683146058958\n",
      "=== epoch:291, train acc:0.99, test acc:0.769 ===\n",
      "train loss:0.05256483315431068\n",
      "train loss:0.09990863306550202\n",
      "train loss:0.08098705603316964\n",
      "=== epoch:292, train acc:0.99, test acc:0.7716 ===\n",
      "train loss:0.10136035205525191\n",
      "train loss:0.047712717802691554\n",
      "train loss:0.0765037857004578\n",
      "=== epoch:293, train acc:0.99, test acc:0.7717 ===\n",
      "train loss:0.10577902894146617\n",
      "train loss:0.06639087016348331\n",
      "train loss:0.1079091991529123\n",
      "=== epoch:294, train acc:0.9833333333333333, test acc:0.7682 ===\n",
      "train loss:0.07196889871490737\n",
      "train loss:0.06809919105575729\n",
      "train loss:0.08185918895577533\n",
      "=== epoch:295, train acc:0.9833333333333333, test acc:0.7699 ===\n",
      "train loss:0.08977636423334893\n",
      "train loss:0.0865834292523031\n",
      "train loss:0.09901197667913182\n",
      "=== epoch:296, train acc:0.99, test acc:0.7722 ===\n",
      "train loss:0.05866753093611084\n",
      "train loss:0.08191150583720364\n",
      "train loss:0.06340839358215056\n",
      "=== epoch:297, train acc:0.99, test acc:0.7709 ===\n",
      "train loss:0.05320435024241235\n",
      "train loss:0.05825204871594305\n",
      "train loss:0.05616506582436518\n",
      "=== epoch:298, train acc:0.99, test acc:0.7735 ===\n",
      "train loss:0.07145344548443719\n",
      "train loss:0.038649999427917754\n",
      "train loss:0.0666227300161927\n",
      "=== epoch:299, train acc:0.99, test acc:0.7751 ===\n",
      "train loss:0.05978505846713872\n",
      "train loss:0.06825186792925869\n",
      "train loss:0.045503955874571254\n",
      "=== epoch:300, train acc:0.99, test acc:0.7744 ===\n",
      "train loss:0.06072517265334643\n",
      "train loss:0.061660115182687186\n",
      "train loss:0.04851051362601647\n",
      "=== epoch:301, train acc:0.99, test acc:0.7726 ===\n",
      "train loss:0.06938849631705087\n",
      "train loss:0.08191049921151389\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7726\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.1\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f325f4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0810cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2959210647840487\n",
      "=== epoch:1, train acc:0.06, test acc:0.0634 ===\n",
      "train loss:2.310403273288428\n",
      "train loss:2.309717111159578\n",
      "train loss:2.301306025939381\n",
      "=== epoch:2, train acc:0.06333333333333334, test acc:0.0656 ===\n",
      "train loss:2.3171689045428097\n",
      "train loss:2.3135769199630234\n",
      "train loss:2.2903067130252794\n",
      "=== epoch:3, train acc:0.06333333333333334, test acc:0.0672 ===\n",
      "train loss:2.301512804903795\n",
      "train loss:2.2937438779694914\n",
      "train loss:2.301702891448014\n",
      "=== epoch:4, train acc:0.06333333333333334, test acc:0.0664 ===\n",
      "train loss:2.303317523288316\n",
      "train loss:2.3028910463682273\n",
      "train loss:2.303676819906938\n",
      "=== epoch:5, train acc:0.06333333333333334, test acc:0.0681 ===\n",
      "train loss:2.302550624868801\n",
      "train loss:2.295723532825746\n",
      "train loss:2.2982014673736275\n",
      "=== epoch:6, train acc:0.06666666666666667, test acc:0.0702 ===\n",
      "train loss:2.313798894624564\n",
      "train loss:2.294495453289609\n",
      "train loss:2.3117153786873157\n",
      "=== epoch:7, train acc:0.06666666666666667, test acc:0.0705 ===\n",
      "train loss:2.303076107896964\n",
      "train loss:2.302335652390609\n",
      "train loss:2.307486675497276\n",
      "=== epoch:8, train acc:0.07333333333333333, test acc:0.0728 ===\n",
      "train loss:2.30207814269406\n",
      "train loss:2.3064332658669406\n",
      "train loss:2.3101012085344426\n",
      "=== epoch:9, train acc:0.07, test acc:0.0718 ===\n",
      "train loss:2.30435211762618\n",
      "train loss:2.2990072976850344\n",
      "train loss:2.3052742729046694\n",
      "=== epoch:10, train acc:0.06333333333333334, test acc:0.0711 ===\n",
      "train loss:2.2964635537156983\n",
      "train loss:2.309586249461594\n",
      "train loss:2.29427422609366\n",
      "=== epoch:11, train acc:0.06333333333333334, test acc:0.07 ===\n",
      "train loss:2.3030987108661143\n",
      "train loss:2.3036821661675106\n",
      "train loss:2.3015939926851767\n",
      "=== epoch:12, train acc:0.06333333333333334, test acc:0.0701 ===\n",
      "train loss:2.3004389880362415\n",
      "train loss:2.3005788082790493\n",
      "train loss:2.2964507657961795\n",
      "=== epoch:13, train acc:0.07, test acc:0.0739 ===\n",
      "train loss:2.305490104836701\n",
      "train loss:2.292263905348486\n",
      "train loss:2.2894203875093293\n",
      "=== epoch:14, train acc:0.07333333333333333, test acc:0.0762 ===\n",
      "train loss:2.3036854860613123\n",
      "train loss:2.3003223066392073\n",
      "train loss:2.302878949206849\n",
      "=== epoch:15, train acc:0.08, test acc:0.0815 ===\n",
      "train loss:2.2952250945305783\n",
      "train loss:2.304812085738685\n",
      "train loss:2.2999992672842655\n",
      "=== epoch:16, train acc:0.07666666666666666, test acc:0.0821 ===\n",
      "train loss:2.296599754782113\n",
      "train loss:2.303393721090042\n",
      "train loss:2.288804220478623\n",
      "=== epoch:17, train acc:0.08666666666666667, test acc:0.0864 ===\n",
      "train loss:2.2943853906477383\n",
      "train loss:2.300221984296144\n",
      "train loss:2.2996628053657315\n",
      "=== epoch:18, train acc:0.08666666666666667, test acc:0.089 ===\n",
      "train loss:2.2967302043868667\n",
      "train loss:2.298903666724759\n",
      "train loss:2.28970980751237\n",
      "=== epoch:19, train acc:0.09, test acc:0.0906 ===\n",
      "train loss:2.30186974712694\n",
      "train loss:2.3012543499990286\n",
      "train loss:2.3047863188571927\n",
      "=== epoch:20, train acc:0.09, test acc:0.0933 ===\n",
      "train loss:2.289581295673129\n",
      "train loss:2.2950694968363075\n",
      "train loss:2.299175786135928\n",
      "=== epoch:21, train acc:0.09, test acc:0.0969 ===\n",
      "train loss:2.3003282821871833\n",
      "train loss:2.293662871844098\n",
      "train loss:2.295323470103624\n",
      "=== epoch:22, train acc:0.09, test acc:0.0977 ===\n",
      "train loss:2.289435466471637\n",
      "train loss:2.2902272178896848\n",
      "train loss:2.2930002767520024\n",
      "=== epoch:23, train acc:0.09, test acc:0.0983 ===\n",
      "train loss:2.294576653668409\n",
      "train loss:2.305874791788363\n",
      "train loss:2.2930476402563063\n",
      "=== epoch:24, train acc:0.09333333333333334, test acc:0.1039 ===\n",
      "train loss:2.2910514740393957\n",
      "train loss:2.2955426330122144\n",
      "train loss:2.300260905676572\n",
      "=== epoch:25, train acc:0.09, test acc:0.1072 ===\n",
      "train loss:2.2850883116624625\n",
      "train loss:2.2924451101494343\n",
      "train loss:2.3039651666017638\n",
      "=== epoch:26, train acc:0.09333333333333334, test acc:0.1075 ===\n",
      "train loss:2.2911860843703558\n",
      "train loss:2.2954109676641146\n",
      "train loss:2.2958004998944683\n",
      "=== epoch:27, train acc:0.09666666666666666, test acc:0.1143 ===\n",
      "train loss:2.291271531641643\n",
      "train loss:2.291802434656523\n",
      "train loss:2.289716194193251\n",
      "=== epoch:28, train acc:0.1, test acc:0.1177 ===\n",
      "train loss:2.3000653084113507\n",
      "train loss:2.290674214837148\n",
      "train loss:2.2975958919526924\n",
      "=== epoch:29, train acc:0.10333333333333333, test acc:0.1185 ===\n",
      "train loss:2.2953266412774678\n",
      "train loss:2.285095269550165\n",
      "train loss:2.3005682648884513\n",
      "=== epoch:30, train acc:0.10333333333333333, test acc:0.1222 ===\n",
      "train loss:2.293549378108912\n",
      "train loss:2.2859070962367887\n",
      "train loss:2.291977607652868\n",
      "=== epoch:31, train acc:0.11333333333333333, test acc:0.1268 ===\n",
      "train loss:2.2943865412156446\n",
      "train loss:2.292379960242773\n",
      "train loss:2.2875323981785325\n",
      "=== epoch:32, train acc:0.10666666666666667, test acc:0.1259 ===\n",
      "train loss:2.2910976758109682\n",
      "train loss:2.2913779789612194\n",
      "train loss:2.2935688722094003\n",
      "=== epoch:33, train acc:0.11666666666666667, test acc:0.1291 ===\n",
      "train loss:2.296315285032957\n",
      "train loss:2.2936384029252315\n",
      "train loss:2.2921460426250264\n",
      "=== epoch:34, train acc:0.13333333333333333, test acc:0.1338 ===\n",
      "train loss:2.284263849794433\n",
      "train loss:2.294642470886613\n",
      "train loss:2.287628961740775\n",
      "=== epoch:35, train acc:0.13333333333333333, test acc:0.1325 ===\n",
      "train loss:2.2936198595116215\n",
      "train loss:2.2897712423991727\n",
      "train loss:2.297125649486114\n",
      "=== epoch:36, train acc:0.14, test acc:0.133 ===\n",
      "train loss:2.293272936046459\n",
      "train loss:2.290121794809491\n",
      "train loss:2.291806638507535\n",
      "=== epoch:37, train acc:0.15333333333333332, test acc:0.1358 ===\n",
      "train loss:2.2916644155232366\n",
      "train loss:2.296997498108446\n",
      "train loss:2.294745314052499\n",
      "=== epoch:38, train acc:0.15666666666666668, test acc:0.1359 ===\n",
      "train loss:2.2900483282606645\n",
      "train loss:2.3013172366155517\n",
      "train loss:2.288466452173415\n",
      "=== epoch:39, train acc:0.15, test acc:0.1381 ===\n",
      "train loss:2.2907890789054868\n",
      "train loss:2.2863209004410523\n",
      "train loss:2.2869806179918664\n",
      "=== epoch:40, train acc:0.14666666666666667, test acc:0.1429 ===\n",
      "train loss:2.2885475653979315\n",
      "train loss:2.2788186724773274\n",
      "train loss:2.286369647788927\n",
      "=== epoch:41, train acc:0.14666666666666667, test acc:0.1464 ===\n",
      "train loss:2.2933854959743654\n",
      "train loss:2.294345612350153\n",
      "train loss:2.284172490038612\n",
      "=== epoch:42, train acc:0.13666666666666666, test acc:0.1443 ===\n",
      "train loss:2.289280287019315\n",
      "train loss:2.28333027964375\n",
      "train loss:2.2939267546056814\n",
      "=== epoch:43, train acc:0.14, test acc:0.1469 ===\n",
      "train loss:2.2937134465982116\n",
      "train loss:2.288760799866408\n",
      "train loss:2.295066850922924\n",
      "=== epoch:44, train acc:0.13666666666666666, test acc:0.147 ===\n",
      "train loss:2.282589897150939\n",
      "train loss:2.2841705024071057\n",
      "train loss:2.2839770478153882\n",
      "=== epoch:45, train acc:0.15, test acc:0.1491 ===\n",
      "train loss:2.2883878952824235\n",
      "train loss:2.286926274089181\n",
      "train loss:2.2863958868436973\n",
      "=== epoch:46, train acc:0.15, test acc:0.1493 ===\n",
      "train loss:2.2793517562067387\n",
      "train loss:2.280008799059533\n",
      "train loss:2.2867080220159695\n",
      "=== epoch:47, train acc:0.15333333333333332, test acc:0.1497 ===\n",
      "train loss:2.2820302573606286\n",
      "train loss:2.2909256911245155\n",
      "train loss:2.284955366260783\n",
      "=== epoch:48, train acc:0.16333333333333333, test acc:0.1504 ===\n",
      "train loss:2.281379042715747\n",
      "train loss:2.2825831507172936\n",
      "train loss:2.290050530499571\n",
      "=== epoch:49, train acc:0.17, test acc:0.1498 ===\n",
      "train loss:2.284204601972079\n",
      "train loss:2.291063715919493\n",
      "train loss:2.2859055695135613\n",
      "=== epoch:50, train acc:0.17, test acc:0.1503 ===\n",
      "train loss:2.2926761101026893\n",
      "train loss:2.2950507815715806\n",
      "train loss:2.2835029076577795\n",
      "=== epoch:51, train acc:0.17, test acc:0.1503 ===\n",
      "train loss:2.2876616951006152\n",
      "train loss:2.2878235275050502\n",
      "train loss:2.279039037359133\n",
      "=== epoch:52, train acc:0.17, test acc:0.1491 ===\n",
      "train loss:2.2940804914554285\n",
      "train loss:2.2828100277925314\n",
      "train loss:2.2887951265254913\n",
      "=== epoch:53, train acc:0.16666666666666666, test acc:0.1481 ===\n",
      "train loss:2.2883364589751904\n",
      "train loss:2.2884825601516634\n",
      "train loss:2.283688880590029\n",
      "=== epoch:54, train acc:0.16666666666666666, test acc:0.1484 ===\n",
      "train loss:2.2814552859954644\n",
      "train loss:2.285823056375803\n",
      "train loss:2.283466369717962\n",
      "=== epoch:55, train acc:0.16666666666666666, test acc:0.1491 ===\n",
      "train loss:2.282524908125053\n",
      "train loss:2.2842539794623398\n",
      "train loss:2.272885175433056\n",
      "=== epoch:56, train acc:0.17, test acc:0.1498 ===\n",
      "train loss:2.2859842567211666\n",
      "train loss:2.2872547932428726\n",
      "train loss:2.283275615392374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.16666666666666666, test acc:0.1487 ===\n",
      "train loss:2.275802165070892\n",
      "train loss:2.2908857977089156\n",
      "train loss:2.282374887941925\n",
      "=== epoch:58, train acc:0.16666666666666666, test acc:0.1504 ===\n",
      "train loss:2.28095827052158\n",
      "train loss:2.2838101412891327\n",
      "train loss:2.2872185115216284\n",
      "=== epoch:59, train acc:0.17, test acc:0.1509 ===\n",
      "train loss:2.2835653945951195\n",
      "train loss:2.2882847893987885\n",
      "train loss:2.2880837724992116\n",
      "=== epoch:60, train acc:0.17, test acc:0.153 ===\n",
      "train loss:2.290100123577364\n",
      "train loss:2.298242782627925\n",
      "train loss:2.282750116603335\n",
      "=== epoch:61, train acc:0.17666666666666667, test acc:0.154 ===\n",
      "train loss:2.273254988776885\n",
      "train loss:2.289447676031024\n",
      "train loss:2.272374007568524\n",
      "=== epoch:62, train acc:0.18333333333333332, test acc:0.155 ===\n",
      "train loss:2.2777473972999065\n",
      "train loss:2.2811134262066592\n",
      "train loss:2.2894144816969946\n",
      "=== epoch:63, train acc:0.18333333333333332, test acc:0.1569 ===\n",
      "train loss:2.27637160495262\n",
      "train loss:2.27866582567482\n",
      "train loss:2.27678398773252\n",
      "=== epoch:64, train acc:0.19666666666666666, test acc:0.1584 ===\n",
      "train loss:2.2774993994406456\n",
      "train loss:2.280471637705402\n",
      "train loss:2.283636597677321\n",
      "=== epoch:65, train acc:0.2, test acc:0.159 ===\n",
      "train loss:2.289207247307384\n",
      "train loss:2.285255839724723\n",
      "train loss:2.282236387623759\n",
      "=== epoch:66, train acc:0.19666666666666666, test acc:0.1617 ===\n",
      "train loss:2.2720872363358513\n",
      "train loss:2.2746569113736697\n",
      "train loss:2.281975228236105\n",
      "=== epoch:67, train acc:0.20333333333333334, test acc:0.1656 ===\n",
      "train loss:2.2717240689195295\n",
      "train loss:2.278765891595342\n",
      "train loss:2.2794019739711078\n",
      "=== epoch:68, train acc:0.20333333333333334, test acc:0.1633 ===\n",
      "train loss:2.293477553833294\n",
      "train loss:2.283057362747065\n",
      "train loss:2.2824473902645686\n",
      "=== epoch:69, train acc:0.21, test acc:0.1672 ===\n",
      "train loss:2.2837795638106018\n",
      "train loss:2.2748566855157613\n",
      "train loss:2.2795773935558303\n",
      "=== epoch:70, train acc:0.21333333333333335, test acc:0.1694 ===\n",
      "train loss:2.2812869492940155\n",
      "train loss:2.2811221239141095\n",
      "train loss:2.27874572763875\n",
      "=== epoch:71, train acc:0.21333333333333335, test acc:0.1715 ===\n",
      "train loss:2.278362954154591\n",
      "train loss:2.2830520008708315\n",
      "train loss:2.29233432011581\n",
      "=== epoch:72, train acc:0.21666666666666667, test acc:0.1728 ===\n",
      "train loss:2.2809680594263755\n",
      "train loss:2.2761231170515583\n",
      "train loss:2.275467196153504\n",
      "=== epoch:73, train acc:0.21, test acc:0.1712 ===\n",
      "train loss:2.2866224676754277\n",
      "train loss:2.2694602090681757\n",
      "train loss:2.2827283786058574\n",
      "=== epoch:74, train acc:0.21666666666666667, test acc:0.1729 ===\n",
      "train loss:2.27273906430341\n",
      "train loss:2.2887703888553674\n",
      "train loss:2.2705999020869316\n",
      "=== epoch:75, train acc:0.22, test acc:0.1736 ===\n",
      "train loss:2.2956883780345065\n",
      "train loss:2.2821136016928865\n",
      "train loss:2.2792109992079075\n",
      "=== epoch:76, train acc:0.21666666666666667, test acc:0.1738 ===\n",
      "train loss:2.2765876481290817\n",
      "train loss:2.2848867853094994\n",
      "train loss:2.2755930292518913\n",
      "=== epoch:77, train acc:0.21666666666666667, test acc:0.174 ===\n",
      "train loss:2.282528839293836\n",
      "train loss:2.27439226845504\n",
      "train loss:2.272922941542467\n",
      "=== epoch:78, train acc:0.22333333333333333, test acc:0.1756 ===\n",
      "train loss:2.2706210445227715\n",
      "train loss:2.2731699309224225\n",
      "train loss:2.274868874624125\n",
      "=== epoch:79, train acc:0.22666666666666666, test acc:0.179 ===\n",
      "train loss:2.280303992826934\n",
      "train loss:2.280280152818405\n",
      "train loss:2.270277813648095\n",
      "=== epoch:80, train acc:0.22333333333333333, test acc:0.1848 ===\n",
      "train loss:2.2749382861851544\n",
      "train loss:2.269614547116435\n",
      "train loss:2.285445537019714\n",
      "=== epoch:81, train acc:0.22333333333333333, test acc:0.1883 ===\n",
      "train loss:2.2891191467532668\n",
      "train loss:2.2786017037323574\n",
      "train loss:2.278592657454243\n",
      "=== epoch:82, train acc:0.23, test acc:0.1883 ===\n",
      "train loss:2.282958681150463\n",
      "train loss:2.2760535352147\n",
      "train loss:2.271023801923548\n",
      "=== epoch:83, train acc:0.22333333333333333, test acc:0.189 ===\n",
      "train loss:2.280495642604137\n",
      "train loss:2.2717797322151223\n",
      "train loss:2.2672037107738343\n",
      "=== epoch:84, train acc:0.22666666666666666, test acc:0.1881 ===\n",
      "train loss:2.2787263180458597\n",
      "train loss:2.274402511819966\n",
      "train loss:2.301009695427286\n",
      "=== epoch:85, train acc:0.23333333333333334, test acc:0.1872 ===\n",
      "train loss:2.277843201181242\n",
      "train loss:2.2823151424739816\n",
      "train loss:2.275751767074566\n",
      "=== epoch:86, train acc:0.23333333333333334, test acc:0.1905 ===\n",
      "train loss:2.2765182742183216\n",
      "train loss:2.287203672057506\n",
      "train loss:2.2771164447220142\n",
      "=== epoch:87, train acc:0.23333333333333334, test acc:0.1943 ===\n",
      "train loss:2.2752833644092005\n",
      "train loss:2.2830499557252955\n",
      "train loss:2.2781236175130495\n",
      "=== epoch:88, train acc:0.23666666666666666, test acc:0.1969 ===\n",
      "train loss:2.2797769062443565\n",
      "train loss:2.2730059673380696\n",
      "train loss:2.278258254249683\n",
      "=== epoch:89, train acc:0.23333333333333334, test acc:0.1929 ===\n",
      "train loss:2.2677608150430952\n",
      "train loss:2.278231121911288\n",
      "train loss:2.2757742047232186\n",
      "=== epoch:90, train acc:0.23333333333333334, test acc:0.1942 ===\n",
      "train loss:2.2570478948557064\n",
      "train loss:2.282109957130385\n",
      "train loss:2.2773050595349345\n",
      "=== epoch:91, train acc:0.22666666666666666, test acc:0.194 ===\n",
      "train loss:2.280825401989618\n",
      "train loss:2.2668567546812053\n",
      "train loss:2.277460740286303\n",
      "=== epoch:92, train acc:0.22666666666666666, test acc:0.193 ===\n",
      "train loss:2.268639544761697\n",
      "train loss:2.2661070873975357\n",
      "train loss:2.2765611373336845\n",
      "=== epoch:93, train acc:0.23, test acc:0.1939 ===\n",
      "train loss:2.2835165592374813\n",
      "train loss:2.2782484874918962\n",
      "train loss:2.2721778995107385\n",
      "=== epoch:94, train acc:0.24, test acc:0.1968 ===\n",
      "train loss:2.271645677638689\n",
      "train loss:2.2787835095533215\n",
      "train loss:2.268201821114344\n",
      "=== epoch:95, train acc:0.23666666666666666, test acc:0.1947 ===\n",
      "train loss:2.2691735161921973\n",
      "train loss:2.2733150699023796\n",
      "train loss:2.27967461169873\n",
      "=== epoch:96, train acc:0.23666666666666666, test acc:0.1956 ===\n",
      "train loss:2.2739492736053277\n",
      "train loss:2.2743981710825905\n",
      "train loss:2.268661166686573\n",
      "=== epoch:97, train acc:0.24, test acc:0.1935 ===\n",
      "train loss:2.267975753234117\n",
      "train loss:2.2752428932050264\n",
      "train loss:2.2737510380142023\n",
      "=== epoch:98, train acc:0.23, test acc:0.1932 ===\n",
      "train loss:2.273700404517817\n",
      "train loss:2.273303792705391\n",
      "train loss:2.2813019135564545\n",
      "=== epoch:99, train acc:0.24, test acc:0.1958 ===\n",
      "train loss:2.2811416328394456\n",
      "train loss:2.2830582618636357\n",
      "train loss:2.291063189508313\n",
      "=== epoch:100, train acc:0.24333333333333335, test acc:0.1966 ===\n",
      "train loss:2.2802147253680896\n",
      "train loss:2.270592239445738\n",
      "train loss:2.271417819080781\n",
      "=== epoch:101, train acc:0.24666666666666667, test acc:0.1976 ===\n",
      "train loss:2.2622757885879192\n",
      "train loss:2.273919256929534\n",
      "train loss:2.2704249501000877\n",
      "=== epoch:102, train acc:0.24333333333333335, test acc:0.1981 ===\n",
      "train loss:2.2687238255859836\n",
      "train loss:2.2645350333313106\n",
      "train loss:2.265607446767143\n",
      "=== epoch:103, train acc:0.2633333333333333, test acc:0.203 ===\n",
      "train loss:2.274986650607884\n",
      "train loss:2.2715873510633084\n",
      "train loss:2.2731533201747025\n",
      "=== epoch:104, train acc:0.2633333333333333, test acc:0.2075 ===\n",
      "train loss:2.2618107373812277\n",
      "train loss:2.2762420902398315\n",
      "train loss:2.270501818861592\n",
      "=== epoch:105, train acc:0.26666666666666666, test acc:0.2089 ===\n",
      "train loss:2.2720437714844595\n",
      "train loss:2.265275253995554\n",
      "train loss:2.2677412888913593\n",
      "=== epoch:106, train acc:0.26666666666666666, test acc:0.2085 ===\n",
      "train loss:2.2805249428106538\n",
      "train loss:2.274177409160133\n",
      "train loss:2.280405768893109\n",
      "=== epoch:107, train acc:0.2733333333333333, test acc:0.2096 ===\n",
      "train loss:2.267173940486025\n",
      "train loss:2.273461241219677\n",
      "train loss:2.2729218045130413\n",
      "=== epoch:108, train acc:0.2733333333333333, test acc:0.212 ===\n",
      "train loss:2.2670778845529247\n",
      "train loss:2.2687793873473527\n",
      "train loss:2.262420260090647\n",
      "=== epoch:109, train acc:0.2733333333333333, test acc:0.2089 ===\n",
      "train loss:2.2690375813904513\n",
      "train loss:2.274913205093956\n",
      "train loss:2.2731208452413743\n",
      "=== epoch:110, train acc:0.28, test acc:0.2096 ===\n",
      "train loss:2.270065589831041\n",
      "train loss:2.27127902728254\n",
      "train loss:2.2704821098928605\n",
      "=== epoch:111, train acc:0.26666666666666666, test acc:0.2084 ===\n",
      "train loss:2.2784127904803357\n",
      "train loss:2.270652157970489\n",
      "train loss:2.2706498930075645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.26, test acc:0.2085 ===\n",
      "train loss:2.2664576342005547\n",
      "train loss:2.2732535096466795\n",
      "train loss:2.2805065228008146\n",
      "=== epoch:113, train acc:0.26, test acc:0.2072 ===\n",
      "train loss:2.270181451612015\n",
      "train loss:2.2797767795929746\n",
      "train loss:2.283542753412449\n",
      "=== epoch:114, train acc:0.26, test acc:0.2042 ===\n",
      "train loss:2.2579051333159064\n",
      "train loss:2.269565854533451\n",
      "train loss:2.2720326974688665\n",
      "=== epoch:115, train acc:0.27, test acc:0.2085 ===\n",
      "train loss:2.2741719094540156\n",
      "train loss:2.260937989114373\n",
      "train loss:2.2611318285312483\n",
      "=== epoch:116, train acc:0.27, test acc:0.2065 ===\n",
      "train loss:2.2634293011664868\n",
      "train loss:2.2683154190393706\n",
      "train loss:2.2811963287383064\n",
      "=== epoch:117, train acc:0.27, test acc:0.2079 ===\n",
      "train loss:2.2694424909744826\n",
      "train loss:2.270139572556597\n",
      "train loss:2.2733723956824\n",
      "=== epoch:118, train acc:0.27666666666666667, test acc:0.2103 ===\n",
      "train loss:2.2538283318262495\n",
      "train loss:2.282233938934948\n",
      "train loss:2.2674222565447724\n",
      "=== epoch:119, train acc:0.28, test acc:0.2109 ===\n",
      "train loss:2.2596537799096397\n",
      "train loss:2.268947045446694\n",
      "train loss:2.258383025619385\n",
      "=== epoch:120, train acc:0.25333333333333335, test acc:0.2064 ===\n",
      "train loss:2.2583943016166894\n",
      "train loss:2.2781968902128282\n",
      "train loss:2.2554478307263266\n",
      "=== epoch:121, train acc:0.25666666666666665, test acc:0.2058 ===\n",
      "train loss:2.2800179332010213\n",
      "train loss:2.2674757549627143\n",
      "train loss:2.285723200703885\n",
      "=== epoch:122, train acc:0.25333333333333335, test acc:0.2044 ===\n",
      "train loss:2.2634988628446377\n",
      "train loss:2.2642972930777288\n",
      "train loss:2.257191113669223\n",
      "=== epoch:123, train acc:0.25666666666666665, test acc:0.2086 ===\n",
      "train loss:2.264086074902193\n",
      "train loss:2.248512644471065\n",
      "train loss:2.278350156190095\n",
      "=== epoch:124, train acc:0.26, test acc:0.2104 ===\n",
      "train loss:2.251930433857599\n",
      "train loss:2.286114268059357\n",
      "train loss:2.2692728836154807\n",
      "=== epoch:125, train acc:0.27666666666666667, test acc:0.214 ===\n",
      "train loss:2.2651473165660563\n",
      "train loss:2.2722415938397087\n",
      "train loss:2.266932559354915\n",
      "=== epoch:126, train acc:0.27666666666666667, test acc:0.2146 ===\n",
      "train loss:2.270538322439569\n",
      "train loss:2.2773112742327926\n",
      "train loss:2.242008621064622\n",
      "=== epoch:127, train acc:0.27, test acc:0.2143 ===\n",
      "train loss:2.2571961847232176\n",
      "train loss:2.2659078373086867\n",
      "train loss:2.269902648446906\n",
      "=== epoch:128, train acc:0.2633333333333333, test acc:0.2126 ===\n",
      "train loss:2.279826498431785\n",
      "train loss:2.258667275511309\n",
      "train loss:2.2525175303523124\n",
      "=== epoch:129, train acc:0.26666666666666666, test acc:0.2111 ===\n",
      "train loss:2.2557546859280633\n",
      "train loss:2.2553027276176034\n",
      "train loss:2.2690531831766565\n",
      "=== epoch:130, train acc:0.26666666666666666, test acc:0.2127 ===\n",
      "train loss:2.2628488216346807\n",
      "train loss:2.2650076053654855\n",
      "train loss:2.2718376309561332\n",
      "=== epoch:131, train acc:0.26666666666666666, test acc:0.2128 ===\n",
      "train loss:2.262009795522964\n",
      "train loss:2.2438073707074326\n",
      "train loss:2.2531819008078413\n",
      "=== epoch:132, train acc:0.2633333333333333, test acc:0.2101 ===\n",
      "train loss:2.2555015301866788\n",
      "train loss:2.2728065568725233\n",
      "train loss:2.2857852256212023\n",
      "=== epoch:133, train acc:0.2633333333333333, test acc:0.2137 ===\n",
      "train loss:2.2564048557841763\n",
      "train loss:2.255930554597261\n",
      "train loss:2.25728582689226\n",
      "=== epoch:134, train acc:0.26666666666666666, test acc:0.2133 ===\n",
      "train loss:2.253214291264564\n",
      "train loss:2.2648101696895666\n",
      "train loss:2.2542624155145496\n",
      "=== epoch:135, train acc:0.27, test acc:0.2159 ===\n",
      "train loss:2.271485742090962\n",
      "train loss:2.253534111667182\n",
      "train loss:2.268619238707014\n",
      "=== epoch:136, train acc:0.2733333333333333, test acc:0.2185 ===\n",
      "train loss:2.2411471584676868\n",
      "train loss:2.2930370496159838\n",
      "train loss:2.256859920526131\n",
      "=== epoch:137, train acc:0.29, test acc:0.2234 ===\n",
      "train loss:2.2652362374766413\n",
      "train loss:2.265309429359937\n",
      "train loss:2.2641111754321\n",
      "=== epoch:138, train acc:0.29, test acc:0.2243 ===\n",
      "train loss:2.270716590242584\n",
      "train loss:2.270764202330193\n",
      "train loss:2.2779701248644524\n",
      "=== epoch:139, train acc:0.29, test acc:0.2259 ===\n",
      "train loss:2.2591378010803074\n",
      "train loss:2.2622446534371883\n",
      "train loss:2.2625911302712254\n",
      "=== epoch:140, train acc:0.2866666666666667, test acc:0.2272 ===\n",
      "train loss:2.2634712152930376\n",
      "train loss:2.2660757283714026\n",
      "train loss:2.2733654541433324\n",
      "=== epoch:141, train acc:0.29, test acc:0.229 ===\n",
      "train loss:2.2575974350942003\n",
      "train loss:2.263371951733467\n",
      "train loss:2.2598005096850184\n",
      "=== epoch:142, train acc:0.29333333333333333, test acc:0.2318 ===\n",
      "train loss:2.262113303454163\n",
      "train loss:2.26282235004899\n",
      "train loss:2.2527712004519787\n",
      "=== epoch:143, train acc:0.29333333333333333, test acc:0.2286 ===\n",
      "train loss:2.2587711298674016\n",
      "train loss:2.2563187264741176\n",
      "train loss:2.259212723945348\n",
      "=== epoch:144, train acc:0.29333333333333333, test acc:0.2279 ===\n",
      "train loss:2.2599392390222635\n",
      "train loss:2.2578913850003555\n",
      "train loss:2.271970943017363\n",
      "=== epoch:145, train acc:0.2833333333333333, test acc:0.2265 ===\n",
      "train loss:2.2569621483717164\n",
      "train loss:2.2661784942300547\n",
      "train loss:2.2579021959039647\n",
      "=== epoch:146, train acc:0.2866666666666667, test acc:0.2264 ===\n",
      "train loss:2.255421680352909\n",
      "train loss:2.2505091673432314\n",
      "train loss:2.2550415687469356\n",
      "=== epoch:147, train acc:0.2866666666666667, test acc:0.2274 ===\n",
      "train loss:2.256934177228486\n",
      "train loss:2.245474604823165\n",
      "train loss:2.277456349829207\n",
      "=== epoch:148, train acc:0.29, test acc:0.228 ===\n",
      "train loss:2.2461258838881295\n",
      "train loss:2.2667907945526657\n",
      "train loss:2.2511374107523507\n",
      "=== epoch:149, train acc:0.2866666666666667, test acc:0.2297 ===\n",
      "train loss:2.267462766473773\n",
      "train loss:2.252086029232406\n",
      "train loss:2.248895112894503\n",
      "=== epoch:150, train acc:0.2866666666666667, test acc:0.2272 ===\n",
      "train loss:2.260446651585122\n",
      "train loss:2.2474389629926077\n",
      "train loss:2.271720382354282\n",
      "=== epoch:151, train acc:0.2866666666666667, test acc:0.2261 ===\n",
      "train loss:2.2589203674461134\n",
      "train loss:2.263830301442636\n",
      "train loss:2.2545887207458306\n",
      "=== epoch:152, train acc:0.29, test acc:0.2288 ===\n",
      "train loss:2.2611155179930016\n",
      "train loss:2.2598327142582795\n",
      "train loss:2.250061965925254\n",
      "=== epoch:153, train acc:0.2866666666666667, test acc:0.2281 ===\n",
      "train loss:2.2488500851727573\n",
      "train loss:2.2620632486242416\n",
      "train loss:2.260665384472267\n",
      "=== epoch:154, train acc:0.29, test acc:0.229 ===\n",
      "train loss:2.244135068686872\n",
      "train loss:2.2530366228937897\n",
      "train loss:2.261291304076991\n",
      "=== epoch:155, train acc:0.29333333333333333, test acc:0.2302 ===\n",
      "train loss:2.257888654838194\n",
      "train loss:2.2668163367489327\n",
      "train loss:2.226710776054668\n",
      "=== epoch:156, train acc:0.29333333333333333, test acc:0.2309 ===\n",
      "train loss:2.2618646461787733\n",
      "train loss:2.255733742654003\n",
      "train loss:2.2401455376143393\n",
      "=== epoch:157, train acc:0.29333333333333333, test acc:0.2303 ===\n",
      "train loss:2.259160543190861\n",
      "train loss:2.24418881458307\n",
      "train loss:2.2584488146694137\n",
      "=== epoch:158, train acc:0.29333333333333333, test acc:0.2309 ===\n",
      "train loss:2.2364094671197163\n",
      "train loss:2.252470376868821\n",
      "train loss:2.2494609498327414\n",
      "=== epoch:159, train acc:0.2833333333333333, test acc:0.2307 ===\n",
      "train loss:2.263776319934301\n",
      "train loss:2.2701410880904738\n",
      "train loss:2.2619686474934833\n",
      "=== epoch:160, train acc:0.2833333333333333, test acc:0.2313 ===\n",
      "train loss:2.2646767908324335\n",
      "train loss:2.2656185321936335\n",
      "train loss:2.261621737488959\n",
      "=== epoch:161, train acc:0.29333333333333333, test acc:0.2342 ===\n",
      "train loss:2.2673119721615405\n",
      "train loss:2.2351993602551348\n",
      "train loss:2.250214910596556\n",
      "=== epoch:162, train acc:0.2866666666666667, test acc:0.2318 ===\n",
      "train loss:2.279844711111404\n",
      "train loss:2.232315875151133\n",
      "train loss:2.2568559769985224\n",
      "=== epoch:163, train acc:0.29333333333333333, test acc:0.2335 ===\n",
      "train loss:2.265833195787756\n",
      "train loss:2.2561799287510578\n",
      "train loss:2.2512517223135826\n",
      "=== epoch:164, train acc:0.29333333333333333, test acc:0.2356 ===\n",
      "train loss:2.264905053204574\n",
      "train loss:2.2497668664541766\n",
      "train loss:2.271182384082854\n",
      "=== epoch:165, train acc:0.29333333333333333, test acc:0.2365 ===\n",
      "train loss:2.269640666399918\n",
      "train loss:2.255748654222011\n",
      "train loss:2.2687977457696684\n",
      "=== epoch:166, train acc:0.29333333333333333, test acc:0.2366 ===\n",
      "train loss:2.2673263144098055\n",
      "train loss:2.2427911596013668\n",
      "train loss:2.257845113654505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.2966666666666667, test acc:0.2365 ===\n",
      "train loss:2.247177852916477\n",
      "train loss:2.250514509452909\n",
      "train loss:2.2662390343007046\n",
      "=== epoch:168, train acc:0.3, test acc:0.2375 ===\n",
      "train loss:2.2408003838936725\n",
      "train loss:2.2679051884480512\n",
      "train loss:2.25304461304386\n",
      "=== epoch:169, train acc:0.3, test acc:0.2381 ===\n",
      "train loss:2.250572605879545\n",
      "train loss:2.252017296224897\n",
      "train loss:2.2675784711634517\n",
      "=== epoch:170, train acc:0.3, test acc:0.2368 ===\n",
      "train loss:2.2517256296095343\n",
      "train loss:2.2583545552683124\n",
      "train loss:2.258025790661401\n",
      "=== epoch:171, train acc:0.3, test acc:0.2375 ===\n",
      "train loss:2.265808194790244\n",
      "train loss:2.235275606123382\n",
      "train loss:2.2436325115730553\n",
      "=== epoch:172, train acc:0.30333333333333334, test acc:0.2377 ===\n",
      "train loss:2.255630509275437\n",
      "train loss:2.245394822558703\n",
      "train loss:2.2393267758743467\n",
      "=== epoch:173, train acc:0.30333333333333334, test acc:0.2382 ===\n",
      "train loss:2.2454180182193286\n",
      "train loss:2.2683261322933923\n",
      "train loss:2.2507467595795574\n",
      "=== epoch:174, train acc:0.30333333333333334, test acc:0.2385 ===\n",
      "train loss:2.265464134710893\n",
      "train loss:2.2572954321833865\n",
      "train loss:2.2528661736955846\n",
      "=== epoch:175, train acc:0.30333333333333334, test acc:0.2403 ===\n",
      "train loss:2.2148411746648358\n",
      "train loss:2.249398128773251\n",
      "train loss:2.24108543729703\n",
      "=== epoch:176, train acc:0.30333333333333334, test acc:0.2441 ===\n",
      "train loss:2.243980404650342\n",
      "train loss:2.239319627025892\n",
      "train loss:2.24328734490516\n",
      "=== epoch:177, train acc:0.30666666666666664, test acc:0.2455 ===\n",
      "train loss:2.229772356927905\n",
      "train loss:2.230317750078816\n",
      "train loss:2.2481552010741206\n",
      "=== epoch:178, train acc:0.31, test acc:0.2465 ===\n",
      "train loss:2.250496343339123\n",
      "train loss:2.2436481844877694\n",
      "train loss:2.2350034814139734\n",
      "=== epoch:179, train acc:0.30333333333333334, test acc:0.2454 ===\n",
      "train loss:2.22719406795213\n",
      "train loss:2.2505916206586267\n",
      "train loss:2.2533908585564486\n",
      "=== epoch:180, train acc:0.30666666666666664, test acc:0.2467 ===\n",
      "train loss:2.243189833054527\n",
      "train loss:2.2447453261668335\n",
      "train loss:2.2360566386536087\n",
      "=== epoch:181, train acc:0.30666666666666664, test acc:0.2482 ===\n",
      "train loss:2.2372101368316324\n",
      "train loss:2.2326635697954145\n",
      "train loss:2.2474072390351623\n",
      "=== epoch:182, train acc:0.31, test acc:0.2506 ===\n",
      "train loss:2.237334309773278\n",
      "train loss:2.220251111019555\n",
      "train loss:2.2461406748250043\n",
      "=== epoch:183, train acc:0.31, test acc:0.2526 ===\n",
      "train loss:2.2504687213435504\n",
      "train loss:2.244576318454843\n",
      "train loss:2.2577908211027005\n",
      "=== epoch:184, train acc:0.32, test acc:0.2546 ===\n",
      "train loss:2.2598572377432773\n",
      "train loss:2.2285302417424133\n",
      "train loss:2.255115525769057\n",
      "=== epoch:185, train acc:0.31666666666666665, test acc:0.2548 ===\n",
      "train loss:2.2249315617426033\n",
      "train loss:2.240824156011098\n",
      "train loss:2.24570516920728\n",
      "=== epoch:186, train acc:0.31666666666666665, test acc:0.2553 ===\n",
      "train loss:2.256405713770158\n",
      "train loss:2.2439375449431553\n",
      "train loss:2.2464994790833486\n",
      "=== epoch:187, train acc:0.31666666666666665, test acc:0.2559 ===\n",
      "train loss:2.2133766880387977\n",
      "train loss:2.2329607962749263\n",
      "train loss:2.220264728613533\n",
      "=== epoch:188, train acc:0.31666666666666665, test acc:0.256 ===\n",
      "train loss:2.2424418133996857\n",
      "train loss:2.21380250375286\n",
      "train loss:2.2233513115007932\n",
      "=== epoch:189, train acc:0.31666666666666665, test acc:0.2556 ===\n",
      "train loss:2.2303228477343873\n",
      "train loss:2.2448285740067138\n",
      "train loss:2.25367540198069\n",
      "=== epoch:190, train acc:0.31333333333333335, test acc:0.2575 ===\n",
      "train loss:2.243621500903525\n",
      "train loss:2.229628414730325\n",
      "train loss:2.238243909590681\n",
      "=== epoch:191, train acc:0.31333333333333335, test acc:0.2567 ===\n",
      "train loss:2.223365228109324\n",
      "train loss:2.223077230854593\n",
      "train loss:2.2136059434750917\n",
      "=== epoch:192, train acc:0.30666666666666664, test acc:0.2549 ===\n",
      "train loss:2.2489382509045845\n",
      "train loss:2.233692187224838\n",
      "train loss:2.2320734860158855\n",
      "=== epoch:193, train acc:0.30666666666666664, test acc:0.2556 ===\n",
      "train loss:2.2497501367333625\n",
      "train loss:2.2501838922844404\n",
      "train loss:2.2154948063672206\n",
      "=== epoch:194, train acc:0.31333333333333335, test acc:0.2556 ===\n",
      "train loss:2.2279375436673203\n",
      "train loss:2.233505690333691\n",
      "train loss:2.2262047025776757\n",
      "=== epoch:195, train acc:0.31666666666666665, test acc:0.257 ===\n",
      "train loss:2.2235522735698137\n",
      "train loss:2.2355186039799686\n",
      "train loss:2.23595847954328\n",
      "=== epoch:196, train acc:0.31333333333333335, test acc:0.2554 ===\n",
      "train loss:2.2357722342429853\n",
      "train loss:2.2498172995508474\n",
      "train loss:2.2363667095810915\n",
      "=== epoch:197, train acc:0.31666666666666665, test acc:0.255 ===\n",
      "train loss:2.2323111449603705\n",
      "train loss:2.2423292878435315\n",
      "train loss:2.2408085480449254\n",
      "=== epoch:198, train acc:0.31333333333333335, test acc:0.2564 ===\n",
      "train loss:2.235281147874651\n",
      "train loss:2.2211292502016438\n",
      "train loss:2.219711210687753\n",
      "=== epoch:199, train acc:0.31333333333333335, test acc:0.2588 ===\n",
      "train loss:2.2486576115185226\n",
      "train loss:2.2216972984779346\n",
      "train loss:2.2267456237224827\n",
      "=== epoch:200, train acc:0.31, test acc:0.2566 ===\n",
      "train loss:2.2236355272993586\n",
      "train loss:2.21821411975366\n",
      "train loss:2.25684945711481\n",
      "=== epoch:201, train acc:0.31, test acc:0.257 ===\n",
      "train loss:2.2452991666318804\n",
      "train loss:2.243196542062628\n",
      "train loss:2.24451045067028\n",
      "=== epoch:202, train acc:0.31, test acc:0.2582 ===\n",
      "train loss:2.2514707013686244\n",
      "train loss:2.2472875783100297\n",
      "train loss:2.2115463248403318\n",
      "=== epoch:203, train acc:0.30666666666666664, test acc:0.2583 ===\n",
      "train loss:2.233436057200233\n",
      "train loss:2.222293891429102\n",
      "train loss:2.23301349223773\n",
      "=== epoch:204, train acc:0.31, test acc:0.2587 ===\n",
      "train loss:2.2261176160119014\n",
      "train loss:2.2334186341737565\n",
      "train loss:2.2276729229834173\n",
      "=== epoch:205, train acc:0.31, test acc:0.2607 ===\n",
      "train loss:2.2392432370480435\n",
      "train loss:2.2449000812304023\n",
      "train loss:2.2511917764568183\n",
      "=== epoch:206, train acc:0.31333333333333335, test acc:0.2632 ===\n",
      "train loss:2.2615810526009703\n",
      "train loss:2.2198853070826825\n",
      "train loss:2.240442943625245\n",
      "=== epoch:207, train acc:0.31666666666666665, test acc:0.2639 ===\n",
      "train loss:2.216961801405724\n",
      "train loss:2.252813058522632\n",
      "train loss:2.213459288261751\n",
      "=== epoch:208, train acc:0.32, test acc:0.2649 ===\n",
      "train loss:2.209426079261774\n",
      "train loss:2.223604387945964\n",
      "train loss:2.21302930198741\n",
      "=== epoch:209, train acc:0.3233333333333333, test acc:0.267 ===\n",
      "train loss:2.2105846074113153\n",
      "train loss:2.206170423698587\n",
      "train loss:2.2259553068390003\n",
      "=== epoch:210, train acc:0.32666666666666666, test acc:0.2683 ===\n",
      "train loss:2.246313965645275\n",
      "train loss:2.218700078531982\n",
      "train loss:2.2296882357911896\n",
      "=== epoch:211, train acc:0.32666666666666666, test acc:0.2689 ===\n",
      "train loss:2.2121839433761745\n",
      "train loss:2.2457394282023966\n",
      "train loss:2.241121172020825\n",
      "=== epoch:212, train acc:0.32666666666666666, test acc:0.2694 ===\n",
      "train loss:2.216447041403325\n",
      "train loss:2.237372322614843\n",
      "train loss:2.2364558048443617\n",
      "=== epoch:213, train acc:0.33, test acc:0.2694 ===\n",
      "train loss:2.215109131419455\n",
      "train loss:2.216574463670353\n",
      "train loss:2.2192185454020468\n",
      "=== epoch:214, train acc:0.3233333333333333, test acc:0.2678 ===\n",
      "train loss:2.2203694651027677\n",
      "train loss:2.250993597874001\n",
      "train loss:2.239980529192355\n",
      "=== epoch:215, train acc:0.3333333333333333, test acc:0.2693 ===\n",
      "train loss:2.2275904070074897\n",
      "train loss:2.210287148032535\n",
      "train loss:2.2000920609326027\n",
      "=== epoch:216, train acc:0.3333333333333333, test acc:0.2685 ===\n",
      "train loss:2.2336396410496078\n",
      "train loss:2.216424162337138\n",
      "train loss:2.2114353662937742\n",
      "=== epoch:217, train acc:0.33, test acc:0.2685 ===\n",
      "train loss:2.244469861085083\n",
      "train loss:2.2120314164489123\n",
      "train loss:2.214174886520019\n",
      "=== epoch:218, train acc:0.3333333333333333, test acc:0.2699 ===\n",
      "train loss:2.2139240103424895\n",
      "train loss:2.2500586028930107\n",
      "train loss:2.2350391127064726\n",
      "=== epoch:219, train acc:0.3333333333333333, test acc:0.2679 ===\n",
      "train loss:2.213905157198898\n",
      "train loss:2.225972127081034\n",
      "train loss:2.176677481584944\n",
      "=== epoch:220, train acc:0.32666666666666666, test acc:0.2695 ===\n",
      "train loss:2.224007724831361\n",
      "train loss:2.1942074822254183\n",
      "train loss:2.2378900581116694\n",
      "=== epoch:221, train acc:0.33666666666666667, test acc:0.2691 ===\n",
      "train loss:2.210029677670168\n",
      "train loss:2.218513812853782\n",
      "train loss:2.207873999335034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.33666666666666667, test acc:0.2709 ===\n",
      "train loss:2.2483869404802506\n",
      "train loss:2.228156527174842\n",
      "train loss:2.2431748745822304\n",
      "=== epoch:223, train acc:0.3333333333333333, test acc:0.2746 ===\n",
      "train loss:2.22933320883052\n",
      "train loss:2.201584754788151\n",
      "train loss:2.214209383602645\n",
      "=== epoch:224, train acc:0.34, test acc:0.2758 ===\n",
      "train loss:2.2396622384363076\n",
      "train loss:2.2562664632895957\n",
      "train loss:2.2280057594487364\n",
      "=== epoch:225, train acc:0.3433333333333333, test acc:0.2789 ===\n",
      "train loss:2.2106885384539496\n",
      "train loss:2.194895450502548\n",
      "train loss:2.2106966443737592\n",
      "=== epoch:226, train acc:0.3433333333333333, test acc:0.2781 ===\n",
      "train loss:2.1929700356995405\n",
      "train loss:2.2202934599203443\n",
      "train loss:2.2135324396230094\n",
      "=== epoch:227, train acc:0.34, test acc:0.2745 ===\n",
      "train loss:2.197237844727532\n",
      "train loss:2.211765962009437\n",
      "train loss:2.2001654417484797\n",
      "=== epoch:228, train acc:0.34, test acc:0.2749 ===\n",
      "train loss:2.215716762201063\n",
      "train loss:2.216259549712003\n",
      "train loss:2.2219351429839875\n",
      "=== epoch:229, train acc:0.34, test acc:0.2736 ===\n",
      "train loss:2.2102335045232406\n",
      "train loss:2.202462329268762\n",
      "train loss:2.2338769094954696\n",
      "=== epoch:230, train acc:0.3433333333333333, test acc:0.2754 ===\n",
      "train loss:2.2253212570908016\n",
      "train loss:2.208717144721896\n",
      "train loss:2.2035755911579384\n",
      "=== epoch:231, train acc:0.34, test acc:0.2732 ===\n",
      "train loss:2.2318876173361644\n",
      "train loss:2.1891401884580937\n",
      "train loss:2.2432724760145386\n",
      "=== epoch:232, train acc:0.3333333333333333, test acc:0.272 ===\n",
      "train loss:2.1731887792699034\n",
      "train loss:2.226076836834347\n",
      "train loss:2.220076446248052\n",
      "=== epoch:233, train acc:0.3333333333333333, test acc:0.2716 ===\n",
      "train loss:2.2168231018731435\n",
      "train loss:2.2283698436395136\n",
      "train loss:2.203853267645924\n",
      "=== epoch:234, train acc:0.33666666666666667, test acc:0.2718 ===\n",
      "train loss:2.225845577261851\n",
      "train loss:2.204744413374478\n",
      "train loss:2.20645503532353\n",
      "=== epoch:235, train acc:0.33666666666666667, test acc:0.2728 ===\n",
      "train loss:2.2143891427910734\n",
      "train loss:2.222798516893344\n",
      "train loss:2.227798999055621\n",
      "=== epoch:236, train acc:0.34, test acc:0.2725 ===\n",
      "train loss:2.24660010694742\n",
      "train loss:2.231631556787537\n",
      "train loss:2.224390781546762\n",
      "=== epoch:237, train acc:0.34, test acc:0.2754 ===\n",
      "train loss:2.2044442795989796\n",
      "train loss:2.217993915566931\n",
      "train loss:2.1949885163882037\n",
      "=== epoch:238, train acc:0.3433333333333333, test acc:0.2771 ===\n",
      "train loss:2.2015395842510332\n",
      "train loss:2.158376236262208\n",
      "train loss:2.1800384512438216\n",
      "=== epoch:239, train acc:0.3433333333333333, test acc:0.2734 ===\n",
      "train loss:2.220213262950259\n",
      "train loss:2.2032530989296366\n",
      "train loss:2.1860272104011558\n",
      "=== epoch:240, train acc:0.3433333333333333, test acc:0.2738 ===\n",
      "train loss:2.1998542557307035\n",
      "train loss:2.208328936877883\n",
      "train loss:2.205107690718457\n",
      "=== epoch:241, train acc:0.34, test acc:0.2732 ===\n",
      "train loss:2.176524012245792\n",
      "train loss:2.194802454574573\n",
      "train loss:2.2106811429286837\n",
      "=== epoch:242, train acc:0.33666666666666667, test acc:0.2732 ===\n",
      "train loss:2.2006189080830687\n",
      "train loss:2.189951006252607\n",
      "train loss:2.2039327389186036\n",
      "=== epoch:243, train acc:0.3333333333333333, test acc:0.2723 ===\n",
      "train loss:2.1951568515848745\n",
      "train loss:2.2082360529173797\n",
      "train loss:2.2207857820233783\n",
      "=== epoch:244, train acc:0.3333333333333333, test acc:0.2718 ===\n",
      "train loss:2.202064809221112\n",
      "train loss:2.2274244794089886\n",
      "train loss:2.232215892141966\n",
      "=== epoch:245, train acc:0.33, test acc:0.2721 ===\n",
      "train loss:2.2132071325593063\n",
      "train loss:2.1995666924911035\n",
      "train loss:2.185878040627543\n",
      "=== epoch:246, train acc:0.34, test acc:0.2745 ===\n",
      "train loss:2.188613377942066\n",
      "train loss:2.2007679021399498\n",
      "train loss:2.2009443886112745\n",
      "=== epoch:247, train acc:0.33666666666666667, test acc:0.2745 ===\n",
      "train loss:2.1871771522664964\n",
      "train loss:2.1914318055715616\n",
      "train loss:2.16759773533775\n",
      "=== epoch:248, train acc:0.3333333333333333, test acc:0.2736 ===\n",
      "train loss:2.194216414685579\n",
      "train loss:2.1929163249580887\n",
      "train loss:2.238929749886548\n",
      "=== epoch:249, train acc:0.33, test acc:0.2715 ===\n",
      "train loss:2.2157691134884057\n",
      "train loss:2.1957325986579375\n",
      "train loss:2.2200154922099613\n",
      "=== epoch:250, train acc:0.33, test acc:0.2728 ===\n",
      "train loss:2.2202578402200808\n",
      "train loss:2.1738410384602744\n",
      "train loss:2.1801591017571873\n",
      "=== epoch:251, train acc:0.33, test acc:0.2728 ===\n",
      "train loss:2.1566567056279635\n",
      "train loss:2.181478898004597\n",
      "train loss:2.203355169327855\n",
      "=== epoch:252, train acc:0.33, test acc:0.2693 ===\n",
      "train loss:2.1969283345927595\n",
      "train loss:2.1913769380190007\n",
      "train loss:2.1902313206245028\n",
      "=== epoch:253, train acc:0.3233333333333333, test acc:0.2685 ===\n",
      "train loss:2.192534312948805\n",
      "train loss:2.221599947813191\n",
      "train loss:2.1701164719967845\n",
      "=== epoch:254, train acc:0.32, test acc:0.2663 ===\n",
      "train loss:2.183723780359826\n",
      "train loss:2.1862097999453622\n",
      "train loss:2.2064400620732445\n",
      "=== epoch:255, train acc:0.3233333333333333, test acc:0.2661 ===\n",
      "train loss:2.1954080154186353\n",
      "train loss:2.1945124987703326\n",
      "train loss:2.169667258680522\n",
      "=== epoch:256, train acc:0.32666666666666666, test acc:0.2678 ===\n",
      "train loss:2.166867730128639\n",
      "train loss:2.153034990837615\n",
      "train loss:2.143417547118445\n",
      "=== epoch:257, train acc:0.3233333333333333, test acc:0.2649 ===\n",
      "train loss:2.1399720991423914\n",
      "train loss:2.1701457422221186\n",
      "train loss:2.2002221627628002\n",
      "=== epoch:258, train acc:0.32, test acc:0.2645 ===\n",
      "train loss:2.2066524174986326\n",
      "train loss:2.1968864426803085\n",
      "train loss:2.13799593554011\n",
      "=== epoch:259, train acc:0.32, test acc:0.2651 ===\n",
      "train loss:2.206507978851745\n",
      "train loss:2.1892898470958757\n",
      "train loss:2.194090987536805\n",
      "=== epoch:260, train acc:0.3233333333333333, test acc:0.2652 ===\n",
      "train loss:2.189616697329909\n",
      "train loss:2.1781152947279607\n",
      "train loss:2.1982507933068822\n",
      "=== epoch:261, train acc:0.32, test acc:0.2652 ===\n",
      "train loss:2.2003686025371194\n",
      "train loss:2.2031006100067008\n",
      "train loss:2.1694525377905185\n",
      "=== epoch:262, train acc:0.32, test acc:0.2657 ===\n",
      "train loss:2.194944488016889\n",
      "train loss:2.2032279215741712\n",
      "train loss:2.175316307310012\n",
      "=== epoch:263, train acc:0.32, test acc:0.2674 ===\n",
      "train loss:2.147019235066132\n",
      "train loss:2.19427931844431\n",
      "train loss:2.1891685187169543\n",
      "=== epoch:264, train acc:0.32, test acc:0.2684 ===\n",
      "train loss:2.178860809435841\n",
      "train loss:2.172800475778975\n",
      "train loss:2.1830952792640654\n",
      "=== epoch:265, train acc:0.32666666666666666, test acc:0.2706 ===\n",
      "train loss:2.1685580964747087\n",
      "train loss:2.147163420244133\n",
      "train loss:2.1948800765914105\n",
      "=== epoch:266, train acc:0.32666666666666666, test acc:0.2704 ===\n",
      "train loss:2.1796913388181185\n",
      "train loss:2.1708051186743633\n",
      "train loss:2.208254124138876\n",
      "=== epoch:267, train acc:0.32666666666666666, test acc:0.269 ===\n",
      "train loss:2.227903354326371\n",
      "train loss:2.1763645615969387\n",
      "train loss:2.1599676606539835\n",
      "=== epoch:268, train acc:0.32666666666666666, test acc:0.2701 ===\n",
      "train loss:2.180138015031706\n",
      "train loss:2.17582866075662\n",
      "train loss:2.1451749903635617\n",
      "=== epoch:269, train acc:0.31666666666666665, test acc:0.2679 ===\n",
      "train loss:2.189572746447876\n",
      "train loss:2.192565110721433\n",
      "train loss:2.1944883222054057\n",
      "=== epoch:270, train acc:0.31666666666666665, test acc:0.2705 ===\n",
      "train loss:2.144679417783002\n",
      "train loss:2.1809841023056165\n",
      "train loss:2.205703786818293\n",
      "=== epoch:271, train acc:0.3233333333333333, test acc:0.2714 ===\n",
      "train loss:2.1804258871245916\n",
      "train loss:2.17309104801028\n",
      "train loss:2.2065132446310742\n",
      "=== epoch:272, train acc:0.3233333333333333, test acc:0.269 ===\n",
      "train loss:2.1562229140172127\n",
      "train loss:2.1808300058969294\n",
      "train loss:2.165736076650381\n",
      "=== epoch:273, train acc:0.32, test acc:0.2682 ===\n",
      "train loss:2.1758962711360157\n",
      "train loss:2.209341550893003\n",
      "train loss:2.1909960317004615\n",
      "=== epoch:274, train acc:0.3233333333333333, test acc:0.2692 ===\n",
      "train loss:2.1642130338262846\n",
      "train loss:2.1709695720869018\n",
      "train loss:2.179019315846176\n",
      "=== epoch:275, train acc:0.3233333333333333, test acc:0.2714 ===\n",
      "train loss:2.1340556314553663\n",
      "train loss:2.179591315737257\n",
      "train loss:2.1406000057221295\n",
      "=== epoch:276, train acc:0.32666666666666666, test acc:0.2722 ===\n",
      "train loss:2.119483284531595\n",
      "train loss:2.1801607926758155\n",
      "train loss:2.161901273775886\n",
      "=== epoch:277, train acc:0.32666666666666666, test acc:0.2734 ===\n",
      "train loss:2.1630979630433624\n",
      "train loss:2.099803600553864\n",
      "train loss:2.1418718883212513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.32666666666666666, test acc:0.2726 ===\n",
      "train loss:2.159356188944273\n",
      "train loss:2.1316138162506095\n",
      "train loss:2.157319241245557\n",
      "=== epoch:279, train acc:0.32666666666666666, test acc:0.275 ===\n",
      "train loss:2.140784522611724\n",
      "train loss:2.1767079996710166\n",
      "train loss:2.20522659550812\n",
      "=== epoch:280, train acc:0.33, test acc:0.2757 ===\n",
      "train loss:2.160542601586604\n",
      "train loss:2.169129854755295\n",
      "train loss:2.18876984941552\n",
      "=== epoch:281, train acc:0.33, test acc:0.2757 ===\n",
      "train loss:2.1238397117560166\n",
      "train loss:2.1649290860298267\n",
      "train loss:2.223674451905357\n",
      "=== epoch:282, train acc:0.33666666666666667, test acc:0.277 ===\n",
      "train loss:2.1238143749490233\n",
      "train loss:2.1737856148207326\n",
      "train loss:2.083409322252959\n",
      "=== epoch:283, train acc:0.32666666666666666, test acc:0.2739 ===\n",
      "train loss:2.1836673670503775\n",
      "train loss:2.1259285755679014\n",
      "train loss:2.1590470822384393\n",
      "=== epoch:284, train acc:0.32666666666666666, test acc:0.2742 ===\n",
      "train loss:2.2101985438219445\n",
      "train loss:2.1797423490174728\n",
      "train loss:2.172314095283718\n",
      "=== epoch:285, train acc:0.32666666666666666, test acc:0.2749 ===\n",
      "train loss:2.183609610712206\n",
      "train loss:2.1071545177585773\n",
      "train loss:2.2105066629498737\n",
      "=== epoch:286, train acc:0.32666666666666666, test acc:0.2763 ===\n",
      "train loss:2.2227352472150246\n",
      "train loss:2.1286386324264153\n",
      "train loss:2.1584360971397647\n",
      "=== epoch:287, train acc:0.32666666666666666, test acc:0.2761 ===\n",
      "train loss:2.13747367961985\n",
      "train loss:2.182910318087277\n",
      "train loss:2.149716458595299\n",
      "=== epoch:288, train acc:0.32666666666666666, test acc:0.2753 ===\n",
      "train loss:2.1365624633371927\n",
      "train loss:2.1408816220130826\n",
      "train loss:2.160424747637977\n",
      "=== epoch:289, train acc:0.3233333333333333, test acc:0.2735 ===\n",
      "train loss:2.1412555451271356\n",
      "train loss:2.1854167129709143\n",
      "train loss:2.1245640436121405\n",
      "=== epoch:290, train acc:0.32666666666666666, test acc:0.2731 ===\n",
      "train loss:2.1614116755343913\n",
      "train loss:2.156229082786841\n",
      "train loss:2.172780124415609\n",
      "=== epoch:291, train acc:0.32666666666666666, test acc:0.2746 ===\n",
      "train loss:2.1298752221652544\n",
      "train loss:2.1148957393402377\n",
      "train loss:2.1436673197492326\n",
      "=== epoch:292, train acc:0.32666666666666666, test acc:0.2731 ===\n",
      "train loss:2.095794935352421\n",
      "train loss:2.103408853748583\n",
      "train loss:2.135482068045842\n",
      "=== epoch:293, train acc:0.32, test acc:0.2725 ===\n",
      "train loss:2.12709309577325\n",
      "train loss:2.1563256221710647\n",
      "train loss:2.1727289172020523\n",
      "=== epoch:294, train acc:0.32, test acc:0.2717 ===\n",
      "train loss:2.1053347772187654\n",
      "train loss:2.12272339766285\n",
      "train loss:2.132504494083762\n",
      "=== epoch:295, train acc:0.32, test acc:0.2698 ===\n",
      "train loss:2.1508861038224514\n",
      "train loss:2.1839558150128453\n",
      "train loss:2.1428165370241765\n",
      "=== epoch:296, train acc:0.32, test acc:0.2706 ===\n",
      "train loss:2.169070044653395\n",
      "train loss:2.1570665901950807\n",
      "train loss:2.120013597898927\n",
      "=== epoch:297, train acc:0.32, test acc:0.2703 ===\n",
      "train loss:2.1563345772498765\n",
      "train loss:2.0899957602143306\n",
      "train loss:2.1818177646668637\n",
      "=== epoch:298, train acc:0.32, test acc:0.2699 ===\n",
      "train loss:2.142702271619924\n",
      "train loss:2.1350125523596715\n",
      "train loss:2.133909124386896\n",
      "=== epoch:299, train acc:0.32, test acc:0.2703 ===\n",
      "train loss:2.1580273927554448\n",
      "train loss:2.1238235239488055\n",
      "train loss:2.15397091144213\n",
      "=== epoch:300, train acc:0.32, test acc:0.2693 ===\n",
      "train loss:2.150282425471232\n",
      "train loss:2.1498360715864475\n",
      "train loss:2.129002669812438\n",
      "=== epoch:301, train acc:0.32, test acc:0.2711 ===\n",
      "train loss:2.13024154581637\n",
      "train loss:2.1534867421225936\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.2713\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.3\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db32e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf1520",
   "metadata": {},
   "source": [
    "- 드롭아웃 0.1은 오버피팅의 여지가 있고\n",
    "- 드롭아웃 0.3은 학습이 저조한 경향이 있어 드롭아웃 0.2가 최적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd52746",
   "metadata": {},
   "source": [
    "### 적절한 하이퍼 파라미터 값 찾기\n",
    "- 검증 데이터 셋 분리하기\n",
    "- 적절한 하이퍼 파라미터 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99b78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    if x.ndim == 2:\n",
    "        x = x[permutation,:]\n",
    "    else:\n",
    "        x = x[permutation,:,:,:]\n",
    "\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f728dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f5649",
   "metadata": {},
   "source": [
    "데이터 뒤섞기 이전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e97624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736cd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 뒤섞기\n",
    "\n",
    "x_train,t_train = shuffle_dataset(x_train,t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df49d3",
   "metadata": {},
   "source": [
    "데이터 뒤섞기 이후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53180b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 7, ..., 0, 3, 9], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459fea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# 검증셋 분리하기\n",
    "valid_rate = 0.2\n",
    "\n",
    "valid_num = int(x_train.shape[0] * valid_rate)\n",
    "\n",
    "print(valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2657e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:valid_num]\n",
    "\n",
    "x_train = x_train[valid_num:]\n",
    "\n",
    "t_val = t_train[:valid_num]\n",
    "\n",
    "t_train = t_train[valid_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ac2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋 데이터 개수  48000\n",
      "검증셋 데이터 개수  48000\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련셋 데이터 개수 \",len(x_train))\n",
    "print(\"검증셋 데이터 개수 \",len(t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28fab361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "from common.trainer import Trainer\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4b887",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 무작위 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bfc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.9688333333333333 | lr:0.006736392077547212, weight decay:4.914114150355648e-08\n",
      "val acc:0.852 | lr:0.00013950531508548087, weight decay:1.4081597054781761e-06\n",
      "val acc:0.13966666666666666 | lr:1.4196575901750695e-06, weight decay:3.4400846846160196e-07\n",
      "val acc:0.8611666666666666 | lr:0.0001339576278800374, weight decay:1.1040389848682265e-06\n",
      "val acc:0.9645833333333333 | lr:0.004980197628233759, weight decay:1.1521876315210054e-06\n",
      "val acc:0.9334166666666667 | lr:0.0005982720382525896, weight decay:1.1230745135054847e-08\n",
      "val acc:0.22983333333333333 | lr:1.2186535925433861e-05, weight decay:4.219517995456539e-08\n",
      "val acc:0.9639166666666666 | lr:0.0032902134730075075, weight decay:8.973025390127156e-05\n"
     ]
    }
   ],
   "source": [
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 하이퍼파라미터의 범위 지정\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4) # 가중치 감소 계수의 범위 10^-8 ~ 10^-4\n",
    "    lr = 10 ** np.random.uniform(-6, -2) # 학습률의 범위 10^-6 ~ 10^-2\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0661af",
   "metadata": {},
   "source": [
    "그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38d5f7",
   "metadata": {},
   "source": [
    "### 오버피팅 방지방법 결합해보기\n",
    "- 학습률 0.01\n",
    "- 드롭아웃 사용\n",
    "- 가중치 감쇠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d02317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f65f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    if x.ndim == 2:\n",
    "        x = x[permutation,:]\n",
    "    else:\n",
    "        x = x[permutation,:,:,:]\n",
    "\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3157d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 훈련 데이터 뒤섞기\n",
    "\n",
    "x_train,t_train = shuffle_dataset(x_train,t_train)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 줄이기\n",
    "x_train = x_train[:20000]\n",
    "t_train = t_train[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc833bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:62.749699836960914\n",
      "=== epoch:1, train acc:0.10075, test acc:0.0959 ===\n",
      "train loss:62.619500133625365\n",
      "train loss:62.503274760189534\n",
      "train loss:62.37773491254179\n",
      "train loss:62.22631639938371\n",
      "train loss:62.08647963980972\n",
      "train loss:61.97214208303095\n",
      "train loss:61.87386410938551\n",
      "train loss:61.7508551322893\n",
      "train loss:61.65773407900303\n",
      "train loss:61.51865316515463\n",
      "train loss:61.40421333665094\n",
      "train loss:61.289777188712286\n",
      "train loss:61.16796130047642\n",
      "train loss:61.04114295184373\n",
      "train loss:60.89780715142061\n",
      "train loss:60.80962308597452\n",
      "train loss:60.68572206981453\n",
      "train loss:60.57038256756276\n",
      "train loss:60.45005383514057\n",
      "train loss:60.33935739939549\n",
      "train loss:60.22445714121642\n",
      "train loss:60.11089610069653\n",
      "train loss:59.98531577369717\n",
      "train loss:59.873912429087014\n",
      "train loss:59.7561475143881\n",
      "train loss:59.63824245393796\n",
      "train loss:59.526596040389805\n",
      "train loss:59.4051963415655\n",
      "train loss:59.295675780933\n",
      "train loss:59.18569490284222\n",
      "train loss:59.057190171328045\n",
      "train loss:58.95821334625607\n",
      "train loss:58.838284545213185\n",
      "train loss:58.728527093705765\n",
      "train loss:58.60472987471272\n",
      "train loss:58.498818645275485\n",
      "train loss:58.38927830703038\n",
      "train loss:58.2598659748669\n",
      "train loss:58.155883273794906\n",
      "train loss:58.03894784841195\n",
      "train loss:57.93748177065587\n",
      "train loss:57.8280706288873\n",
      "train loss:57.69723896397672\n",
      "train loss:57.6081360341082\n",
      "train loss:57.49322803599779\n",
      "train loss:57.37749553569763\n",
      "train loss:57.278975323938205\n",
      "train loss:57.16097452498659\n",
      "train loss:57.04015086356869\n",
      "train loss:56.936712278913035\n",
      "train loss:56.82966879398561\n",
      "train loss:56.722575289883245\n",
      "train loss:56.61106952263642\n",
      "train loss:56.50441501823161\n",
      "train loss:56.392629904577184\n",
      "train loss:56.28241865848421\n",
      "train loss:56.17563301287495\n",
      "train loss:56.06570835786702\n",
      "train loss:55.9600653133935\n",
      "train loss:55.844995782080915\n",
      "train loss:55.737837453505975\n",
      "train loss:55.63315958153759\n",
      "train loss:55.53479008996473\n",
      "train loss:55.42622134265737\n",
      "train loss:55.31774979870229\n",
      "train loss:55.205384337370184\n",
      "train loss:55.105701393018094\n",
      "train loss:54.99575764554637\n",
      "train loss:54.893846485975466\n",
      "train loss:54.79577848005988\n",
      "train loss:54.682913403485806\n",
      "train loss:54.57667901393366\n",
      "train loss:54.46700932653002\n",
      "train loss:54.37207377565228\n",
      "train loss:54.27092370379404\n",
      "train loss:54.16193999562977\n",
      "train loss:54.054593853429125\n",
      "train loss:53.95318635569336\n",
      "train loss:53.852589237844754\n",
      "train loss:53.75585743869468\n",
      "train loss:53.640929593167854\n",
      "train loss:53.54217735468818\n",
      "train loss:53.446871888105576\n",
      "train loss:53.342520225489515\n",
      "train loss:53.236846989399865\n",
      "train loss:53.13256288755075\n",
      "train loss:53.039103555604584\n",
      "train loss:52.92741384735439\n",
      "train loss:52.83187693475681\n",
      "train loss:52.73672119062161\n",
      "train loss:52.630153848661074\n",
      "train loss:52.52742288753981\n",
      "train loss:52.424926754012155\n",
      "train loss:52.32859008820571\n",
      "train loss:52.22779163779333\n",
      "train loss:52.12817803031911\n",
      "train loss:52.027202037465536\n",
      "train loss:51.923780677569056\n",
      "train loss:51.83093897712235\n",
      "train loss:51.73322803843856\n",
      "train loss:51.6303935267075\n",
      "train loss:51.53378987928844\n",
      "train loss:51.43089368197506\n",
      "train loss:51.33967747288278\n",
      "train loss:51.23632389083644\n",
      "train loss:51.13374021606848\n",
      "train loss:51.04356444364976\n",
      "train loss:50.9518117338124\n",
      "train loss:50.848461864814844\n",
      "train loss:50.74595378771468\n",
      "train loss:50.65284868776865\n",
      "train loss:50.559652424472944\n",
      "train loss:50.448523917773336\n",
      "train loss:50.365660151549925\n",
      "train loss:50.26870275596448\n",
      "train loss:50.17734674011563\n",
      "train loss:50.079223347804394\n",
      "train loss:49.98339323077257\n",
      "train loss:49.88964852659454\n",
      "train loss:49.79315330549226\n",
      "train loss:49.693298000751014\n",
      "train loss:49.601626036335354\n",
      "train loss:49.50519934819368\n",
      "train loss:49.40815531095\n",
      "train loss:49.31908691085238\n",
      "train loss:49.2266153366098\n",
      "train loss:49.13116611819467\n",
      "train loss:49.03455504384559\n",
      "train loss:48.94103774911826\n",
      "train loss:48.85261524472047\n",
      "train loss:48.75705911727312\n",
      "train loss:48.668417244040505\n",
      "train loss:48.570710043291754\n",
      "train loss:48.476847477316696\n",
      "train loss:48.38655380129823\n",
      "train loss:48.29288443499004\n",
      "train loss:48.20138722862501\n",
      "train loss:48.107847496849125\n",
      "train loss:48.02035934292012\n",
      "train loss:47.9274771491495\n",
      "train loss:47.842172106911434\n",
      "train loss:47.748763951480896\n",
      "train loss:47.651780362092644\n",
      "train loss:47.56473100180207\n",
      "train loss:47.47606626919772\n",
      "train loss:47.38239076579939\n",
      "train loss:47.29443576964957\n",
      "train loss:47.2048636708486\n",
      "train loss:47.113201002289614\n",
      "train loss:47.02606765488897\n",
      "train loss:46.936007884225376\n",
      "train loss:46.84357213661281\n",
      "train loss:46.755608215930415\n",
      "train loss:46.66738559627384\n",
      "train loss:46.57883879569486\n",
      "train loss:46.492590265031566\n",
      "train loss:46.40221373660165\n",
      "train loss:46.31377651972386\n",
      "train loss:46.22754864011736\n",
      "train loss:46.14040057026212\n",
      "train loss:46.050664208916324\n",
      "train loss:45.96480071186683\n",
      "train loss:45.880331841206086\n",
      "train loss:45.788742759435124\n",
      "train loss:45.703136075489546\n",
      "train loss:45.612255336290104\n",
      "train loss:45.52876326319913\n",
      "train loss:45.44504991501109\n",
      "train loss:45.353320775915876\n",
      "train loss:45.273671148211655\n",
      "train loss:45.18401424800284\n",
      "train loss:45.097597053221016\n",
      "train loss:45.013188863161616\n",
      "train loss:44.928083736581875\n",
      "train loss:44.84414748691011\n",
      "train loss:44.75726709858431\n",
      "train loss:44.67274278648636\n",
      "train loss:44.59099115526892\n",
      "train loss:44.50628785310573\n",
      "train loss:44.420095703406375\n",
      "train loss:44.33667576816364\n",
      "train loss:44.25457847490203\n",
      "train loss:44.167821483152\n",
      "train loss:44.0864439555608\n",
      "train loss:44.002005523471084\n",
      "train loss:43.91585477032152\n",
      "train loss:43.83437977502949\n",
      "train loss:43.753577545881214\n",
      "train loss:43.66791248143713\n",
      "train loss:43.58593207786827\n",
      "train loss:43.50412913037514\n",
      "train loss:43.42206641519467\n",
      "train loss:43.33973911446637\n",
      "train loss:43.25539091163824\n",
      "train loss:43.17602310674812\n",
      "train loss:43.094612808957095\n",
      "train loss:43.01113100116513\n",
      "train loss:42.92931190806535\n",
      "train loss:42.848195552143984\n",
      "train loss:42.769645117980296\n",
      "=== epoch:2, train acc:0.24095, test acc:0.2442 ===\n",
      "train loss:42.68650872083458\n",
      "train loss:42.60640863251841\n",
      "train loss:42.53046548713706\n",
      "train loss:42.446080158396434\n",
      "train loss:42.36610474595447\n",
      "train loss:42.284142261540495\n",
      "train loss:42.204051142032355\n",
      "train loss:42.12610890863712\n",
      "train loss:42.04796058467748\n",
      "train loss:41.96844503097919\n",
      "train loss:41.88686356097534\n",
      "train loss:41.810659188507536\n",
      "train loss:41.727748923282476\n",
      "train loss:41.65211979869015\n",
      "train loss:41.571423780076195\n",
      "train loss:41.49335976668303\n",
      "train loss:41.421243474942806\n",
      "train loss:41.33908022816008\n",
      "train loss:41.26201340029324\n",
      "train loss:41.18125328213649\n",
      "train loss:41.10668184608801\n",
      "train loss:41.02638742154862\n",
      "train loss:40.95005703579352\n",
      "train loss:40.87269508440187\n",
      "train loss:40.79506858832142\n",
      "train loss:40.71619390307005\n",
      "train loss:40.641572428887514\n",
      "train loss:40.56657894877926\n",
      "train loss:40.49214666087042\n",
      "train loss:40.41038419660057\n",
      "train loss:40.33727722663358\n",
      "train loss:40.259305183812714\n",
      "train loss:40.18334275642487\n",
      "train loss:40.109834746934176\n",
      "train loss:40.03378453820585\n",
      "train loss:39.95557159819281\n",
      "train loss:39.88325096870181\n",
      "train loss:39.806561664469314\n",
      "train loss:39.733519226145404\n",
      "train loss:39.662283460980476\n",
      "train loss:39.58365734814129\n",
      "train loss:39.5084914591586\n",
      "train loss:39.433540448856476\n",
      "train loss:39.36252375347251\n",
      "train loss:39.28743195356537\n",
      "train loss:39.21407733993116\n",
      "train loss:39.140660646582795\n",
      "train loss:39.06459070238393\n",
      "train loss:38.99115294569761\n",
      "train loss:38.921464307448325\n",
      "train loss:38.84697806151276\n",
      "train loss:38.7722021621903\n",
      "train loss:38.69879182484304\n",
      "train loss:38.625337004208134\n",
      "train loss:38.555694569999915\n",
      "train loss:38.48164064617192\n",
      "train loss:38.41304329968383\n",
      "train loss:38.334469485833196\n",
      "train loss:38.26272087992186\n",
      "train loss:38.19483316382074\n",
      "train loss:38.12004583303901\n",
      "train loss:38.05126907825233\n",
      "train loss:37.97983484726622\n",
      "train loss:37.90776171310051\n",
      "train loss:37.83924734947161\n",
      "train loss:37.76361365408342\n",
      "train loss:37.695212072790845\n",
      "train loss:37.624267530310256\n",
      "train loss:37.55379471887258\n",
      "train loss:37.48517545990659\n",
      "train loss:37.41562442695989\n",
      "train loss:37.3449088507533\n",
      "train loss:37.27142906509392\n",
      "train loss:37.20636627034017\n",
      "train loss:37.13333480395782\n",
      "train loss:37.05992470330045\n",
      "train loss:36.9934925692459\n",
      "train loss:36.92376442232477\n",
      "train loss:36.857173856398084\n",
      "train loss:36.78553250180784\n",
      "train loss:36.71518557537234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:36.651083544229735\n",
      "train loss:36.57653460932748\n",
      "train loss:36.51174094032987\n",
      "train loss:36.4474310466676\n",
      "train loss:36.37616984744049\n",
      "train loss:36.30841424263046\n",
      "train loss:36.239798959029166\n",
      "train loss:36.17216181020773\n",
      "train loss:36.1007997244408\n",
      "train loss:36.03582723152475\n",
      "train loss:35.96676299990218\n",
      "train loss:35.90534258757076\n",
      "train loss:35.83675538746763\n",
      "train loss:35.768261886185314\n",
      "train loss:35.70219728739488\n",
      "train loss:35.63243474914295\n",
      "train loss:35.566290596313436\n",
      "train loss:35.50246642828683\n",
      "train loss:35.43458701256148\n",
      "train loss:35.37182593387608\n",
      "train loss:35.299876078348554\n",
      "train loss:35.23181874523647\n",
      "train loss:35.169449719289965\n",
      "train loss:35.105077683712096\n",
      "train loss:35.04397235985681\n",
      "train loss:34.97473693219937\n",
      "train loss:34.907940786056905\n",
      "train loss:34.845891799923805\n",
      "train loss:34.7806672971653\n",
      "train loss:34.71487317931262\n",
      "train loss:34.65074874392734\n",
      "train loss:34.587591643470056\n",
      "train loss:34.519631778380315\n",
      "train loss:34.45908077874473\n",
      "train loss:34.39079945281744\n",
      "train loss:34.329435353756594\n",
      "train loss:34.264230374225164\n",
      "train loss:34.20046103014532\n",
      "train loss:34.13851958939572\n",
      "train loss:34.07402296325838\n",
      "train loss:34.0077524523931\n",
      "train loss:33.947484990134015\n",
      "train loss:33.88418982791912\n",
      "train loss:33.82089901737974\n",
      "train loss:33.758081403201956\n",
      "train loss:33.69251180472801\n",
      "train loss:33.63150885758712\n",
      "train loss:33.56625493042587\n",
      "train loss:33.50568527696783\n",
      "train loss:33.446888711197076\n",
      "train loss:33.38024540942817\n",
      "train loss:33.32112894356725\n",
      "train loss:33.261686071449205\n",
      "train loss:33.19493169027693\n",
      "train loss:33.13573912502455\n",
      "train loss:33.071152341351144\n",
      "train loss:33.01139317792473\n",
      "train loss:32.95157299772105\n",
      "train loss:32.88902621510612\n",
      "train loss:32.8300468623133\n",
      "train loss:32.76934097170947\n",
      "train loss:32.70907958028768\n",
      "train loss:32.64896606191695\n",
      "train loss:32.58203101903766\n",
      "train loss:32.52322727084112\n",
      "train loss:32.46541686504433\n",
      "train loss:32.404758623849915\n",
      "train loss:32.344617847239135\n",
      "train loss:32.2825008113877\n",
      "train loss:32.225826568358414\n",
      "train loss:32.159699403711954\n",
      "train loss:32.10213325750914\n",
      "train loss:32.04276022721238\n",
      "train loss:31.984380422931096\n",
      "train loss:31.92695203833892\n",
      "train loss:31.86656131751763\n",
      "train loss:31.808150216987155\n",
      "train loss:31.746223585819443\n",
      "train loss:31.68956062054004\n",
      "train loss:31.635177000733204\n",
      "train loss:31.575253439095874\n",
      "train loss:31.51572466858609\n",
      "train loss:31.451398892721713\n",
      "train loss:31.39716769157931\n",
      "train loss:31.34156507614938\n",
      "train loss:31.279448148271886\n",
      "train loss:31.22294872394794\n",
      "train loss:31.16872965409953\n",
      "train loss:31.108387771348834\n",
      "train loss:31.055391620455406\n",
      "train loss:30.992415740796872\n",
      "train loss:30.935976030781966\n",
      "train loss:30.878745507493868\n",
      "train loss:30.82357081371268\n",
      "train loss:30.761815440584623\n",
      "train loss:30.705984283079687\n",
      "train loss:30.649297672570114\n",
      "train loss:30.591809366455653\n",
      "train loss:30.537150343469172\n",
      "train loss:30.480552724779297\n",
      "train loss:30.42297650053248\n",
      "train loss:30.371314169340533\n",
      "train loss:30.311766789609536\n",
      "train loss:30.259097003424795\n",
      "train loss:30.19719860864637\n",
      "train loss:30.146131672015024\n",
      "train loss:30.080700154135833\n",
      "train loss:30.035699493197903\n",
      "train loss:29.974218740685586\n",
      "train loss:29.92306074707302\n",
      "train loss:29.870078333214394\n",
      "train loss:29.815532209539928\n",
      "train loss:29.762485998050977\n",
      "train loss:29.704075796351354\n",
      "train loss:29.650770174651097\n",
      "train loss:29.59551067049082\n",
      "train loss:29.5432954926722\n",
      "train loss:29.491028697166342\n",
      "train loss:29.434865131144953\n",
      "=== epoch:3, train acc:0.11685, test acc:0.1213 ===\n",
      "train loss:29.376628727235207\n",
      "train loss:29.32445021785006\n",
      "train loss:29.27347111692314\n",
      "train loss:29.214382835916282\n",
      "train loss:29.165184106838357\n",
      "train loss:29.104403435455403\n",
      "train loss:29.053091123463897\n",
      "train loss:28.99901085456071\n",
      "train loss:28.941374859439104\n",
      "train loss:28.894585162281654\n",
      "train loss:28.839974781009197\n",
      "train loss:28.7865933690675\n",
      "train loss:28.736615232330454\n",
      "train loss:28.685573220255417\n",
      "train loss:28.627726753626725\n",
      "train loss:28.578116098177674\n",
      "train loss:28.522240398254382\n",
      "train loss:28.473586762372243\n",
      "train loss:28.414967294281208\n",
      "train loss:28.36762613230599\n",
      "train loss:28.31880723317544\n",
      "train loss:28.26503490793459\n",
      "train loss:28.209781553011847\n",
      "train loss:28.16220231744162\n",
      "train loss:28.10874084727761\n",
      "train loss:28.052871753141886\n",
      "train loss:28.006465592688652\n",
      "train loss:27.958124841266624\n",
      "train loss:27.9018208255522\n",
      "train loss:27.849402250951854\n",
      "train loss:27.800122181985284\n",
      "train loss:27.75350092138493\n",
      "train loss:27.69907092238813\n",
      "train loss:27.64848956307784\n",
      "train loss:27.593863564743966\n",
      "train loss:27.54620804469028\n",
      "train loss:27.497258310966053\n",
      "train loss:27.44963815296843\n",
      "train loss:27.391785977202638\n",
      "train loss:27.346177162669157\n",
      "train loss:27.294549074537475\n",
      "train loss:27.247949583534727\n",
      "train loss:27.19986591323311\n",
      "train loss:27.148994781911448\n",
      "train loss:27.09771876889412\n",
      "train loss:27.05036997407931\n",
      "train loss:26.999816476077974\n",
      "train loss:26.94715302812537\n",
      "train loss:26.902997196367593\n",
      "train loss:26.84898299868702\n",
      "train loss:26.796836323428288\n",
      "train loss:26.75134467036766\n",
      "train loss:26.700522307143977\n",
      "train loss:26.65644878572665\n",
      "train loss:26.60582933418157\n",
      "train loss:26.558699216936372\n",
      "train loss:26.50940973840293\n",
      "train loss:26.459016072863445\n",
      "train loss:26.414280694850127\n",
      "train loss:26.364445542731964\n",
      "train loss:26.318068368546676\n",
      "train loss:26.2664868101793\n",
      "train loss:26.221899002716523\n",
      "train loss:26.17589397368932\n",
      "train loss:26.12332729614988\n",
      "train loss:26.073396443933454\n",
      "train loss:26.029762117363624\n",
      "train loss:25.983606915206067\n",
      "train loss:25.93350433065165\n",
      "train loss:25.88372297530341\n",
      "train loss:25.83957599424707\n",
      "train loss:25.790263464998603\n",
      "train loss:25.74262836944658\n",
      "train loss:25.70117988329953\n",
      "train loss:25.65651433502711\n",
      "train loss:25.604548094587482\n",
      "train loss:25.56068563679803\n",
      "train loss:25.510988975811383\n",
      "train loss:25.468518469324778\n",
      "train loss:25.42131576483706\n",
      "train loss:25.375468711827445\n",
      "train loss:25.331746539884186\n",
      "train loss:25.288146643977363\n",
      "train loss:25.235135454452053\n",
      "train loss:25.189789095879263\n",
      "train loss:25.14208712227602\n",
      "train loss:25.103117941867215\n",
      "train loss:25.05645937472252\n",
      "train loss:25.005076851910317\n",
      "train loss:24.966339623875125\n",
      "train loss:24.919379646710635\n",
      "train loss:24.873761558628743\n",
      "train loss:24.827551074782907\n",
      "train loss:24.779966698772334\n",
      "train loss:24.735585610899737\n",
      "train loss:24.69339896159517\n",
      "train loss:24.6481331363713\n",
      "train loss:24.60798083968523\n",
      "train loss:24.559118613698036\n",
      "train loss:24.515724491001055\n",
      "train loss:24.47139354457968\n",
      "train loss:24.43127671014789\n",
      "train loss:24.382481498868593\n",
      "train loss:24.339792043459298\n",
      "train loss:24.296730798303035\n",
      "train loss:24.252999793286605\n",
      "train loss:24.205742880616135\n",
      "train loss:24.15967345695956\n",
      "train loss:24.119045572353485\n",
      "train loss:24.07762958514766\n",
      "train loss:24.031239810682152\n",
      "train loss:23.98940978765208\n",
      "train loss:23.942840217164374\n",
      "train loss:23.89664308420418\n",
      "train loss:23.857508223739984\n",
      "train loss:23.813990212186514\n",
      "train loss:23.770316287741096\n",
      "train loss:23.730100279980036\n",
      "train loss:23.69064268341881\n",
      "train loss:23.644840140248537\n",
      "train loss:23.60040846391203\n",
      "train loss:23.562484581730423\n",
      "train loss:23.516031508175924\n",
      "train loss:23.47416454118626\n",
      "train loss:23.429428990726773\n",
      "train loss:23.390770429326437\n",
      "train loss:23.34538212313175\n",
      "train loss:23.30381144419566\n",
      "train loss:23.265760250733845\n",
      "train loss:23.21935830488793\n",
      "train loss:23.184784474490087\n",
      "train loss:23.142648298504994\n",
      "train loss:23.093233659365737\n",
      "train loss:23.06075850821488\n",
      "train loss:23.014066430885535\n",
      "train loss:22.972429887190213\n",
      "train loss:22.93126261843722\n",
      "train loss:22.89061335211615\n",
      "train loss:22.850092716657805\n",
      "train loss:22.81071863779383\n",
      "train loss:22.77137943053013\n",
      "train loss:22.722929582856263\n",
      "train loss:22.681447000442315\n",
      "train loss:22.646177244604065\n",
      "train loss:22.60483299074434\n",
      "train loss:22.56370706239225\n",
      "train loss:22.51805229183122\n",
      "train loss:22.482214497110633\n",
      "train loss:22.43747909330566\n",
      "train loss:22.39783758983483\n",
      "train loss:22.362638700617275\n",
      "train loss:22.31276617711238\n",
      "train loss:22.281608403261323\n",
      "train loss:22.241820978904762\n",
      "train loss:22.20024797308266\n",
      "train loss:22.164231679631925\n",
      "train loss:22.11497131601762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:22.08538434404327\n",
      "train loss:22.04464957015882\n",
      "train loss:22.000703946386345\n",
      "train loss:21.96778367261368\n",
      "train loss:21.925515391636857\n",
      "train loss:21.882117051815314\n",
      "train loss:21.848018741954846\n",
      "train loss:21.809116574646655\n",
      "train loss:21.773698302827672\n",
      "train loss:21.729550743141647\n",
      "train loss:21.694946505452968\n",
      "train loss:21.650419482625406\n",
      "train loss:21.61768627410693\n",
      "train loss:21.578431440864772\n",
      "train loss:21.536921091518348\n",
      "train loss:21.49933910239487\n",
      "train loss:21.45742565277782\n",
      "train loss:21.421961956116835\n",
      "train loss:21.381567729469722\n",
      "train loss:21.348572144219947\n",
      "train loss:21.302880283471364\n",
      "train loss:21.26810906779285\n",
      "train loss:21.229527053618625\n",
      "train loss:21.19745091368381\n",
      "train loss:21.155460624799062\n",
      "train loss:21.11353905431296\n",
      "train loss:21.085636138951106\n",
      "train loss:21.045063978852717\n",
      "train loss:21.009773095928935\n",
      "train loss:20.97151298649191\n",
      "train loss:20.929323584483488\n",
      "train loss:20.89536253006775\n",
      "train loss:20.85865822975021\n",
      "train loss:20.820783933122566\n",
      "train loss:20.78690770974358\n",
      "train loss:20.74295658548885\n",
      "train loss:20.70973795693862\n",
      "train loss:20.671544924934782\n",
      "train loss:20.634937798893244\n",
      "train loss:20.59500676027845\n",
      "train loss:20.562112587255037\n",
      "train loss:20.52577238078069\n",
      "train loss:20.492306378058064\n",
      "=== epoch:4, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:20.447847915465932\n",
      "train loss:20.418264526299215\n",
      "train loss:20.379300445597753\n",
      "train loss:20.349225467335447\n",
      "train loss:20.309878552613853\n",
      "train loss:20.27467752021871\n",
      "train loss:20.23655587247291\n",
      "train loss:20.199445222722407\n",
      "train loss:20.160484014966823\n",
      "train loss:20.129079959352218\n",
      "train loss:20.096570884553653\n",
      "train loss:20.051485382589973\n",
      "train loss:20.015082825176865\n",
      "train loss:19.986372490007096\n",
      "train loss:19.953566161586348\n",
      "train loss:19.916787006436852\n",
      "train loss:19.88106605044554\n",
      "train loss:19.847724555798344\n",
      "train loss:19.812069486729783\n",
      "train loss:19.776758870055616\n",
      "train loss:19.744220470267386\n",
      "train loss:19.708575128765307\n",
      "train loss:19.675634811533897\n",
      "train loss:19.631297721378594\n",
      "train loss:19.60850962129433\n",
      "train loss:19.568125749129326\n",
      "train loss:19.533899343725643\n",
      "train loss:19.505534497243826\n",
      "train loss:19.467413831309543\n",
      "train loss:19.42873054916939\n",
      "train loss:19.393299878923273\n",
      "train loss:19.352262946391626\n",
      "train loss:19.328348314725822\n",
      "train loss:19.29336928960506\n",
      "train loss:19.261172481267828\n",
      "train loss:19.223545759028678\n",
      "train loss:19.188135846556296\n",
      "train loss:19.15976073378244\n",
      "train loss:19.12356662706841\n",
      "train loss:19.096137649906748\n",
      "train loss:19.05847966667536\n",
      "train loss:19.021042753413198\n",
      "train loss:18.9927854937697\n",
      "train loss:18.95872576109949\n",
      "train loss:18.92780200509404\n",
      "train loss:18.894068136269247\n",
      "train loss:18.85515886920859\n",
      "train loss:18.82793229297681\n",
      "train loss:18.78683090080223\n",
      "train loss:18.760388749018546\n",
      "train loss:18.724597659833375\n",
      "train loss:18.69698394996536\n",
      "train loss:18.65603836014082\n",
      "train loss:18.6331443717034\n",
      "train loss:18.59862708541999\n",
      "train loss:18.558936317405742\n",
      "train loss:18.532631635945904\n",
      "train loss:18.4894376418629\n",
      "train loss:18.4652768705722\n",
      "train loss:18.42512583120186\n",
      "train loss:18.405115257563416\n",
      "train loss:18.366752028228007\n",
      "train loss:18.333734403305236\n",
      "train loss:18.302598201440063\n",
      "train loss:18.27692037933526\n",
      "train loss:18.243385354916718\n",
      "train loss:18.205904518034608\n",
      "train loss:18.173163759450006\n",
      "train loss:18.145698493616102\n",
      "train loss:18.112151297942667\n",
      "train loss:18.084115048444932\n",
      "train loss:18.05056776000567\n",
      "train loss:18.019578899103415\n",
      "train loss:17.98559135007413\n",
      "train loss:17.95831157549798\n",
      "train loss:17.92573209715534\n",
      "train loss:17.89407280826065\n",
      "train loss:17.86602471157288\n",
      "train loss:17.829609134105787\n",
      "train loss:17.79753344012795\n",
      "train loss:17.766703831160996\n",
      "train loss:17.741018413691467\n",
      "train loss:17.70983968091794\n",
      "train loss:17.673947496630348\n",
      "train loss:17.642706104112676\n",
      "train loss:17.61162091399878\n",
      "train loss:17.582545847275732\n",
      "train loss:17.55436688293633\n",
      "train loss:17.518716463423125\n",
      "train loss:17.489654806946863\n",
      "train loss:17.462005034293902\n",
      "train loss:17.43069443730895\n",
      "train loss:17.405262385645553\n",
      "train loss:17.37152829787544\n",
      "train loss:17.34716355112333\n",
      "train loss:17.31571569807561\n",
      "train loss:17.28011147459388\n",
      "train loss:17.252836564279345\n",
      "train loss:17.22396529827383\n",
      "train loss:17.195058206572618\n",
      "train loss:17.163776571679897\n",
      "train loss:17.133195160067004\n",
      "train loss:17.10393174537649\n",
      "train loss:17.070035709940722\n",
      "train loss:17.046193066663825\n",
      "train loss:17.016095120880532\n",
      "train loss:16.984980832217623\n",
      "train loss:16.958937617501203\n",
      "train loss:16.92620176069159\n",
      "train loss:16.898243585528114\n",
      "train loss:16.86854960237124\n",
      "train loss:16.840824069587587\n",
      "train loss:16.815310262361827\n",
      "train loss:16.778087634950868\n",
      "train loss:16.750524783097948\n",
      "train loss:16.72160710462914\n",
      "train loss:16.690953991838803\n",
      "train loss:16.66733345103416\n",
      "train loss:16.638836045443007\n",
      "train loss:16.605767580096103\n",
      "train loss:16.585875044350544\n",
      "train loss:16.549891075772013\n",
      "train loss:16.51567060973544\n",
      "train loss:16.49664465827573\n",
      "train loss:16.465475132168226\n",
      "train loss:16.435744210241324\n",
      "train loss:16.4129968969672\n",
      "train loss:16.380294132211482\n",
      "train loss:16.350837756666465\n",
      "train loss:16.326478182903145\n",
      "train loss:16.300233235249156\n",
      "train loss:16.272333843406432\n",
      "train loss:16.246735994504178\n",
      "train loss:16.211191376809047\n",
      "train loss:16.18578586797397\n",
      "train loss:16.15710309158259\n",
      "train loss:16.132467666061792\n",
      "train loss:16.10734526673263\n",
      "train loss:16.078231881786806\n",
      "train loss:16.041394860812282\n",
      "train loss:16.018875167794068\n",
      "train loss:15.992835305271607\n",
      "train loss:15.967145791944874\n",
      "train loss:15.931944214903744\n",
      "train loss:15.909693221628768\n",
      "train loss:15.880730002130061\n",
      "train loss:15.859241889906079\n",
      "train loss:15.830799697144604\n",
      "train loss:15.798342244906081\n",
      "train loss:15.771481419432527\n",
      "train loss:15.75037597836683\n",
      "train loss:15.719492546022652\n",
      "train loss:15.693036027651775\n",
      "train loss:15.665871716473761\n",
      "train loss:15.645796166206754\n",
      "train loss:15.620140921629213\n",
      "train loss:15.583921375963795\n",
      "train loss:15.563213202342215\n",
      "train loss:15.531928465789843\n",
      "train loss:15.506821078692433\n",
      "train loss:15.478172815069252\n",
      "train loss:15.460925134997002\n",
      "train loss:15.424901498037089\n",
      "train loss:15.408019462658547\n",
      "train loss:15.38381600347494\n",
      "train loss:15.35282799592025\n",
      "train loss:15.331161881868582\n",
      "train loss:15.295843426289096\n",
      "train loss:15.2752396832855\n",
      "train loss:15.245066849363358\n",
      "train loss:15.223455585952287\n",
      "train loss:15.196564643271621\n",
      "train loss:15.163931472723771\n",
      "train loss:15.140117867789392\n",
      "train loss:15.121546397827561\n",
      "train loss:15.090273798063258\n",
      "train loss:15.063623751850525\n",
      "train loss:15.048613890810994\n",
      "train loss:15.022088151318961\n",
      "train loss:14.992192470671242\n",
      "train loss:14.963932132249154\n",
      "train loss:14.936929392662432\n",
      "train loss:14.915542429305297\n",
      "train loss:14.88438621741939\n",
      "train loss:14.8601499542883\n",
      "train loss:14.8343286326499\n",
      "train loss:14.814883334709236\n",
      "train loss:14.780717681887698\n",
      "train loss:14.764518485598863\n",
      "train loss:14.742121096233515\n",
      "train loss:14.715172965495583\n",
      "train loss:14.688246748604968\n",
      "train loss:14.665325069443067\n",
      "train loss:14.635525103981966\n",
      "train loss:14.617987839993528\n",
      "train loss:14.595797211876082\n",
      "train loss:14.563547024538213\n",
      "train loss:14.547953410930319\n",
      "train loss:14.51778609510435\n",
      "train loss:14.489573363555738\n",
      "=== epoch:5, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:14.469077319097513\n",
      "train loss:14.448621621008177\n",
      "train loss:14.422035028298199\n",
      "train loss:14.39155500071457\n",
      "train loss:14.365509892372279\n",
      "train loss:14.351528582291076\n",
      "train loss:14.323473466697106\n",
      "train loss:14.29861627818504\n",
      "train loss:14.277224353256441\n",
      "train loss:14.249180700367997\n",
      "train loss:14.232230564440396\n",
      "train loss:14.206979494507912\n",
      "train loss:14.181793791691241\n",
      "train loss:14.152710539211467\n",
      "train loss:14.13734804127974\n",
      "train loss:14.112397265881114\n",
      "train loss:14.0850431300758\n",
      "train loss:14.060411321736407\n",
      "train loss:14.041249257414059\n",
      "train loss:14.01693332059022\n",
      "train loss:13.99155083720894\n",
      "train loss:13.967723967457744\n",
      "train loss:13.945116053726517\n",
      "train loss:13.918691017556153\n",
      "train loss:13.895274993375281\n",
      "train loss:13.879833556017617\n",
      "train loss:13.855240343059494\n",
      "train loss:13.829358790268854\n",
      "train loss:13.810223920005518\n",
      "train loss:13.776779696626395\n",
      "train loss:13.75703334945166\n",
      "train loss:13.733105122654647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:13.718077820578031\n",
      "train loss:13.683872343284282\n",
      "train loss:13.673110806308733\n",
      "train loss:13.649868121359024\n",
      "train loss:13.622821550425222\n",
      "train loss:13.602440851495428\n",
      "train loss:13.578462622130093\n",
      "train loss:13.552949419445738\n",
      "train loss:13.531592896442923\n",
      "train loss:13.510144768227553\n",
      "train loss:13.488180617188466\n",
      "train loss:13.467272677486243\n",
      "train loss:13.4434374868524\n",
      "train loss:13.427501443002406\n",
      "train loss:13.399946702194892\n",
      "train loss:13.376072766731365\n",
      "train loss:13.357876601155327\n",
      "train loss:13.334849323096378\n",
      "train loss:13.313376512221481\n",
      "train loss:13.289528477734184\n",
      "train loss:13.269200451932967\n",
      "train loss:13.246464491095384\n",
      "train loss:13.226204412723208\n",
      "train loss:13.198420086050017\n",
      "train loss:13.180882411731632\n",
      "train loss:13.160463599803254\n",
      "train loss:13.131206976336228\n",
      "train loss:13.11881278182138\n",
      "train loss:13.09182847041273\n",
      "train loss:13.066959559716002\n",
      "train loss:13.05003159308906\n",
      "train loss:13.025034862932863\n",
      "train loss:13.004713027981444\n",
      "train loss:12.990093840681613\n",
      "train loss:12.971023317637158\n",
      "train loss:12.940818757097707\n",
      "train loss:12.917140866139382\n",
      "train loss:12.900608539838046\n",
      "train loss:12.878030147216258\n",
      "train loss:12.855348399850357\n",
      "train loss:12.840158995809626\n",
      "train loss:12.820839937343822\n",
      "train loss:12.796025938316241\n",
      "train loss:12.773429618751921\n",
      "train loss:12.75162001805021\n",
      "train loss:12.732234850291544\n",
      "train loss:12.706304547543343\n",
      "train loss:12.696382593718386\n",
      "train loss:12.671443477381096\n",
      "train loss:12.653262948982556\n",
      "train loss:12.635371905738452\n",
      "train loss:12.604747411536646\n",
      "train loss:12.58395244573286\n",
      "train loss:12.56564785775015\n",
      "train loss:12.54640470072852\n",
      "train loss:12.524958444334272\n",
      "train loss:12.505193215736215\n",
      "train loss:12.490860950452367\n",
      "train loss:12.465651510689515\n",
      "train loss:12.446463150491594\n",
      "train loss:12.418773099963682\n",
      "train loss:12.402759744078027\n",
      "train loss:12.383910532135687\n",
      "train loss:12.361613355247199\n",
      "train loss:12.33979390188866\n",
      "train loss:12.320245264726136\n",
      "train loss:12.303166677949552\n",
      "train loss:12.275749171278362\n",
      "train loss:12.259635967879646\n",
      "train loss:12.2472871587653\n",
      "train loss:12.220045583032718\n",
      "train loss:12.202273512229944\n",
      "train loss:12.182974851878868\n",
      "train loss:12.164461132008078\n",
      "train loss:12.144879182291877\n",
      "train loss:12.118118057810022\n",
      "train loss:12.099539907929994\n",
      "train loss:12.084251421892251\n",
      "train loss:12.065096156605192\n",
      "train loss:12.052888953425832\n",
      "train loss:12.02758697405873\n",
      "train loss:12.009295159411261\n",
      "train loss:11.983412122216953\n",
      "train loss:11.970270944743623\n",
      "train loss:11.952824257149675\n",
      "train loss:11.926765413543897\n",
      "train loss:11.908142493374832\n",
      "train loss:11.89653280510763\n",
      "train loss:11.87793438744908\n",
      "train loss:11.849488509755432\n",
      "train loss:11.836365809118519\n",
      "train loss:11.819302160654436\n",
      "train loss:11.792451466265586\n",
      "train loss:11.776373529219327\n",
      "train loss:11.75896847073592\n",
      "train loss:11.735444888541839\n",
      "train loss:11.722568816188463\n",
      "train loss:11.69985181119906\n",
      "train loss:11.679230119778989\n",
      "train loss:11.66451352720144\n",
      "train loss:11.639730925015249\n",
      "train loss:11.622885072328337\n",
      "train loss:11.603218555002375\n",
      "train loss:11.595170700927863\n",
      "train loss:11.57446213550571\n",
      "train loss:11.553642255607244\n",
      "train loss:11.534800736784334\n",
      "train loss:11.515598955938955\n",
      "train loss:11.49689233586028\n",
      "train loss:11.479150794416306\n",
      "train loss:11.460542365552403\n",
      "train loss:11.441119854863894\n",
      "train loss:11.415490406027503\n",
      "train loss:11.405462799930113\n",
      "train loss:11.384805037279211\n",
      "train loss:11.373551186039352\n",
      "train loss:11.349945699618257\n",
      "train loss:11.332235454737805\n",
      "train loss:11.317429480280241\n",
      "train loss:11.295829467718441\n",
      "train loss:11.277770806721335\n",
      "train loss:11.260524084141762\n",
      "train loss:11.240960166342667\n",
      "train loss:11.220502128028112\n",
      "train loss:11.208283727500131\n",
      "train loss:11.185730865847608\n",
      "train loss:11.162286259973751\n",
      "train loss:11.159723195726158\n",
      "train loss:11.136788121945672\n",
      "train loss:11.117068809671865\n",
      "train loss:11.10127857027632\n",
      "train loss:11.0819064127834\n",
      "train loss:11.066399870913486\n",
      "train loss:11.036600882577495\n",
      "train loss:11.036059272886225\n",
      "train loss:11.013275259083851\n",
      "train loss:10.993378739617857\n",
      "train loss:10.985453370126258\n",
      "train loss:10.961194499590977\n",
      "train loss:10.946840790334576\n",
      "train loss:10.930165592531125\n",
      "train loss:10.912066831309893\n",
      "train loss:10.88834779393146\n",
      "train loss:10.875510702673008\n",
      "train loss:10.85771741658748\n",
      "train loss:10.832306689049402\n",
      "train loss:10.82378730364973\n",
      "train loss:10.81137772355684\n",
      "train loss:10.781934706689885\n",
      "train loss:10.766889499428872\n",
      "train loss:10.751035462396677\n",
      "train loss:10.737590330708086\n",
      "train loss:10.723155050494878\n",
      "train loss:10.698904350033139\n",
      "train loss:10.693673632585863\n",
      "train loss:10.674188876190897\n",
      "train loss:10.656930184677613\n",
      "train loss:10.631327271731081\n",
      "train loss:10.622861134772714\n",
      "train loss:10.600842940586425\n",
      "train loss:10.588370706641943\n",
      "train loss:10.566956192889826\n",
      "train loss:10.557611951386033\n",
      "train loss:10.53473297022615\n",
      "train loss:10.519554167266758\n",
      "train loss:10.507986715192175\n",
      "train loss:10.493211842359516\n",
      "train loss:10.47372384763781\n",
      "=== epoch:6, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:10.458360546220387\n",
      "train loss:10.439526302598903\n",
      "train loss:10.423177190437588\n",
      "train loss:10.405901000919172\n",
      "train loss:10.387744361654555\n",
      "train loss:10.369755700298326\n",
      "train loss:10.36342135392007\n",
      "train loss:10.338102839033834\n",
      "train loss:10.330160851676991\n",
      "train loss:10.308101864826025\n",
      "train loss:10.295312334744425\n",
      "train loss:10.286306091493996\n",
      "train loss:10.260031036395866\n",
      "train loss:10.242730458755918\n",
      "train loss:10.234632345955934\n",
      "train loss:10.215435663487192\n",
      "train loss:10.202803794231505\n",
      "train loss:10.17929101180005\n",
      "train loss:10.16861169715378\n",
      "train loss:10.154511721362727\n",
      "train loss:10.14132327234896\n",
      "train loss:10.115480552999824\n",
      "train loss:10.10357394414673\n",
      "train loss:10.086791165834578\n",
      "train loss:10.069500400107742\n",
      "train loss:10.06140815154696\n",
      "train loss:10.052231328389402\n",
      "train loss:10.025678368808466\n",
      "train loss:10.009891045208905\n",
      "train loss:9.997061793758897\n",
      "train loss:9.981798403865664\n",
      "train loss:9.968694172870938\n",
      "train loss:9.954886528321161\n",
      "train loss:9.925419933653622\n",
      "train loss:9.92276656057686\n",
      "train loss:9.909954733212528\n",
      "train loss:9.892284024234147\n",
      "train loss:9.871966954916852\n",
      "train loss:9.859682168705545\n",
      "train loss:9.839813813572967\n",
      "train loss:9.833195813480756\n",
      "train loss:9.81779055591519\n",
      "train loss:9.798128447477788\n",
      "train loss:9.784646424611726\n",
      "train loss:9.766336227653744\n",
      "train loss:9.757814122869455\n",
      "train loss:9.739111582499326\n",
      "train loss:9.726328949554267\n",
      "train loss:9.713061838991429\n",
      "train loss:9.69759787812189\n",
      "train loss:9.681178649214186\n",
      "train loss:9.67266608100115\n",
      "train loss:9.654116575822588\n",
      "train loss:9.634065762974464\n",
      "train loss:9.620802887663125\n",
      "train loss:9.613190831965083\n",
      "train loss:9.588411844879786\n",
      "train loss:9.57730654292522\n",
      "train loss:9.56866693636832\n",
      "train loss:9.545155524587285\n",
      "train loss:9.534402729650402\n",
      "train loss:9.521062927066078\n",
      "train loss:9.502723081343614\n",
      "train loss:9.49030614189306\n",
      "train loss:9.475089801485007\n",
      "train loss:9.457711265901926\n",
      "train loss:9.450700162643628\n",
      "train loss:9.433624102281193\n",
      "train loss:9.414873343039593\n",
      "train loss:9.400412110575491\n",
      "train loss:9.391799440382322\n",
      "train loss:9.372564627064426\n",
      "train loss:9.358431271602699\n",
      "train loss:9.349531155671144\n",
      "train loss:9.335636960768294\n",
      "train loss:9.319473894559813\n",
      "train loss:9.311593128095975\n",
      "train loss:9.289175849850713\n",
      "train loss:9.280236951760639\n",
      "train loss:9.266171838479632\n",
      "train loss:9.245786048425757\n",
      "train loss:9.240140531234342\n",
      "train loss:9.224139231841198\n",
      "train loss:9.206352431810997\n",
      "train loss:9.19662506493479\n",
      "train loss:9.176543524652732\n",
      "train loss:9.166927568547115\n",
      "train loss:9.15430605490133\n",
      "train loss:9.141131964200843\n",
      "train loss:9.127347114003255\n",
      "train loss:9.114434752344964\n",
      "train loss:9.104838200121549\n",
      "train loss:9.08490437994436\n",
      "train loss:9.066995203772482\n",
      "train loss:9.061651741658359\n",
      "train loss:9.044319810202781\n",
      "train loss:9.030524722359598\n",
      "train loss:9.019503963456177\n",
      "train loss:9.010762720672206\n",
      "train loss:8.989515684548982\n",
      "train loss:8.978064922652083\n",
      "train loss:8.969739452540368\n",
      "train loss:8.95477461485102\n",
      "train loss:8.941883217128261\n",
      "train loss:8.923846048646107\n",
      "train loss:8.909671635730271\n",
      "train loss:8.896431778887411\n",
      "train loss:8.8885233801365\n",
      "train loss:8.872385568353527\n",
      "train loss:8.854515295787907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:8.852589060357253\n",
      "train loss:8.831706487584531\n",
      "train loss:8.823731235326527\n",
      "train loss:8.805667059366721\n",
      "train loss:8.79656154061816\n",
      "train loss:8.78019260581134\n",
      "train loss:8.766428879224614\n",
      "train loss:8.754963367228491\n",
      "train loss:8.736914022494403\n",
      "train loss:8.728534913109913\n",
      "train loss:8.71417286229424\n",
      "train loss:8.70270035422999\n",
      "train loss:8.6842455950503\n",
      "train loss:8.679101392428356\n",
      "train loss:8.665504707726049\n",
      "train loss:8.650231085804421\n",
      "train loss:8.642767053368031\n",
      "train loss:8.623322640671036\n",
      "train loss:8.609628081397215\n",
      "train loss:8.602097484484457\n",
      "train loss:8.597426892566244\n",
      "train loss:8.580906825909599\n",
      "train loss:8.567289751963918\n",
      "train loss:8.555515344776037\n",
      "train loss:8.543247590192758\n",
      "train loss:8.532546531350242\n",
      "train loss:8.506855933798235\n",
      "train loss:8.502209345743111\n",
      "train loss:8.492266181409576\n",
      "train loss:8.476290576372389\n",
      "train loss:8.469707998849302\n",
      "train loss:8.450427156905\n",
      "train loss:8.438218401765587\n",
      "train loss:8.434131375412033\n",
      "train loss:8.412266621003605\n",
      "train loss:8.40365416319702\n",
      "train loss:8.39387090379218\n",
      "train loss:8.37504971617397\n",
      "train loss:8.358844573292833\n",
      "train loss:8.358981830006405\n",
      "train loss:8.339661203022786\n",
      "train loss:8.333288038257962\n",
      "train loss:8.313164365150696\n",
      "train loss:8.305769773198978\n",
      "train loss:8.297805748243999\n",
      "train loss:8.28171995559714\n",
      "train loss:8.278320158218621\n",
      "train loss:8.264623470582293\n",
      "train loss:8.247755619949205\n",
      "train loss:8.232478395534372\n",
      "train loss:8.224898865673016\n",
      "train loss:8.209953198012569\n",
      "train loss:8.198313482923515\n",
      "train loss:8.18102082368404\n",
      "train loss:8.176489380588938\n",
      "train loss:8.159975491291412\n",
      "train loss:8.151135111957862\n",
      "train loss:8.139371779148595\n",
      "train loss:8.131836298627253\n",
      "train loss:8.113272238240587\n",
      "train loss:8.097265181597852\n",
      "train loss:8.098531229312718\n",
      "train loss:8.08843840384045\n",
      "train loss:8.070993843407539\n",
      "train loss:8.064215164722889\n",
      "train loss:8.050480666596345\n",
      "train loss:8.0309779365205\n",
      "train loss:8.021168227012993\n",
      "train loss:8.016267476082074\n",
      "train loss:8.00102699506902\n",
      "train loss:7.9851880574938825\n",
      "train loss:7.973811439723724\n",
      "train loss:7.97039090362222\n",
      "train loss:7.964347578422487\n",
      "train loss:7.94681218682873\n",
      "train loss:7.9285371364787665\n",
      "train loss:7.9260583084535705\n",
      "train loss:7.912082949039347\n",
      "train loss:7.9010922229979865\n",
      "train loss:7.888945592084607\n",
      "train loss:7.873218319049022\n",
      "train loss:7.856270263187877\n",
      "train loss:7.854880835630084\n",
      "train loss:7.846661537039651\n",
      "train loss:7.839412123881656\n",
      "train loss:7.832140996923409\n",
      "train loss:7.805955622918429\n",
      "train loss:7.797421821934619\n",
      "train loss:7.784692879850844\n",
      "train loss:7.777496905989288\n",
      "=== epoch:7, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:7.770700616406131\n",
      "train loss:7.7552982534825645\n",
      "train loss:7.746231596412594\n",
      "train loss:7.734366331256411\n",
      "train loss:7.731818904840237\n",
      "train loss:7.7158912698985755\n",
      "train loss:7.701571998068146\n",
      "train loss:7.685942460870046\n",
      "train loss:7.683229190798043\n",
      "train loss:7.671941914304271\n",
      "train loss:7.659283332001338\n",
      "train loss:7.646377367668325\n",
      "train loss:7.644999408532348\n",
      "train loss:7.630409391331561\n",
      "train loss:7.618933982273766\n",
      "train loss:7.606145529337493\n",
      "train loss:7.593360695272104\n",
      "train loss:7.57902347930881\n",
      "train loss:7.573021419970139\n",
      "train loss:7.558077084318247\n",
      "train loss:7.548671995379117\n",
      "train loss:7.545155718123025\n",
      "train loss:7.536939562405051\n",
      "train loss:7.529098193709358\n",
      "train loss:7.506120859407243\n",
      "train loss:7.500993793300801\n",
      "train loss:7.490546387619624\n",
      "train loss:7.4872805571040395\n",
      "train loss:7.476189826445074\n",
      "train loss:7.45852518215486\n",
      "train loss:7.4524909514343705\n",
      "train loss:7.442378863733369\n",
      "train loss:7.4264250523477715\n",
      "train loss:7.421111643369093\n",
      "train loss:7.412080254870092\n",
      "train loss:7.393699739578307\n",
      "train loss:7.390712459228785\n",
      "train loss:7.373632574777627\n",
      "train loss:7.36903459285984\n",
      "train loss:7.356630259010267\n",
      "train loss:7.351717203192759\n",
      "train loss:7.32550306956917\n",
      "train loss:7.330498020286802\n",
      "train loss:7.317177917962134\n",
      "train loss:7.30941030037494\n",
      "train loss:7.297287515928289\n",
      "train loss:7.285172008145672\n",
      "train loss:7.284558142652996\n",
      "train loss:7.265722052982587\n",
      "train loss:7.2618139893431835\n",
      "train loss:7.255610508207006\n",
      "train loss:7.236617367276946\n",
      "train loss:7.230219971725552\n",
      "train loss:7.218676608613038\n",
      "train loss:7.203293602422553\n",
      "train loss:7.194038390154402\n",
      "train loss:7.187696066504772\n",
      "train loss:7.174079554845685\n",
      "train loss:7.170642515163985\n",
      "train loss:7.1604758267270405\n",
      "train loss:7.1529729056827165\n",
      "train loss:7.135691196220775\n",
      "train loss:7.129569728147912\n",
      "train loss:7.116879099827889\n",
      "train loss:7.110253923947352\n",
      "train loss:7.098288018777866\n",
      "train loss:7.086179617834095\n",
      "train loss:7.082409537209921\n",
      "train loss:7.070874650731719\n",
      "train loss:7.055003552993444\n",
      "train loss:7.055079272865749\n",
      "train loss:7.0339567284409785\n",
      "train loss:7.032107313566071\n",
      "train loss:7.023286225435744\n",
      "train loss:7.018356670454452\n",
      "train loss:7.001238992474677\n",
      "train loss:7.000614527733194\n",
      "train loss:6.987511299077248\n",
      "train loss:6.980624994440223\n",
      "train loss:6.962346887685706\n",
      "train loss:6.9631089456411885\n",
      "train loss:6.951213385690255\n",
      "train loss:6.943854357929771\n",
      "train loss:6.935954213010239\n",
      "train loss:6.92174039420563\n",
      "train loss:6.912066664040082\n",
      "train loss:6.902036043938843\n",
      "train loss:6.900682423945762\n",
      "train loss:6.891260159080953\n",
      "train loss:6.8795443673107055\n",
      "train loss:6.866437882928539\n",
      "train loss:6.85421487511422\n",
      "train loss:6.846747040008019\n",
      "train loss:6.841497797781114\n",
      "train loss:6.835249729468492\n",
      "train loss:6.823854777098687\n",
      "train loss:6.811253693089176\n",
      "train loss:6.805205222987535\n",
      "train loss:6.7959841543710855\n",
      "train loss:6.787925721699882\n",
      "train loss:6.7777543623692456\n",
      "train loss:6.766632384867154\n",
      "train loss:6.756891216067857\n",
      "train loss:6.7413313522706275\n",
      "train loss:6.750162567071462\n",
      "train loss:6.731668629271943\n",
      "train loss:6.72052477682099\n",
      "train loss:6.717252736206823\n",
      "train loss:6.700531203255633\n",
      "train loss:6.6937359695666245\n",
      "train loss:6.6812907971305595\n",
      "train loss:6.6797035487832055\n",
      "train loss:6.666871771647779\n",
      "train loss:6.660982473425046\n",
      "train loss:6.652473184841724\n",
      "train loss:6.6421110136240475\n",
      "train loss:6.640193551578479\n",
      "train loss:6.628858119835431\n",
      "train loss:6.6147265929243915\n",
      "train loss:6.605256149267919\n",
      "train loss:6.598769545168724\n",
      "train loss:6.588298498617908\n",
      "train loss:6.579765091349051\n",
      "train loss:6.574645466154151\n",
      "train loss:6.5682984235245545\n",
      "train loss:6.55715675068279\n",
      "train loss:6.5423161200016136\n",
      "train loss:6.5461334848706105\n",
      "train loss:6.531650885588257\n",
      "train loss:6.519462766866915\n",
      "train loss:6.514601161559716\n",
      "train loss:6.507457279710925\n",
      "train loss:6.4946149707461185\n",
      "train loss:6.485465505419448\n",
      "train loss:6.484133623848924\n",
      "train loss:6.468889805804704\n",
      "train loss:6.4714447400348\n",
      "train loss:6.462705934663785\n",
      "train loss:6.449159532717189\n",
      "train loss:6.443997787645424\n",
      "train loss:6.4394053135945875\n",
      "train loss:6.431209455122623\n",
      "train loss:6.417962565041776\n",
      "train loss:6.405083741603985\n",
      "train loss:6.396196075878942\n",
      "train loss:6.392880065557749\n",
      "train loss:6.383207529563716\n",
      "train loss:6.379774569566657\n",
      "train loss:6.369159490389713\n",
      "train loss:6.357298842817732\n",
      "train loss:6.350361876373045\n",
      "train loss:6.34116520272397\n",
      "train loss:6.336855944739513\n",
      "train loss:6.322928184992044\n",
      "train loss:6.319774949102946\n",
      "train loss:6.30939002277082\n",
      "train loss:6.30029846344769\n",
      "train loss:6.29233779999632\n",
      "train loss:6.284893920495309\n",
      "train loss:6.272654245098803\n",
      "train loss:6.27466890718879\n",
      "train loss:6.2611597410517845\n",
      "train loss:6.246135019567312\n",
      "train loss:6.243744132165921\n",
      "train loss:6.244530870280189\n",
      "train loss:6.226391017511921\n",
      "train loss:6.22079287299063\n",
      "train loss:6.214328096530929\n",
      "train loss:6.214685273253178\n",
      "train loss:6.192540658304925\n",
      "train loss:6.192587293279786\n",
      "train loss:6.175114118882169\n",
      "train loss:6.175089332010989\n",
      "train loss:6.164731469475656\n",
      "train loss:6.15885687513404\n",
      "train loss:6.160058660111304\n",
      "train loss:6.13925446472148\n",
      "train loss:6.141305285079999\n",
      "train loss:6.128194262707822\n",
      "train loss:6.116589696336497\n",
      "train loss:6.112209606916382\n",
      "train loss:6.115549639819972\n",
      "train loss:6.093521977184323\n",
      "train loss:6.089312977115826\n",
      "train loss:6.079932270010248\n",
      "train loss:6.0745484182365015\n",
      "train loss:6.071527846604624\n",
      "train loss:6.059956740825395\n",
      "train loss:6.050879547924982\n",
      "train loss:6.049978483402621\n",
      "train loss:6.041730991518648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:6.027581556963465\n",
      "train loss:6.019862890222976\n",
      "train loss:6.013103694733779\n",
      "train loss:6.004909221713458\n",
      "train loss:6.009127379506394\n",
      "train loss:5.997140226915706\n",
      "train loss:5.987441378495852\n",
      "train loss:5.978919184247748\n",
      "train loss:5.973441563285521\n",
      "=== epoch:8, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:5.971094325612192\n",
      "train loss:5.953195541276447\n",
      "train loss:5.944563396710171\n",
      "train loss:5.937195964172078\n",
      "train loss:5.942233514611315\n",
      "train loss:5.928635347719544\n",
      "train loss:5.918327007419592\n",
      "train loss:5.9203214773076835\n",
      "train loss:5.903370334113078\n",
      "train loss:5.89301479273543\n",
      "train loss:5.897642384866228\n",
      "train loss:5.886906781527463\n",
      "train loss:5.88057639858885\n",
      "train loss:5.874699883409685\n",
      "train loss:5.861447437321277\n",
      "train loss:5.855899903986732\n",
      "train loss:5.851136523747554\n",
      "train loss:5.848403515310965\n",
      "train loss:5.834283556707221\n",
      "train loss:5.826343854364003\n",
      "train loss:5.816682495747789\n",
      "train loss:5.816640594234545\n",
      "train loss:5.799221580393279\n",
      "train loss:5.798596632161289\n",
      "train loss:5.793009883662435\n",
      "train loss:5.787363415986292\n",
      "train loss:5.775418751504153\n",
      "train loss:5.769122353372778\n",
      "train loss:5.7705297151751616\n",
      "train loss:5.770741234669539\n",
      "train loss:5.753216945558883\n",
      "train loss:5.7426984582185145\n",
      "train loss:5.7309773991555755\n",
      "train loss:5.73586908464367\n",
      "train loss:5.721725732744382\n",
      "train loss:5.71554842659655\n",
      "train loss:5.713819268411155\n",
      "train loss:5.701695847931574\n",
      "train loss:5.687992498479\n",
      "train loss:5.682692668379133\n",
      "train loss:5.679823138944867\n",
      "train loss:5.683683524317561\n",
      "train loss:5.664836140826914\n",
      "train loss:5.664885733798926\n",
      "train loss:5.661892615958601\n",
      "train loss:5.649638440059196\n",
      "train loss:5.645322756334015\n",
      "train loss:5.640972098402038\n",
      "train loss:5.627148756691591\n",
      "train loss:5.617980387368065\n",
      "train loss:5.616250823882849\n",
      "train loss:5.606182993274091\n",
      "train loss:5.600773902385077\n",
      "train loss:5.59506714359361\n",
      "train loss:5.5896836570688375\n",
      "train loss:5.582555338327845\n",
      "train loss:5.57288199006157\n",
      "train loss:5.572731683351643\n",
      "train loss:5.5652065590829505\n",
      "train loss:5.560513051889162\n",
      "train loss:5.55518156934212\n",
      "train loss:5.54614741138665\n",
      "train loss:5.536956347618704\n",
      "train loss:5.526845085125808\n",
      "train loss:5.522126969225603\n",
      "train loss:5.516437527030732\n",
      "train loss:5.511133461419205\n",
      "train loss:5.497659981538444\n",
      "train loss:5.498057909400355\n",
      "train loss:5.491462099816163\n",
      "train loss:5.48862367944475\n",
      "train loss:5.476621984742887\n",
      "train loss:5.472268987777884\n",
      "train loss:5.469977155240514\n",
      "train loss:5.45235065287569\n",
      "train loss:5.451513149336347\n",
      "train loss:5.451982475574368\n",
      "train loss:5.443078554840726\n",
      "train loss:5.439002333697125\n",
      "train loss:5.430532686459761\n",
      "train loss:5.425699787411071\n",
      "train loss:5.414012156836273\n",
      "train loss:5.408907526399609\n",
      "train loss:5.403491413589623\n",
      "train loss:5.398259896240178\n",
      "train loss:5.3800314763123565\n",
      "train loss:5.3889887010285165\n",
      "train loss:5.378891049090655\n",
      "train loss:5.3704594013627185\n",
      "train loss:5.362789745730797\n",
      "train loss:5.36221877858151\n",
      "train loss:5.348596540346708\n",
      "train loss:5.348646949631855\n",
      "train loss:5.336177521812589\n",
      "train loss:5.343302814524707\n",
      "train loss:5.329619271894655\n",
      "train loss:5.3256332024930355\n",
      "train loss:5.316839722726935\n",
      "train loss:5.3139802222625185\n",
      "train loss:5.306491759328826\n",
      "train loss:5.3025672669613435\n",
      "train loss:5.2889093844253985\n",
      "train loss:5.286743493735183\n",
      "train loss:5.281870016111034\n",
      "train loss:5.28159125902482\n",
      "train loss:5.2768155120771425\n",
      "train loss:5.270484430439066\n",
      "train loss:5.259503143918366\n",
      "train loss:5.254609297552423\n",
      "train loss:5.243395599452432\n",
      "train loss:5.243205817711189\n",
      "train loss:5.237066563924611\n",
      "train loss:5.228749383971765\n",
      "train loss:5.225575649476102\n",
      "train loss:5.2193949265224475\n",
      "train loss:5.213136691171836\n",
      "train loss:5.20363397074877\n",
      "train loss:5.1950402678726455\n",
      "train loss:5.2008745588914085\n",
      "train loss:5.193972153015562\n",
      "train loss:5.179550192229635\n",
      "train loss:5.178804196540641\n",
      "train loss:5.165158262958146\n",
      "train loss:5.165881686665447\n",
      "train loss:5.1567404572709545\n",
      "train loss:5.158557160247412\n",
      "train loss:5.152224490217225\n",
      "train loss:5.1410151039973995\n",
      "train loss:5.137325296447643\n",
      "train loss:5.1319524747794985\n",
      "train loss:5.126556210959602\n",
      "train loss:5.121431294138766\n",
      "train loss:5.114553453198253\n",
      "train loss:5.1062047206732935\n",
      "train loss:5.099263810620451\n",
      "train loss:5.094445073469645\n",
      "train loss:5.0844950614404425\n",
      "train loss:5.079957158279528\n",
      "train loss:5.079045197552576\n",
      "train loss:5.0717447792000865\n",
      "train loss:5.070228290158518\n",
      "train loss:5.069513384079576\n",
      "train loss:5.062554310666037\n",
      "train loss:5.058665394914529\n",
      "train loss:5.046937398376768\n",
      "train loss:5.043073578896982\n",
      "train loss:5.037302608124878\n",
      "train loss:5.022376130868995\n",
      "train loss:5.025568477919766\n",
      "train loss:5.026853151551554\n",
      "train loss:5.011916439582608\n",
      "train loss:5.009445336232575\n",
      "train loss:5.008560871383112\n",
      "train loss:4.994547331111956\n",
      "train loss:4.995692647233318\n",
      "train loss:4.980940069711577\n",
      "train loss:4.978361473819848\n",
      "train loss:4.9795440514139635\n",
      "train loss:4.969873158314938\n",
      "train loss:4.959555916052681\n",
      "train loss:4.960934897159365\n",
      "train loss:4.948803758726937\n",
      "train loss:4.950317432054787\n",
      "train loss:4.950906051587744\n",
      "train loss:4.942356970348585\n",
      "train loss:4.93189019291382\n",
      "train loss:4.934325329002797\n",
      "train loss:4.929776242872816\n",
      "train loss:4.921175113681869\n",
      "train loss:4.914376713241817\n",
      "train loss:4.901614787764346\n",
      "train loss:4.905395032406487\n",
      "train loss:4.904695983239323\n",
      "train loss:4.89101960572301\n",
      "train loss:4.889187442875768\n",
      "train loss:4.882859767681842\n",
      "train loss:4.8808509477198605\n",
      "train loss:4.875020936092923\n",
      "train loss:4.867952349738083\n",
      "train loss:4.868853757017179\n",
      "train loss:4.8573225349884765\n",
      "train loss:4.852951153314471\n",
      "train loss:4.844271735223329\n",
      "train loss:4.8389631314367785\n",
      "train loss:4.838825975806692\n",
      "train loss:4.833018681816386\n",
      "train loss:4.823481890681148\n",
      "train loss:4.81845391898465\n",
      "train loss:4.815018230712348\n",
      "train loss:4.8110199519921455\n",
      "train loss:4.802425046850416\n",
      "train loss:4.799796968819612\n",
      "train loss:4.7923926340849485\n",
      "train loss:4.788763048280877\n",
      "train loss:4.784348764845366\n",
      "train loss:4.77606817650396\n",
      "train loss:4.776894320753529\n",
      "train loss:4.775208806182445\n",
      "train loss:4.773408884333762\n",
      "train loss:4.766357240013377\n",
      "=== epoch:9, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:4.7516215389943435\n",
      "train loss:4.752731668675324\n",
      "train loss:4.750896161276935\n",
      "train loss:4.744964552372393\n",
      "train loss:4.7384027976396474\n",
      "train loss:4.733949004735441\n",
      "train loss:4.735013499704319\n",
      "train loss:4.727853260063576\n",
      "train loss:4.714868170166822\n",
      "train loss:4.7090252356967675\n",
      "train loss:4.710417597675352\n",
      "train loss:4.7014531148023195\n",
      "train loss:4.7011037754249\n",
      "train loss:4.692072044445984\n",
      "train loss:4.689396019974893\n",
      "train loss:4.688715899001207\n",
      "train loss:4.683582204996002\n",
      "train loss:4.673972021632439\n",
      "train loss:4.676105394089511\n",
      "train loss:4.662087557014013\n",
      "train loss:4.663699514373063\n",
      "train loss:4.66007450827596\n",
      "train loss:4.6488443723769635\n",
      "train loss:4.644362702977253\n",
      "train loss:4.640896848471497\n",
      "train loss:4.6337539082079715\n",
      "train loss:4.629148084448294\n",
      "train loss:4.6265872620339845\n",
      "train loss:4.632074740576719\n",
      "train loss:4.609856422925228\n",
      "train loss:4.621917478497256\n",
      "train loss:4.613576009121688\n",
      "train loss:4.598653705850504\n",
      "train loss:4.596476767733634\n",
      "train loss:4.589417144688178\n",
      "train loss:4.592014950568549\n",
      "train loss:4.585134317377817\n",
      "train loss:4.580620853248611\n",
      "train loss:4.582040995772083\n",
      "train loss:4.571866693492687\n",
      "train loss:4.569343719285255\n",
      "train loss:4.560229398289974\n",
      "train loss:4.55499644752314\n",
      "train loss:4.556845765007753\n",
      "train loss:4.5473515868438685\n",
      "train loss:4.547008309050598\n",
      "train loss:4.545698244264358\n",
      "train loss:4.532346814227584\n",
      "train loss:4.535002878406059\n",
      "train loss:4.524652673919757\n",
      "train loss:4.522702122009823\n",
      "train loss:4.5181212704076135\n",
      "train loss:4.5101466411109605\n",
      "train loss:4.509744208455764\n",
      "train loss:4.506631122910315\n",
      "train loss:4.5043073939825655\n",
      "train loss:4.494918683623551\n",
      "train loss:4.490860164675511\n",
      "train loss:4.496615369194849\n",
      "train loss:4.484425354933757\n",
      "train loss:4.483445117272947\n",
      "train loss:4.478758718345572\n",
      "train loss:4.461293520107996\n",
      "train loss:4.465613304323894\n",
      "train loss:4.461812505804899\n",
      "train loss:4.460644935970416\n",
      "train loss:4.458741799363466\n",
      "train loss:4.451675748239452\n",
      "train loss:4.451722422910801\n",
      "train loss:4.446516337935885\n",
      "train loss:4.434288844284913\n",
      "train loss:4.4289572946075015\n",
      "train loss:4.425977368855374\n",
      "train loss:4.421916452129477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.411989481216278\n",
      "train loss:4.409308950017884\n",
      "train loss:4.419323624200198\n",
      "train loss:4.403561594880346\n",
      "train loss:4.399213028685331\n",
      "train loss:4.399508154555748\n",
      "train loss:4.392493689025842\n",
      "train loss:4.387891453205306\n",
      "train loss:4.3874928999928\n",
      "train loss:4.380586007791448\n",
      "train loss:4.378014393143074\n",
      "train loss:4.374263732617671\n",
      "train loss:4.368679721892127\n",
      "train loss:4.368664725612019\n",
      "train loss:4.351497454724864\n",
      "train loss:4.356219020434672\n",
      "train loss:4.352640014190239\n",
      "train loss:4.347666133309712\n",
      "train loss:4.333365890097764\n",
      "train loss:4.342832540179881\n",
      "train loss:4.334511407601141\n",
      "train loss:4.324117091966537\n",
      "train loss:4.329781723423162\n",
      "train loss:4.320598246393048\n",
      "train loss:4.317756539501805\n",
      "train loss:4.319460575286518\n",
      "train loss:4.3169920852904085\n",
      "train loss:4.3052476707963745\n",
      "train loss:4.3026260990673775\n",
      "train loss:4.292492407292315\n",
      "train loss:4.292795312910624\n",
      "train loss:4.293159064275029\n",
      "train loss:4.288030662053297\n",
      "train loss:4.281640530750185\n",
      "train loss:4.280434798189731\n",
      "train loss:4.2739654629103185\n",
      "train loss:4.274837119868829\n",
      "train loss:4.269899815237357\n",
      "train loss:4.264813549893406\n",
      "train loss:4.256152236843468\n",
      "train loss:4.25279743764793\n",
      "train loss:4.252807680041496\n",
      "train loss:4.246260212456684\n",
      "train loss:4.239724718057428\n",
      "train loss:4.240698129911835\n",
      "train loss:4.242298019429981\n",
      "train loss:4.227852284487046\n",
      "train loss:4.231156449576515\n",
      "train loss:4.224119035940912\n",
      "train loss:4.218909020960959\n",
      "train loss:4.216100305643987\n",
      "train loss:4.2118801613584065\n",
      "train loss:4.218811053118916\n",
      "train loss:4.203275174150074\n",
      "train loss:4.202125946232844\n",
      "train loss:4.202723603674329\n",
      "train loss:4.19063362892091\n",
      "train loss:4.1894541960244585\n",
      "train loss:4.182116057036374\n",
      "train loss:4.182471293806504\n",
      "train loss:4.181370018292687\n",
      "train loss:4.168310153287144\n",
      "train loss:4.165149596098653\n",
      "train loss:4.174919096440474\n",
      "train loss:4.163640741631591\n",
      "train loss:4.159804736280261\n",
      "train loss:4.155974293744573\n",
      "train loss:4.153348856149748\n",
      "train loss:4.1466785792845045\n",
      "train loss:4.147577816487809\n",
      "train loss:4.1420839680763075\n",
      "train loss:4.137639536781038\n",
      "train loss:4.133500701648099\n",
      "train loss:4.131603464210319\n",
      "train loss:4.125191625387506\n",
      "train loss:4.1296332218087874\n",
      "train loss:4.118705320526348\n",
      "train loss:4.112243613723497\n",
      "train loss:4.110186723092582\n",
      "train loss:4.113922882061068\n",
      "train loss:4.101863377378139\n",
      "train loss:4.102715607424104\n",
      "train loss:4.097700134290901\n",
      "train loss:4.096340832310427\n",
      "train loss:4.090115516144742\n",
      "train loss:4.09180068995523\n",
      "train loss:4.084150687631068\n",
      "train loss:4.091575622328947\n",
      "train loss:4.080890743116033\n",
      "train loss:4.070187511938494\n",
      "train loss:4.071921754118906\n",
      "train loss:4.06258019159424\n",
      "train loss:4.061741732426232\n",
      "train loss:4.053996225419445\n",
      "train loss:4.05470311646347\n",
      "train loss:4.047947592766368\n",
      "train loss:4.049272730894094\n",
      "train loss:4.048850485179206\n",
      "train loss:4.042261607591765\n",
      "train loss:4.0418078553866845\n",
      "train loss:4.032721832852787\n",
      "train loss:4.032304084515525\n",
      "train loss:4.033567491610999\n",
      "train loss:4.016621250522519\n",
      "train loss:4.022471057729311\n",
      "train loss:4.018915831304724\n",
      "train loss:4.015000591109672\n",
      "train loss:4.012357170204686\n",
      "train loss:4.01032713225762\n",
      "train loss:4.000910063814395\n",
      "train loss:4.003714681326626\n",
      "train loss:3.990233043690039\n",
      "train loss:3.994807020185224\n",
      "train loss:3.992959487113684\n",
      "train loss:3.9876790300307228\n",
      "train loss:3.987178417731511\n",
      "train loss:3.981472886451233\n",
      "train loss:3.9680753935869397\n",
      "train loss:3.975986138546845\n",
      "train loss:3.968796953354946\n",
      "train loss:3.963476980496998\n",
      "train loss:3.971424556137103\n",
      "train loss:3.9644572294367455\n",
      "train loss:3.956704823729882\n",
      "train loss:3.953218771012393\n",
      "train loss:3.9418162244428157\n",
      "=== epoch:10, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:3.9484911745160316\n",
      "train loss:3.9446282981063336\n",
      "train loss:3.9343589304522686\n",
      "train loss:3.9426851132296097\n",
      "train loss:3.936860141217961\n",
      "train loss:3.9267156166351245\n",
      "train loss:3.9225758049298083\n",
      "train loss:3.926794224846149\n",
      "train loss:3.905206720433648\n",
      "train loss:3.918808202690361\n",
      "train loss:3.9127155132353204\n",
      "train loss:3.9133661994610165\n",
      "train loss:3.904538483927366\n",
      "train loss:3.912465250554365\n",
      "train loss:3.90721856049852\n",
      "train loss:3.9049636944220274\n",
      "train loss:3.893121536441247\n",
      "train loss:3.8916323760447913\n",
      "train loss:3.8896318399553995\n",
      "train loss:3.8866258105272635\n",
      "train loss:3.882163560723003\n",
      "train loss:3.8876010222109967\n",
      "train loss:3.8742490298084418\n",
      "train loss:3.8780517474470715\n",
      "train loss:3.8685528036797496\n",
      "train loss:3.8596389336175925\n",
      "train loss:3.859137418251591\n",
      "train loss:3.855566933731766\n",
      "train loss:3.865333441191901\n",
      "train loss:3.8538596432162273\n",
      "train loss:3.847743385065261\n",
      "train loss:3.8496270849660297\n",
      "train loss:3.8507196065696014\n",
      "train loss:3.8373214043146424\n",
      "train loss:3.83697757696733\n",
      "train loss:3.820077735949517\n",
      "train loss:3.8322311070821398\n",
      "train loss:3.826189365009923\n",
      "train loss:3.8196494800114404\n",
      "train loss:3.8287710778736175\n",
      "train loss:3.820485144759961\n",
      "train loss:3.821592948884986\n",
      "train loss:3.8109663363665383\n",
      "train loss:3.8152303976068738\n",
      "train loss:3.8056088334480944\n",
      "train loss:3.8118973785333594\n",
      "train loss:3.8056402703513466\n",
      "train loss:3.7998625317903416\n",
      "train loss:3.806028548827099\n",
      "train loss:3.7994585797432947\n",
      "train loss:3.787463953742858\n",
      "train loss:3.78627656748838\n",
      "train loss:3.7761578893367025\n",
      "train loss:3.7826116341321856\n",
      "train loss:3.772691757015859\n",
      "train loss:3.7692132926467785\n",
      "train loss:3.769117555304242\n",
      "train loss:3.769698059118645\n",
      "train loss:3.765536316191946\n",
      "train loss:3.7733166658801\n",
      "train loss:3.7603770503329015\n",
      "train loss:3.753484503454131\n",
      "train loss:3.7454060443489183\n",
      "train loss:3.7550999491285135\n",
      "train loss:3.7494128847561625\n",
      "train loss:3.7447335257447323\n",
      "train loss:3.7386345696207712\n",
      "train loss:3.745824387972003\n",
      "train loss:3.735304148565672\n",
      "train loss:3.7300132000095014\n",
      "train loss:3.734511388733048\n",
      "train loss:3.7318191845174935\n",
      "train loss:3.715639562514313\n",
      "train loss:3.7177803409007555\n",
      "train loss:3.7209053134090384\n",
      "train loss:3.7176077086524453\n",
      "train loss:3.7134302397494814\n",
      "train loss:3.7095498289341395\n",
      "train loss:3.7047152957419227\n",
      "train loss:3.705978458919303\n",
      "train loss:3.7049356569911236\n",
      "train loss:3.7022506220309244\n",
      "train loss:3.6980812366162077\n",
      "train loss:3.69339601337723\n",
      "train loss:3.693527806180528\n",
      "train loss:3.689192224188436\n",
      "train loss:3.686071215895897\n",
      "train loss:3.689232016948866\n",
      "train loss:3.6835349004425035\n",
      "train loss:3.6824699864094557\n",
      "train loss:3.6752573260122663\n",
      "train loss:3.669674190539486\n",
      "train loss:3.674295492029695\n",
      "train loss:3.666154678559953\n",
      "train loss:3.665955391125876\n",
      "train loss:3.66610136568261\n",
      "train loss:3.6572786505835504\n",
      "train loss:3.6558524399488714\n",
      "train loss:3.65893048704066\n",
      "train loss:3.6465501676060583\n",
      "train loss:3.649514772332663\n",
      "train loss:3.644655175265237\n",
      "train loss:3.6399955763104703\n",
      "train loss:3.639773297381884\n",
      "train loss:3.6333266724484377\n",
      "train loss:3.634739896789389\n",
      "train loss:3.6325493814114704\n",
      "train loss:3.6221313949941276\n",
      "train loss:3.622947026509899\n",
      "train loss:3.627501596612019\n",
      "train loss:3.618574252544269\n",
      "train loss:3.6134704280624437\n",
      "train loss:3.614704712625917\n",
      "train loss:3.6164110701632968\n",
      "train loss:3.6041868269920565\n",
      "train loss:3.6104079382044247\n",
      "train loss:3.6049227470193443\n",
      "train loss:3.601629411172198\n",
      "train loss:3.5982007190199115\n",
      "train loss:3.600260227449202\n",
      "train loss:3.594987777990503\n",
      "train loss:3.5880298797870873\n",
      "train loss:3.5811896589599272\n",
      "train loss:3.5924664112553844\n",
      "train loss:3.5833810896367044\n",
      "train loss:3.580352574386219\n",
      "train loss:3.575528637510106\n",
      "train loss:3.580976477583435\n",
      "train loss:3.5816143527633724\n",
      "train loss:3.5739346407604184\n",
      "train loss:3.5725961919425027\n",
      "train loss:3.5660768644791876\n",
      "train loss:3.567439445142214\n",
      "train loss:3.5650526828609244\n",
      "train loss:3.5627512745773906\n",
      "train loss:3.5543673891072656\n",
      "train loss:3.5566225179722406\n",
      "train loss:3.548112921020357\n",
      "train loss:3.553563061167054\n",
      "train loss:3.5534198120167675\n",
      "train loss:3.5404487667666893\n",
      "train loss:3.5471813949574496\n",
      "train loss:3.543230141705157\n",
      "train loss:3.5433284963858203\n",
      "train loss:3.537942708759796\n",
      "train loss:3.53537317377024\n",
      "train loss:3.5358294883289116\n",
      "train loss:3.5379232636943163\n",
      "train loss:3.5185337178088085\n",
      "train loss:3.521273872644007\n",
      "train loss:3.5176102076824916\n",
      "train loss:3.516006509098432\n",
      "train loss:3.518335500641868\n",
      "train loss:3.5085328461361525\n",
      "train loss:3.5095278440844835\n",
      "train loss:3.5050118677729163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:3.506993961092923\n",
      "train loss:3.5067376287678353\n",
      "train loss:3.5043746287224122\n",
      "train loss:3.4912795530009815\n",
      "train loss:3.4914663359918965\n",
      "train loss:3.4957455140054714\n",
      "train loss:3.486279520572719\n",
      "train loss:3.4823851538959234\n",
      "train loss:3.4908323083949897\n",
      "train loss:3.4838046740908473\n",
      "train loss:3.4855271181327536\n",
      "train loss:3.478600144108886\n",
      "train loss:3.474794826095016\n",
      "train loss:3.4688162529134683\n",
      "train loss:3.478448588477434\n",
      "train loss:3.4696775530554516\n",
      "train loss:3.4753892852699177\n",
      "train loss:3.467823497821649\n",
      "train loss:3.4625357770515786\n",
      "train loss:3.4672137772502745\n",
      "train loss:3.4582244865072833\n",
      "train loss:3.458247532016065\n",
      "train loss:3.444483244649856\n",
      "train loss:3.4499925453760545\n",
      "train loss:3.4498370347877194\n",
      "train loss:3.446343585704489\n",
      "train loss:3.4379437368595975\n",
      "train loss:3.4453090098280947\n",
      "train loss:3.4440273100404575\n",
      "train loss:3.43904543029457\n",
      "train loss:3.427156069812175\n",
      "train loss:3.440512430908198\n",
      "train loss:3.434051835669951\n",
      "train loss:3.4333233791095727\n",
      "train loss:3.4311173712130483\n",
      "train loss:3.419803334776193\n",
      "train loss:3.425400078184732\n",
      "train loss:3.410452922929609\n",
      "train loss:3.410549173916557\n",
      "train loss:3.424481206065701\n",
      "train loss:3.406176374049579\n",
      "train loss:3.407724696143285\n",
      "train loss:3.4037909890591704\n",
      "train loss:3.405127528857868\n",
      "=== epoch:11, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:3.407840389964486\n",
      "train loss:3.4021638526620626\n",
      "train loss:3.402619283218425\n",
      "train loss:3.3946174078722473\n",
      "train loss:3.390839693274956\n",
      "train loss:3.391475414372885\n",
      "train loss:3.3867761216473857\n",
      "train loss:3.391526746321029\n",
      "train loss:3.384268068556939\n",
      "train loss:3.377988344394428\n",
      "train loss:3.379202076445368\n",
      "train loss:3.382985074397453\n",
      "train loss:3.3755114266471535\n",
      "train loss:3.3816349271934234\n",
      "train loss:3.369225623198189\n",
      "train loss:3.3695208117982465\n",
      "train loss:3.3736519378302345\n",
      "train loss:3.3748933381350446\n",
      "train loss:3.360205926144384\n",
      "train loss:3.3513935479901615\n",
      "train loss:3.358093681941418\n",
      "train loss:3.367416229624715\n",
      "train loss:3.359523437574264\n",
      "train loss:3.3548446202011055\n",
      "train loss:3.3541295576259853\n",
      "train loss:3.342455120569953\n",
      "train loss:3.351029818371395\n",
      "train loss:3.3438581186811103\n",
      "train loss:3.345628930034459\n",
      "train loss:3.3406602505583054\n",
      "train loss:3.340254445508785\n",
      "train loss:3.341374224315074\n",
      "train loss:3.335229119541717\n",
      "train loss:3.3287087755497913\n",
      "train loss:3.3292471231667675\n",
      "train loss:3.32913189617088\n",
      "train loss:3.3345370787382063\n",
      "train loss:3.32425010521123\n",
      "train loss:3.323491498268866\n",
      "train loss:3.3235386149546122\n",
      "train loss:3.3204067308039056\n",
      "train loss:3.319832517622544\n",
      "train loss:3.3203369285874578\n",
      "train loss:3.3169557592947294\n",
      "train loss:3.3014240216557047\n",
      "train loss:3.3175100993968503\n",
      "train loss:3.3046110014044467\n",
      "train loss:3.3072166896866717\n",
      "train loss:3.3016821304444552\n",
      "train loss:3.305612214603312\n",
      "train loss:3.292538699098113\n",
      "train loss:3.303781041991567\n",
      "train loss:3.296168835985661\n",
      "train loss:3.2935787495609423\n",
      "train loss:3.286228086726318\n",
      "train loss:3.2827672400902825\n",
      "train loss:3.292928284477728\n",
      "train loss:3.286029745849385\n",
      "train loss:3.2899621383075885\n",
      "train loss:3.2862780882060934\n",
      "train loss:3.285111651168743\n",
      "train loss:3.2807760838048\n",
      "train loss:3.2782808006339272\n",
      "train loss:3.2800362737159547\n",
      "train loss:3.2741379250209217\n",
      "train loss:3.263800098825396\n",
      "train loss:3.268194023992409\n",
      "train loss:3.266609900554223\n",
      "train loss:3.255424089580954\n",
      "train loss:3.2665546245396078\n",
      "train loss:3.2698065325355703\n",
      "train loss:3.2554885481539637\n",
      "train loss:3.257367205617768\n",
      "train loss:3.2512863439346886\n",
      "train loss:3.2482979485348666\n",
      "train loss:3.24895288396208\n",
      "train loss:3.2576793989298816\n",
      "train loss:3.243109040144018\n",
      "train loss:3.2435545239299386\n",
      "train loss:3.242513041431692\n",
      "train loss:3.2454375229584698\n",
      "train loss:3.241321564683821\n",
      "train loss:3.236490070621982\n",
      "train loss:3.234885459754664\n",
      "train loss:3.2359933097264326\n",
      "train loss:3.230788034650327\n",
      "train loss:3.229642151717098\n",
      "train loss:3.2268013520988217\n",
      "train loss:3.2281318805789443\n",
      "train loss:3.227287678583085\n",
      "train loss:3.225099592103122\n",
      "train loss:3.2220723167638847\n",
      "train loss:3.2243160168070633\n",
      "train loss:3.2235986777632792\n",
      "train loss:3.216830741929786\n",
      "train loss:3.214641776456868\n",
      "train loss:3.21172293288683\n",
      "train loss:3.2081128197892363\n",
      "train loss:3.2111433592178793\n",
      "train loss:3.2036047866387936\n",
      "train loss:3.2061494010940685\n",
      "train loss:3.20376506498169\n",
      "train loss:3.198779026676342\n",
      "train loss:3.18971136137691\n",
      "train loss:3.193968448977239\n",
      "train loss:3.1944727881784916\n",
      "train loss:3.191022895992272\n",
      "train loss:3.183697688145237\n",
      "train loss:3.188083002462139\n",
      "train loss:3.1854529585904814\n",
      "train loss:3.180450596484386\n",
      "train loss:3.1853001096666747\n",
      "train loss:3.1833033026835613\n",
      "train loss:3.174228578816636\n",
      "train loss:3.177200456777481\n",
      "train loss:3.187531561862194\n",
      "train loss:3.1736295940130157\n",
      "train loss:3.1655221046618687\n",
      "train loss:3.1692186746799407\n",
      "train loss:3.1802339441926315\n",
      "train loss:3.164691054028297\n",
      "train loss:3.157780620258824\n",
      "train loss:3.165004030675843\n",
      "train loss:3.1606133138063317\n",
      "train loss:3.163049046776921\n",
      "train loss:3.1558187810957574\n",
      "train loss:3.161932006199752\n",
      "train loss:3.1570443122105996\n",
      "train loss:3.156496930432028\n",
      "train loss:3.1527980333326058\n",
      "train loss:3.1515924013594026\n",
      "train loss:3.1511384450850746\n",
      "train loss:3.1464178395747613\n",
      "train loss:3.1504271277096523\n",
      "train loss:3.147453806977778\n",
      "train loss:3.1387519079348536\n",
      "train loss:3.143123820583308\n",
      "train loss:3.1317081396999145\n",
      "train loss:3.1453370635126676\n",
      "train loss:3.145093178505933\n",
      "train loss:3.132621746642907\n",
      "train loss:3.1351177323285397\n",
      "train loss:3.1302569563907556\n",
      "train loss:3.136273616731442\n",
      "train loss:3.1264231191275287\n",
      "train loss:3.1298969668849907\n",
      "train loss:3.1300259408386797\n",
      "train loss:3.127527491014294\n",
      "train loss:3.1229581148994736\n",
      "train loss:3.1163225868886473\n",
      "train loss:3.122178387135422\n",
      "train loss:3.1186494038826553\n",
      "train loss:3.1123235743160924\n",
      "train loss:3.111734039181913\n",
      "train loss:3.1163162947374556\n",
      "train loss:3.11215439744742\n",
      "train loss:3.1055719783256808\n",
      "train loss:3.108157183794681\n",
      "train loss:3.107641365978937\n",
      "train loss:3.1031450156829847\n",
      "train loss:3.0986373576430295\n",
      "train loss:3.097415053968584\n",
      "train loss:3.107703844258074\n",
      "train loss:3.0960066093529504\n",
      "train loss:3.0883027291022307\n",
      "train loss:3.097401177202433\n",
      "train loss:3.094761153706321\n",
      "train loss:3.0825964426969277\n",
      "train loss:3.0899494880542213\n",
      "train loss:3.088453917998675\n",
      "train loss:3.084961697490349\n",
      "train loss:3.0856243983616856\n",
      "train loss:3.0876711428936083\n",
      "train loss:3.0835528979412175\n",
      "train loss:3.0826570447012887\n",
      "train loss:3.0754723823680017\n",
      "train loss:3.0752678231056194\n",
      "train loss:3.0735977366010063\n",
      "train loss:3.0651864380783223\n",
      "train loss:3.0730217570753178\n",
      "train loss:3.0763847299605103\n",
      "train loss:3.0705013697316677\n",
      "train loss:3.064065570395608\n",
      "train loss:3.065039700087664\n",
      "train loss:3.0581056348523017\n",
      "train loss:3.070900083650385\n",
      "train loss:3.0576165803586712\n",
      "train loss:3.06285887779808\n",
      "train loss:3.058260517981912\n",
      "train loss:3.0604514575388393\n",
      "train loss:3.052362429094967\n",
      "train loss:3.0478905495631485\n",
      "train loss:3.0562175768873536\n",
      "train loss:3.0453906367403003\n",
      "train loss:3.0509518769851596\n",
      "train loss:3.049506320955404\n",
      "train loss:3.0422856291245863\n",
      "train loss:3.0433415372626396\n",
      "train loss:3.0457116743557826\n",
      "train loss:3.0483912921712504\n",
      "=== epoch:12, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:3.0405992773415105\n",
      "train loss:3.0326637624854604\n",
      "train loss:3.03703844984509\n",
      "train loss:3.037305780766717\n",
      "train loss:3.029907119956003\n",
      "train loss:3.0351710263738765\n",
      "train loss:3.0250371381806063\n",
      "train loss:3.0268065043034778\n",
      "train loss:3.027235585778015\n",
      "train loss:3.02879089823745\n",
      "train loss:3.0205109036298206\n",
      "train loss:3.0258250244214127\n",
      "train loss:3.01789384991947\n",
      "train loss:3.029060103033839\n",
      "train loss:3.0218775550985875\n",
      "train loss:3.017382407139433\n",
      "train loss:3.0175008998028705\n",
      "train loss:3.0210931811389914\n",
      "train loss:3.019387910489276\n",
      "train loss:3.0082511485011882\n",
      "train loss:3.014787053738385\n",
      "train loss:3.0047591596816994\n",
      "train loss:3.008547540688013\n",
      "train loss:3.006506562332822\n",
      "train loss:3.003999230261888\n",
      "train loss:3.0002645655031683\n",
      "train loss:3.0035330209539284\n",
      "train loss:2.993650618027525\n",
      "train loss:2.996706350936516\n",
      "train loss:2.99867014617145\n",
      "train loss:3.0007678700031395\n",
      "train loss:2.999882894626386\n",
      "train loss:2.991562876455207\n",
      "train loss:3.0041471037805865\n",
      "train loss:2.989174391010712\n",
      "train loss:2.9912577619259433\n",
      "train loss:2.985480257346336\n",
      "train loss:2.980805617721234\n",
      "train loss:2.980605790930899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.976639981448925\n",
      "train loss:2.983446407161016\n",
      "train loss:2.9787331173207114\n",
      "train loss:2.9753600171763424\n",
      "train loss:2.9782146170327275\n",
      "train loss:2.9773225913761996\n",
      "train loss:2.9739535523697773\n",
      "train loss:2.975755909196101\n",
      "train loss:2.975680424663511\n",
      "train loss:2.96997395061027\n",
      "train loss:2.975574895415025\n",
      "train loss:2.9652328751852135\n",
      "train loss:2.9684976781027106\n",
      "train loss:2.9634574838479386\n",
      "train loss:2.9654761959813856\n",
      "train loss:2.9665906500338584\n",
      "train loss:2.9691579112575712\n",
      "train loss:2.958160637493554\n",
      "train loss:2.956147072806567\n",
      "train loss:2.9562259465254845\n",
      "train loss:2.963211682992214\n",
      "train loss:2.9495881500472834\n",
      "train loss:2.95748154408107\n",
      "train loss:2.9505761328195494\n",
      "train loss:2.9536876989708922\n",
      "train loss:2.94756003810084\n",
      "train loss:2.9506016226264946\n",
      "train loss:2.9550792828295775\n",
      "train loss:2.946436090807375\n",
      "train loss:2.9448532335260036\n",
      "train loss:2.948284668450045\n",
      "train loss:2.945561714052424\n",
      "train loss:2.9401319786979165\n",
      "train loss:2.9456215219326185\n",
      "train loss:2.937005798948679\n",
      "train loss:2.9418802700702162\n",
      "train loss:2.94219505893112\n",
      "train loss:2.9416338937197066\n",
      "train loss:2.9281189876108495\n",
      "train loss:2.936208019006809\n",
      "train loss:2.939212848520805\n",
      "train loss:2.9329256179525935\n",
      "train loss:2.926326396146039\n",
      "train loss:2.9278356916378563\n",
      "train loss:2.923124316832173\n",
      "train loss:2.9270663684039864\n",
      "train loss:2.919395817506489\n",
      "train loss:2.923194468153147\n",
      "train loss:2.9261192491587478\n",
      "train loss:2.9150117464857814\n",
      "train loss:2.9138302396669733\n",
      "train loss:2.9222712403273783\n",
      "train loss:2.9160730608201346\n",
      "train loss:2.9210664750816253\n",
      "train loss:2.9142989585395345\n",
      "train loss:2.9137521008730234\n",
      "train loss:2.91174731236313\n",
      "train loss:2.914163882744561\n",
      "train loss:2.909461735830392\n",
      "train loss:2.912652561840608\n",
      "train loss:2.904385553369396\n",
      "train loss:2.9020115066261027\n",
      "train loss:2.8959110680939255\n",
      "train loss:2.904402007749495\n",
      "train loss:2.902375335070225\n",
      "train loss:2.9002163976867834\n",
      "train loss:2.902556046755854\n",
      "train loss:2.9013116373033694\n",
      "train loss:2.898099052595386\n",
      "train loss:2.8972244628115105\n",
      "train loss:2.905729015655705\n",
      "train loss:2.8978601785436586\n",
      "train loss:2.892644064730039\n",
      "train loss:2.8933445627599887\n",
      "train loss:2.8858412451367994\n",
      "train loss:2.8892861271715384\n",
      "train loss:2.885646245025538\n",
      "train loss:2.88843066037836\n",
      "train loss:2.8864960792487775\n",
      "train loss:2.882047930586655\n",
      "train loss:2.8781842181717834\n",
      "train loss:2.8887279267291617\n",
      "train loss:2.8833542579662255\n",
      "train loss:2.8783183254765135\n",
      "train loss:2.867136842251303\n",
      "train loss:2.8825467245270033\n",
      "train loss:2.8764603736993344\n",
      "train loss:2.8779815767251717\n",
      "train loss:2.8786012735440103\n",
      "train loss:2.8755897119198988\n",
      "train loss:2.876550635247841\n",
      "train loss:2.873447628770942\n",
      "train loss:2.872980667747113\n",
      "train loss:2.8719646576589737\n",
      "train loss:2.863275627289208\n",
      "train loss:2.873695246899968\n",
      "train loss:2.8649992800904305\n",
      "train loss:2.8654696068899894\n",
      "train loss:2.8543214373354684\n",
      "train loss:2.8548379873294154\n",
      "train loss:2.8527516488833022\n",
      "train loss:2.858213378440667\n",
      "train loss:2.851709499605364\n",
      "train loss:2.858362985504203\n",
      "train loss:2.852876002278609\n",
      "train loss:2.8545177304766423\n",
      "train loss:2.8527569435912348\n",
      "train loss:2.859763964604599\n",
      "train loss:2.8471107271444795\n",
      "train loss:2.849387218704422\n",
      "train loss:2.8529289138005653\n",
      "train loss:2.8465137870464523\n",
      "train loss:2.8392874307270395\n",
      "train loss:2.839623157311613\n",
      "train loss:2.841218194392794\n",
      "train loss:2.847840046802297\n",
      "train loss:2.848914128696456\n",
      "train loss:2.8475727214110824\n",
      "train loss:2.83704603228684\n",
      "train loss:2.838676151285191\n",
      "train loss:2.842686316146879\n",
      "train loss:2.8309681353133174\n",
      "train loss:2.837639867811463\n",
      "train loss:2.838064530428259\n",
      "train loss:2.8392035109547766\n",
      "train loss:2.836936924827897\n",
      "train loss:2.833653308363204\n",
      "train loss:2.831937497560234\n",
      "train loss:2.8319432100025432\n",
      "train loss:2.8267281735234295\n",
      "train loss:2.82877442387841\n",
      "train loss:2.833222707238777\n",
      "train loss:2.8225743568118267\n",
      "train loss:2.8256119030168545\n",
      "train loss:2.825765099580064\n",
      "train loss:2.8298337141155896\n",
      "train loss:2.82060562754197\n",
      "train loss:2.8205421329922657\n",
      "train loss:2.819222096590467\n",
      "train loss:2.8200560998062745\n",
      "train loss:2.815178631521145\n",
      "train loss:2.817676120494438\n",
      "train loss:2.8130258244207615\n",
      "train loss:2.819430752559846\n",
      "train loss:2.8191452708118265\n",
      "train loss:2.8088674450103928\n",
      "train loss:2.8131654338910317\n",
      "train loss:2.8067172956477098\n",
      "train loss:2.81055654362261\n",
      "train loss:2.8064362802447826\n",
      "train loss:2.8082763130975605\n",
      "train loss:2.8056717180811486\n",
      "train loss:2.799050648896918\n",
      "train loss:2.798794734714888\n",
      "train loss:2.799794304260649\n",
      "train loss:2.7984800088587285\n",
      "train loss:2.797153570769011\n",
      "train loss:2.810538446791172\n",
      "train loss:2.796807341713773\n",
      "train loss:2.794894948354015\n",
      "train loss:2.7985717825781364\n",
      "=== epoch:13, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.8021614948826654\n",
      "train loss:2.7997648492753604\n",
      "train loss:2.804499482016336\n",
      "train loss:2.7942260190945594\n",
      "train loss:2.7918077902357488\n",
      "train loss:2.796996309797411\n",
      "train loss:2.7920796360604667\n",
      "train loss:2.796683815569064\n",
      "train loss:2.7833407953882228\n",
      "train loss:2.7877779072225115\n",
      "train loss:2.7772312939140247\n",
      "train loss:2.789776689658133\n",
      "train loss:2.784430195031867\n",
      "train loss:2.7849566660647884\n",
      "train loss:2.7874683180370865\n",
      "train loss:2.7850127249057692\n",
      "train loss:2.777840747835096\n",
      "train loss:2.7737084642390277\n",
      "train loss:2.7848353417092566\n",
      "train loss:2.7818220779104617\n",
      "train loss:2.7795406296827045\n",
      "train loss:2.774864202710689\n",
      "train loss:2.778423636540565\n",
      "train loss:2.7762553602986153\n",
      "train loss:2.776408471693621\n",
      "train loss:2.7740956957117784\n",
      "train loss:2.7685473939837486\n",
      "train loss:2.7732112242702516\n",
      "train loss:2.7683974287712982\n",
      "train loss:2.7710357575803823\n",
      "train loss:2.76832489886756\n",
      "train loss:2.7646664695087404\n",
      "train loss:2.7717274278493877\n",
      "train loss:2.76195831432124\n",
      "train loss:2.7646208859003396\n",
      "train loss:2.7692601165592263\n",
      "train loss:2.7615426834305263\n",
      "train loss:2.7603193354110123\n",
      "train loss:2.770988739994688\n",
      "train loss:2.7595701119493508\n",
      "train loss:2.760140254619115\n",
      "train loss:2.753551921523931\n",
      "train loss:2.7567585481267836\n",
      "train loss:2.7518698718595016\n",
      "train loss:2.7478276001276036\n",
      "train loss:2.7639637863066224\n",
      "train loss:2.752154956492649\n",
      "train loss:2.7588634534404233\n",
      "train loss:2.7515856512436705\n",
      "train loss:2.754886229570826\n",
      "train loss:2.7574710984322883\n",
      "train loss:2.75329680666161\n",
      "train loss:2.7445244598183365\n",
      "train loss:2.741573874372298\n",
      "train loss:2.745763959201582\n",
      "train loss:2.7425214941930807\n",
      "train loss:2.7400336239646514\n",
      "train loss:2.74244950896485\n",
      "train loss:2.7462502494252585\n",
      "train loss:2.743021863695615\n",
      "train loss:2.7428985375735655\n",
      "train loss:2.7383722995378754\n",
      "train loss:2.734830125831335\n",
      "train loss:2.735981844127299\n",
      "train loss:2.7294491041310027\n",
      "train loss:2.7357942490118576\n",
      "train loss:2.731142738780718\n",
      "train loss:2.7329191572613296\n",
      "train loss:2.731273564031994\n",
      "train loss:2.7323683472873213\n",
      "train loss:2.728034953701405\n",
      "train loss:2.734922930949143\n",
      "train loss:2.7326874528636864\n",
      "train loss:2.7250016655818694\n",
      "train loss:2.7294165841221076\n",
      "train loss:2.730822505285083\n",
      "train loss:2.7242811789698234\n",
      "train loss:2.7357208366584365\n",
      "train loss:2.725686466658993\n",
      "train loss:2.720206380521673\n",
      "train loss:2.7240809786111546\n",
      "train loss:2.716790178411841\n",
      "train loss:2.716627408551415\n",
      "train loss:2.726176364780877\n",
      "train loss:2.718819132583469\n",
      "train loss:2.72179918818783\n",
      "train loss:2.7059104308716004\n",
      "train loss:2.7221825120589807\n",
      "train loss:2.709382566974808\n",
      "train loss:2.7206760122309\n",
      "train loss:2.721488135774592\n",
      "train loss:2.7137877526136527\n",
      "train loss:2.70682190590922\n",
      "train loss:2.7102849097986197\n",
      "train loss:2.715252589437042\n",
      "train loss:2.708318234273959\n",
      "train loss:2.704777749980269\n",
      "train loss:2.7046215227278547\n",
      "train loss:2.7069268887075726\n",
      "train loss:2.7060774697303556\n",
      "train loss:2.708818504543541\n",
      "train loss:2.6990847206860775\n",
      "train loss:2.7022413908323357\n",
      "train loss:2.704283930253189\n",
      "train loss:2.703648759038223\n",
      "train loss:2.7029313145666247\n",
      "train loss:2.7082640877915303\n",
      "train loss:2.6985811858633553\n",
      "train loss:2.691113047023851\n",
      "train loss:2.7000440991438843\n",
      "train loss:2.6996216539101616\n",
      "train loss:2.705294618323312\n",
      "train loss:2.7000405286593083\n",
      "train loss:2.691994407611824\n",
      "train loss:2.7059773189709033\n",
      "train loss:2.6964014933048857\n",
      "train loss:2.6934641748019987\n",
      "train loss:2.6914290756875285\n",
      "train loss:2.6890268562369113\n",
      "train loss:2.6947861989309034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.6946847454869634\n",
      "train loss:2.6821559442835117\n",
      "train loss:2.6932739862991815\n",
      "train loss:2.6989070022697077\n",
      "train loss:2.682096234424223\n",
      "train loss:2.6868760827625184\n",
      "train loss:2.6809222713025553\n",
      "train loss:2.688438027127056\n",
      "train loss:2.6760477488257255\n",
      "train loss:2.694492857237318\n",
      "train loss:2.690078102455413\n",
      "train loss:2.676921896473519\n",
      "train loss:2.679823725636977\n",
      "train loss:2.677296473358645\n",
      "train loss:2.684278694359397\n",
      "train loss:2.6819180938022416\n",
      "train loss:2.677534433501274\n",
      "train loss:2.675188718395214\n",
      "train loss:2.678957173075719\n",
      "train loss:2.673231858409459\n",
      "train loss:2.674047772294455\n",
      "train loss:2.6808019248522106\n",
      "train loss:2.6728664257998975\n",
      "train loss:2.6761822595080242\n",
      "train loss:2.674159085860472\n",
      "train loss:2.666230310469118\n",
      "train loss:2.6713081751894268\n",
      "train loss:2.663728910980119\n",
      "train loss:2.669302900313901\n",
      "train loss:2.668800033255277\n",
      "train loss:2.6596634770981638\n",
      "train loss:2.6665462108770144\n",
      "train loss:2.6706098377067584\n",
      "train loss:2.6687227278569994\n",
      "train loss:2.6633256210207197\n",
      "train loss:2.658553018896414\n",
      "train loss:2.6670280898486283\n",
      "train loss:2.6679967760928083\n",
      "train loss:2.659395134034164\n",
      "train loss:2.664858263564266\n",
      "train loss:2.6631615016297667\n",
      "train loss:2.659739895471578\n",
      "train loss:2.6630096356931974\n",
      "train loss:2.651431710912337\n",
      "train loss:2.6565734028489048\n",
      "train loss:2.6576999350924537\n",
      "train loss:2.6515265274202067\n",
      "train loss:2.662504306175827\n",
      "train loss:2.6507087485100014\n",
      "train loss:2.663464637839045\n",
      "train loss:2.6537809359936415\n",
      "train loss:2.6525348652647827\n",
      "train loss:2.651738478181854\n",
      "train loss:2.652792716434721\n",
      "train loss:2.651575356360889\n",
      "train loss:2.6457013147543638\n",
      "train loss:2.6503112749073163\n",
      "train loss:2.641032121930678\n",
      "train loss:2.647220023196013\n",
      "train loss:2.6474842449005602\n",
      "train loss:2.6417856817368137\n",
      "train loss:2.643852125284821\n",
      "train loss:2.641956647609638\n",
      "train loss:2.6385263680483773\n",
      "train loss:2.63909051154254\n",
      "train loss:2.641711079030352\n",
      "train loss:2.638383406158769\n",
      "train loss:2.63638193361978\n",
      "train loss:2.644886770073748\n",
      "train loss:2.6432115166196017\n",
      "train loss:2.6442999174194166\n",
      "train loss:2.6467125621265617\n",
      "train loss:2.640714057501597\n",
      "train loss:2.639257858340004\n",
      "train loss:2.6314910475893885\n",
      "train loss:2.6379796715229107\n",
      "train loss:2.6359071813378434\n",
      "train loss:2.6332606712753925\n",
      "train loss:2.637300054554475\n",
      "train loss:2.634206643048784\n",
      "=== epoch:14, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.6351418246180374\n",
      "train loss:2.6334268219725807\n",
      "train loss:2.6352030056204856\n",
      "train loss:2.626268952486597\n",
      "train loss:2.642355714189958\n",
      "train loss:2.6269602088511053\n",
      "train loss:2.626897368831926\n",
      "train loss:2.627364293366114\n",
      "train loss:2.631349111958297\n",
      "train loss:2.6242572808464404\n",
      "train loss:2.6250485923496663\n",
      "train loss:2.630878561972799\n",
      "train loss:2.6289936524033215\n",
      "train loss:2.619295772190426\n",
      "train loss:2.6216944214385354\n",
      "train loss:2.6214111775529774\n",
      "train loss:2.618450557437046\n",
      "train loss:2.6276645216560763\n",
      "train loss:2.632210142966202\n",
      "train loss:2.6221305196289744\n",
      "train loss:2.619522226846947\n",
      "train loss:2.6162768813951542\n",
      "train loss:2.6247187493441926\n",
      "train loss:2.6177037643360537\n",
      "train loss:2.6244108449599337\n",
      "train loss:2.617357544628611\n",
      "train loss:2.6169963713266813\n",
      "train loss:2.615250376351642\n",
      "train loss:2.6119794920449957\n",
      "train loss:2.611231763569927\n",
      "train loss:2.61482463876469\n",
      "train loss:2.6175876976598484\n",
      "train loss:2.612064018762254\n",
      "train loss:2.612383896656529\n",
      "train loss:2.6115102118326576\n",
      "train loss:2.6152042762479355\n",
      "train loss:2.609239192177018\n",
      "train loss:2.612282160353435\n",
      "train loss:2.615025887571724\n",
      "train loss:2.6110067060711177\n",
      "train loss:2.6016888649010492\n",
      "train loss:2.6091121610838646\n",
      "train loss:2.614014269251849\n",
      "train loss:2.602556412734968\n",
      "train loss:2.6061323027859187\n",
      "train loss:2.612032175477426\n",
      "train loss:2.6081082637727926\n",
      "train loss:2.602587410558861\n",
      "train loss:2.60313155706414\n",
      "train loss:2.612559781230993\n",
      "train loss:2.597131082490334\n",
      "train loss:2.6026765613136145\n",
      "train loss:2.602771299762787\n",
      "train loss:2.603686009527427\n",
      "train loss:2.6005012282794415\n",
      "train loss:2.602576021844616\n",
      "train loss:2.5969271494222\n",
      "train loss:2.6033074929343645\n",
      "train loss:2.601229422587916\n",
      "train loss:2.5969905797316453\n",
      "train loss:2.591658655707683\n",
      "train loss:2.5942203041564866\n",
      "train loss:2.6032771538219848\n",
      "train loss:2.588180125590124\n",
      "train loss:2.5994264262916884\n",
      "train loss:2.597218053539421\n",
      "train loss:2.5874610612768096\n",
      "train loss:2.5949615361448846\n",
      "train loss:2.5905684562116646\n",
      "train loss:2.595677421160118\n",
      "train loss:2.5789101360671385\n",
      "train loss:2.591672425450964\n",
      "train loss:2.599681713122187\n",
      "train loss:2.578991982527248\n",
      "train loss:2.5938136833075993\n",
      "train loss:2.5885058321265824\n",
      "train loss:2.5901059434993634\n",
      "train loss:2.575356938315191\n",
      "train loss:2.587204151597244\n",
      "train loss:2.578576397036382\n",
      "train loss:2.5858260053574664\n",
      "train loss:2.5812111524781693\n",
      "train loss:2.579756564501362\n",
      "train loss:2.582502500038408\n",
      "train loss:2.5804964783416664\n",
      "train loss:2.5765176503724363\n",
      "train loss:2.5918823715779284\n",
      "train loss:2.5862332642676007\n",
      "train loss:2.5780425012744574\n",
      "train loss:2.5766493603951206\n",
      "train loss:2.5776037554495996\n",
      "train loss:2.5783158495004925\n",
      "train loss:2.5680354553578844\n",
      "train loss:2.570690645509062\n",
      "train loss:2.5746725948813918\n",
      "train loss:2.5797861076466537\n",
      "train loss:2.578380178816029\n",
      "train loss:2.5683264441268836\n",
      "train loss:2.578126593082926\n",
      "train loss:2.5801493133814843\n",
      "train loss:2.563614076322449\n",
      "train loss:2.5805982866163046\n",
      "train loss:2.5734492057444567\n",
      "train loss:2.574825754408925\n",
      "train loss:2.561116269979572\n",
      "train loss:2.5660023989498275\n",
      "train loss:2.57493777200036\n",
      "train loss:2.574173188834794\n",
      "train loss:2.56416737736754\n",
      "train loss:2.577270372806621\n",
      "train loss:2.5686340614252456\n",
      "train loss:2.566552100026469\n",
      "train loss:2.564186949742259\n",
      "train loss:2.571824613238328\n",
      "train loss:2.5782220896419674\n",
      "train loss:2.5673657494941056\n",
      "train loss:2.5651559984850247\n",
      "train loss:2.5657115812485953\n",
      "train loss:2.5643045848283603\n",
      "train loss:2.5663073707417845\n",
      "train loss:2.5628250484184063\n",
      "train loss:2.5620941335641936\n",
      "train loss:2.5639614237457042\n",
      "train loss:2.5569064682153417\n",
      "train loss:2.5540271252112974\n",
      "train loss:2.5685483374888456\n",
      "train loss:2.5505334399574227\n",
      "train loss:2.548668893355294\n",
      "train loss:2.559533262452892\n",
      "train loss:2.557854347537014\n",
      "train loss:2.551840457197694\n",
      "train loss:2.5509448525434895\n",
      "train loss:2.5545463424051755\n",
      "train loss:2.56073331358776\n",
      "train loss:2.5516492940511215\n",
      "train loss:2.5503620462554806\n",
      "train loss:2.5544607926574807\n",
      "train loss:2.5547020069774327\n",
      "train loss:2.557794412852781\n",
      "train loss:2.545434922025147\n",
      "train loss:2.547073923728076\n",
      "train loss:2.549274016226683\n",
      "train loss:2.5526860464781445\n",
      "train loss:2.551088335200149\n",
      "train loss:2.5495348700153286\n",
      "train loss:2.5545116209486203\n",
      "train loss:2.5479750917433157\n",
      "train loss:2.5491421041016027\n",
      "train loss:2.5431803698303894\n",
      "train loss:2.5482744180071153\n",
      "train loss:2.545550739308993\n",
      "train loss:2.544619846085915\n",
      "train loss:2.5381525088435635\n",
      "train loss:2.551011820176743\n",
      "train loss:2.5397782067115657\n",
      "train loss:2.549543712921568\n",
      "train loss:2.5429231776875287\n",
      "train loss:2.537864702440031\n",
      "train loss:2.5421334004366303\n",
      "train loss:2.535481455750958\n",
      "train loss:2.548530287451071\n",
      "train loss:2.5431148853088352\n",
      "train loss:2.540526957743301\n",
      "train loss:2.5488465281846704\n",
      "train loss:2.542908924428956\n",
      "train loss:2.5373552451093393\n",
      "train loss:2.540765955002205\n",
      "train loss:2.5398049499711695\n",
      "train loss:2.540544311138654\n",
      "train loss:2.5417056135003415\n",
      "train loss:2.5384940109083405\n",
      "train loss:2.542689708994429\n",
      "train loss:2.535975954083749\n",
      "train loss:2.5380823260532974\n",
      "train loss:2.5451934321809495\n",
      "train loss:2.526874158603384\n",
      "train loss:2.5369893826153183\n",
      "train loss:2.53158743039268\n",
      "train loss:2.532605733121872\n",
      "train loss:2.5345865863656263\n",
      "train loss:2.5325908436264464\n",
      "train loss:2.5356818606500298\n",
      "train loss:2.533980879171619\n",
      "train loss:2.5232705755745806\n",
      "train loss:2.530035945575333\n",
      "train loss:2.52374973989594\n",
      "train loss:2.5356729600750314\n",
      "train loss:2.524798324972178\n",
      "train loss:2.529331268845302\n",
      "train loss:2.5294237501492485\n",
      "train loss:2.5318234894352725\n",
      "train loss:2.5390690831302645\n",
      "train loss:2.531087428110302\n",
      "train loss:2.5295986676301485\n",
      "train loss:2.529747753353672\n",
      "train loss:2.5363483211696893\n",
      "train loss:2.5259161275064796\n",
      "train loss:2.53559459477209\n",
      "train loss:2.523459721940336\n",
      "train loss:2.528515258345177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:15, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.513842117105434\n",
      "train loss:2.5171871940125885\n",
      "train loss:2.5289568392525474\n",
      "train loss:2.5201811954591555\n",
      "train loss:2.5139863368492112\n",
      "train loss:2.524819115708654\n",
      "train loss:2.513741182883042\n",
      "train loss:2.523015286518033\n",
      "train loss:2.523635141394774\n",
      "train loss:2.5244320370426676\n",
      "train loss:2.52079672039044\n",
      "train loss:2.5169137590574384\n",
      "train loss:2.5217783223439305\n",
      "train loss:2.523493423095575\n",
      "train loss:2.5130811353376687\n",
      "train loss:2.5144329660530933\n",
      "train loss:2.5144213873966854\n",
      "train loss:2.5245816843850015\n",
      "train loss:2.5139584485286046\n",
      "train loss:2.5166753143096035\n",
      "train loss:2.515252561458298\n",
      "train loss:2.510654170709952\n",
      "train loss:2.5155269046714337\n",
      "train loss:2.5086727036915386\n",
      "train loss:2.516528136389025\n",
      "train loss:2.5182290517985635\n",
      "train loss:2.515351508974511\n",
      "train loss:2.5135069935416547\n",
      "train loss:2.5103342459149314\n",
      "train loss:2.516171556989727\n",
      "train loss:2.509269313846136\n",
      "train loss:2.5187456609654113\n",
      "train loss:2.510869871933813\n",
      "train loss:2.5143923713352203\n",
      "train loss:2.5114194325162638\n",
      "train loss:2.5125749308390444\n",
      "train loss:2.5084371143218522\n",
      "train loss:2.5083140804375144\n",
      "train loss:2.5039109612145407\n",
      "train loss:2.506920175805654\n",
      "train loss:2.5059867072040305\n",
      "train loss:2.5018662309601245\n",
      "train loss:2.5103428858355423\n",
      "train loss:2.507468222598361\n",
      "train loss:2.501181525167568\n",
      "train loss:2.509221630989843\n",
      "train loss:2.5040138944090296\n",
      "train loss:2.500045236243367\n",
      "train loss:2.5045021814961146\n",
      "train loss:2.4963007224429252\n",
      "train loss:2.5029748293210163\n",
      "train loss:2.5023959996207172\n",
      "train loss:2.5014069580910685\n",
      "train loss:2.5032778217098537\n",
      "train loss:2.497770128724842\n",
      "train loss:2.5010516598560972\n",
      "train loss:2.505268737745064\n",
      "train loss:2.5093421931219067\n",
      "train loss:2.496448325451841\n",
      "train loss:2.500905529738741\n",
      "train loss:2.500665464875505\n",
      "train loss:2.4965574228179226\n",
      "train loss:2.498795132097918\n",
      "train loss:2.5003703757577\n",
      "train loss:2.493649793645861\n",
      "train loss:2.4982895117913655\n",
      "train loss:2.4974571740554605\n",
      "train loss:2.4975326300060745\n",
      "train loss:2.4891873009112073\n",
      "train loss:2.4944634308804305\n",
      "train loss:2.4913220252118626\n",
      "train loss:2.4948647281738547\n",
      "train loss:2.487769389095513\n",
      "train loss:2.488487531069215\n",
      "train loss:2.4989888865562286\n",
      "train loss:2.4910779348594194\n",
      "train loss:2.4923828075776493\n",
      "train loss:2.494376874878061\n",
      "train loss:2.489253563850084\n",
      "train loss:2.492481915191383\n",
      "train loss:2.4858892416653284\n",
      "train loss:2.4878707027551834\n",
      "train loss:2.486992050109248\n",
      "train loss:2.4825649855153413\n",
      "train loss:2.4953576842873013\n",
      "train loss:2.4840986744265194\n",
      "train loss:2.4832830214638317\n",
      "train loss:2.490498744822772\n",
      "train loss:2.47957513649464\n",
      "train loss:2.48637686276495\n",
      "train loss:2.4796910147156788\n",
      "train loss:2.482039114413977\n",
      "train loss:2.4829276559904336\n",
      "train loss:2.4918115282528506\n",
      "train loss:2.481922187652096\n",
      "train loss:2.4823580048998592\n",
      "train loss:2.489425412873441\n",
      "train loss:2.485701161778155\n",
      "train loss:2.4901465082570557\n",
      "train loss:2.481856611323625\n",
      "train loss:2.4853325662093844\n",
      "train loss:2.4798434358786134\n",
      "train loss:2.485064676698058\n",
      "train loss:2.4751898919133506\n",
      "train loss:2.4790843364098922\n",
      "train loss:2.4740353504583124\n",
      "train loss:2.4720106850917576\n",
      "train loss:2.4770842774655804\n",
      "train loss:2.4891570379575247\n",
      "train loss:2.4893603183599122\n",
      "train loss:2.479091058523849\n",
      "train loss:2.482371924129312\n",
      "train loss:2.4738897797077524\n",
      "train loss:2.4832515318403288\n",
      "train loss:2.4697816502739305\n",
      "train loss:2.4825057051961963\n",
      "train loss:2.4727946939944117\n",
      "train loss:2.4803919316988186\n",
      "train loss:2.4675956655425573\n",
      "train loss:2.4809897410289654\n",
      "train loss:2.476732345552509\n",
      "train loss:2.4802037911318835\n",
      "train loss:2.475487058098874\n",
      "train loss:2.4710091302814714\n",
      "train loss:2.470836354642521\n",
      "train loss:2.472782078574161\n",
      "train loss:2.4669918214640565\n",
      "train loss:2.468343010539323\n",
      "train loss:2.476211930676908\n",
      "train loss:2.4707212201467126\n",
      "train loss:2.4724879909711475\n",
      "train loss:2.4732022889574443\n",
      "train loss:2.473191521976665\n",
      "train loss:2.481275163183154\n",
      "train loss:2.473707337023309\n",
      "train loss:2.4718482573437495\n",
      "train loss:2.4758805831871475\n",
      "train loss:2.4647500946153102\n",
      "train loss:2.4701210962902125\n",
      "train loss:2.46812523497116\n",
      "train loss:2.4695169263404764\n",
      "train loss:2.473654650926512\n",
      "train loss:2.464775761288652\n",
      "train loss:2.4598117709259477\n",
      "train loss:2.467136342135099\n",
      "train loss:2.4580971809753636\n",
      "train loss:2.4710826645752606\n",
      "train loss:2.4709040760765943\n",
      "train loss:2.4633053275993584\n",
      "train loss:2.4612956776740433\n",
      "train loss:2.4672315435328946\n",
      "train loss:2.468923809492977\n",
      "train loss:2.463436485814536\n",
      "train loss:2.4585748721697636\n",
      "train loss:2.4660846413778095\n",
      "train loss:2.4695897071761013\n",
      "train loss:2.456374716561688\n",
      "train loss:2.458838395964302\n",
      "train loss:2.4520405402750303\n",
      "train loss:2.458562768614637\n",
      "train loss:2.4642744406696995\n",
      "train loss:2.4692372275329606\n",
      "train loss:2.459222430115015\n",
      "train loss:2.4573944345851975\n",
      "train loss:2.4653704880153358\n",
      "train loss:2.4550145087788753\n",
      "train loss:2.4595679184171546\n",
      "train loss:2.458780940338695\n",
      "train loss:2.4628949422095796\n",
      "train loss:2.457845348357997\n",
      "train loss:2.463452688479339\n",
      "train loss:2.4698310831609427\n",
      "train loss:2.4593251449148403\n",
      "train loss:2.458564546496227\n",
      "train loss:2.457336769456485\n",
      "train loss:2.4609777408418942\n",
      "train loss:2.4522587688352617\n",
      "train loss:2.457617661785112\n",
      "train loss:2.4601203827024287\n",
      "train loss:2.451124463395022\n",
      "train loss:2.457598639950262\n",
      "train loss:2.451152904319682\n",
      "train loss:2.4615768349393274\n",
      "train loss:2.453891923252012\n",
      "train loss:2.4544476123461703\n",
      "train loss:2.456098724882635\n",
      "train loss:2.4564182809355857\n",
      "train loss:2.4516526175468214\n",
      "train loss:2.4534752327208067\n",
      "train loss:2.456311696973481\n",
      "train loss:2.453935460980469\n",
      "train loss:2.4465865344792954\n",
      "train loss:2.4456830168278754\n",
      "train loss:2.4514678168961703\n",
      "train loss:2.4556088115993604\n",
      "train loss:2.4448980172802544\n",
      "train loss:2.4576279842582824\n",
      "train loss:2.4525148990776415\n",
      "train loss:2.451326528303224\n",
      "train loss:2.45952578089955\n",
      "=== epoch:16, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.446520986002167\n",
      "train loss:2.4496171161862685\n",
      "train loss:2.444460156577208\n",
      "train loss:2.4401945922293815\n",
      "train loss:2.4337139359463165\n",
      "train loss:2.457795577744461\n",
      "train loss:2.4445967261546544\n",
      "train loss:2.4491083902472366\n",
      "train loss:2.44531461457546\n",
      "train loss:2.4455376869197156\n",
      "train loss:2.4588836860350716\n",
      "train loss:2.4449020293484853\n",
      "train loss:2.439710120904248\n",
      "train loss:2.444654928351155\n",
      "train loss:2.4474876535705996\n",
      "train loss:2.4412271542228727\n",
      "train loss:2.4506260015969974\n",
      "train loss:2.440782046925207\n",
      "train loss:2.455801467349103\n",
      "train loss:2.445189566203924\n",
      "train loss:2.443634270611954\n",
      "train loss:2.44382400896815\n",
      "train loss:2.4521179282067416\n",
      "train loss:2.4482160094034473\n",
      "train loss:2.446606554743912\n",
      "train loss:2.443858606843521\n",
      "train loss:2.4463190474825915\n",
      "train loss:2.4391040632593435\n",
      "train loss:2.4404068394083342\n",
      "train loss:2.447078299630262\n",
      "train loss:2.433158335982627\n",
      "train loss:2.443126350114864\n",
      "train loss:2.4429448546948\n",
      "train loss:2.4397899075836196\n",
      "train loss:2.4431928802172957\n",
      "train loss:2.4350366170101516\n",
      "train loss:2.4426122480155756\n",
      "train loss:2.437528021133593\n",
      "train loss:2.439562456068899\n",
      "train loss:2.439840702003344\n",
      "train loss:2.4407804367697556\n",
      "train loss:2.4256191869610353\n",
      "train loss:2.436212153161982\n",
      "train loss:2.4359741232020067\n",
      "train loss:2.437173465717531\n",
      "train loss:2.429560599316052\n",
      "train loss:2.444280525268467\n",
      "train loss:2.4414925001626404\n",
      "train loss:2.4410621819508402\n",
      "train loss:2.429421403517519\n",
      "train loss:2.4412875988334597\n",
      "train loss:2.434599592248394\n",
      "train loss:2.4375477578319606\n",
      "train loss:2.429936430447044\n",
      "train loss:2.432239676172519\n",
      "train loss:2.4390762075414116\n",
      "train loss:2.4300728937636134\n",
      "train loss:2.4327768317099117\n",
      "train loss:2.433715038306431\n",
      "train loss:2.4364455649242545\n",
      "train loss:2.434942868197161\n",
      "train loss:2.438169173721709\n",
      "train loss:2.430807340567686\n",
      "train loss:2.430494889330651\n",
      "train loss:2.4240716243010483\n",
      "train loss:2.4298087164867037\n",
      "train loss:2.4419303563534305\n",
      "train loss:2.431555620702824\n",
      "train loss:2.4295537703912835\n",
      "train loss:2.4282902137875064\n",
      "train loss:2.4331873049407164\n",
      "train loss:2.4283402314913953\n",
      "train loss:2.421935565179968\n",
      "train loss:2.4320107100431048\n",
      "train loss:2.4285805512678973\n",
      "train loss:2.4378479017463457\n",
      "train loss:2.4239798174788896\n",
      "train loss:2.4203014938631986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.4252010936733503\n",
      "train loss:2.4303818359213087\n",
      "train loss:2.432842491697305\n",
      "train loss:2.4263035672087856\n",
      "train loss:2.4178570978037994\n",
      "train loss:2.4276160509699602\n",
      "train loss:2.4222722579128844\n",
      "train loss:2.4273379461669276\n",
      "train loss:2.4209371720041286\n",
      "train loss:2.428498200733453\n",
      "train loss:2.428032978112804\n",
      "train loss:2.4256598043780118\n",
      "train loss:2.419056232089167\n",
      "train loss:2.4278308684030616\n",
      "train loss:2.419200106504039\n",
      "train loss:2.4231591454672143\n",
      "train loss:2.4275570959308483\n",
      "train loss:2.4196027409761593\n",
      "train loss:2.4337571146005357\n",
      "train loss:2.4250285628533037\n",
      "train loss:2.4172103889262937\n",
      "train loss:2.4347615370920925\n",
      "train loss:2.423105766373376\n",
      "train loss:2.4291390953945324\n",
      "train loss:2.4159391007970443\n",
      "train loss:2.421751479408103\n",
      "train loss:2.418428485512001\n",
      "train loss:2.4155137799099364\n",
      "train loss:2.4239092677523675\n",
      "train loss:2.4197249770174603\n",
      "train loss:2.4222310799822586\n",
      "train loss:2.416730117205452\n",
      "train loss:2.4107241822458185\n",
      "train loss:2.4248150155185426\n",
      "train loss:2.4205835583394006\n",
      "train loss:2.4273676301553793\n",
      "train loss:2.4237208615148234\n",
      "train loss:2.420421595061164\n",
      "train loss:2.429658938903985\n",
      "train loss:2.4261604616793977\n",
      "train loss:2.4183887605865\n",
      "train loss:2.4180580331465276\n",
      "train loss:2.41652731762548\n",
      "train loss:2.418377660468935\n",
      "train loss:2.4177493116739908\n",
      "train loss:2.42030150681749\n",
      "train loss:2.432115268734334\n",
      "train loss:2.4136473788502832\n",
      "train loss:2.4105481980054493\n",
      "train loss:2.413189961489909\n",
      "train loss:2.418315462875817\n",
      "train loss:2.407827199513174\n",
      "train loss:2.4203178637204394\n",
      "train loss:2.4238026085727746\n",
      "train loss:2.4248799830975214\n",
      "train loss:2.4217782016557705\n",
      "train loss:2.416562123422583\n",
      "train loss:2.421957088376788\n",
      "train loss:2.417174780531139\n",
      "train loss:2.4168610726804425\n",
      "train loss:2.4131103441520168\n",
      "train loss:2.4143170550948665\n",
      "train loss:2.4097367972239194\n",
      "train loss:2.4111347646273287\n",
      "train loss:2.426614186992119\n",
      "train loss:2.4132927155333044\n",
      "train loss:2.4101110784442765\n",
      "train loss:2.4125657964206173\n",
      "train loss:2.4096318172014315\n",
      "train loss:2.4115233269715137\n",
      "train loss:2.410981440984804\n",
      "train loss:2.4016358520360095\n",
      "train loss:2.4051345808853206\n",
      "train loss:2.4199012832950424\n",
      "train loss:2.4073938613601897\n",
      "train loss:2.408332424477941\n",
      "train loss:2.399365465777964\n",
      "train loss:2.4080516723281997\n",
      "train loss:2.4067097503752994\n",
      "train loss:2.416404756116148\n",
      "train loss:2.4034209646309166\n",
      "train loss:2.401770332146286\n",
      "train loss:2.4111362096088516\n",
      "train loss:2.4043815429535824\n",
      "train loss:2.417760003781667\n",
      "train loss:2.404984849211003\n",
      "train loss:2.4139987521828288\n",
      "train loss:2.4018697388759915\n",
      "train loss:2.403559245498839\n",
      "train loss:2.4147804400796637\n",
      "train loss:2.399539255982901\n",
      "train loss:2.405597673820543\n",
      "train loss:2.4036762654102697\n",
      "train loss:2.4136967831533873\n",
      "train loss:2.409783367026542\n",
      "train loss:2.4058907860592265\n",
      "train loss:2.4040095588119517\n",
      "train loss:2.4056403775767756\n",
      "train loss:2.412022843957235\n",
      "train loss:2.4029303319171116\n",
      "train loss:2.407422381555158\n",
      "train loss:2.397772996971194\n",
      "train loss:2.404463768742905\n",
      "train loss:2.409609643325201\n",
      "train loss:2.4119767270381347\n",
      "train loss:2.3999330518095547\n",
      "train loss:2.3938566780885795\n",
      "train loss:2.401976560895951\n",
      "train loss:2.4090254334945325\n",
      "train loss:2.40422435566653\n",
      "train loss:2.402253946456007\n",
      "train loss:2.389647969181416\n",
      "train loss:2.3985790973019627\n",
      "train loss:2.403376709824113\n",
      "train loss:2.4105335180090672\n",
      "train loss:2.400917976676188\n",
      "train loss:2.403409040640614\n",
      "train loss:2.4016702812597397\n",
      "train loss:2.3975464623837133\n",
      "train loss:2.3991746023714056\n",
      "train loss:2.4038977012034466\n",
      "train loss:2.4005958706589903\n",
      "=== epoch:17, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.402692402822585\n",
      "train loss:2.403147653083352\n",
      "train loss:2.410812856757757\n",
      "train loss:2.3979153104475555\n",
      "train loss:2.4010473550325813\n",
      "train loss:2.3991259932503572\n",
      "train loss:2.39917234303437\n",
      "train loss:2.401537191251849\n",
      "train loss:2.395720874492558\n",
      "train loss:2.406758422288858\n",
      "train loss:2.396329016769875\n",
      "train loss:2.4019212039938624\n",
      "train loss:2.3993470394356207\n",
      "train loss:2.399364986493214\n",
      "train loss:2.396257005750475\n",
      "train loss:2.3958155068143268\n",
      "train loss:2.393560007939564\n",
      "train loss:2.3975416127475935\n",
      "train loss:2.402564497020562\n",
      "train loss:2.397830577413295\n",
      "train loss:2.393474123783505\n",
      "train loss:2.3941568015361585\n",
      "train loss:2.4000358584036645\n",
      "train loss:2.400029847299892\n",
      "train loss:2.3904125164357337\n",
      "train loss:2.3931592663374612\n",
      "train loss:2.3930767255957313\n",
      "train loss:2.398889697189191\n",
      "train loss:2.3979574614052583\n",
      "train loss:2.3981593411318753\n",
      "train loss:2.3965029329672887\n",
      "train loss:2.3926495506232364\n",
      "train loss:2.39976412003903\n",
      "train loss:2.3936666329693375\n",
      "train loss:2.3965242745612003\n",
      "train loss:2.384667347604426\n",
      "train loss:2.391640418445168\n",
      "train loss:2.397587844576037\n",
      "train loss:2.393685393074326\n",
      "train loss:2.3860688305555358\n",
      "train loss:2.405015217425554\n",
      "train loss:2.393644415138471\n",
      "train loss:2.3982402347997898\n",
      "train loss:2.4009837409806276\n",
      "train loss:2.397773080692575\n",
      "train loss:2.3898631500762346\n",
      "train loss:2.3951850686187774\n",
      "train loss:2.386938679591606\n",
      "train loss:2.389281196882684\n",
      "train loss:2.3990610015343834\n",
      "train loss:2.386519246202465\n",
      "train loss:2.3900888812610988\n",
      "train loss:2.3929743953718807\n",
      "train loss:2.394902044478191\n",
      "train loss:2.396145954414869\n",
      "train loss:2.3953968206214578\n",
      "train loss:2.3860010117735513\n",
      "train loss:2.385042199749243\n",
      "train loss:2.395764619334752\n",
      "train loss:2.3946586078298586\n",
      "train loss:2.3964948697602044\n",
      "train loss:2.3892243856553312\n",
      "train loss:2.391397327744265\n",
      "train loss:2.3929177849893426\n",
      "train loss:2.395923946343151\n",
      "train loss:2.391000232719425\n",
      "train loss:2.3962827369854622\n",
      "train loss:2.3860262397504295\n",
      "train loss:2.3852025465516586\n",
      "train loss:2.392192822869858\n",
      "train loss:2.3907641138201683\n",
      "train loss:2.3806707144236317\n",
      "train loss:2.3983995618435356\n",
      "train loss:2.383544265459733\n",
      "train loss:2.3872674466921997\n",
      "train loss:2.387218218872057\n",
      "train loss:2.393168537259143\n",
      "train loss:2.3946436481275515\n",
      "train loss:2.387033923702799\n",
      "train loss:2.3826277379911316\n",
      "train loss:2.3929718626704877\n",
      "train loss:2.3844842794898846\n",
      "train loss:2.383574598663127\n",
      "train loss:2.3777406913752697\n",
      "train loss:2.384711293358893\n",
      "train loss:2.3798167213488\n",
      "train loss:2.378807518866447\n",
      "train loss:2.398240356716099\n",
      "train loss:2.3845016493026727\n",
      "train loss:2.3767754317629977\n",
      "train loss:2.392301506651251\n",
      "train loss:2.3885783204759625\n",
      "train loss:2.3859327465421245\n",
      "train loss:2.374704883679732\n",
      "train loss:2.368047309258935\n",
      "train loss:2.3780830269597875\n",
      "train loss:2.388043590203886\n",
      "train loss:2.3759437421007616\n",
      "train loss:2.387687465322494\n",
      "train loss:2.392185096129091\n",
      "train loss:2.3861126747236696\n",
      "train loss:2.3857781601421713\n",
      "train loss:2.381851704474829\n",
      "train loss:2.383002634595732\n",
      "train loss:2.3837881033841026\n",
      "train loss:2.3805458697193247\n",
      "train loss:2.3834695825922645\n",
      "train loss:2.383379662242384\n",
      "train loss:2.370652935270361\n",
      "train loss:2.381256024554786\n",
      "train loss:2.3847147715909225\n",
      "train loss:2.381752994987682\n",
      "train loss:2.376191353212852\n",
      "train loss:2.382534586406703\n",
      "train loss:2.383936362369672\n",
      "train loss:2.3826835711168384\n",
      "train loss:2.3808542355763826\n",
      "train loss:2.3747787907491222\n",
      "train loss:2.3750972640496326\n",
      "train loss:2.3827266418451445\n",
      "train loss:2.381176797913445\n",
      "train loss:2.381795072903354\n",
      "train loss:2.3795890920172624\n",
      "train loss:2.3693450296315346\n",
      "train loss:2.378322347084614\n",
      "train loss:2.382556491164643\n",
      "train loss:2.3788006123710645\n",
      "train loss:2.378777006289088\n",
      "train loss:2.3819623941965444\n",
      "train loss:2.3781970247355866\n",
      "train loss:2.379026112636096\n",
      "train loss:2.375036572383253\n",
      "train loss:2.382435935525044\n",
      "train loss:2.3738688389302425\n",
      "train loss:2.3708231308457823\n",
      "train loss:2.3860913386210942\n",
      "train loss:2.3819492461135625\n",
      "train loss:2.3737469164020446\n",
      "train loss:2.3827516122424814\n",
      "train loss:2.3747294356241806\n",
      "train loss:2.3694803702305216\n",
      "train loss:2.3755423463158185\n",
      "train loss:2.38385083483502\n",
      "train loss:2.377869104898301\n",
      "train loss:2.374034023520813\n",
      "train loss:2.3767941663774645\n",
      "train loss:2.3789893707373433\n",
      "train loss:2.3726530059445925\n",
      "train loss:2.381034901577162\n",
      "train loss:2.3650036256517124\n",
      "train loss:2.3820243085443353\n",
      "train loss:2.379607852847499\n",
      "train loss:2.3701195327998708\n",
      "train loss:2.3739665940167036\n",
      "train loss:2.376279557927304\n",
      "train loss:2.379224654033195\n",
      "train loss:2.362120812900476\n",
      "train loss:2.3796426537754254\n",
      "train loss:2.377228424654256\n",
      "train loss:2.377640420330124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3748166372060697\n",
      "train loss:2.3694776263138104\n",
      "train loss:2.366114867222958\n",
      "train loss:2.373714250910107\n",
      "train loss:2.3650736724108135\n",
      "train loss:2.372214738667972\n",
      "train loss:2.371221506399641\n",
      "train loss:2.371712741958341\n",
      "train loss:2.3772777677582893\n",
      "train loss:2.370385332748823\n",
      "train loss:2.3617159826996073\n",
      "train loss:2.3710902768968203\n",
      "train loss:2.3786434606407125\n",
      "train loss:2.367372833573186\n",
      "train loss:2.362489119834957\n",
      "train loss:2.38102245444698\n",
      "train loss:2.368815574483356\n",
      "train loss:2.367873576546218\n",
      "train loss:2.376711728471036\n",
      "train loss:2.375038911906552\n",
      "train loss:2.373588811092647\n",
      "train loss:2.374054447506567\n",
      "train loss:2.371778836447702\n",
      "train loss:2.364926815676978\n",
      "train loss:2.3735880347934817\n",
      "train loss:2.366286293380673\n",
      "train loss:2.378006017606157\n",
      "train loss:2.3749008304794286\n",
      "train loss:2.3641533867831326\n",
      "train loss:2.362031927400837\n",
      "train loss:2.367973336397139\n",
      "train loss:2.3654627850889636\n",
      "train loss:2.3656795572833818\n",
      "train loss:2.366225425659541\n",
      "train loss:2.363154819907623\n",
      "train loss:2.3672234177597526\n",
      "train loss:2.3681927785270216\n",
      "train loss:2.370740907280131\n",
      "train loss:2.372889955524757\n",
      "train loss:2.3613195658122317\n",
      "=== epoch:18, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3739863974925672\n",
      "train loss:2.3727389125131815\n",
      "train loss:2.3700488818545624\n",
      "train loss:2.370345891960287\n",
      "train loss:2.3645396082432213\n",
      "train loss:2.3681935264478677\n",
      "train loss:2.369773784229735\n",
      "train loss:2.358456492379906\n",
      "train loss:2.365641995262204\n",
      "train loss:2.357966427613634\n",
      "train loss:2.3636605730947746\n",
      "train loss:2.362540603783025\n",
      "train loss:2.373130776775568\n",
      "train loss:2.364093915332953\n",
      "train loss:2.366302134861724\n",
      "train loss:2.366649615917277\n",
      "train loss:2.368660631666586\n",
      "train loss:2.3685859880067475\n",
      "train loss:2.366264280708305\n",
      "train loss:2.378617856800722\n",
      "train loss:2.3675043425520372\n",
      "train loss:2.372089590421792\n",
      "train loss:2.368240017891449\n",
      "train loss:2.3670353108305813\n",
      "train loss:2.3675087044459526\n",
      "train loss:2.3561001491717857\n",
      "train loss:2.357359860316564\n",
      "train loss:2.3696647490953597\n",
      "train loss:2.3624598485778288\n",
      "train loss:2.368128738591219\n",
      "train loss:2.356481387382075\n",
      "train loss:2.364472755060513\n",
      "train loss:2.3683377067826514\n",
      "train loss:2.35791668360371\n",
      "train loss:2.3669231871385974\n",
      "train loss:2.3633685773386666\n",
      "train loss:2.3555882237382746\n",
      "train loss:2.3517650915332933\n",
      "train loss:2.360247029215828\n",
      "train loss:2.3484319990060136\n",
      "train loss:2.3600682243335265\n",
      "train loss:2.360054358962057\n",
      "train loss:2.367704132236145\n",
      "train loss:2.365398065808961\n",
      "train loss:2.3648449317288733\n",
      "train loss:2.3625819907654684\n",
      "train loss:2.3698670343952264\n",
      "train loss:2.3645145859065217\n",
      "train loss:2.3583282260422487\n",
      "train loss:2.357456788608884\n",
      "train loss:2.3698328323405895\n",
      "train loss:2.363717971267363\n",
      "train loss:2.3614493336596905\n",
      "train loss:2.3573674172737564\n",
      "train loss:2.36019030545759\n",
      "train loss:2.365460515524862\n",
      "train loss:2.355988850654556\n",
      "train loss:2.360853524679774\n",
      "train loss:2.361694066565074\n",
      "train loss:2.355960040313773\n",
      "train loss:2.3666006160819046\n",
      "train loss:2.362490237025759\n",
      "train loss:2.35541006415763\n",
      "train loss:2.3606490548567516\n",
      "train loss:2.3553799206030126\n",
      "train loss:2.354937293968797\n",
      "train loss:2.349762491226927\n",
      "train loss:2.353933759915038\n",
      "train loss:2.367009599478507\n",
      "train loss:2.360523074425539\n",
      "train loss:2.355910738559511\n",
      "train loss:2.356502977031514\n",
      "train loss:2.3580046343785526\n",
      "train loss:2.348307341146543\n",
      "train loss:2.36452760891341\n",
      "train loss:2.364144614604668\n",
      "train loss:2.3584741723044136\n",
      "train loss:2.350522963383182\n",
      "train loss:2.3682232468677684\n",
      "train loss:2.3620118299462125\n",
      "train loss:2.3608880827890135\n",
      "train loss:2.3580092131858224\n",
      "train loss:2.3618026925066853\n",
      "train loss:2.3594994690848403\n",
      "train loss:2.3535156764968734\n",
      "train loss:2.34432779215122\n",
      "train loss:2.357577195996924\n",
      "train loss:2.3592688381617264\n",
      "train loss:2.355517822126972\n",
      "train loss:2.35656401570457\n",
      "train loss:2.3537558302597392\n",
      "train loss:2.355699312385595\n",
      "train loss:2.3589116506320185\n",
      "train loss:2.3640614993588365\n",
      "train loss:2.361384412671128\n",
      "train loss:2.3438745596727504\n",
      "train loss:2.3547787446812816\n",
      "train loss:2.3579710178953905\n",
      "train loss:2.3543838150998386\n",
      "train loss:2.356479041247605\n",
      "train loss:2.3573828996599575\n",
      "train loss:2.3545094333246883\n",
      "train loss:2.3474621031271057\n",
      "train loss:2.3468796837519794\n",
      "train loss:2.3560098698200425\n",
      "train loss:2.3446381773277283\n",
      "train loss:2.3524695915472265\n",
      "train loss:2.3580123722269106\n",
      "train loss:2.3609773468443818\n",
      "train loss:2.346318348745951\n",
      "train loss:2.353597195619773\n",
      "train loss:2.3630702958404686\n",
      "train loss:2.360684997836806\n",
      "train loss:2.3478680165924137\n",
      "train loss:2.361739005627167\n",
      "train loss:2.3585956073651233\n",
      "train loss:2.3485282241035788\n",
      "train loss:2.3545024899503812\n",
      "train loss:2.344462040966275\n",
      "train loss:2.3583791956578795\n",
      "train loss:2.3576265514469594\n",
      "train loss:2.3515604353947412\n",
      "train loss:2.3551265471825436\n",
      "train loss:2.3448786741368335\n",
      "train loss:2.356077441507175\n",
      "train loss:2.353032651020514\n",
      "train loss:2.3561436280775494\n",
      "train loss:2.3438221266545916\n",
      "train loss:2.361158426207113\n",
      "train loss:2.3518693807124658\n",
      "train loss:2.3571440311372513\n",
      "train loss:2.344810726095616\n",
      "train loss:2.351130737876637\n",
      "train loss:2.3584565112678755\n",
      "train loss:2.3549435306161466\n",
      "train loss:2.3511706792488924\n",
      "train loss:2.3512881220228525\n",
      "train loss:2.357777500731786\n",
      "train loss:2.350919168599219\n",
      "train loss:2.3571336479786265\n",
      "train loss:2.3513484974993526\n",
      "train loss:2.346349944434053\n",
      "train loss:2.3518018281979267\n",
      "train loss:2.3506490422232456\n",
      "train loss:2.3485806541441447\n",
      "train loss:2.355206857059527\n",
      "train loss:2.3417316960733605\n",
      "train loss:2.345861712199334\n",
      "train loss:2.349230656677037\n",
      "train loss:2.3518800082554514\n",
      "train loss:2.349000721278501\n",
      "train loss:2.3416162119394484\n",
      "train loss:2.339748860350511\n",
      "train loss:2.358830862406962\n",
      "train loss:2.3542667462009863\n",
      "train loss:2.337545093949152\n",
      "train loss:2.3586064746860576\n",
      "train loss:2.3516568451839026\n",
      "train loss:2.3443245385085847\n",
      "train loss:2.354470314725456\n",
      "train loss:2.3468319410774017\n",
      "train loss:2.356453512174216\n",
      "train loss:2.35249771139942\n",
      "train loss:2.3524423089526074\n",
      "train loss:2.3578782322102603\n",
      "train loss:2.3402078484984727\n",
      "train loss:2.3487154706815887\n",
      "train loss:2.3518991476609337\n",
      "train loss:2.3458634800711944\n",
      "train loss:2.351232861419445\n",
      "train loss:2.361490063945916\n",
      "train loss:2.3489397164665053\n",
      "train loss:2.351733396276158\n",
      "train loss:2.352134665821182\n",
      "train loss:2.3615253267839154\n",
      "train loss:2.3418250073247373\n",
      "train loss:2.36043114746379\n",
      "train loss:2.350685222656252\n",
      "train loss:2.353170293748504\n",
      "train loss:2.351967226017258\n",
      "train loss:2.35719061492145\n",
      "train loss:2.340135681046703\n",
      "train loss:2.345569695128629\n",
      "train loss:2.3420980242877634\n",
      "train loss:2.338604606045851\n",
      "train loss:2.3456061870712053\n",
      "train loss:2.3440687686694326\n",
      "train loss:2.3493100685891277\n",
      "train loss:2.3505256140121253\n",
      "train loss:2.3429700109898284\n",
      "train loss:2.3391817094681966\n",
      "train loss:2.349904705996862\n",
      "train loss:2.351821669141441\n",
      "train loss:2.3523800025275228\n",
      "train loss:2.3527934672989446\n",
      "train loss:2.34713672936583\n",
      "train loss:2.3418137204457494\n",
      "train loss:2.352565071068618\n",
      "train loss:2.349045858172091\n",
      "train loss:2.3454036760882793\n",
      "=== epoch:19, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3476300780395527\n",
      "train loss:2.348874446263885\n",
      "train loss:2.3419363790185064\n",
      "train loss:2.3472494191963715\n",
      "train loss:2.3386643788321253\n",
      "train loss:2.3538719243570716\n",
      "train loss:2.351383949505874\n",
      "train loss:2.3458603384021197\n",
      "train loss:2.3470839074613554\n",
      "train loss:2.336230269948643\n",
      "train loss:2.336991353507266\n",
      "train loss:2.3437436103469906\n",
      "train loss:2.3554621865895835\n",
      "train loss:2.3419208738469335\n",
      "train loss:2.3481593908818352\n",
      "train loss:2.3414234151096442\n",
      "train loss:2.353456159570132\n",
      "train loss:2.343455726828083\n",
      "train loss:2.348707817785796\n",
      "train loss:2.350158953185447\n",
      "train loss:2.3355460044236604\n",
      "train loss:2.351837926208327\n",
      "train loss:2.3349334368857986\n",
      "train loss:2.3416406366398963\n",
      "train loss:2.3367889192116236\n",
      "train loss:2.3495868074531066\n",
      "train loss:2.334709604657448\n",
      "train loss:2.34174946581103\n",
      "train loss:2.3388238789587503\n",
      "train loss:2.328347268098472\n",
      "train loss:2.3506439480129457\n",
      "train loss:2.348780608979196\n",
      "train loss:2.345182602088919\n",
      "train loss:2.3450448041902456\n",
      "train loss:2.3409282407413246\n",
      "train loss:2.3456668581875473\n",
      "train loss:2.3456097091981234\n",
      "train loss:2.341277992517956\n",
      "train loss:2.341953178210652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.343621821689632\n",
      "train loss:2.3430709528423437\n",
      "train loss:2.344607408891151\n",
      "train loss:2.350763373717784\n",
      "train loss:2.3352743992765452\n",
      "train loss:2.352769449474593\n",
      "train loss:2.3502770829155466\n",
      "train loss:2.344180993032282\n",
      "train loss:2.3294201530614647\n",
      "train loss:2.352634199249802\n",
      "train loss:2.3446387959467816\n",
      "train loss:2.3392608768735426\n",
      "train loss:2.3316649145546604\n",
      "train loss:2.3379941926427725\n",
      "train loss:2.336805545385568\n",
      "train loss:2.3391966153803847\n",
      "train loss:2.3463673993879315\n",
      "train loss:2.3386629931656446\n",
      "train loss:2.34214402310915\n",
      "train loss:2.344388522228585\n",
      "train loss:2.348657132793486\n",
      "train loss:2.340835054570804\n",
      "train loss:2.341748104086433\n",
      "train loss:2.3357762164573015\n",
      "train loss:2.3410264165516605\n",
      "train loss:2.3408932212363203\n",
      "train loss:2.3457567360140787\n",
      "train loss:2.3428037271487336\n",
      "train loss:2.3452503632101713\n",
      "train loss:2.3480322991614213\n",
      "train loss:2.3461103793342435\n",
      "train loss:2.343774639752967\n",
      "train loss:2.3383737671837794\n",
      "train loss:2.334816452799521\n",
      "train loss:2.32938544724352\n",
      "train loss:2.3349776787145284\n",
      "train loss:2.3426704978791726\n",
      "train loss:2.3396593861738686\n",
      "train loss:2.3443154829448876\n",
      "train loss:2.337999846656932\n",
      "train loss:2.3398378768433656\n",
      "train loss:2.3372099337784205\n",
      "train loss:2.339228776835729\n",
      "train loss:2.339215127270885\n",
      "train loss:2.340617657037342\n",
      "train loss:2.3374134896398915\n",
      "train loss:2.3380174274716583\n",
      "train loss:2.3297830533084642\n",
      "train loss:2.3203483342366287\n",
      "train loss:2.340267189569523\n",
      "train loss:2.345227477073036\n",
      "train loss:2.333347903362061\n",
      "train loss:2.3402278252867688\n",
      "train loss:2.3400800864693836\n",
      "train loss:2.3385715708021815\n",
      "train loss:2.3374441627651574\n",
      "train loss:2.333343310925549\n",
      "train loss:2.332297428787301\n",
      "train loss:2.333660513904811\n",
      "train loss:2.331740578967833\n",
      "train loss:2.3387528003696643\n",
      "train loss:2.335850097363783\n",
      "train loss:2.3293772646003768\n",
      "train loss:2.339815861507067\n",
      "train loss:2.3361954279083244\n",
      "train loss:2.346309702751938\n",
      "train loss:2.344583231769071\n",
      "train loss:2.346126305633111\n",
      "train loss:2.344075704867325\n",
      "train loss:2.328928229489248\n",
      "train loss:2.332684567300994\n",
      "train loss:2.3350343116618855\n",
      "train loss:2.336500455159682\n",
      "train loss:2.3395700897213607\n",
      "train loss:2.3411303583051652\n",
      "train loss:2.3431412373939087\n",
      "train loss:2.3462844625139114\n",
      "train loss:2.337416285395132\n",
      "train loss:2.337791147318228\n",
      "train loss:2.3495738205960435\n",
      "train loss:2.335722969179836\n",
      "train loss:2.3301311120280395\n",
      "train loss:2.345871344830014\n",
      "train loss:2.335955374615154\n",
      "train loss:2.345833772067416\n",
      "train loss:2.335790165383579\n",
      "train loss:2.3463897261194093\n",
      "train loss:2.3394763698864653\n",
      "train loss:2.3431175914431335\n",
      "train loss:2.338957466115653\n",
      "train loss:2.3338405238469475\n",
      "train loss:2.3398591581685375\n",
      "train loss:2.336446558895867\n",
      "train loss:2.3342321211687413\n",
      "train loss:2.3292943800352157\n",
      "train loss:2.3444648312613716\n",
      "train loss:2.3331285712968883\n",
      "train loss:2.335644255300273\n",
      "train loss:2.339638917285167\n",
      "train loss:2.3301819375174584\n",
      "train loss:2.334163378651061\n",
      "train loss:2.3362515441911174\n",
      "train loss:2.342453180971376\n",
      "train loss:2.339065083491929\n",
      "train loss:2.3354494957173952\n",
      "train loss:2.338037581382654\n",
      "train loss:2.3412272440141466\n",
      "train loss:2.326920917933405\n",
      "train loss:2.3325517667960187\n",
      "train loss:2.3297809443954085\n",
      "train loss:2.334110939849222\n",
      "train loss:2.3308246126731658\n",
      "train loss:2.331621038687078\n",
      "train loss:2.3299267717997134\n",
      "train loss:2.3364207426841066\n",
      "train loss:2.3321444326065244\n",
      "train loss:2.3358684040551085\n",
      "train loss:2.342111569024087\n",
      "train loss:2.334108031161656\n",
      "train loss:2.3276627212796646\n",
      "train loss:2.3411449654681675\n",
      "train loss:2.3318912714057207\n",
      "train loss:2.3365876717838643\n",
      "train loss:2.332381533643558\n",
      "train loss:2.3393708196128182\n",
      "train loss:2.3364544309933923\n",
      "train loss:2.3259233376697073\n",
      "train loss:2.3309407159405504\n",
      "train loss:2.3356067248625307\n",
      "train loss:2.337891967070217\n",
      "train loss:2.3317439834763105\n",
      "train loss:2.336557705999123\n",
      "train loss:2.3277919316324813\n",
      "train loss:2.3388086390756526\n",
      "train loss:2.3394943598638034\n",
      "train loss:2.338937319018264\n",
      "train loss:2.3413515543092176\n",
      "train loss:2.3198084159844266\n",
      "train loss:2.3296340148239487\n",
      "train loss:2.3350414114470475\n",
      "train loss:2.324049485887114\n",
      "train loss:2.3378328584558736\n",
      "train loss:2.334242693898513\n",
      "train loss:2.33092420703195\n",
      "train loss:2.3270863987789294\n",
      "train loss:2.3300447186646718\n",
      "train loss:2.327369337662917\n",
      "train loss:2.3397048791805397\n",
      "train loss:2.338350282223363\n",
      "train loss:2.329018369556217\n",
      "train loss:2.3424016321507004\n",
      "train loss:2.331394948678591\n",
      "train loss:2.331479231051715\n",
      "train loss:2.331690642202638\n",
      "train loss:2.3234953622295116\n",
      "train loss:2.326114448747016\n",
      "train loss:2.3314333725267615\n",
      "train loss:2.332985980250045\n",
      "train loss:2.334035729413871\n",
      "train loss:2.3324993269911967\n",
      "train loss:2.3387637489515654\n",
      "=== epoch:20, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.333578803255121\n",
      "train loss:2.3301072300675374\n",
      "train loss:2.32680111993871\n",
      "train loss:2.3241051027814503\n",
      "train loss:2.3279824358942127\n",
      "train loss:2.334024703930865\n",
      "train loss:2.3259754729387137\n",
      "train loss:2.330044951334847\n",
      "train loss:2.328384130776128\n",
      "train loss:2.335312807702286\n",
      "train loss:2.3365595651599835\n",
      "train loss:2.3286207698303567\n",
      "train loss:2.3290645904721665\n",
      "train loss:2.3256587903219725\n",
      "train loss:2.328731392417589\n",
      "train loss:2.3333765143241285\n",
      "train loss:2.338502954234811\n",
      "train loss:2.33291328317493\n",
      "train loss:2.3299302313378534\n",
      "train loss:2.327014919240589\n",
      "train loss:2.321712659620151\n",
      "train loss:2.3272183233202095\n",
      "train loss:2.330730524262138\n",
      "train loss:2.3278255412788638\n",
      "train loss:2.324353639842455\n",
      "train loss:2.333849267343933\n",
      "train loss:2.3252443646919945\n",
      "train loss:2.339673667340907\n",
      "train loss:2.3326899394576617\n",
      "train loss:2.330194055646987\n",
      "train loss:2.321683017741726\n",
      "train loss:2.3270056525585883\n",
      "train loss:2.3284523825880052\n",
      "train loss:2.3284786984235555\n",
      "train loss:2.3289093467677255\n",
      "train loss:2.338419044261545\n",
      "train loss:2.318230144088947\n",
      "train loss:2.334765484439383\n",
      "train loss:2.322482474749741\n",
      "train loss:2.328953135232459\n",
      "train loss:2.332134937904494\n",
      "train loss:2.3270568163978544\n",
      "train loss:2.331747283796155\n",
      "train loss:2.3314165346576616\n",
      "train loss:2.328001779780505\n",
      "train loss:2.3323945726320945\n",
      "train loss:2.334926390005561\n",
      "train loss:2.3312104779600293\n",
      "train loss:2.326340852954425\n",
      "train loss:2.3224576662972507\n",
      "train loss:2.330813607951347\n",
      "train loss:2.3363260921018334\n",
      "train loss:2.3207751172435898\n",
      "train loss:2.334391765915659\n",
      "train loss:2.326050595746968\n",
      "train loss:2.3369920265782564\n",
      "train loss:2.325191306053369\n",
      "train loss:2.3212348032703187\n",
      "train loss:2.323933694975231\n",
      "train loss:2.324678275064838\n",
      "train loss:2.328073004021309\n",
      "train loss:2.3319982128684686\n",
      "train loss:2.3234167582390426\n",
      "train loss:2.323465905357794\n",
      "train loss:2.3334070778342606\n",
      "train loss:2.3315715456246395\n",
      "train loss:2.326687606417262\n",
      "train loss:2.3243118432947902\n",
      "train loss:2.329749418841\n",
      "train loss:2.3206225732455152\n",
      "train loss:2.32860253366098\n",
      "train loss:2.329223819448526\n",
      "train loss:2.3200896509759574\n",
      "train loss:2.3241626434485734\n",
      "train loss:2.332672219885609\n",
      "train loss:2.334895955567959\n",
      "train loss:2.327109457804347\n",
      "train loss:2.3282033013193586\n",
      "train loss:2.3294854248840378\n",
      "train loss:2.32811510878553\n",
      "train loss:2.3235684158210392\n",
      "train loss:2.3314367307213226\n",
      "train loss:2.325174728683552\n",
      "train loss:2.32340476770766\n",
      "train loss:2.3275597274215047\n",
      "train loss:2.3202831464239173\n",
      "train loss:2.3239273615092815\n",
      "train loss:2.317932124126495\n",
      "train loss:2.3272684003293738\n",
      "train loss:2.3247851894734715\n",
      "train loss:2.3256244551044905\n",
      "train loss:2.322873173218294\n",
      "train loss:2.326372858252386\n",
      "train loss:2.331735428648611\n",
      "train loss:2.337084065563985\n",
      "train loss:2.327663039192709\n",
      "train loss:2.320784922599893\n",
      "train loss:2.3243008425635394\n",
      "train loss:2.324216102342665\n",
      "train loss:2.3181713632731507\n",
      "train loss:2.324633092068109\n",
      "train loss:2.3214280061752044\n",
      "train loss:2.319990837896179\n",
      "train loss:2.3203180896744264\n",
      "train loss:2.3230632705361107\n",
      "train loss:2.3254665705611144\n",
      "train loss:2.3245626357336993\n",
      "train loss:2.3229138807809755\n",
      "train loss:2.329296983768628\n",
      "train loss:2.319195902950756\n",
      "train loss:2.3248850236940117\n",
      "train loss:2.3325075535549176\n",
      "train loss:2.3195690052823474\n",
      "train loss:2.3270167330708604\n",
      "train loss:2.330757282695372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3259859924747546\n",
      "train loss:2.3295957228671624\n",
      "train loss:2.325378360081498\n",
      "train loss:2.3225903398213505\n",
      "train loss:2.3215424302546483\n",
      "train loss:2.317673027142617\n",
      "train loss:2.3234612204352305\n",
      "train loss:2.3356008996472073\n",
      "train loss:2.324978323602678\n",
      "train loss:2.321431373375548\n",
      "train loss:2.3159271991404413\n",
      "train loss:2.336122551957229\n",
      "train loss:2.3332933359905863\n",
      "train loss:2.3261831903981576\n",
      "train loss:2.3292983208242317\n",
      "train loss:2.317776022105545\n",
      "train loss:2.3212886933269736\n",
      "train loss:2.31036716491506\n",
      "train loss:2.3289813001895636\n",
      "train loss:2.3297923522859945\n",
      "train loss:2.3199196296384503\n",
      "train loss:2.325214662270092\n",
      "train loss:2.3275652508658986\n",
      "train loss:2.321283440763516\n",
      "train loss:2.330300612362387\n",
      "train loss:2.330990449653711\n",
      "train loss:2.321334805568397\n",
      "train loss:2.3264726437367367\n",
      "train loss:2.3313065512863944\n",
      "train loss:2.319732392406744\n",
      "train loss:2.3300430240664185\n",
      "train loss:2.318365593891748\n",
      "train loss:2.3219900891143572\n",
      "train loss:2.3243395963968836\n",
      "train loss:2.3141746288359157\n",
      "train loss:2.314265546738644\n",
      "train loss:2.324184147912781\n",
      "train loss:2.322589407817421\n",
      "train loss:2.3221230582912433\n",
      "train loss:2.3266154795607115\n",
      "train loss:2.3197408263551775\n",
      "train loss:2.326352156790658\n",
      "train loss:2.330141448917225\n",
      "train loss:2.3252777109937144\n",
      "train loss:2.3201893196497507\n",
      "train loss:2.3222787924246675\n",
      "train loss:2.326578534272833\n",
      "train loss:2.3269627480627997\n",
      "train loss:2.3251679906815874\n",
      "train loss:2.3239061525678597\n",
      "train loss:2.3205478902881187\n",
      "train loss:2.3167497108329558\n",
      "train loss:2.3219294156080763\n",
      "train loss:2.3266328596595827\n",
      "train loss:2.322249220367726\n",
      "train loss:2.3257989129264622\n",
      "train loss:2.3215903337512347\n",
      "train loss:2.3249036834692047\n",
      "train loss:2.324236502095353\n",
      "train loss:2.3313206721898516\n",
      "train loss:2.321419829255958\n",
      "train loss:2.315387333893977\n",
      "train loss:2.3227560909085696\n",
      "train loss:2.328244835411348\n",
      "train loss:2.325234556667408\n",
      "train loss:2.3168304616622715\n",
      "train loss:2.3228295688971268\n",
      "train loss:2.316332353056782\n",
      "train loss:2.323897067118209\n",
      "train loss:2.326382333744542\n",
      "train loss:2.320421699200723\n",
      "train loss:2.321046156625786\n",
      "train loss:2.323603137670045\n",
      "train loss:2.3151342414429856\n",
      "train loss:2.32217339595045\n",
      "train loss:2.3110782082302572\n",
      "train loss:2.323731560650835\n",
      "train loss:2.3203519943559514\n",
      "train loss:2.3224588099238255\n",
      "train loss:2.321606988755463\n",
      "train loss:2.323481102502439\n",
      "train loss:2.320990002011193\n",
      "train loss:2.3216018516820887\n",
      "train loss:2.320949635306052\n",
      "train loss:2.3232646969473363\n",
      "=== epoch:21, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3119075653417394\n",
      "train loss:2.315356918921472\n",
      "train loss:2.3157241515311484\n",
      "train loss:2.321352438491474\n",
      "train loss:2.316316351788598\n",
      "train loss:2.3216041828383625\n",
      "train loss:2.323283639432354\n",
      "train loss:2.326636081788635\n",
      "train loss:2.3240969223312753\n",
      "train loss:2.3152105268482353\n",
      "train loss:2.327832516625379\n",
      "train loss:2.3215210303549263\n",
      "train loss:2.3202177775410555\n",
      "train loss:2.3270598856758196\n",
      "train loss:2.3204521946498833\n",
      "train loss:2.3071857381014036\n",
      "train loss:2.319803319019708\n",
      "train loss:2.32273040202172\n",
      "train loss:2.3140232764631854\n",
      "train loss:2.321054254343552\n",
      "train loss:2.3248167601838596\n",
      "train loss:2.320530300716445\n",
      "train loss:2.318338319537152\n",
      "train loss:2.3231159073774785\n",
      "train loss:2.3203321580789997\n",
      "train loss:2.322617466087402\n",
      "train loss:2.321243322430858\n",
      "train loss:2.315096923303063\n",
      "train loss:2.322477951169407\n",
      "train loss:2.3227385686548105\n",
      "train loss:2.31522040391871\n",
      "train loss:2.3179527174120955\n",
      "train loss:2.326561974124743\n",
      "train loss:2.325721520895046\n",
      "train loss:2.327280645542722\n",
      "train loss:2.3199682269994817\n",
      "train loss:2.3139064820457165\n",
      "train loss:2.317348027312277\n",
      "train loss:2.3219040160652176\n",
      "train loss:2.3259328366746956\n",
      "train loss:2.3219266124011173\n",
      "train loss:2.316724666335468\n",
      "train loss:2.307926601357433\n",
      "train loss:2.318034073023212\n",
      "train loss:2.31700126412239\n",
      "train loss:2.3170051411388215\n",
      "train loss:2.320029529318592\n",
      "train loss:2.3187193859994326\n",
      "train loss:2.3260911311000396\n",
      "train loss:2.3107230683268516\n",
      "train loss:2.3118825939476815\n",
      "train loss:2.3145580759812416\n",
      "train loss:2.3188491307140957\n",
      "train loss:2.3218942093529154\n",
      "train loss:2.3213040732547863\n",
      "train loss:2.31105174674637\n",
      "train loss:2.3173491231479586\n",
      "train loss:2.3140957366397865\n",
      "train loss:2.3173915476847227\n",
      "train loss:2.32348383997032\n",
      "train loss:2.3061809175680414\n",
      "train loss:2.3232175445435512\n",
      "train loss:2.3298664242390896\n",
      "train loss:2.3252386885452623\n",
      "train loss:2.311268589091895\n",
      "train loss:2.309504106484907\n",
      "train loss:2.3212178897938474\n",
      "train loss:2.314219564078162\n",
      "train loss:2.3251917047450372\n",
      "train loss:2.3154678603165935\n",
      "train loss:2.3316222120149837\n",
      "train loss:2.3151200736106197\n",
      "train loss:2.3243624241218575\n",
      "train loss:2.317533212181585\n",
      "train loss:2.324786911619517\n",
      "train loss:2.314111012554132\n",
      "train loss:2.3171221529595853\n",
      "train loss:2.314443413013119\n",
      "train loss:2.319672844514634\n",
      "train loss:2.3286460451310895\n",
      "train loss:2.3096747580369237\n",
      "train loss:2.319311201556296\n",
      "train loss:2.321864972519946\n",
      "train loss:2.317698556170216\n",
      "train loss:2.312131708298352\n",
      "train loss:2.322533816480218\n",
      "train loss:2.3159444057586938\n",
      "train loss:2.3049413370111913\n",
      "train loss:2.3202039147631686\n",
      "train loss:2.31217700713035\n",
      "train loss:2.3253003701953343\n",
      "train loss:2.312936775592579\n",
      "train loss:2.3246350320271145\n",
      "train loss:2.3201776118981727\n",
      "train loss:2.317752171428948\n",
      "train loss:2.3161239616625555\n",
      "train loss:2.3218172556531256\n",
      "train loss:2.3223222327698876\n",
      "train loss:2.319704719257847\n",
      "train loss:2.3255180158318054\n",
      "train loss:2.3165297077822133\n",
      "train loss:2.3185993012110293\n",
      "train loss:2.3169406634085132\n",
      "train loss:2.319768573213374\n",
      "train loss:2.313184669695805\n",
      "train loss:2.3043579432678762\n",
      "train loss:2.312910876116104\n",
      "train loss:2.31156056601019\n",
      "train loss:2.329403223116697\n",
      "train loss:2.317764651299516\n",
      "train loss:2.314399043187626\n",
      "train loss:2.3247833074181035\n",
      "train loss:2.3206659286466578\n",
      "train loss:2.3213236391603886\n",
      "train loss:2.3173011315320404\n",
      "train loss:2.3134729295266165\n",
      "train loss:2.3195388690155543\n",
      "train loss:2.318406265072638\n",
      "train loss:2.3195327105443764\n",
      "train loss:2.309727832909114\n",
      "train loss:2.311257342415424\n",
      "train loss:2.320328759544635\n",
      "train loss:2.3191762788696098\n",
      "train loss:2.317387343377674\n",
      "train loss:2.3182414326280334\n",
      "train loss:2.3141762560120687\n",
      "train loss:2.3202954388834818\n",
      "train loss:2.3112303251539243\n",
      "train loss:2.330107207269127\n",
      "train loss:2.3187241812829202\n",
      "train loss:2.321402162764619\n",
      "train loss:2.3086256719605376\n",
      "train loss:2.3232747747702795\n",
      "train loss:2.3093041058313917\n",
      "train loss:2.3123929969875996\n",
      "train loss:2.3222047699127035\n",
      "train loss:2.3113851169901567\n",
      "train loss:2.3146698079612342\n",
      "train loss:2.316656545090554\n",
      "train loss:2.3185960737082065\n",
      "train loss:2.324504042046981\n",
      "train loss:2.3136909774933736\n",
      "train loss:2.316102422318755\n",
      "train loss:2.309585982241998\n",
      "train loss:2.319838677953497\n",
      "train loss:2.3133206800347974\n",
      "train loss:2.3152456770820105\n",
      "train loss:2.3188724460455177\n",
      "train loss:2.31673775389895\n",
      "train loss:2.313052515411735\n",
      "train loss:2.3154830980025394\n",
      "train loss:2.325593973806967\n",
      "train loss:2.328676656497584\n",
      "train loss:2.3217828375048537\n",
      "train loss:2.3251806236357697\n",
      "train loss:2.3249878506968327\n",
      "train loss:2.320599221310673\n",
      "train loss:2.3112132559106735\n",
      "train loss:2.3198143148211696\n",
      "train loss:2.322869000358013\n",
      "train loss:2.31803778740031\n",
      "train loss:2.320507916370265\n",
      "train loss:2.3150879415498395\n",
      "train loss:2.3150876955956843\n",
      "train loss:2.3124947211737696\n",
      "train loss:2.3187415093544796\n",
      "train loss:2.3175620203714704\n",
      "train loss:2.306618527491267\n",
      "train loss:2.3142978454630474\n",
      "train loss:2.3154568939616316\n",
      "train loss:2.324216131971273\n",
      "train loss:2.3103181829464803\n",
      "train loss:2.318901214716956\n",
      "train loss:2.3063815412480504\n",
      "train loss:2.309092221357581\n",
      "train loss:2.3124017065016385\n",
      "train loss:2.3157519032619565\n",
      "train loss:2.306989364679444\n",
      "train loss:2.3060645172392156\n",
      "train loss:2.3159253438389618\n",
      "train loss:2.3133849892918796\n",
      "train loss:2.3155481812698127\n",
      "train loss:2.31332924953928\n",
      "train loss:2.307202122987048\n",
      "train loss:2.3170235296585746\n",
      "train loss:2.316370205925333\n",
      "train loss:2.3092286088098954\n",
      "train loss:2.312001574763767\n",
      "train loss:2.3132962285349827\n",
      "train loss:2.3193391031399013\n",
      "train loss:2.3077561289753614\n",
      "train loss:2.3123184127905603\n",
      "train loss:2.323627171251203\n",
      "train loss:2.3132476479584696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3160769378879142\n",
      "train loss:2.3162163254931296\n",
      "train loss:2.314202828164135\n",
      "train loss:2.319151344262763\n",
      "train loss:2.3205260605368907\n",
      "train loss:2.3063173200818805\n",
      "=== epoch:22, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.315139505641401\n",
      "train loss:2.313442543154131\n",
      "train loss:2.3276479886512647\n",
      "train loss:2.3172338275380286\n",
      "train loss:2.3091213090384906\n",
      "train loss:2.3211955683064676\n",
      "train loss:2.3093907340202686\n",
      "train loss:2.3132439474938407\n",
      "train loss:2.3265069483665424\n",
      "train loss:2.3181061491716455\n",
      "train loss:2.3167539847940417\n",
      "train loss:2.3068467078522827\n",
      "train loss:2.31190626775235\n",
      "train loss:2.3130396509437605\n",
      "train loss:2.3166421102923116\n",
      "train loss:2.3062694639484436\n",
      "train loss:2.3219498755597896\n",
      "train loss:2.3097606246654103\n",
      "train loss:2.3166674198651886\n",
      "train loss:2.3103334053628224\n",
      "train loss:2.3095336033925293\n",
      "train loss:2.317381069884117\n",
      "train loss:2.3207203441976243\n",
      "train loss:2.3165930874164062\n",
      "train loss:2.319272461690766\n",
      "train loss:2.310975792165392\n",
      "train loss:2.3223865180983907\n",
      "train loss:2.3088755031745616\n",
      "train loss:2.314332183512578\n",
      "train loss:2.316706144139963\n",
      "train loss:2.318923230390768\n",
      "train loss:2.3226950100152663\n",
      "train loss:2.308573356473544\n",
      "train loss:2.3126153475673643\n",
      "train loss:2.3145614664345304\n",
      "train loss:2.311437735451004\n",
      "train loss:2.319894075907609\n",
      "train loss:2.3027247189287006\n",
      "train loss:2.3141558684642596\n",
      "train loss:2.3076176856924606\n",
      "train loss:2.315001592159125\n",
      "train loss:2.318246362069463\n",
      "train loss:2.303787203800047\n",
      "train loss:2.3158831190412523\n",
      "train loss:2.31977217805758\n",
      "train loss:2.308424854079309\n",
      "train loss:2.318514919071348\n",
      "train loss:2.3034047701468636\n",
      "train loss:2.3111034722181167\n",
      "train loss:2.3189485683469506\n",
      "train loss:2.3131827885880045\n",
      "train loss:2.3080398975229124\n",
      "train loss:2.3113204784825014\n",
      "train loss:2.3184857963478156\n",
      "train loss:2.323275564402215\n",
      "train loss:2.3138088759719446\n",
      "train loss:2.321072127665514\n",
      "train loss:2.3141683767150325\n",
      "train loss:2.308486440405266\n",
      "train loss:2.3189536424184176\n",
      "train loss:2.3169638691406176\n",
      "train loss:2.31481501728216\n",
      "train loss:2.3152509834275934\n",
      "train loss:2.3231170235863616\n",
      "train loss:2.308218019254316\n",
      "train loss:2.310711524275529\n",
      "train loss:2.311579974345031\n",
      "train loss:2.3084282347879204\n",
      "train loss:2.3180489595010734\n",
      "train loss:2.317050296384552\n",
      "train loss:2.3116506019729846\n",
      "train loss:2.3107069005748393\n",
      "train loss:2.3132289671752346\n",
      "train loss:2.3138358242946095\n",
      "train loss:2.3152742659491374\n",
      "train loss:2.3143332878131555\n",
      "train loss:2.32060222528392\n",
      "train loss:2.3105686975641127\n",
      "train loss:2.3127669630122245\n",
      "train loss:2.3182172068456395\n",
      "train loss:2.3183159700442455\n",
      "train loss:2.3176768318515353\n",
      "train loss:2.3130404554337414\n",
      "train loss:2.311795778677669\n",
      "train loss:2.321243110270143\n",
      "train loss:2.31922381011319\n",
      "train loss:2.3000706247224763\n",
      "train loss:2.3075096583830494\n",
      "train loss:2.3167111285359043\n",
      "train loss:2.305807162233392\n",
      "train loss:2.3156762000038684\n",
      "train loss:2.3196272696811056\n",
      "train loss:2.3132728829841986\n",
      "train loss:2.317773712879682\n",
      "train loss:2.309376123148443\n",
      "train loss:2.314775669662012\n",
      "train loss:2.305854901189163\n",
      "train loss:2.3049342147266727\n",
      "train loss:2.3108534281103017\n",
      "train loss:2.306462254067723\n",
      "train loss:2.3099908287094526\n",
      "train loss:2.3107327047553117\n",
      "train loss:2.3172830627074505\n",
      "train loss:2.316047222835933\n",
      "train loss:2.309001033767382\n",
      "train loss:2.315860043674512\n",
      "train loss:2.3125912298567264\n",
      "train loss:2.3102948373755807\n",
      "train loss:2.3041202106703658\n",
      "train loss:2.3161543329889303\n",
      "train loss:2.3124776656700963\n",
      "train loss:2.3061942160591005\n",
      "train loss:2.309053073866143\n",
      "train loss:2.298213717058332\n",
      "train loss:2.309230763724799\n",
      "train loss:2.3155481051516458\n",
      "train loss:2.3106535765036273\n",
      "train loss:2.3001647476705784\n",
      "train loss:2.3062509208998345\n",
      "train loss:2.3168040467926487\n",
      "train loss:2.30784701106289\n",
      "train loss:2.3167726403076134\n",
      "train loss:2.312135792391844\n",
      "train loss:2.305900241851268\n",
      "train loss:2.3124669943690894\n",
      "train loss:2.315761350486704\n",
      "train loss:2.3063662564824523\n",
      "train loss:2.3031451346560776\n",
      "train loss:2.3147734713633232\n",
      "train loss:2.3127109726376744\n",
      "train loss:2.3198132219450085\n",
      "train loss:2.3061162086410327\n",
      "train loss:2.3105815133987075\n",
      "train loss:2.298599353031797\n",
      "train loss:2.3027888626165307\n",
      "train loss:2.303500125948238\n",
      "train loss:2.308889123403402\n",
      "train loss:2.314562342847838\n",
      "train loss:2.3143825009401153\n",
      "train loss:2.315600600145807\n",
      "train loss:2.31053708946716\n",
      "train loss:2.303743218855287\n",
      "train loss:2.3131404137695806\n",
      "train loss:2.312893642639407\n",
      "train loss:2.31488651606041\n",
      "train loss:2.3081820055505866\n",
      "train loss:2.315139227089662\n",
      "train loss:2.3082498739202433\n",
      "train loss:2.32108299195031\n",
      "train loss:2.310176437944466\n",
      "train loss:2.2990840043957923\n",
      "train loss:2.311805327677831\n",
      "train loss:2.305831771758729\n",
      "train loss:2.3118010870823733\n",
      "train loss:2.316345629966075\n",
      "train loss:2.3154666021116133\n",
      "train loss:2.320560326740259\n",
      "train loss:2.308824062002347\n",
      "train loss:2.3058825958597033\n",
      "train loss:2.3111489862574603\n",
      "train loss:2.31325662895833\n",
      "train loss:2.3154758949509304\n",
      "train loss:2.3129194266833357\n",
      "train loss:2.315538923458828\n",
      "train loss:2.3142704809009618\n",
      "train loss:2.310849722519206\n",
      "train loss:2.3073748360412027\n",
      "train loss:2.3102720197838087\n",
      "train loss:2.3122577613611504\n",
      "train loss:2.3043868108660623\n",
      "train loss:2.3080285669937117\n",
      "train loss:2.3067191677298045\n",
      "train loss:2.3105005588114675\n",
      "train loss:2.311174160100767\n",
      "train loss:2.3171233119461254\n",
      "train loss:2.3134128244868433\n",
      "train loss:2.312378522279182\n",
      "train loss:2.3134611063204464\n",
      "train loss:2.308213641664339\n",
      "train loss:2.3138180705399516\n",
      "train loss:2.3117829094443847\n",
      "train loss:2.3087334814325673\n",
      "train loss:2.318590338793263\n",
      "train loss:2.32160993189177\n",
      "train loss:2.3038296223054573\n",
      "train loss:2.317046128813366\n",
      "train loss:2.3141613392814597\n",
      "train loss:2.315126166823055\n",
      "train loss:2.3109534583570386\n",
      "train loss:2.3089860108589195\n",
      "train loss:2.3068776608310824\n",
      "train loss:2.3058796531415915\n",
      "train loss:2.310889908104654\n",
      "train loss:2.3147710022508416\n",
      "train loss:2.321261379072656\n",
      "train loss:2.3026196337507185\n",
      "train loss:2.312089030022243\n",
      "train loss:2.3111632678546354\n",
      "train loss:2.306028652154886\n",
      "train loss:2.305681041089773\n",
      "=== epoch:23, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3145481576376157\n",
      "train loss:2.3119689470392966\n",
      "train loss:2.3085302924481588\n",
      "train loss:2.3135907004169503\n",
      "train loss:2.308140597655761\n",
      "train loss:2.3072171917098445\n",
      "train loss:2.3071917445018606\n",
      "train loss:2.307979268963753\n",
      "train loss:2.311286890568795\n",
      "train loss:2.310742741153103\n",
      "train loss:2.3211908481205294\n",
      "train loss:2.312956166432338\n",
      "train loss:2.3102218411198234\n",
      "train loss:2.305716319809005\n",
      "train loss:2.3098268962563786\n",
      "train loss:2.3149372235158334\n",
      "train loss:2.304100916665078\n",
      "train loss:2.3082127572636946\n",
      "train loss:2.31543528425153\n",
      "train loss:2.3103590229305007\n",
      "train loss:2.314160537560017\n",
      "train loss:2.3081573055819264\n",
      "train loss:2.3109108590166416\n",
      "train loss:2.3081484229360982\n",
      "train loss:2.30762421607564\n",
      "train loss:2.3024203864843926\n",
      "train loss:2.310867728257231\n",
      "train loss:2.304560389041\n",
      "train loss:2.322196900909116\n",
      "train loss:2.3137735633842422\n",
      "train loss:2.3164107729839616\n",
      "train loss:2.3144646918352523\n",
      "train loss:2.31349354481368\n",
      "train loss:2.3085940723584817\n",
      "train loss:2.3052604796895086\n",
      "train loss:2.309470443711482\n",
      "train loss:2.303905719086137\n",
      "train loss:2.3063191816303235\n",
      "train loss:2.3002348649539934\n",
      "train loss:2.3097499043356096\n",
      "train loss:2.3021193508936126\n",
      "train loss:2.318593105685298\n",
      "train loss:2.307124067245434\n",
      "train loss:2.314753411956387\n",
      "train loss:2.315848956726878\n",
      "train loss:2.3137264793757737\n",
      "train loss:2.3061253652847946\n",
      "train loss:2.3177984253009236\n",
      "train loss:2.308803783290466\n",
      "train loss:2.3058044556595574\n",
      "train loss:2.3125400845817374\n",
      "train loss:2.307887434162995\n",
      "train loss:2.311945956738244\n",
      "train loss:2.3075450826422004\n",
      "train loss:2.308899899075495\n",
      "train loss:2.3189015335289462\n",
      "train loss:2.300116145904601\n",
      "train loss:2.3166621775744276\n",
      "train loss:2.318823649247389\n",
      "train loss:2.3104453247729877\n",
      "train loss:2.32017049749571\n",
      "train loss:2.307374519738493\n",
      "train loss:2.3072502464421194\n",
      "train loss:2.314892483951098\n",
      "train loss:2.3081813994720983\n",
      "train loss:2.3041145023610916\n",
      "train loss:2.3094531867000927\n",
      "train loss:2.31373019692443\n",
      "train loss:2.3011528629946247\n",
      "train loss:2.3068135995320116\n",
      "train loss:2.3088070176261626\n",
      "train loss:2.2992560091126464\n",
      "train loss:2.3129083658518863\n",
      "train loss:2.309735304521389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.307287405233683\n",
      "train loss:2.3042320450510756\n",
      "train loss:2.303319068398903\n",
      "train loss:2.3148178468515797\n",
      "train loss:2.3057248307203633\n",
      "train loss:2.3189312992447153\n",
      "train loss:2.314717392116383\n",
      "train loss:2.3079190225798647\n",
      "train loss:2.3076888209461237\n",
      "train loss:2.312349810689324\n",
      "train loss:2.3070791415101306\n",
      "train loss:2.311163976474576\n",
      "train loss:2.3126297393756214\n",
      "train loss:2.305644907116881\n",
      "train loss:2.3198829812532145\n",
      "train loss:2.3110135576963193\n",
      "train loss:2.3092359590381033\n",
      "train loss:2.3119194432966474\n",
      "train loss:2.306561526454285\n",
      "train loss:2.303059827423733\n",
      "train loss:2.3034009380102183\n",
      "train loss:2.3124333384829683\n",
      "train loss:2.303871361578259\n",
      "train loss:2.3105293624133436\n",
      "train loss:2.302597203554207\n",
      "train loss:2.3130320609057935\n",
      "train loss:2.305578337071769\n",
      "train loss:2.3002656601834754\n",
      "train loss:2.3069582115996163\n",
      "train loss:2.3117088146341516\n",
      "train loss:2.313193940094198\n",
      "train loss:2.306044140602015\n",
      "train loss:2.299792653720476\n",
      "train loss:2.295829084451279\n",
      "train loss:2.3083227031897495\n",
      "train loss:2.3059299992521667\n",
      "train loss:2.309580674277098\n",
      "train loss:2.3110830638789355\n",
      "train loss:2.3066098130754598\n",
      "train loss:2.311539293002246\n",
      "train loss:2.314142342502246\n",
      "train loss:2.3150707657541987\n",
      "train loss:2.312947606824454\n",
      "train loss:2.3106072610903334\n",
      "train loss:2.3084346936829774\n",
      "train loss:2.3017595714688848\n",
      "train loss:2.3114870499937696\n",
      "train loss:2.307156343289348\n",
      "train loss:2.303284045419952\n",
      "train loss:2.3100691560354885\n",
      "train loss:2.3042509026255495\n",
      "train loss:2.298769748190805\n",
      "train loss:2.314024173119715\n",
      "train loss:2.310025305477125\n",
      "train loss:2.32067212570105\n",
      "train loss:2.30261508945842\n",
      "train loss:2.3021992689240682\n",
      "train loss:2.3126017248550146\n",
      "train loss:2.3145274591885214\n",
      "train loss:2.322929567779426\n",
      "train loss:2.314099301117685\n",
      "train loss:2.3133830057590816\n",
      "train loss:2.3070611943980475\n",
      "train loss:2.3092112454282345\n",
      "train loss:2.310784977605563\n",
      "train loss:2.3160188024076107\n",
      "train loss:2.310260339686289\n",
      "train loss:2.309555784500431\n",
      "train loss:2.3113056674668435\n",
      "train loss:2.30315454290987\n",
      "train loss:2.3150568369011246\n",
      "train loss:2.301481092042262\n",
      "train loss:2.29835561771353\n",
      "train loss:2.305879211176875\n",
      "train loss:2.3128373836147635\n",
      "train loss:2.3055451608773705\n",
      "train loss:2.3071889903582785\n",
      "train loss:2.309408799985242\n",
      "train loss:2.3111433543824385\n",
      "train loss:2.3026392376284748\n",
      "train loss:2.315277838660369\n",
      "train loss:2.3064064539993385\n",
      "train loss:2.313133348893552\n",
      "train loss:2.3072617972009093\n",
      "train loss:2.314439995964708\n",
      "train loss:2.30401533534113\n",
      "train loss:2.3134627433402906\n",
      "train loss:2.2992902945133853\n",
      "train loss:2.3014494967279546\n",
      "train loss:2.3094418039346243\n",
      "train loss:2.317958298495576\n",
      "train loss:2.3133412779670617\n",
      "train loss:2.309163972471664\n",
      "train loss:2.3093672389199495\n",
      "train loss:2.3143894805236975\n",
      "train loss:2.2990746214989795\n",
      "train loss:2.3064654326547878\n",
      "train loss:2.3002661794932493\n",
      "train loss:2.309492064718253\n",
      "train loss:2.299437435969461\n",
      "train loss:2.3100158417175765\n",
      "train loss:2.314630743674256\n",
      "train loss:2.302695411698296\n",
      "train loss:2.306930554599962\n",
      "train loss:2.3062138319241097\n",
      "train loss:2.3081511826459504\n",
      "train loss:2.309608578753819\n",
      "train loss:2.3088344881539773\n",
      "train loss:2.3104212813824607\n",
      "train loss:2.30098148476985\n",
      "train loss:2.3039824249910494\n",
      "train loss:2.3032231471189237\n",
      "train loss:2.30315165492869\n",
      "train loss:2.305466289129708\n",
      "train loss:2.3076742459936077\n",
      "train loss:2.3145360725397275\n",
      "train loss:2.303139901227596\n",
      "train loss:2.314007269186849\n",
      "train loss:2.2992940425218027\n",
      "train loss:2.31016969077231\n",
      "train loss:2.3150242758606674\n",
      "train loss:2.3160904599160905\n",
      "train loss:2.3014907570157677\n",
      "train loss:2.302658726534717\n",
      "train loss:2.315123258905459\n",
      "train loss:2.301166577969174\n",
      "=== epoch:24, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3092987502283955\n",
      "train loss:2.302281524489272\n",
      "train loss:2.310523386929595\n",
      "train loss:2.3133541585434614\n",
      "train loss:2.312622119115967\n",
      "train loss:2.306235469474655\n",
      "train loss:2.3019178230084925\n",
      "train loss:2.304178383515971\n",
      "train loss:2.3029462714968205\n",
      "train loss:2.2952948139336655\n",
      "train loss:2.310614549826981\n",
      "train loss:2.3059673547091526\n",
      "train loss:2.2921084039437067\n",
      "train loss:2.297923642205293\n",
      "train loss:2.316933828775161\n",
      "train loss:2.305917810748056\n",
      "train loss:2.2990691476832725\n",
      "train loss:2.297409601112349\n",
      "train loss:2.3115235302548722\n",
      "train loss:2.3039213495431197\n",
      "train loss:2.3100732189406243\n",
      "train loss:2.3055786456656078\n",
      "train loss:2.3043774192076376\n",
      "train loss:2.313169817552622\n",
      "train loss:2.3121040574510916\n",
      "train loss:2.306216585152737\n",
      "train loss:2.2961050541515213\n",
      "train loss:2.3119005733065467\n",
      "train loss:2.303373295606893\n",
      "train loss:2.3033676649630936\n",
      "train loss:2.3038734153588787\n",
      "train loss:2.306847497672166\n",
      "train loss:2.299083176794734\n",
      "train loss:2.3185414593652034\n",
      "train loss:2.3107129641190207\n",
      "train loss:2.309377470083941\n",
      "train loss:2.3089288183083436\n",
      "train loss:2.308960976484643\n",
      "train loss:2.311082449562308\n",
      "train loss:2.3107755250073754\n",
      "train loss:2.294045075464223\n",
      "train loss:2.3042353764606744\n",
      "train loss:2.302408696143675\n",
      "train loss:2.3042150893356976\n",
      "train loss:2.3132618745566242\n",
      "train loss:2.3029329208449387\n",
      "train loss:2.3161275816054534\n",
      "train loss:2.3064141225465544\n",
      "train loss:2.3154370686446595\n",
      "train loss:2.316843255669708\n",
      "train loss:2.3050086696865257\n",
      "train loss:2.3089496566448005\n",
      "train loss:2.298943511963687\n",
      "train loss:2.302278219134248\n",
      "train loss:2.3072811038359644\n",
      "train loss:2.311819092170496\n",
      "train loss:2.3085540493550107\n",
      "train loss:2.3122140410025356\n",
      "train loss:2.3135510099057495\n",
      "train loss:2.3069150086319965\n",
      "train loss:2.3139614131805937\n",
      "train loss:2.3011811635374846\n",
      "train loss:2.2990927970233517\n",
      "train loss:2.303292676222217\n",
      "train loss:2.303093387900183\n",
      "train loss:2.30999030395918\n",
      "train loss:2.3031504924986215\n",
      "train loss:2.3090048086782624\n",
      "train loss:2.310992291438508\n",
      "train loss:2.3076425298745398\n",
      "train loss:2.3075969155838694\n",
      "train loss:2.2992895463251295\n",
      "train loss:2.304645111636077\n",
      "train loss:2.3077012074104726\n",
      "train loss:2.3011753503203263\n",
      "train loss:2.2993951460994815\n",
      "train loss:2.3082334242207385\n",
      "train loss:2.3096196414685592\n",
      "train loss:2.3109390813357367\n",
      "train loss:2.3113541569460616\n",
      "train loss:2.3080877303021756\n",
      "train loss:2.3162193487157916\n",
      "train loss:2.3048770068568554\n",
      "train loss:2.303909540849812\n",
      "train loss:2.3101914841561637\n",
      "train loss:2.2961789676641287\n",
      "train loss:2.3049823174284514\n",
      "train loss:2.300266753294032\n",
      "train loss:2.306337149658215\n",
      "train loss:2.3022852820503927\n",
      "train loss:2.304205094523396\n",
      "train loss:2.3085693785642007\n",
      "train loss:2.310313964934865\n",
      "train loss:2.301823402638302\n",
      "train loss:2.3029355931080784\n",
      "train loss:2.3093548198373854\n",
      "train loss:2.297969872848625\n",
      "train loss:2.306666781564812\n",
      "train loss:2.317197248377035\n",
      "train loss:2.3090922808657908\n",
      "train loss:2.30059643719284\n",
      "train loss:2.307234334182102\n",
      "train loss:2.3074400040300542\n",
      "train loss:2.3136293530623866\n",
      "train loss:2.3015826647724604\n",
      "train loss:2.305733560523947\n",
      "train loss:2.3038847640160895\n",
      "train loss:2.298138654818423\n",
      "train loss:2.2948346848879373\n",
      "train loss:2.3100757542134933\n",
      "train loss:2.302403212444218\n",
      "train loss:2.313067461183733\n",
      "train loss:2.314693353794363\n",
      "train loss:2.3002187578957134\n",
      "train loss:2.3121429587541322\n",
      "train loss:2.304691537075173\n",
      "train loss:2.29934121643241\n",
      "train loss:2.3055840489087687\n",
      "train loss:2.302077106208299\n",
      "train loss:2.301694928456995\n",
      "train loss:2.3050031377850377\n",
      "train loss:2.3128850909861853\n",
      "train loss:2.3072846817669848\n",
      "train loss:2.3114219030591765\n",
      "train loss:2.3105888582886704\n",
      "train loss:2.3122540726198397\n",
      "train loss:2.3053511894887952\n",
      "train loss:2.303716256108405\n",
      "train loss:2.300754581628736\n",
      "train loss:2.302992439730964\n",
      "train loss:2.3085424205376426\n",
      "train loss:2.3044945557191125\n",
      "train loss:2.310074748823974\n",
      "train loss:2.3031620839056703\n",
      "train loss:2.31628667293243\n",
      "train loss:2.3107335888894385\n",
      "train loss:2.3119549032411615\n",
      "train loss:2.311423628806273\n",
      "train loss:2.308460767037381\n",
      "train loss:2.308369891700458\n",
      "train loss:2.298503698832726\n",
      "train loss:2.2978304076140725\n",
      "train loss:2.309614282068272\n",
      "train loss:2.316591847524766\n",
      "train loss:2.3100696112936547\n",
      "train loss:2.3065570302913536\n",
      "train loss:2.302028449052305\n",
      "train loss:2.3069176254154367\n",
      "train loss:2.313928695491412\n",
      "train loss:2.303916730628217\n",
      "train loss:2.3061356227209524\n",
      "train loss:2.2996113290564115\n",
      "train loss:2.3054911287989523\n",
      "train loss:2.314358656781821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3070998838346997\n",
      "train loss:2.3033339668863047\n",
      "train loss:2.3011003602059006\n",
      "train loss:2.3006620007036584\n",
      "train loss:2.307580638265519\n",
      "train loss:2.2982280504115185\n",
      "train loss:2.306240985106583\n",
      "train loss:2.3100527432068207\n",
      "train loss:2.3081276405658375\n",
      "train loss:2.301473491376687\n",
      "train loss:2.3034717002607024\n",
      "train loss:2.313955664996359\n",
      "train loss:2.305538432431756\n",
      "train loss:2.311652801620685\n",
      "train loss:2.3002264464071835\n",
      "train loss:2.3043457520715256\n",
      "train loss:2.3046484474090794\n",
      "train loss:2.3064278294113687\n",
      "train loss:2.305832295961119\n",
      "train loss:2.300949444850927\n",
      "train loss:2.3135610785079628\n",
      "train loss:2.302265515208599\n",
      "train loss:2.3017773558792305\n",
      "train loss:2.2988337819285167\n",
      "train loss:2.29972807782438\n",
      "train loss:2.3097871783040014\n",
      "train loss:2.310037844624333\n",
      "train loss:2.3079209752431598\n",
      "train loss:2.3127623106021984\n",
      "train loss:2.3047921457410485\n",
      "train loss:2.3003833225045476\n",
      "train loss:2.3118351532430768\n",
      "train loss:2.3090085683594403\n",
      "train loss:2.3094813784057546\n",
      "train loss:2.3117771457197445\n",
      "train loss:2.3097929717834798\n",
      "train loss:2.302198640228707\n",
      "train loss:2.304237158574918\n",
      "train loss:2.2943467846434844\n",
      "train loss:2.3027763350645816\n",
      "train loss:2.2943704292143017\n",
      "train loss:2.3133883544296245\n",
      "train loss:2.307550333965573\n",
      "train loss:2.3028280669202736\n",
      "train loss:2.303267363996247\n",
      "train loss:2.310906978819401\n",
      "=== epoch:25, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3033298942643805\n",
      "train loss:2.3031798862907165\n",
      "train loss:2.2988861613126668\n",
      "train loss:2.302981385249507\n",
      "train loss:2.303630557449496\n",
      "train loss:2.3020080891366352\n",
      "train loss:2.2925370102631364\n",
      "train loss:2.306799299000652\n",
      "train loss:2.3032572808322347\n",
      "train loss:2.303744016293092\n",
      "train loss:2.2978859412650743\n",
      "train loss:2.303211004151315\n",
      "train loss:2.304101463237102\n",
      "train loss:2.310292558127414\n",
      "train loss:2.313986275580221\n",
      "train loss:2.307747238372434\n",
      "train loss:2.3152282559961295\n",
      "train loss:2.307671838495961\n",
      "train loss:2.311306641457362\n",
      "train loss:2.2985793891124633\n",
      "train loss:2.304259969264475\n",
      "train loss:2.3053986978125063\n",
      "train loss:2.300880247079342\n",
      "train loss:2.30481511282216\n",
      "train loss:2.309530296025187\n",
      "train loss:2.308069058835539\n",
      "train loss:2.3090779716673735\n",
      "train loss:2.305032391758784\n",
      "train loss:2.3041533623277095\n",
      "train loss:2.302270791729835\n",
      "train loss:2.314378300992369\n",
      "train loss:2.310154811833906\n",
      "train loss:2.3109602642553577\n",
      "train loss:2.2945967214757017\n",
      "train loss:2.293561842221614\n",
      "train loss:2.308931863008742\n",
      "train loss:2.302824010597175\n",
      "train loss:2.3138637511916382\n",
      "train loss:2.3018952411723927\n",
      "train loss:2.3075368671399024\n",
      "train loss:2.3008259620449856\n",
      "train loss:2.3105060819728975\n",
      "train loss:2.304114910861536\n",
      "train loss:2.3026718018779206\n",
      "train loss:2.3049660361133926\n",
      "train loss:2.3038691950092356\n",
      "train loss:2.302996319518542\n",
      "train loss:2.3038192064742526\n",
      "train loss:2.3024841260828546\n",
      "train loss:2.3091646545226627\n",
      "train loss:2.3121702095791927\n",
      "train loss:2.312515767984619\n",
      "train loss:2.3047660785642434\n",
      "train loss:2.308297336911281\n",
      "train loss:2.301130727409488\n",
      "train loss:2.304850829131033\n",
      "train loss:2.300813007528289\n",
      "train loss:2.2991777077851587\n",
      "train loss:2.302721052724181\n",
      "train loss:2.3040388670715184\n",
      "train loss:2.309441010130028\n",
      "train loss:2.3064461772905744\n",
      "train loss:2.301487430252444\n",
      "train loss:2.304799223984342\n",
      "train loss:2.30537854181404\n",
      "train loss:2.3073060637371783\n",
      "train loss:2.3040490787969587\n",
      "train loss:2.3064089990395558\n",
      "train loss:2.2989842151007487\n",
      "train loss:2.293967398190238\n",
      "train loss:2.3014776955377445\n",
      "train loss:2.306925873742985\n",
      "train loss:2.304240775589479\n",
      "train loss:2.3192373480706587\n",
      "train loss:2.3066390981441254\n",
      "train loss:2.304221314949885\n",
      "train loss:2.305890300792257\n",
      "train loss:2.3011719231960073\n",
      "train loss:2.294761541277636\n",
      "train loss:2.303537563766638\n",
      "train loss:2.3071660571955417\n",
      "train loss:2.3038591221302354\n",
      "train loss:2.300566588148304\n",
      "train loss:2.3075908234452935\n",
      "train loss:2.301780943592705\n",
      "train loss:2.297338686189765\n",
      "train loss:2.3010377102122535\n",
      "train loss:2.307552601211996\n",
      "train loss:2.30748449721293\n",
      "train loss:2.3090700274763356\n",
      "train loss:2.303763029586665\n",
      "train loss:2.3041115432239856\n",
      "train loss:2.3061578064226924\n",
      "train loss:2.305031751020928\n",
      "train loss:2.3069735794294743\n",
      "train loss:2.2979058195083018\n",
      "train loss:2.3108628346089506\n",
      "train loss:2.3098947181628806\n",
      "train loss:2.306907829377039\n",
      "train loss:2.3030753668995545\n",
      "train loss:2.311537774770383\n",
      "train loss:2.3085245328320223\n",
      "train loss:2.2995855192813393\n",
      "train loss:2.3112412773715545\n",
      "train loss:2.307856178838759\n",
      "train loss:2.3012778690396103\n",
      "train loss:2.3082956324266397\n",
      "train loss:2.306683883988299\n",
      "train loss:2.301764754815359\n",
      "train loss:2.3073741014165856\n",
      "train loss:2.309825601596227\n",
      "train loss:2.3034482822891915\n",
      "train loss:2.308975007061052\n",
      "train loss:2.3019860262521785\n",
      "train loss:2.3081387926151598\n",
      "train loss:2.2978421722964555\n",
      "train loss:2.3091468702832794\n",
      "train loss:2.2983473766954208\n",
      "train loss:2.3005197471135514\n",
      "train loss:2.301073823095856\n",
      "train loss:2.3014032993009\n",
      "train loss:2.297462206763586\n",
      "train loss:2.312082525266127\n",
      "train loss:2.3055374182063666\n",
      "train loss:2.3122970524093884\n",
      "train loss:2.3086849623207595\n",
      "train loss:2.305398459902537\n",
      "train loss:2.3109236437036826\n",
      "train loss:2.3054541641282964\n",
      "train loss:2.300134722462505\n",
      "train loss:2.298857174758533\n",
      "train loss:2.3064990369989333\n",
      "train loss:2.30715708224575\n",
      "train loss:2.3013834393919974\n",
      "train loss:2.3064441628325714\n",
      "train loss:2.303167219217742\n",
      "train loss:2.3013867570174664\n",
      "train loss:2.3113510851458945\n",
      "train loss:2.3057077844438463\n",
      "train loss:2.3006668608207814\n",
      "train loss:2.301285951138064\n",
      "train loss:2.296562356798623\n",
      "train loss:2.31118868676179\n",
      "train loss:2.3043926302897737\n",
      "train loss:2.298557525911914\n",
      "train loss:2.294723019347777\n",
      "train loss:2.302173925377205\n",
      "train loss:2.299894832323623\n",
      "train loss:2.3032323925381113\n",
      "train loss:2.2968877839009942\n",
      "train loss:2.3057976023163618\n",
      "train loss:2.3026858453730377\n",
      "train loss:2.3047934444421627\n",
      "train loss:2.3166276665607186\n",
      "train loss:2.3052729233552056\n",
      "train loss:2.3021073710573345\n",
      "train loss:2.3137643922989573\n",
      "train loss:2.308301784032972\n",
      "train loss:2.305133017068425\n",
      "train loss:2.3063018116982295\n",
      "train loss:2.299349206791348\n",
      "train loss:2.3044030868652507\n",
      "train loss:2.310629470433338\n",
      "train loss:2.3055972751866225\n",
      "train loss:2.308410050071051\n",
      "train loss:2.3094854561979528\n",
      "train loss:2.310959334999284\n",
      "train loss:2.3137935412472275\n",
      "train loss:2.3033549350668125\n",
      "train loss:2.3073236408164974\n",
      "train loss:2.3022484102608245\n",
      "train loss:2.303094408404359\n",
      "train loss:2.301343217265469\n",
      "train loss:2.311612950574762\n",
      "train loss:2.3105150206631793\n",
      "train loss:2.3082124598662332\n",
      "train loss:2.3084570919941587\n",
      "train loss:2.3025821838698586\n",
      "train loss:2.29774709313544\n",
      "train loss:2.300175388935342\n",
      "train loss:2.3167586059457497\n",
      "train loss:2.303293588630917\n",
      "train loss:2.3070517951293095\n",
      "train loss:2.306076039388989\n",
      "train loss:2.3007503362179396\n",
      "train loss:2.303099554306042\n",
      "train loss:2.315394157247276\n",
      "train loss:2.3141832290454283\n",
      "train loss:2.301308472837839\n",
      "train loss:2.305438964164772\n",
      "train loss:2.3068318524300366\n",
      "train loss:2.314548531932542\n",
      "train loss:2.301368589082162\n",
      "train loss:2.2987243285613457\n",
      "train loss:2.3118299637429587\n",
      "train loss:2.306132087774767\n",
      "train loss:2.311155743922814\n",
      "train loss:2.3074326975899484\n",
      "train loss:2.315149966984892\n",
      "train loss:2.293818165522631\n",
      "=== epoch:26, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2947605423730355\n",
      "train loss:2.312741654877771\n",
      "train loss:2.306345060180923\n",
      "train loss:2.3035935605326725\n",
      "train loss:2.305209771938982\n",
      "train loss:2.299910471085119\n",
      "train loss:2.2961911622349223\n",
      "train loss:2.3066423433176637\n",
      "train loss:2.3067747043468323\n",
      "train loss:2.3019581491675734\n",
      "train loss:2.3058774534731654\n",
      "train loss:2.307687178345722\n",
      "train loss:2.301566531864825\n",
      "train loss:2.3031284497150812\n",
      "train loss:2.3039361966467484\n",
      "train loss:2.3123758020557528\n",
      "train loss:2.3078400476919474\n",
      "train loss:2.3047345028785524\n",
      "train loss:2.301289531783298\n",
      "train loss:2.3086615582816004\n",
      "train loss:2.2973652699812677\n",
      "train loss:2.301545636513545\n",
      "train loss:2.3086408132442813\n",
      "train loss:2.298975054061797\n",
      "train loss:2.3048753112974287\n",
      "train loss:2.3076505945981483\n",
      "train loss:2.303056140433267\n",
      "train loss:2.3010552745163544\n",
      "train loss:2.311166139024335\n",
      "train loss:2.3008716387929558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.295316528166316\n",
      "train loss:2.3073155180803244\n",
      "train loss:2.3009226655571875\n",
      "train loss:2.311647141202254\n",
      "train loss:2.306701907790781\n",
      "train loss:2.2993853290928\n",
      "train loss:2.308476672845778\n",
      "train loss:2.3040095386087405\n",
      "train loss:2.303091767738381\n",
      "train loss:2.307944203087981\n",
      "train loss:2.3068881449458947\n",
      "train loss:2.3068255181144908\n",
      "train loss:2.3033119235956865\n",
      "train loss:2.2987507044714524\n",
      "train loss:2.3011576948228356\n",
      "train loss:2.3024819301706763\n",
      "train loss:2.2924782806588153\n",
      "train loss:2.31119442808452\n",
      "train loss:2.3095653729186307\n",
      "train loss:2.306022484606597\n",
      "train loss:2.3021872776216097\n",
      "train loss:2.3026781142222426\n",
      "train loss:2.307955813748486\n",
      "train loss:2.312832344031546\n",
      "train loss:2.2985480058060985\n",
      "train loss:2.305205601658895\n",
      "train loss:2.303169977123659\n",
      "train loss:2.307922719265571\n",
      "train loss:2.3032146837908485\n",
      "train loss:2.3068012918499705\n",
      "train loss:2.2963748577239023\n",
      "train loss:2.303322834573487\n",
      "train loss:2.3037393027643653\n",
      "train loss:2.3030113806485266\n",
      "train loss:2.306242831189208\n",
      "train loss:2.2987826382057355\n",
      "train loss:2.3022229418541147\n",
      "train loss:2.2946595178694484\n",
      "train loss:2.3076029905183315\n",
      "train loss:2.309410759180791\n",
      "train loss:2.3070290560761024\n",
      "train loss:2.3112883834590097\n",
      "train loss:2.3067215298216923\n",
      "train loss:2.3021551543390437\n",
      "train loss:2.3027055742524807\n",
      "train loss:2.304056757990903\n",
      "train loss:2.3118602882652532\n",
      "train loss:2.2978890506780627\n",
      "train loss:2.309972252855454\n",
      "train loss:2.3075980411254378\n",
      "train loss:2.2971223949300152\n",
      "train loss:2.295026729890564\n",
      "train loss:2.3041272328622364\n",
      "train loss:2.307622974105922\n",
      "train loss:2.3115845747778634\n",
      "train loss:2.3075487785592506\n",
      "train loss:2.3032081125540147\n",
      "train loss:2.3047811902070383\n",
      "train loss:2.3090845199693733\n",
      "train loss:2.2965102359131917\n",
      "train loss:2.2973663544927776\n",
      "train loss:2.2905535696325887\n",
      "train loss:2.303969836819537\n",
      "train loss:2.2886966430910465\n",
      "train loss:2.302207140624316\n",
      "train loss:2.3074509395409315\n",
      "train loss:2.2930118890677433\n",
      "train loss:2.305549509414537\n",
      "train loss:2.30504014724353\n",
      "train loss:2.306009560923484\n",
      "train loss:2.3087571652954395\n",
      "train loss:2.3055604794046927\n",
      "train loss:2.3088927271972173\n",
      "train loss:2.297628275276586\n",
      "train loss:2.2993727660872842\n",
      "train loss:2.2991614543748917\n",
      "train loss:2.3086426791227423\n",
      "train loss:2.3039475671880547\n",
      "train loss:2.3029522115320558\n",
      "train loss:2.3021607012096794\n",
      "train loss:2.297346858613511\n",
      "train loss:2.304423599302624\n",
      "train loss:2.299516597117975\n",
      "train loss:2.301566811257862\n",
      "train loss:2.3068412700260192\n",
      "train loss:2.2996631981940463\n",
      "train loss:2.2983988781546567\n",
      "train loss:2.301374423118973\n",
      "train loss:2.2994137257385794\n",
      "train loss:2.3072690743046067\n",
      "train loss:2.3004830605254636\n",
      "train loss:2.307459366903888\n",
      "train loss:2.298025514701254\n",
      "train loss:2.296240726669561\n",
      "train loss:2.298653842358293\n",
      "train loss:2.298388314766036\n",
      "train loss:2.3014658405282065\n",
      "train loss:2.3108971594109344\n",
      "train loss:2.297473771872106\n",
      "train loss:2.300967105209207\n",
      "train loss:2.298999257273671\n",
      "train loss:2.309457690090747\n",
      "train loss:2.3051147899697213\n",
      "train loss:2.3055247674943344\n",
      "train loss:2.2943291687580754\n",
      "train loss:2.3047994034931065\n",
      "train loss:2.3020325199969984\n",
      "train loss:2.29726759453816\n",
      "train loss:2.3099725076150057\n",
      "train loss:2.301131252353481\n",
      "train loss:2.307115765818468\n",
      "train loss:2.302248947972809\n",
      "train loss:2.30360981843967\n",
      "train loss:2.3019261344385216\n",
      "train loss:2.3074322355300874\n",
      "train loss:2.299920800399331\n",
      "train loss:2.3067789580951708\n",
      "train loss:2.304720073544806\n",
      "train loss:2.307448376871301\n",
      "train loss:2.3039635533158753\n",
      "train loss:2.3067732407321224\n",
      "train loss:2.3023628873594832\n",
      "train loss:2.299504831419895\n",
      "train loss:2.3085636636099833\n",
      "train loss:2.2988269579272385\n",
      "train loss:2.3111186318180788\n",
      "train loss:2.2972555921659787\n",
      "train loss:2.30141017814698\n",
      "train loss:2.3114146127694943\n",
      "train loss:2.3009783561408264\n",
      "train loss:2.300997242273682\n",
      "train loss:2.3073392647457025\n",
      "train loss:2.3024954337012047\n",
      "train loss:2.297625973020555\n",
      "train loss:2.298508404477178\n",
      "train loss:2.3148865534517067\n",
      "train loss:2.3054218671923987\n",
      "train loss:2.3074543372941116\n",
      "train loss:2.299036851732168\n",
      "train loss:2.299413754038629\n",
      "train loss:2.3085979721070435\n",
      "train loss:2.3012803712821466\n",
      "train loss:2.288513113344708\n",
      "train loss:2.3075888729225955\n",
      "train loss:2.301300600947731\n",
      "train loss:2.3053277893815394\n",
      "train loss:2.30563585713619\n",
      "train loss:2.29968315367869\n",
      "train loss:2.304224252419119\n",
      "train loss:2.31178482678952\n",
      "train loss:2.3064977333329257\n",
      "train loss:2.297839595954881\n",
      "train loss:2.3048451427615855\n",
      "train loss:2.3096050243216855\n",
      "train loss:2.302136927137463\n",
      "train loss:2.3061254978715295\n",
      "train loss:2.298007060736803\n",
      "train loss:2.307835623257957\n",
      "train loss:2.3059710305087666\n",
      "train loss:2.305003158174537\n",
      "train loss:2.3075481877260846\n",
      "train loss:2.3051421169585207\n",
      "train loss:2.299661742886229\n",
      "train loss:2.3005464958801194\n",
      "train loss:2.30662343479423\n",
      "train loss:2.3019917441984648\n",
      "train loss:2.301613683829017\n",
      "train loss:2.298884382703297\n",
      "train loss:2.3069792112588505\n",
      "train loss:2.2999662943494568\n",
      "=== epoch:27, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3072170158445147\n",
      "train loss:2.3111084405731797\n",
      "train loss:2.309174684612792\n",
      "train loss:2.3000190811067283\n",
      "train loss:2.297823935750408\n",
      "train loss:2.3081553463831668\n",
      "train loss:2.302467333572478\n",
      "train loss:2.302452499812125\n",
      "train loss:2.3002185220610887\n",
      "train loss:2.3089758849682545\n",
      "train loss:2.313604846330699\n",
      "train loss:2.3002900827241026\n",
      "train loss:2.2949940955927577\n",
      "train loss:2.2965893374061377\n",
      "train loss:2.308666185583002\n",
      "train loss:2.3118451617594027\n",
      "train loss:2.310075639036077\n",
      "train loss:2.3108660150193576\n",
      "train loss:2.304974074547956\n",
      "train loss:2.301845410828871\n",
      "train loss:2.304989617540652\n",
      "train loss:2.299120621548548\n",
      "train loss:2.3083779360794754\n",
      "train loss:2.294380868846809\n",
      "train loss:2.2976375860103464\n",
      "train loss:2.302646126877193\n",
      "train loss:2.3066483018230075\n",
      "train loss:2.307032840556142\n",
      "train loss:2.3096998358111036\n",
      "train loss:2.3096563460320603\n",
      "train loss:2.3056900722439573\n",
      "train loss:2.310330051244263\n",
      "train loss:2.3003663786752644\n",
      "train loss:2.2983832661076637\n",
      "train loss:2.2999647649282244\n",
      "train loss:2.3032730737345832\n",
      "train loss:2.300351797865602\n",
      "train loss:2.301629774714308\n",
      "train loss:2.3140735525620086\n",
      "train loss:2.3031736785984274\n",
      "train loss:2.3052771192204107\n",
      "train loss:2.2999508387979364\n",
      "train loss:2.3073716061074805\n",
      "train loss:2.3075558710119854\n",
      "train loss:2.298169941477622\n",
      "train loss:2.287492316414639\n",
      "train loss:2.3044347712014934\n",
      "train loss:2.2997025652330705\n",
      "train loss:2.3047592009777853\n",
      "train loss:2.303043228889601\n",
      "train loss:2.305897634466108\n",
      "train loss:2.297824548152597\n",
      "train loss:2.3083142277295705\n",
      "train loss:2.3020107837299815\n",
      "train loss:2.3093958129709033\n",
      "train loss:2.2924435658613675\n",
      "train loss:2.302374500054795\n",
      "train loss:2.3055453180456853\n",
      "train loss:2.2961691148153682\n",
      "train loss:2.3004309094203657\n",
      "train loss:2.2985677131198528\n",
      "train loss:2.3072059058637695\n",
      "train loss:2.301799993307949\n",
      "train loss:2.302328999579067\n",
      "train loss:2.3058129236020135\n",
      "train loss:2.3037720264115626\n",
      "train loss:2.3016328206819634\n",
      "train loss:2.306704211814534\n",
      "train loss:2.3046880457001735\n",
      "train loss:2.297240322808458\n",
      "train loss:2.310923562329426\n",
      "train loss:2.301729564854362\n",
      "train loss:2.305866852933946\n",
      "train loss:2.2973083164341497\n",
      "train loss:2.307764496077122\n",
      "train loss:2.297986231031824\n",
      "train loss:2.302011581842115\n",
      "train loss:2.2973481638837643\n",
      "train loss:2.3087920471835846\n",
      "train loss:2.3130562482908723\n",
      "train loss:2.305058885186089\n",
      "train loss:2.3101938342330293\n",
      "train loss:2.3042759968651354\n",
      "train loss:2.308355340779441\n",
      "train loss:2.2994409607873654\n",
      "train loss:2.307370785314695\n",
      "train loss:2.302408342478498\n",
      "train loss:2.3009583023319267\n",
      "train loss:2.301466879875632\n",
      "train loss:2.305446362113039\n",
      "train loss:2.3030442383334075\n",
      "train loss:2.3037109326567906\n",
      "train loss:2.2991547180463674\n",
      "train loss:2.2984765427217306\n",
      "train loss:2.299035896341794\n",
      "train loss:2.2936929157333554\n",
      "train loss:2.2993338199952165\n",
      "train loss:2.3067640436221946\n",
      "train loss:2.308070259605606\n",
      "train loss:2.305243579157642\n",
      "train loss:2.301270286826189\n",
      "train loss:2.3106799797683326\n",
      "train loss:2.298656686017522\n",
      "train loss:2.291183325348665\n",
      "train loss:2.300558528602363\n",
      "train loss:2.3037513038440127\n",
      "train loss:2.3057367218893106\n",
      "train loss:2.303255038987308\n",
      "train loss:2.293165311640898\n",
      "train loss:2.3081845455436687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3021151520454906\n",
      "train loss:2.3083855848315173\n",
      "train loss:2.298020122197251\n",
      "train loss:2.2970877196346273\n",
      "train loss:2.299715799282616\n",
      "train loss:2.291711571270831\n",
      "train loss:2.3003411635066136\n",
      "train loss:2.303538543409165\n",
      "train loss:2.3055129244906514\n",
      "train loss:2.303746129923204\n",
      "train loss:2.29627972481867\n",
      "train loss:2.302754618165701\n",
      "train loss:2.304263378068274\n",
      "train loss:2.3041502343060722\n",
      "train loss:2.3077256847695873\n",
      "train loss:2.302390256997415\n",
      "train loss:2.3018949844230256\n",
      "train loss:2.295068044220519\n",
      "train loss:2.302459546355678\n",
      "train loss:2.3011974851204755\n",
      "train loss:2.3099339544433475\n",
      "train loss:2.3059369960139176\n",
      "train loss:2.303625663278385\n",
      "train loss:2.3068604441916127\n",
      "train loss:2.2982850574424325\n",
      "train loss:2.3002442625911357\n",
      "train loss:2.301649042604708\n",
      "train loss:2.302478380859693\n",
      "train loss:2.2998910810854793\n",
      "train loss:2.307580820692612\n",
      "train loss:2.297338626268404\n",
      "train loss:2.290947714032937\n",
      "train loss:2.300737704040922\n",
      "train loss:2.3009607130300336\n",
      "train loss:2.30725331813312\n",
      "train loss:2.2990359745039637\n",
      "train loss:2.298571128498393\n",
      "train loss:2.30315671989893\n",
      "train loss:2.2999058106094576\n",
      "train loss:2.3064203325888846\n",
      "train loss:2.3100584338196066\n",
      "train loss:2.2998531011524364\n",
      "train loss:2.3022500588730868\n",
      "train loss:2.3020726382626786\n",
      "train loss:2.295921367967034\n",
      "train loss:2.2982939668464897\n",
      "train loss:2.302683316715959\n",
      "train loss:2.2968570149063092\n",
      "train loss:2.296580405403015\n",
      "train loss:2.2959430345845213\n",
      "train loss:2.2952049315088603\n",
      "train loss:2.299826158874826\n",
      "train loss:2.30164598678053\n",
      "train loss:2.299055477018148\n",
      "train loss:2.304049374597286\n",
      "train loss:2.300778752191039\n",
      "train loss:2.3089053634136167\n",
      "train loss:2.301443948999481\n",
      "train loss:2.3139164869829534\n",
      "train loss:2.30145915431904\n",
      "train loss:2.3090000683405028\n",
      "train loss:2.2991255950536846\n",
      "train loss:2.2994040614539104\n",
      "train loss:2.3035490208863774\n",
      "train loss:2.29483411598378\n",
      "train loss:2.2970247320642367\n",
      "train loss:2.30086672840641\n",
      "train loss:2.308620599103879\n",
      "train loss:2.3085017835387753\n",
      "train loss:2.297122857347859\n",
      "train loss:2.3083111555482554\n",
      "train loss:2.3021246115508895\n",
      "train loss:2.303032067540136\n",
      "train loss:2.3073807234973254\n",
      "train loss:2.2995870528317255\n",
      "train loss:2.307052604809119\n",
      "train loss:2.299586612027194\n",
      "train loss:2.3038587709295437\n",
      "train loss:2.303003919161634\n",
      "train loss:2.302660092648262\n",
      "train loss:2.302122238304583\n",
      "train loss:2.3072139546651576\n",
      "train loss:2.3051448775648407\n",
      "train loss:2.3077401051396316\n",
      "train loss:2.3034677602796934\n",
      "train loss:2.3083009465253594\n",
      "train loss:2.299697421505792\n",
      "train loss:2.298133191026798\n",
      "train loss:2.3067363658312066\n",
      "train loss:2.304617895122159\n",
      "=== epoch:28, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.307878415730945\n",
      "train loss:2.309961355384975\n",
      "train loss:2.307891262328482\n",
      "train loss:2.298349039643733\n",
      "train loss:2.294952094099514\n",
      "train loss:2.31045428765663\n",
      "train loss:2.3059815509484025\n",
      "train loss:2.3065648723705827\n",
      "train loss:2.30355924308027\n",
      "train loss:2.305424164768302\n",
      "train loss:2.3079383692375854\n",
      "train loss:2.306642129534425\n",
      "train loss:2.306792359607167\n",
      "train loss:2.301130663994097\n",
      "train loss:2.300616933465419\n",
      "train loss:2.302687019360942\n",
      "train loss:2.3109283534774727\n",
      "train loss:2.3049595551659303\n",
      "train loss:2.297424891482262\n",
      "train loss:2.306314567961812\n",
      "train loss:2.3021591853299186\n",
      "train loss:2.298675327853813\n",
      "train loss:2.3010252226928674\n",
      "train loss:2.310697269158869\n",
      "train loss:2.3019680751767\n",
      "train loss:2.305067452818774\n",
      "train loss:2.3118830547445226\n",
      "train loss:2.297715261151082\n",
      "train loss:2.3015945265892825\n",
      "train loss:2.299381868444171\n",
      "train loss:2.302263468346751\n",
      "train loss:2.304034452111854\n",
      "train loss:2.311410340081128\n",
      "train loss:2.303896346899428\n",
      "train loss:2.305783350524887\n",
      "train loss:2.3024869106329757\n",
      "train loss:2.305575155937146\n",
      "train loss:2.306466167225849\n",
      "train loss:2.298509814610812\n",
      "train loss:2.3011140272188952\n",
      "train loss:2.305073857763153\n",
      "train loss:2.3051202753918205\n",
      "train loss:2.30329501450649\n",
      "train loss:2.307975622277072\n",
      "train loss:2.302130715198796\n",
      "train loss:2.300476124824206\n",
      "train loss:2.2933897138511576\n",
      "train loss:2.3040610528018752\n",
      "train loss:2.3006522102849862\n",
      "train loss:2.3044439630682816\n",
      "train loss:2.305496881510613\n",
      "train loss:2.3121929701089567\n",
      "train loss:2.2969239659633454\n",
      "train loss:2.3085851153418258\n",
      "train loss:2.3047580153152616\n",
      "train loss:2.305759021508745\n",
      "train loss:2.30576336246086\n",
      "train loss:2.3076602031080053\n",
      "train loss:2.3061478671855835\n",
      "train loss:2.301443312360957\n",
      "train loss:2.3028257578399622\n",
      "train loss:2.2999167546830472\n",
      "train loss:2.301806924697652\n",
      "train loss:2.3000019852816433\n",
      "train loss:2.3050109827877137\n",
      "train loss:2.3056440657177353\n",
      "train loss:2.291235107909534\n",
      "train loss:2.3001275621656236\n",
      "train loss:2.3071315528870397\n",
      "train loss:2.300238480575407\n",
      "train loss:2.303726697895484\n",
      "train loss:2.310964994777811\n",
      "train loss:2.300291459973208\n",
      "train loss:2.2997219179265436\n",
      "train loss:2.3013824491018675\n",
      "train loss:2.3018261802800923\n",
      "train loss:2.309555272832911\n",
      "train loss:2.3036146307610488\n",
      "train loss:2.304949790373529\n",
      "train loss:2.2964929989625977\n",
      "train loss:2.301719418112675\n",
      "train loss:2.3018627079785556\n",
      "train loss:2.3066214430843637\n",
      "train loss:2.3015922683985606\n",
      "train loss:2.303629580025073\n",
      "train loss:2.30329547236528\n",
      "train loss:2.301010025798217\n",
      "train loss:2.3031614566745366\n",
      "train loss:2.3062509574726255\n",
      "train loss:2.2971917317275894\n",
      "train loss:2.3043016117751414\n",
      "train loss:2.3044140636277604\n",
      "train loss:2.305360060208351\n",
      "train loss:2.2960177800619084\n",
      "train loss:2.305050091247776\n",
      "train loss:2.2936772005768695\n",
      "train loss:2.2968494767057988\n",
      "train loss:2.3005753477164355\n",
      "train loss:2.298142519446835\n",
      "train loss:2.2999947477043854\n",
      "train loss:2.2999273537744114\n",
      "train loss:2.306251352054\n",
      "train loss:2.301202592904621\n",
      "train loss:2.308741698475332\n",
      "train loss:2.3019239497773323\n",
      "train loss:2.3014637972723055\n",
      "train loss:2.3067305803506417\n",
      "train loss:2.3035815210086366\n",
      "train loss:2.3051117152887257\n",
      "train loss:2.3048256137661594\n",
      "train loss:2.2979300327257124\n",
      "train loss:2.30004796260638\n",
      "train loss:2.30652211181951\n",
      "train loss:2.2952515441834604\n",
      "train loss:2.3000253388028136\n",
      "train loss:2.2925649214100967\n",
      "train loss:2.312369462729305\n",
      "train loss:2.3086071539244553\n",
      "train loss:2.3125799584465376\n",
      "train loss:2.303369838372148\n",
      "train loss:2.3099647025680015\n",
      "train loss:2.295999099041778\n",
      "train loss:2.3058745237319624\n",
      "train loss:2.3052420863053347\n",
      "train loss:2.3084852491757264\n",
      "train loss:2.3019451401484994\n",
      "train loss:2.3066535013987806\n",
      "train loss:2.304525626298726\n",
      "train loss:2.2974123450442887\n",
      "train loss:2.3049937138157977\n",
      "train loss:2.2928624897074923\n",
      "train loss:2.306540402203279\n",
      "train loss:2.300892592489021\n",
      "train loss:2.3029361668099617\n",
      "train loss:2.293992455160299\n",
      "train loss:2.2978967747879686\n",
      "train loss:2.29235544986768\n",
      "train loss:2.3088115728192835\n",
      "train loss:2.30357140976944\n",
      "train loss:2.305017346632676\n",
      "train loss:2.305799523578172\n",
      "train loss:2.2970442366676678\n",
      "train loss:2.3034750691144255\n",
      "train loss:2.294530490194889\n",
      "train loss:2.3097664770966517\n",
      "train loss:2.300702519145074\n",
      "train loss:2.3076120628955916\n",
      "train loss:2.301610637179499\n",
      "train loss:2.3029823868977197\n",
      "train loss:2.3013814651315436\n",
      "train loss:2.308031370837779\n",
      "train loss:2.307519055485244\n",
      "train loss:2.3014997880463754\n",
      "train loss:2.3072662583221164\n",
      "train loss:2.297348560430932\n",
      "train loss:2.299152484830491\n",
      "train loss:2.300453944945949\n",
      "train loss:2.2995232620268022\n",
      "train loss:2.3025907682434945\n",
      "train loss:2.300163326411737\n",
      "train loss:2.302904801729879\n",
      "train loss:2.3042546590698954\n",
      "train loss:2.295850316013408\n",
      "train loss:2.3026914233389397\n",
      "train loss:2.3057621507409514\n",
      "train loss:2.294667124788998\n",
      "train loss:2.3013072340910536\n",
      "train loss:2.3104887323981065\n",
      "train loss:2.312864838694421\n",
      "train loss:2.303050296980269\n",
      "train loss:2.2940880439076707\n",
      "train loss:2.3053133453277113\n",
      "train loss:2.3092129722036376\n",
      "train loss:2.3131885888294654\n",
      "train loss:2.2906228995975257\n",
      "train loss:2.3048359204841047\n",
      "train loss:2.3041095612217264\n",
      "train loss:2.3014401430946947\n",
      "train loss:2.299729523248393\n",
      "train loss:2.3010994332741683\n",
      "train loss:2.3014623342845013\n",
      "train loss:2.29730002769987\n",
      "train loss:2.294773782565517\n",
      "train loss:2.2891497763946558\n",
      "train loss:2.2968671276586425\n",
      "train loss:2.3084686144866318\n",
      "train loss:2.299880718520993\n",
      "train loss:2.299977090210655\n",
      "train loss:2.302170541440146\n",
      "train loss:2.3076067846195376\n",
      "train loss:2.302044256722948\n",
      "train loss:2.3033665017847604\n",
      "train loss:2.302080729523457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3030708621114826\n",
      "train loss:2.3061768053483\n",
      "train loss:2.3054020080524444\n",
      "train loss:2.292996654237069\n",
      "train loss:2.3106855519990663\n",
      "train loss:2.29784121553769\n",
      "train loss:2.3007788025812066\n",
      "=== epoch:29, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.299506014747133\n",
      "train loss:2.308782080615911\n",
      "train loss:2.3036019443008184\n",
      "train loss:2.300321158098311\n",
      "train loss:2.3024327531313977\n",
      "train loss:2.307896906858793\n",
      "train loss:2.3065659297358305\n",
      "train loss:2.3001313655641997\n",
      "train loss:2.2987788433837406\n",
      "train loss:2.3087209075651463\n",
      "train loss:2.288990987522713\n",
      "train loss:2.3023471957721213\n",
      "train loss:2.3040515363747747\n",
      "train loss:2.30181635123973\n",
      "train loss:2.304459517444959\n",
      "train loss:2.3028501839288302\n",
      "train loss:2.302799668118365\n",
      "train loss:2.3076255307812357\n",
      "train loss:2.301404940398911\n",
      "train loss:2.3110763391235793\n",
      "train loss:2.3001731479335157\n",
      "train loss:2.3018448137376417\n",
      "train loss:2.2953244007726807\n",
      "train loss:2.2977437026430763\n",
      "train loss:2.3047678278847052\n",
      "train loss:2.3000575380308956\n",
      "train loss:2.3035533482561483\n",
      "train loss:2.302651885907857\n",
      "train loss:2.3042128836576246\n",
      "train loss:2.301886197883998\n",
      "train loss:2.301939940809187\n",
      "train loss:2.309938426313184\n",
      "train loss:2.2974717231084014\n",
      "train loss:2.303061377241902\n",
      "train loss:2.3038479733943324\n",
      "train loss:2.3035803649730924\n",
      "train loss:2.3086586106174156\n",
      "train loss:2.3048812807154535\n",
      "train loss:2.308825326148827\n",
      "train loss:2.2970303461552346\n",
      "train loss:2.3027170540149386\n",
      "train loss:2.3049037320098127\n",
      "train loss:2.2923398765866416\n",
      "train loss:2.3009995433756902\n",
      "train loss:2.2878185883606634\n",
      "train loss:2.299801037855075\n",
      "train loss:2.3048970071196733\n",
      "train loss:2.3014679305822456\n",
      "train loss:2.2990593526972694\n",
      "train loss:2.30414208171764\n",
      "train loss:2.29670426083098\n",
      "train loss:2.3075537786360982\n",
      "train loss:2.3086985949470944\n",
      "train loss:2.300136991396002\n",
      "train loss:2.301019724636108\n",
      "train loss:2.2976758109022497\n",
      "train loss:2.2886605405461364\n",
      "train loss:2.307338303345833\n",
      "train loss:2.3029671928501205\n",
      "train loss:2.299047880193836\n",
      "train loss:2.307322049185625\n",
      "train loss:2.2997466519758287\n",
      "train loss:2.305541403001934\n",
      "train loss:2.303118333466555\n",
      "train loss:2.30440734405958\n",
      "train loss:2.299572438332374\n",
      "train loss:2.3006287404336847\n",
      "train loss:2.300178487415544\n",
      "train loss:2.298644008965596\n",
      "train loss:2.3015411373653065\n",
      "train loss:2.305427761877473\n",
      "train loss:2.299492603405021\n",
      "train loss:2.304321653511828\n",
      "train loss:2.3076504593184413\n",
      "train loss:2.3046657685707213\n",
      "train loss:2.2996482398873987\n",
      "train loss:2.300508619850868\n",
      "train loss:2.3027009102783595\n",
      "train loss:2.3028123237871374\n",
      "train loss:2.3097148012816153\n",
      "train loss:2.2935991607616844\n",
      "train loss:2.3070276484905192\n",
      "train loss:2.2898200178199546\n",
      "train loss:2.288193899835424\n",
      "train loss:2.30429112165863\n",
      "train loss:2.3059557219842612\n",
      "train loss:2.3077923990353075\n",
      "train loss:2.303279418999278\n",
      "train loss:2.2959956065997376\n",
      "train loss:2.2968965393807568\n",
      "train loss:2.3095127861813314\n",
      "train loss:2.3056314049487763\n",
      "train loss:2.3014574390803735\n",
      "train loss:2.307366751317388\n",
      "train loss:2.3018447038216827\n",
      "train loss:2.3103981589502167\n",
      "train loss:2.307493447862588\n",
      "train loss:2.2927634878309577\n",
      "train loss:2.3035206841884137\n",
      "train loss:2.290996580194006\n",
      "train loss:2.303264234245285\n",
      "train loss:2.2976538203595984\n",
      "train loss:2.3073883291092647\n",
      "train loss:2.296934562846505\n",
      "train loss:2.3002426829981326\n",
      "train loss:2.302645814154627\n",
      "train loss:2.304966780503496\n",
      "train loss:2.302295021847348\n",
      "train loss:2.304643803562837\n",
      "train loss:2.2962869563006407\n",
      "train loss:2.295289946816038\n",
      "train loss:2.304922371381127\n",
      "train loss:2.3083806771050424\n",
      "train loss:2.299299038514701\n",
      "train loss:2.3061812639451285\n",
      "train loss:2.3049843190907766\n",
      "train loss:2.299090681700159\n",
      "train loss:2.302207173556037\n",
      "train loss:2.307950822045421\n",
      "train loss:2.296834770388052\n",
      "train loss:2.3022843930362957\n",
      "train loss:2.3029507168126484\n",
      "train loss:2.295506451137373\n",
      "train loss:2.308997036043273\n",
      "train loss:2.3090800708809756\n",
      "train loss:2.301698334774529\n",
      "train loss:2.306013225027389\n",
      "train loss:2.3023815972622943\n",
      "train loss:2.295865106096197\n",
      "train loss:2.292374856661119\n",
      "train loss:2.303728859199223\n",
      "train loss:2.299651207894864\n",
      "train loss:2.301565047124078\n",
      "train loss:2.302418826848598\n",
      "train loss:2.303051210727218\n",
      "train loss:2.30922591920615\n",
      "train loss:2.2989664607134785\n",
      "train loss:2.307109790963971\n",
      "train loss:2.3101486001692253\n",
      "train loss:2.3038717535884508\n",
      "train loss:2.3021058972454482\n",
      "train loss:2.2978101625198075\n",
      "train loss:2.306207108568147\n",
      "train loss:2.2993804186698434\n",
      "train loss:2.3009862978517557\n",
      "train loss:2.303695995237808\n",
      "train loss:2.3059793808354283\n",
      "train loss:2.3032778774424565\n",
      "train loss:2.3129930452790464\n",
      "train loss:2.303024149021048\n",
      "train loss:2.2997827969449984\n",
      "train loss:2.303197555737668\n",
      "train loss:2.302739296433192\n",
      "train loss:2.3059776268482377\n",
      "train loss:2.299790370321398\n",
      "train loss:2.3036036504064787\n",
      "train loss:2.3008280980310425\n",
      "train loss:2.297519074915933\n",
      "train loss:2.3130439862762837\n",
      "train loss:2.2956119202830765\n",
      "train loss:2.3122797344769186\n",
      "train loss:2.3052826919924927\n",
      "train loss:2.303637905332446\n",
      "train loss:2.2913947088902775\n",
      "train loss:2.305943742072359\n",
      "train loss:2.2964840224936216\n",
      "train loss:2.3001267981333426\n",
      "train loss:2.2985277458046003\n",
      "train loss:2.3061678881767804\n",
      "train loss:2.3139852492871342\n",
      "train loss:2.302187313452462\n",
      "train loss:2.30431716204835\n",
      "train loss:2.297118996038467\n",
      "train loss:2.2980429105082307\n",
      "train loss:2.3081560306010394\n",
      "train loss:2.2979463376498646\n",
      "train loss:2.2957205634136133\n",
      "train loss:2.294347555975833\n",
      "train loss:2.303253978670105\n",
      "train loss:2.30406508105165\n",
      "train loss:2.299147641620304\n",
      "train loss:2.3075832548486623\n",
      "train loss:2.3007152503789805\n",
      "train loss:2.3022691310693264\n",
      "train loss:2.304923058830735\n",
      "train loss:2.301890083356631\n",
      "train loss:2.3022142354345627\n",
      "train loss:2.303798737703818\n",
      "train loss:2.312236345205089\n",
      "train loss:2.304231871708954\n",
      "train loss:2.305864484165619\n",
      "train loss:2.297869582676816\n",
      "train loss:2.303275371890653\n",
      "train loss:2.300444730480463\n",
      "train loss:2.3018405588048525\n",
      "train loss:2.2985765869896913\n",
      "train loss:2.2916346690494676\n",
      "train loss:2.3049966394317813\n",
      "train loss:2.299376232552705\n",
      "train loss:2.297671367441649\n",
      "=== epoch:30, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3020733198528283\n",
      "train loss:2.3047305634425443\n",
      "train loss:2.303902632786384\n",
      "train loss:2.3072320522765835\n",
      "train loss:2.304536057731437\n",
      "train loss:2.3060129092023467\n",
      "train loss:2.301851612284501\n",
      "train loss:2.3062060512354563\n",
      "train loss:2.2978773827460413\n",
      "train loss:2.3037628242832384\n",
      "train loss:2.312093745148134\n",
      "train loss:2.2983428768263883\n",
      "train loss:2.3069697265484272\n",
      "train loss:2.3057042359910707\n",
      "train loss:2.2981588703117\n",
      "train loss:2.3016872283669803\n",
      "train loss:2.3012150063399175\n",
      "train loss:2.306606686031254\n",
      "train loss:2.3020217458536094\n",
      "train loss:2.294266678132841\n",
      "train loss:2.2986346252221344\n",
      "train loss:2.2990422110473476\n",
      "train loss:2.3118496175325727\n",
      "train loss:2.3071779474834266\n",
      "train loss:2.3081996861435825\n",
      "train loss:2.3032733145644424\n",
      "train loss:2.300879289276311\n",
      "train loss:2.302480918475201\n",
      "train loss:2.2931997730205698\n",
      "train loss:2.306984354163006\n",
      "train loss:2.3006169917365424\n",
      "train loss:2.3015615364434376\n",
      "train loss:2.2895579460572875\n",
      "train loss:2.2986422128825126\n",
      "train loss:2.3022562425235336\n",
      "train loss:2.305329184137284\n",
      "train loss:2.313552169242466\n",
      "train loss:2.3016167872725135\n",
      "train loss:2.296674779299453\n",
      "train loss:2.2963689562941476\n",
      "train loss:2.2991286169047553\n",
      "train loss:2.300843279208237\n",
      "train loss:2.300539924034524\n",
      "train loss:2.303313543815205\n",
      "train loss:2.306422008534837\n",
      "train loss:2.302086974019421\n",
      "train loss:2.305518996453664\n",
      "train loss:2.3010149090081415\n",
      "train loss:2.3046233847249025\n",
      "train loss:2.3055273550390085\n",
      "train loss:2.3005195363162256\n",
      "train loss:2.303623971179842\n",
      "train loss:2.298795124153826\n",
      "train loss:2.3123625717768626\n",
      "train loss:2.305125133398714\n",
      "train loss:2.3061045611240294\n",
      "train loss:2.301225956176779\n",
      "train loss:2.3021801228002925\n",
      "train loss:2.302103888147755\n",
      "train loss:2.296729091143963\n",
      "train loss:2.2982854255893956\n",
      "train loss:2.2928849612535247\n",
      "train loss:2.304548070460804\n",
      "train loss:2.2972543698988557\n",
      "train loss:2.30529176225515\n",
      "train loss:2.291403169214549\n",
      "train loss:2.3024821368680373\n",
      "train loss:2.3047911548066065\n",
      "train loss:2.3037139445018884\n",
      "train loss:2.313806899902865\n",
      "train loss:2.306758114365764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3003014656228156\n",
      "train loss:2.3027047551360904\n",
      "train loss:2.3011481903846356\n",
      "train loss:2.3040975092349605\n",
      "train loss:2.3069995305103927\n",
      "train loss:2.310858604020663\n",
      "train loss:2.307454515363339\n",
      "train loss:2.301669530882661\n",
      "train loss:2.290855793232074\n",
      "train loss:2.2985926596519226\n",
      "train loss:2.307584010652694\n",
      "train loss:2.303378973864335\n",
      "train loss:2.2994413673577894\n",
      "train loss:2.3004436884643584\n",
      "train loss:2.293464975180541\n",
      "train loss:2.297982554602406\n",
      "train loss:2.29915431949369\n",
      "train loss:2.306883899382991\n",
      "train loss:2.3060638890105336\n",
      "train loss:2.2927413078298\n",
      "train loss:2.3038803846843696\n",
      "train loss:2.29550342402691\n",
      "train loss:2.3048732929494498\n",
      "train loss:2.302653844537707\n",
      "train loss:2.2987697160285894\n",
      "train loss:2.295469647333353\n",
      "train loss:2.302029595008219\n",
      "train loss:2.301002041892938\n",
      "train loss:2.3001169191429534\n",
      "train loss:2.303158069604441\n",
      "train loss:2.2937888751529885\n",
      "train loss:2.29502320401049\n",
      "train loss:2.305487345825754\n",
      "train loss:2.3051667756827072\n",
      "train loss:2.295665203703428\n",
      "train loss:2.301423910865848\n",
      "train loss:2.303873130814538\n",
      "train loss:2.2991969706849735\n",
      "train loss:2.3031044060797106\n",
      "train loss:2.30046265692903\n",
      "train loss:2.304459241552687\n",
      "train loss:2.3036262052838397\n",
      "train loss:2.303826037496463\n",
      "train loss:2.304268995914933\n",
      "train loss:2.290658415710469\n",
      "train loss:2.2975205308196323\n",
      "train loss:2.2974148778688273\n",
      "train loss:2.307522776622744\n",
      "train loss:2.3030888243777623\n",
      "train loss:2.301210042529072\n",
      "train loss:2.2993254068339413\n",
      "train loss:2.3013818844780314\n",
      "train loss:2.3039715048482363\n",
      "train loss:2.295916674269856\n",
      "train loss:2.302417475792441\n",
      "train loss:2.2971587727466014\n",
      "train loss:2.295350382090753\n",
      "train loss:2.296890646707166\n",
      "train loss:2.2994796666102624\n",
      "train loss:2.2981848523723225\n",
      "train loss:2.293956662743519\n",
      "train loss:2.3082445879016626\n",
      "train loss:2.302743251723982\n",
      "train loss:2.2940881339646544\n",
      "train loss:2.2931830862630727\n",
      "train loss:2.3089759128145197\n",
      "train loss:2.2995264766444983\n",
      "train loss:2.3001466014107153\n",
      "train loss:2.30570135464875\n",
      "train loss:2.2918782757580294\n",
      "train loss:2.3058443801398685\n",
      "train loss:2.3071172580941175\n",
      "train loss:2.303811546018621\n",
      "train loss:2.2979101634545884\n",
      "train loss:2.305905691245911\n",
      "train loss:2.2994485953883683\n",
      "train loss:2.3015068580174995\n",
      "train loss:2.2982128251042866\n",
      "train loss:2.308047092202679\n",
      "train loss:2.3026221371464066\n",
      "train loss:2.2972136746792335\n",
      "train loss:2.2980632490902044\n",
      "train loss:2.3034881592772534\n",
      "train loss:2.297512746029426\n",
      "train loss:2.3032697209406594\n",
      "train loss:2.2962593821236013\n",
      "train loss:2.308378775755831\n",
      "train loss:2.291699964208391\n",
      "train loss:2.3059986800765\n",
      "train loss:2.296645468898848\n",
      "train loss:2.3056184693569026\n",
      "train loss:2.300251725090045\n",
      "train loss:2.3014194756491886\n",
      "train loss:2.304132504418409\n",
      "train loss:2.308761118180004\n",
      "train loss:2.3072244914108544\n",
      "train loss:2.3050769642400715\n",
      "train loss:2.3134128819525923\n",
      "train loss:2.3047422265193847\n",
      "train loss:2.3058151990646514\n",
      "train loss:2.2938786749350406\n",
      "train loss:2.298842230639737\n",
      "train loss:2.300474884711074\n",
      "train loss:2.300797899838021\n",
      "train loss:2.3084634843186005\n",
      "train loss:2.2956164988155816\n",
      "train loss:2.308222925464542\n",
      "train loss:2.304158266762114\n",
      "train loss:2.310763989497823\n",
      "train loss:2.2990712728302602\n",
      "train loss:2.2919696925130153\n",
      "train loss:2.2956269356216272\n",
      "train loss:2.3051916997884034\n",
      "train loss:2.293999916820616\n",
      "train loss:2.298289991318676\n",
      "train loss:2.303551893717877\n",
      "train loss:2.3001617696489376\n",
      "train loss:2.295207075528603\n",
      "train loss:2.30184893160106\n",
      "train loss:2.310440600016111\n",
      "train loss:2.303133494801913\n",
      "train loss:2.3060918221119038\n",
      "train loss:2.30475307799071\n",
      "train loss:2.3032678695548974\n",
      "train loss:2.3065821925126198\n",
      "train loss:2.2983803756499257\n",
      "train loss:2.3015625764283576\n",
      "train loss:2.2997252720747925\n",
      "train loss:2.296320454146682\n",
      "=== epoch:31, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2951457755900164\n",
      "train loss:2.30835598293751\n",
      "train loss:2.293939362156785\n",
      "train loss:2.294483363531317\n",
      "train loss:2.301713230291615\n",
      "train loss:2.302924567910564\n",
      "train loss:2.2973360251761386\n",
      "train loss:2.305436723880071\n",
      "train loss:2.3031370202138013\n",
      "train loss:2.3015367567010108\n",
      "train loss:2.303335663310252\n",
      "train loss:2.2986538536432866\n",
      "train loss:2.300328449485452\n",
      "train loss:2.2935248914922783\n",
      "train loss:2.2998169450541712\n",
      "train loss:2.301068283012377\n",
      "train loss:2.307798477126262\n",
      "train loss:2.301070498720155\n",
      "train loss:2.2969360847792544\n",
      "train loss:2.2943551167263405\n",
      "train loss:2.2995056583295783\n",
      "train loss:2.2992718277820097\n",
      "train loss:2.303758092905651\n",
      "train loss:2.2986269884256596\n",
      "train loss:2.303916499199004\n",
      "train loss:2.306849340287191\n",
      "train loss:2.2990816225561153\n",
      "train loss:2.2967058751318303\n",
      "train loss:2.306932865210169\n",
      "train loss:2.3012849035067253\n",
      "train loss:2.3094497870777633\n",
      "train loss:2.302198522526008\n",
      "train loss:2.297175869808611\n",
      "train loss:2.3067714032264792\n",
      "train loss:2.2874091034287196\n",
      "train loss:2.2964508053349215\n",
      "train loss:2.2964158999482898\n",
      "train loss:2.294120832950624\n",
      "train loss:2.3034573286847553\n",
      "train loss:2.302624681960854\n",
      "train loss:2.297874181739978\n",
      "train loss:2.3048347307080275\n",
      "train loss:2.306215009519282\n",
      "train loss:2.3107415633209283\n",
      "train loss:2.3084276834702355\n",
      "train loss:2.307450402357602\n",
      "train loss:2.3019832614666575\n",
      "train loss:2.2990867102074866\n",
      "train loss:2.2984398784171853\n",
      "train loss:2.295389424510135\n",
      "train loss:2.304582905179059\n",
      "train loss:2.302888373435791\n",
      "train loss:2.3032534972481242\n",
      "train loss:2.306502960451073\n",
      "train loss:2.30841211837177\n",
      "train loss:2.3025766137581236\n",
      "train loss:2.3008552063301373\n",
      "train loss:2.2969531885179473\n",
      "train loss:2.3088817951110925\n",
      "train loss:2.305267993181929\n",
      "train loss:2.3073412941565445\n",
      "train loss:2.3053910729449387\n",
      "train loss:2.301305674266889\n",
      "train loss:2.288229114809348\n",
      "train loss:2.296755926670257\n",
      "train loss:2.298442880519707\n",
      "train loss:2.30240457955053\n",
      "train loss:2.314102485255532\n",
      "train loss:2.29905882690443\n",
      "train loss:2.300483496534424\n",
      "train loss:2.3033731115065494\n",
      "train loss:2.300340942331766\n",
      "train loss:2.3027393599023265\n",
      "train loss:2.3034508947469177\n",
      "train loss:2.3018358583983036\n",
      "train loss:2.3046722302186042\n",
      "train loss:2.3023842887774215\n",
      "train loss:2.3054705111360656\n",
      "train loss:2.300730645379085\n",
      "train loss:2.2970691657393707\n",
      "train loss:2.2937331928774847\n",
      "train loss:2.2978475540279355\n",
      "train loss:2.3010083878147825\n",
      "train loss:2.2959569925445225\n",
      "train loss:2.294269298663341\n",
      "train loss:2.2978961893977083\n",
      "train loss:2.301232676881153\n",
      "train loss:2.3024034540665426\n",
      "train loss:2.306811422744265\n",
      "train loss:2.300749220444927\n",
      "train loss:2.3010894101376422\n",
      "train loss:2.3062848665880233\n",
      "train loss:2.310935518914306\n",
      "train loss:2.3015721928341057\n",
      "train loss:2.300474851879933\n",
      "train loss:2.2999483395675595\n",
      "train loss:2.307435340970494\n",
      "train loss:2.311242240828169\n",
      "train loss:2.3025890416453803\n",
      "train loss:2.3048745169874296\n",
      "train loss:2.302585056214486\n",
      "train loss:2.3085465228578945\n",
      "train loss:2.3054926087159338\n",
      "train loss:2.303621443615265\n",
      "train loss:2.3004190229085033\n",
      "train loss:2.302410376038699\n",
      "train loss:2.304347975347329\n",
      "train loss:2.304669701516189\n",
      "train loss:2.302883263967203\n",
      "train loss:2.30311896538837\n",
      "train loss:2.2994719554742353\n",
      "train loss:2.299544527607871\n",
      "train loss:2.307855375923465\n",
      "train loss:2.2984416085394788\n",
      "train loss:2.2990614020818256\n",
      "train loss:2.2983326164717046\n",
      "train loss:2.307618156460359\n",
      "train loss:2.2990577189983115\n",
      "train loss:2.2945637877995497\n",
      "train loss:2.299545016072567\n",
      "train loss:2.30798641480664\n",
      "train loss:2.3079409995444244\n",
      "train loss:2.303729587656598\n",
      "train loss:2.3016210680219675\n",
      "train loss:2.3036974688502156\n",
      "train loss:2.3030820689221447\n",
      "train loss:2.3001196695428603\n",
      "train loss:2.3038988101313382\n",
      "train loss:2.3052923131379965\n",
      "train loss:2.301061864671586\n",
      "train loss:2.305341066883415\n",
      "train loss:2.30730559813564\n",
      "train loss:2.30810441172485\n",
      "train loss:2.2978268280747915\n",
      "train loss:2.299416120412044\n",
      "train loss:2.3001219089808433\n",
      "train loss:2.3162419555000735\n",
      "train loss:2.302545603018665\n",
      "train loss:2.304181120194324\n",
      "train loss:2.305155904429759\n",
      "train loss:2.3066622246126074\n",
      "train loss:2.2911771549388167\n",
      "train loss:2.2982315925738703\n",
      "train loss:2.2961606212671\n",
      "train loss:2.302858619188234\n",
      "train loss:2.3018729248719025\n",
      "train loss:2.304341957784359\n",
      "train loss:2.2981738303508488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3045246693229684\n",
      "train loss:2.2994061567219464\n",
      "train loss:2.3131728098589894\n",
      "train loss:2.3025020695655245\n",
      "train loss:2.3014096994342346\n",
      "train loss:2.2990620509767528\n",
      "train loss:2.3020250500948105\n",
      "train loss:2.304273570100749\n",
      "train loss:2.306465814930088\n",
      "train loss:2.2974807372489523\n",
      "train loss:2.3073826992442443\n",
      "train loss:2.3045014439757288\n",
      "train loss:2.3010993923988985\n",
      "train loss:2.301819655512102\n",
      "train loss:2.305229092573021\n",
      "train loss:2.304553163643033\n",
      "train loss:2.2983469523490205\n",
      "train loss:2.2977298555514616\n",
      "train loss:2.296000440697333\n",
      "train loss:2.2951699068969282\n",
      "train loss:2.2977432099136923\n",
      "train loss:2.3026212297034894\n",
      "train loss:2.2962598800513816\n",
      "train loss:2.306329343419942\n",
      "train loss:2.3054879285531897\n",
      "train loss:2.2974080384455378\n",
      "train loss:2.2976916722428116\n",
      "train loss:2.302027398973557\n",
      "train loss:2.296147211909284\n",
      "train loss:2.300617121846143\n",
      "train loss:2.2954000142590756\n",
      "train loss:2.2973366698814095\n",
      "train loss:2.3086307254057052\n",
      "train loss:2.297501018379776\n",
      "train loss:2.3056702685021526\n",
      "train loss:2.28861166040182\n",
      "train loss:2.2982339007621997\n",
      "train loss:2.3060490340736037\n",
      "train loss:2.3007278328613476\n",
      "train loss:2.304176451441298\n",
      "train loss:2.2983760868495273\n",
      "train loss:2.2987308109250963\n",
      "train loss:2.308768970680675\n",
      "train loss:2.298170147926209\n",
      "train loss:2.3077281856104572\n",
      "train loss:2.3073197370471092\n",
      "train loss:2.29828675309926\n",
      "train loss:2.3017554798516047\n",
      "train loss:2.3029413976320643\n",
      "train loss:2.3042620330469856\n",
      "train loss:2.3036237654823366\n",
      "train loss:2.297612383589303\n",
      "=== epoch:32, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2990469216887295\n",
      "train loss:2.2947845143660928\n",
      "train loss:2.299927188261807\n",
      "train loss:2.3039950361722163\n",
      "train loss:2.2993001198873984\n",
      "train loss:2.296623655720536\n",
      "train loss:2.2949431913196667\n",
      "train loss:2.3030862736797437\n",
      "train loss:2.297768628287574\n",
      "train loss:2.3028375324895842\n",
      "train loss:2.301053888115458\n",
      "train loss:2.2944872266919174\n",
      "train loss:2.300686537345588\n",
      "train loss:2.297226496206328\n",
      "train loss:2.3016836271894596\n",
      "train loss:2.3054012839060536\n",
      "train loss:2.306607461560164\n",
      "train loss:2.3036523329559064\n",
      "train loss:2.3031640005966483\n",
      "train loss:2.305549801404969\n",
      "train loss:2.2952825604926788\n",
      "train loss:2.2987462303064765\n",
      "train loss:2.306083309894696\n",
      "train loss:2.29169969138478\n",
      "train loss:2.3032975578832455\n",
      "train loss:2.3005316649150744\n",
      "train loss:2.302896360735415\n",
      "train loss:2.2948423414568024\n",
      "train loss:2.3096855502968636\n",
      "train loss:2.2962411406438377\n",
      "train loss:2.290390716683455\n",
      "train loss:2.30886072330674\n",
      "train loss:2.305330233780676\n",
      "train loss:2.3105194115347243\n",
      "train loss:2.3001995090126903\n",
      "train loss:2.3040020867192927\n",
      "train loss:2.298156240642889\n",
      "train loss:2.3033344084644614\n",
      "train loss:2.303211012570595\n",
      "train loss:2.3051842537925804\n",
      "train loss:2.3047400185994684\n",
      "train loss:2.307990355200185\n",
      "train loss:2.303774647250052\n",
      "train loss:2.3032569025028553\n",
      "train loss:2.296755854829573\n",
      "train loss:2.293853778624497\n",
      "train loss:2.290406189480384\n",
      "train loss:2.303894388232647\n",
      "train loss:2.298445625276266\n",
      "train loss:2.305305606471888\n",
      "train loss:2.2975465164865287\n",
      "train loss:2.2987867566224605\n",
      "train loss:2.2974724987322337\n",
      "train loss:2.3007924783946976\n",
      "train loss:2.2961525967180094\n",
      "train loss:2.2999306426376753\n",
      "train loss:2.298888656881226\n",
      "train loss:2.3030481037222326\n",
      "train loss:2.30931075673392\n",
      "train loss:2.3035738249007447\n",
      "train loss:2.3048662395120454\n",
      "train loss:2.307925249662433\n",
      "train loss:2.3108766623057697\n",
      "train loss:2.300587054214372\n",
      "train loss:2.300888679307371\n",
      "train loss:2.305828107947464\n",
      "train loss:2.297696562315846\n",
      "train loss:2.302757331007423\n",
      "train loss:2.301710814121218\n",
      "train loss:2.2998074443464067\n",
      "train loss:2.3057695939356\n",
      "train loss:2.3080151864021614\n",
      "train loss:2.3018453849788827\n",
      "train loss:2.310382214446902\n",
      "train loss:2.3064773400403507\n",
      "train loss:2.296301257502442\n",
      "train loss:2.308442971894534\n",
      "train loss:2.305501793369064\n",
      "train loss:2.3041016397952507\n",
      "train loss:2.2955158305454155\n",
      "train loss:2.301833750668065\n",
      "train loss:2.2975727049922767\n",
      "train loss:2.3041761844226802\n",
      "train loss:2.3032591582693427\n",
      "train loss:2.3045738813140444\n",
      "train loss:2.3083243331985983\n",
      "train loss:2.2993721108792378\n",
      "train loss:2.307677972884091\n",
      "train loss:2.3082358666141487\n",
      "train loss:2.2979914000133754\n",
      "train loss:2.2997740529067805\n",
      "train loss:2.302438319405968\n",
      "train loss:2.301549275560124\n",
      "train loss:2.3088214091389108\n",
      "train loss:2.300087712918837\n",
      "train loss:2.294986367512624\n",
      "train loss:2.294101540880133\n",
      "train loss:2.3125139143163898\n",
      "train loss:2.3052898000053985\n",
      "train loss:2.305309777880927\n",
      "train loss:2.3003670385816637\n",
      "train loss:2.3001288616710074\n",
      "train loss:2.302851786067346\n",
      "train loss:2.311989594308865\n",
      "train loss:2.292064055274302\n",
      "train loss:2.306365535370459\n",
      "train loss:2.30692119973799\n",
      "train loss:2.306365440435716\n",
      "train loss:2.3024282292521514\n",
      "train loss:2.3013994197682353\n",
      "train loss:2.2987113141093083\n",
      "train loss:2.3044661954090997\n",
      "train loss:2.302440037534526\n",
      "train loss:2.3114254400240224\n",
      "train loss:2.3094107710311294\n",
      "train loss:2.299038710593812\n",
      "train loss:2.3075844401671204\n",
      "train loss:2.298671502813604\n",
      "train loss:2.3074564404624005\n",
      "train loss:2.310585256973649\n",
      "train loss:2.2921449728862924\n",
      "train loss:2.3100596826355164\n",
      "train loss:2.2946744161087134\n",
      "train loss:2.2959818522962294\n",
      "train loss:2.305651539655751\n",
      "train loss:2.306992029758994\n",
      "train loss:2.300941327694381\n",
      "train loss:2.2972644773273294\n",
      "train loss:2.300919358732731\n",
      "train loss:2.2982813448870294\n",
      "train loss:2.301950170948086\n",
      "train loss:2.3071380249499303\n",
      "train loss:2.3005702158738264\n",
      "train loss:2.3012463283862004\n",
      "train loss:2.3066086664020817\n",
      "train loss:2.2986403498088777\n",
      "train loss:2.298424737160122\n",
      "train loss:2.3001569680932015\n",
      "train loss:2.306023219965141\n",
      "train loss:2.294916438399919\n",
      "train loss:2.303793711923808\n",
      "train loss:2.301315521657547\n",
      "train loss:2.2941217005951664\n",
      "train loss:2.299894060275058\n",
      "train loss:2.305053897257075\n",
      "train loss:2.3055090895737598\n",
      "train loss:2.304913998985565\n",
      "train loss:2.3071850073253755\n",
      "train loss:2.3048104401406633\n",
      "train loss:2.3032453375383812\n",
      "train loss:2.3092089595857703\n",
      "train loss:2.301546900402033\n",
      "train loss:2.2979636139638546\n",
      "train loss:2.3074906965746442\n",
      "train loss:2.2981809681740213\n",
      "train loss:2.3004223635390635\n",
      "train loss:2.299194432477782\n",
      "train loss:2.2993720389400423\n",
      "train loss:2.302606543719593\n",
      "train loss:2.3137340807987528\n",
      "train loss:2.2985786474287226\n",
      "train loss:2.3086401012460427\n",
      "train loss:2.3055401120374355\n",
      "train loss:2.3030231754870036\n",
      "train loss:2.3029058332433276\n",
      "train loss:2.303501937847777\n",
      "train loss:2.3028204409886532\n",
      "train loss:2.2995162583332025\n",
      "train loss:2.309351097051878\n",
      "train loss:2.3086194667586843\n",
      "train loss:2.2911376351827477\n",
      "train loss:2.303725778536969\n",
      "train loss:2.2923309140221675\n",
      "train loss:2.3077224881129275\n",
      "train loss:2.300264244504083\n",
      "train loss:2.301722947970394\n",
      "train loss:2.300577366275244\n",
      "train loss:2.301070001484127\n",
      "train loss:2.299610610178425\n",
      "train loss:2.303635349845528\n",
      "train loss:2.3066983696810253\n",
      "train loss:2.3054130162454167\n",
      "train loss:2.3067172774003146\n",
      "train loss:2.301710165396469\n",
      "train loss:2.304223317315932\n",
      "train loss:2.303686914999283\n",
      "train loss:2.3012325345936615\n",
      "train loss:2.3047576185499925\n",
      "train loss:2.303699778413443\n",
      "train loss:2.2958252834058435\n",
      "train loss:2.300568504787311\n",
      "train loss:2.3008323617924793\n",
      "train loss:2.2937899302956373\n",
      "train loss:2.3051738443252066\n",
      "train loss:2.301234131586126\n",
      "train loss:2.295359933592623\n",
      "train loss:2.3047289465657097\n",
      "train loss:2.302612919577394\n",
      "train loss:2.31435602151619\n",
      "train loss:2.295153629801661\n",
      "=== epoch:33, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3064194932606856\n",
      "train loss:2.2950008198468237\n",
      "train loss:2.2926896333808506\n",
      "train loss:2.3024969987377863\n",
      "train loss:2.3002317312579135\n",
      "train loss:2.2977666916999984\n",
      "train loss:2.3003788075156755\n",
      "train loss:2.2991170097161495\n",
      "train loss:2.297312612465245\n",
      "train loss:2.303098396977705\n",
      "train loss:2.302066893139662\n",
      "train loss:2.301645227433059\n",
      "train loss:2.3039342617724032\n",
      "train loss:2.295984545731733\n",
      "train loss:2.300508442862721\n",
      "train loss:2.3000369857608653\n",
      "train loss:2.303570729579347\n",
      "train loss:2.302932535368514\n",
      "train loss:2.3006833530918573\n",
      "train loss:2.3050408740009676\n",
      "train loss:2.3042163754465803\n",
      "train loss:2.3033261110528818\n",
      "train loss:2.3012430961192476\n",
      "train loss:2.2980955260369726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3038243315244102\n",
      "train loss:2.3028377394814723\n",
      "train loss:2.3038785535314625\n",
      "train loss:2.301194414639023\n",
      "train loss:2.3027738148157657\n",
      "train loss:2.302956218617061\n",
      "train loss:2.3074145363258527\n",
      "train loss:2.3017968625435317\n",
      "train loss:2.300069250378737\n",
      "train loss:2.3025861231623073\n",
      "train loss:2.3047417179638057\n",
      "train loss:2.298163109107248\n",
      "train loss:2.306964026897933\n",
      "train loss:2.3085145201049153\n",
      "train loss:2.3031517085445934\n",
      "train loss:2.294003888647709\n",
      "train loss:2.2983816627485383\n",
      "train loss:2.2968535867003688\n",
      "train loss:2.301332571152201\n",
      "train loss:2.2987726832585795\n",
      "train loss:2.303138121497356\n",
      "train loss:2.3100887330138633\n",
      "train loss:2.299834488807878\n",
      "train loss:2.302672235401986\n",
      "train loss:2.3043929729533654\n",
      "train loss:2.302639093497215\n",
      "train loss:2.2979708308965017\n",
      "train loss:2.3030204554634786\n",
      "train loss:2.3068305332331076\n",
      "train loss:2.297619327451962\n",
      "train loss:2.3009865366536717\n",
      "train loss:2.30123926755415\n",
      "train loss:2.302422257724599\n",
      "train loss:2.3023533458359093\n",
      "train loss:2.2979637503091146\n",
      "train loss:2.3000859761625088\n",
      "train loss:2.2974911714371027\n",
      "train loss:2.301570728278185\n",
      "train loss:2.2875118097970506\n",
      "train loss:2.2968263878086046\n",
      "train loss:2.3000770903787586\n",
      "train loss:2.2977817592533434\n",
      "train loss:2.2995404211958044\n",
      "train loss:2.302218200804906\n",
      "train loss:2.309226113872615\n",
      "train loss:2.306221030936599\n",
      "train loss:2.3017654493765773\n",
      "train loss:2.2995679703019785\n",
      "train loss:2.2992346071738856\n",
      "train loss:2.303062413244781\n",
      "train loss:2.3031030389503306\n",
      "train loss:2.297729848985754\n",
      "train loss:2.300247263012015\n",
      "train loss:2.3003759357420317\n",
      "train loss:2.3059026512614094\n",
      "train loss:2.2930482600933377\n",
      "train loss:2.3039679641152437\n",
      "train loss:2.2992464232988303\n",
      "train loss:2.3040448896474084\n",
      "train loss:2.2976547685531505\n",
      "train loss:2.3048471093722713\n",
      "train loss:2.3072609376295192\n",
      "train loss:2.295773248871125\n",
      "train loss:2.3003019835340845\n",
      "train loss:2.2992687595239802\n",
      "train loss:2.303162796612863\n",
      "train loss:2.298904803489053\n",
      "train loss:2.304392842108185\n",
      "train loss:2.3038607613863413\n",
      "train loss:2.3046786199498084\n",
      "train loss:2.306100755066679\n",
      "train loss:2.3077047162643702\n",
      "train loss:2.3072929293039617\n",
      "train loss:2.3101723753745453\n",
      "train loss:2.3036969725326153\n",
      "train loss:2.2942126510474465\n",
      "train loss:2.2995703452956926\n",
      "train loss:2.3034098087231443\n",
      "train loss:2.2958958559111178\n",
      "train loss:2.294635079799476\n",
      "train loss:2.2996330635151923\n",
      "train loss:2.299867809835387\n",
      "train loss:2.303527400372258\n",
      "train loss:2.2970685852707615\n",
      "train loss:2.3020561963856374\n",
      "train loss:2.306379753529831\n",
      "train loss:2.304007829966089\n",
      "train loss:2.302352029434628\n",
      "train loss:2.308907837190858\n",
      "train loss:2.301461345556665\n",
      "train loss:2.3012848060962536\n",
      "train loss:2.299293701557362\n",
      "train loss:2.3065977626105845\n",
      "train loss:2.297733597806288\n",
      "train loss:2.3001766229829603\n",
      "train loss:2.2982323577317136\n",
      "train loss:2.3084101184703756\n",
      "train loss:2.3029673467185305\n",
      "train loss:2.3036997376201596\n",
      "train loss:2.3031569201370927\n",
      "train loss:2.3010611089280952\n",
      "train loss:2.3025268373878416\n",
      "train loss:2.307845429559434\n",
      "train loss:2.3066215523277176\n",
      "train loss:2.305547279959675\n",
      "train loss:2.2974358946608913\n",
      "train loss:2.3086602297608185\n",
      "train loss:2.2961602471113123\n",
      "train loss:2.2956175022601673\n",
      "train loss:2.2994026157752856\n",
      "train loss:2.2984082150150287\n",
      "train loss:2.2970658336725216\n",
      "train loss:2.299730671464819\n",
      "train loss:2.2994612809832846\n",
      "train loss:2.2999131939775164\n",
      "train loss:2.301586808466177\n",
      "train loss:2.3030282771448505\n",
      "train loss:2.3018835708312575\n",
      "train loss:2.2993913235297843\n",
      "train loss:2.298175297453778\n",
      "train loss:2.300538089371086\n",
      "train loss:2.3008975085083683\n",
      "train loss:2.2986480905829882\n",
      "train loss:2.3087765009112413\n",
      "train loss:2.2986133405363893\n",
      "train loss:2.29917928609349\n",
      "train loss:2.305498301502356\n",
      "train loss:2.304883475757039\n",
      "train loss:2.296600920973997\n",
      "train loss:2.3054467734744293\n",
      "train loss:2.2969778106056875\n",
      "train loss:2.2877755501057853\n",
      "train loss:2.293038129380567\n",
      "train loss:2.2983545802326644\n",
      "train loss:2.3015411596588775\n",
      "train loss:2.303055369515729\n",
      "train loss:2.3048644162598064\n",
      "train loss:2.296766057431605\n",
      "train loss:2.3085139667979555\n",
      "train loss:2.306661873183643\n",
      "train loss:2.295958337867525\n",
      "train loss:2.301332967290617\n",
      "train loss:2.303333080174029\n",
      "train loss:2.3010753715332064\n",
      "train loss:2.3027599510156858\n",
      "train loss:2.2940962043045823\n",
      "train loss:2.2908786828347747\n",
      "train loss:2.3002461669118244\n",
      "train loss:2.2980856452861707\n",
      "train loss:2.302054009428954\n",
      "train loss:2.307324964267782\n",
      "train loss:2.299678071320919\n",
      "train loss:2.3018958622713552\n",
      "train loss:2.2999686073170587\n",
      "train loss:2.300201561818766\n",
      "train loss:2.29951210251767\n",
      "train loss:2.2991837684128518\n",
      "train loss:2.294972101652398\n",
      "train loss:2.3068996007849267\n",
      "train loss:2.2915989797196716\n",
      "train loss:2.3035061028494153\n",
      "train loss:2.3016617477626777\n",
      "train loss:2.3004600974080867\n",
      "train loss:2.296228523844736\n",
      "train loss:2.3089690634656392\n",
      "train loss:2.3008763484061077\n",
      "train loss:2.2933886611173353\n",
      "train loss:2.3021267055540546\n",
      "train loss:2.2917019173072695\n",
      "train loss:2.301294790879091\n",
      "train loss:2.3059376791241437\n",
      "train loss:2.30323885945984\n",
      "train loss:2.295201271565853\n",
      "train loss:2.299358057658617\n",
      "train loss:2.306867242791562\n",
      "train loss:2.306322548598952\n",
      "=== epoch:34, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3047881555769902\n",
      "train loss:2.300528517359624\n",
      "train loss:2.2926995154548733\n",
      "train loss:2.2982736357580498\n",
      "train loss:2.305651347942043\n",
      "train loss:2.3004651199837416\n",
      "train loss:2.3030693210402897\n",
      "train loss:2.3012903747552786\n",
      "train loss:2.305090241443613\n",
      "train loss:2.299500662686101\n",
      "train loss:2.3082847368917268\n",
      "train loss:2.304864388988801\n",
      "train loss:2.29852011500007\n",
      "train loss:2.3037057167983783\n",
      "train loss:2.3039305667559113\n",
      "train loss:2.301761287023522\n",
      "train loss:2.3048171985500963\n",
      "train loss:2.2977182506141265\n",
      "train loss:2.2934954082537167\n",
      "train loss:2.300593527717578\n",
      "train loss:2.2986276852483782\n",
      "train loss:2.302530111201783\n",
      "train loss:2.311412015451088\n",
      "train loss:2.306977157568049\n",
      "train loss:2.295812871273028\n",
      "train loss:2.30162765602494\n",
      "train loss:2.3022607538546307\n",
      "train loss:2.301751785857431\n",
      "train loss:2.2962387308769645\n",
      "train loss:2.300185558331589\n",
      "train loss:2.304035953236521\n",
      "train loss:2.31188290905029\n",
      "train loss:2.3013697711463768\n",
      "train loss:2.300176358927579\n",
      "train loss:2.301075983307395\n",
      "train loss:2.295116802741126\n",
      "train loss:2.309116010860685\n",
      "train loss:2.3019424421350627\n",
      "train loss:2.2981779378288016\n",
      "train loss:2.3066715577627503\n",
      "train loss:2.290130849884705\n",
      "train loss:2.301583630632636\n",
      "train loss:2.2980451149220387\n",
      "train loss:2.3062234465290072\n",
      "train loss:2.300936280062858\n",
      "train loss:2.300400286138291\n",
      "train loss:2.298948421544387\n",
      "train loss:2.304121637752081\n",
      "train loss:2.303199915623554\n",
      "train loss:2.2879710310542984\n",
      "train loss:2.3072120407124816\n",
      "train loss:2.3047588097314873\n",
      "train loss:2.3060200346465094\n",
      "train loss:2.3001665048649116\n",
      "train loss:2.3008961523264606\n",
      "train loss:2.3044592941471382\n",
      "train loss:2.29296224535712\n",
      "train loss:2.2992281290855376\n",
      "train loss:2.30525453715903\n",
      "train loss:2.2981543766828145\n",
      "train loss:2.3077915212622875\n",
      "train loss:2.303850597060048\n",
      "train loss:2.2970105153365963\n",
      "train loss:2.300193124349523\n",
      "train loss:2.305056754257313\n",
      "train loss:2.293178479668859\n",
      "train loss:2.301117750174856\n",
      "train loss:2.305520767243647\n",
      "train loss:2.296644681826113\n",
      "train loss:2.295898003024713\n",
      "train loss:2.297355117982187\n",
      "train loss:2.3055451490263437\n",
      "train loss:2.3101660357831646\n",
      "train loss:2.3087683839937236\n",
      "train loss:2.3044898152131488\n",
      "train loss:2.2963395987515733\n",
      "train loss:2.297372659854398\n",
      "train loss:2.3024210809787693\n",
      "train loss:2.300984742981421\n",
      "train loss:2.3021949576197684\n",
      "train loss:2.3015128862372416\n",
      "train loss:2.303992730875742\n",
      "train loss:2.3076998563969493\n",
      "train loss:2.3000782118404364\n",
      "train loss:2.300050707229281\n",
      "train loss:2.304210043510617\n",
      "train loss:2.3029001592749725\n",
      "train loss:2.3041081240645855\n",
      "train loss:2.2955422090206206\n",
      "train loss:2.3063146559811116\n",
      "train loss:2.3042321306390168\n",
      "train loss:2.3034311740831206\n",
      "train loss:2.302845381567767\n",
      "train loss:2.2986816732470228\n",
      "train loss:2.29962832875997\n",
      "train loss:2.3044745355413228\n",
      "train loss:2.3051398604150553\n",
      "train loss:2.302409663254284\n",
      "train loss:2.3007947186869293\n",
      "train loss:2.304656155931878\n",
      "train loss:2.3036862339289175\n",
      "train loss:2.2992527465818178\n",
      "train loss:2.306432160274092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299171741372967\n",
      "train loss:2.2999914568554782\n",
      "train loss:2.3003225102658846\n",
      "train loss:2.308405659288764\n",
      "train loss:2.3009891193429515\n",
      "train loss:2.296181117061631\n",
      "train loss:2.3102602687346625\n",
      "train loss:2.307281454034005\n",
      "train loss:2.3112320868125313\n",
      "train loss:2.3096021152389903\n",
      "train loss:2.309080249646192\n",
      "train loss:2.302862256636187\n",
      "train loss:2.2927688963726456\n",
      "train loss:2.2948047552916\n",
      "train loss:2.2990768865152984\n",
      "train loss:2.3010737228035634\n",
      "train loss:2.3006060992323\n",
      "train loss:2.3036608576266966\n",
      "train loss:2.2967089356903125\n",
      "train loss:2.2910043970332143\n",
      "train loss:2.3008883280495342\n",
      "train loss:2.2981285550196207\n",
      "train loss:2.289461500334299\n",
      "train loss:2.302892347587361\n",
      "train loss:2.3000691467048653\n",
      "train loss:2.297678228900098\n",
      "train loss:2.3046256836710954\n",
      "train loss:2.300286419371727\n",
      "train loss:2.301175041058244\n",
      "train loss:2.3002179211537443\n",
      "train loss:2.300535679749118\n",
      "train loss:2.3016072448955556\n",
      "train loss:2.3005570077111437\n",
      "train loss:2.303307720684358\n",
      "train loss:2.299390028935056\n",
      "train loss:2.2997299912282005\n",
      "train loss:2.297008139106916\n",
      "train loss:2.302289850795287\n",
      "train loss:2.302085804393408\n",
      "train loss:2.289064083957041\n",
      "train loss:2.297862341786619\n",
      "train loss:2.2912389451544883\n",
      "train loss:2.298508268114771\n",
      "train loss:2.305880430212254\n",
      "train loss:2.294465433170018\n",
      "train loss:2.2993603835708436\n",
      "train loss:2.2958758932008254\n",
      "train loss:2.301408758327848\n",
      "train loss:2.306133432969464\n",
      "train loss:2.3051208084736925\n",
      "train loss:2.300188891022407\n",
      "train loss:2.2985386033700874\n",
      "train loss:2.290993584567398\n",
      "train loss:2.29862178753442\n",
      "train loss:2.3037471071829265\n",
      "train loss:2.313001613816139\n",
      "train loss:2.3031312972388496\n",
      "train loss:2.2996489810822096\n",
      "train loss:2.3083718553263\n",
      "train loss:2.307515706158605\n",
      "train loss:2.2956893793667157\n",
      "train loss:2.306893407601923\n",
      "train loss:2.310306411118662\n",
      "train loss:2.300882590701622\n",
      "train loss:2.3013366500679875\n",
      "train loss:2.3047454204551445\n",
      "train loss:2.298776776490059\n",
      "train loss:2.2964341073372805\n",
      "train loss:2.3068047990698433\n",
      "train loss:2.3089808330139068\n",
      "train loss:2.3074489824107105\n",
      "train loss:2.296481875559235\n",
      "train loss:2.295837509147596\n",
      "train loss:2.3059186071999167\n",
      "train loss:2.3023182072990886\n",
      "train loss:2.302710095285241\n",
      "train loss:2.304608763248192\n",
      "train loss:2.301176800842588\n",
      "train loss:2.3089670317188262\n",
      "train loss:2.3006458951982873\n",
      "train loss:2.3104642333095806\n",
      "train loss:2.305090183386795\n",
      "train loss:2.3051627190783064\n",
      "train loss:2.2963936876017454\n",
      "train loss:2.2936785653877414\n",
      "train loss:2.2914980228176374\n",
      "train loss:2.3106546581382528\n",
      "train loss:2.2988290321617253\n",
      "train loss:2.2957806108358896\n",
      "train loss:2.299168395370237\n",
      "train loss:2.3012304048695587\n",
      "train loss:2.301291452731602\n",
      "train loss:2.2985453528828423\n",
      "train loss:2.3017410837235266\n",
      "train loss:2.302358617976464\n",
      "train loss:2.3015315786326425\n",
      "train loss:2.297628684816329\n",
      "=== epoch:35, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3049577661686373\n",
      "train loss:2.2961073157497864\n",
      "train loss:2.2984750436165635\n",
      "train loss:2.302102081248291\n",
      "train loss:2.300542245930637\n",
      "train loss:2.304285807355704\n",
      "train loss:2.3075072623454562\n",
      "train loss:2.3045632976287025\n",
      "train loss:2.304054397352427\n",
      "train loss:2.300671506073794\n",
      "train loss:2.3013212703920085\n",
      "train loss:2.3065746340436393\n",
      "train loss:2.3042764023647075\n",
      "train loss:2.2994270485130275\n",
      "train loss:2.30318427426005\n",
      "train loss:2.303720060551243\n",
      "train loss:2.304008556471379\n",
      "train loss:2.2992176897100514\n",
      "train loss:2.2949555948131573\n",
      "train loss:2.304748523526961\n",
      "train loss:2.3050472846613195\n",
      "train loss:2.3018832460550183\n",
      "train loss:2.2999928841626067\n",
      "train loss:2.2945417523395144\n",
      "train loss:2.2989493800709755\n",
      "train loss:2.2941319906565574\n",
      "train loss:2.301855472991484\n",
      "train loss:2.2993701349351414\n",
      "train loss:2.3005722189282953\n",
      "train loss:2.3025671202804423\n",
      "train loss:2.308180885576784\n",
      "train loss:2.2974464779340917\n",
      "train loss:2.3053926890968253\n",
      "train loss:2.303327856493205\n",
      "train loss:2.2999565860825597\n",
      "train loss:2.3021549162373076\n",
      "train loss:2.300785335767796\n",
      "train loss:2.297163728230045\n",
      "train loss:2.3032984117632953\n",
      "train loss:2.2935929582540235\n",
      "train loss:2.309677133842598\n",
      "train loss:2.2923109426613175\n",
      "train loss:2.29826782779491\n",
      "train loss:2.29866817771715\n",
      "train loss:2.31127575366728\n",
      "train loss:2.306000046627376\n",
      "train loss:2.305557109002865\n",
      "train loss:2.304586577395714\n",
      "train loss:2.2949763952790794\n",
      "train loss:2.296816096767509\n",
      "train loss:2.299664573547894\n",
      "train loss:2.299073568813912\n",
      "train loss:2.300380986625119\n",
      "train loss:2.3035254497664277\n",
      "train loss:2.302369877654426\n",
      "train loss:2.3020196670819555\n",
      "train loss:2.300791759833096\n",
      "train loss:2.298023846547109\n",
      "train loss:2.3061860781894716\n",
      "train loss:2.2988464801314756\n",
      "train loss:2.289565386449477\n",
      "train loss:2.307708991887068\n",
      "train loss:2.2914815480473365\n",
      "train loss:2.301834799279602\n",
      "train loss:2.2953389357607143\n",
      "train loss:2.30500732327552\n",
      "train loss:2.2999999838813014\n",
      "train loss:2.3049125502491874\n",
      "train loss:2.292607732616964\n",
      "train loss:2.3132591090516517\n",
      "train loss:2.3073093967309406\n",
      "train loss:2.30217473995766\n",
      "train loss:2.2982618353697504\n",
      "train loss:2.304539322521612\n",
      "train loss:2.2919187457124535\n",
      "train loss:2.3007056172927416\n",
      "train loss:2.300015477952335\n",
      "train loss:2.3032271880048327\n",
      "train loss:2.3032469587403805\n",
      "train loss:2.293737963447566\n",
      "train loss:2.305608968701223\n",
      "train loss:2.30226757759483\n",
      "train loss:2.294429066625496\n",
      "train loss:2.2937286768471488\n",
      "train loss:2.299889982075151\n",
      "train loss:2.3044554603893452\n",
      "train loss:2.294083522697021\n",
      "train loss:2.3007018711527976\n",
      "train loss:2.294264043088216\n",
      "train loss:2.305664145297981\n",
      "train loss:2.298660875444068\n",
      "train loss:2.3082735163778434\n",
      "train loss:2.2961102047465833\n",
      "train loss:2.3033770722344493\n",
      "train loss:2.3032954542995148\n",
      "train loss:2.3024179832654723\n",
      "train loss:2.3025859133338265\n",
      "train loss:2.3021655497812237\n",
      "train loss:2.299668469946307\n",
      "train loss:2.304142459887324\n",
      "train loss:2.301679511015547\n",
      "train loss:2.301598592663624\n",
      "train loss:2.299854314577659\n",
      "train loss:2.302644646747493\n",
      "train loss:2.304532555093352\n",
      "train loss:2.298330317352456\n",
      "train loss:2.306533562929756\n",
      "train loss:2.299970896748581\n",
      "train loss:2.295355044642058\n",
      "train loss:2.3079694949621055\n",
      "train loss:2.300126078239406\n",
      "train loss:2.297802244682396\n",
      "train loss:2.298977584319798\n",
      "train loss:2.3037697956000645\n",
      "train loss:2.295400437972952\n",
      "train loss:2.2994594227592557\n",
      "train loss:2.2992762249306917\n",
      "train loss:2.296655442832206\n",
      "train loss:2.3108935097866112\n",
      "train loss:2.292559257781814\n",
      "train loss:2.3064099415036097\n",
      "train loss:2.302734448465404\n",
      "train loss:2.3064418769728525\n",
      "train loss:2.304189720247276\n",
      "train loss:2.303558457530549\n",
      "train loss:2.2998287924288086\n",
      "train loss:2.3062697449433087\n",
      "train loss:2.2971838138578935\n",
      "train loss:2.304579291810744\n",
      "train loss:2.3081220667873708\n",
      "train loss:2.2965388482104294\n",
      "train loss:2.3059414714879924\n",
      "train loss:2.291792818365891\n",
      "train loss:2.298473056178952\n",
      "train loss:2.2999607405382476\n",
      "train loss:2.3020435213951504\n",
      "train loss:2.300344282030094\n",
      "train loss:2.304564342624234\n",
      "train loss:2.304996001844508\n",
      "train loss:2.295188543530354\n",
      "train loss:2.296854477181404\n",
      "train loss:2.2992271956573345\n",
      "train loss:2.2954186151468585\n",
      "train loss:2.304501447135311\n",
      "train loss:2.303093010035956\n",
      "train loss:2.3026488909482317\n",
      "train loss:2.3045937288527347\n",
      "train loss:2.29617263329496\n",
      "train loss:2.2964452651207803\n",
      "train loss:2.3045343364452924\n",
      "train loss:2.301747708973494\n",
      "train loss:2.290107563295016\n",
      "train loss:2.306904211499799\n",
      "train loss:2.299846520683576\n",
      "train loss:2.3077518617505492\n",
      "train loss:2.2979936974907025\n",
      "train loss:2.3052250389015927\n",
      "train loss:2.302182804385675\n",
      "train loss:2.306821357987106\n",
      "train loss:2.2994136579484\n",
      "train loss:2.2965956272603028\n",
      "train loss:2.299075325521793\n",
      "train loss:2.296437488167238\n",
      "train loss:2.303902344176048\n",
      "train loss:2.3090047130771643\n",
      "train loss:2.304455788668614\n",
      "train loss:2.309393506855246\n",
      "train loss:2.3045036071678107\n",
      "train loss:2.2960363243045836\n",
      "train loss:2.298634923064254\n",
      "train loss:2.2998055120000855\n",
      "train loss:2.313819423506851\n",
      "train loss:2.3058423913671473\n",
      "train loss:2.2998285078736487\n",
      "train loss:2.298737623775869\n",
      "train loss:2.3061809548155123\n",
      "train loss:2.3022924240077196\n",
      "train loss:2.2943472183148494\n",
      "train loss:2.296256085691319\n",
      "train loss:2.2999667676111075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.30019675369127\n",
      "train loss:2.299455801745824\n",
      "train loss:2.299088853834027\n",
      "train loss:2.3046373902453445\n",
      "train loss:2.29852694316334\n",
      "train loss:2.295148259874583\n",
      "train loss:2.297725957405369\n",
      "train loss:2.3016009670285347\n",
      "train loss:2.3072441852253505\n",
      "train loss:2.3050305991861486\n",
      "train loss:2.3024783742399144\n",
      "train loss:2.304119696955891\n",
      "train loss:2.2979596022428903\n",
      "train loss:2.308367180364348\n",
      "train loss:2.295556472510163\n",
      "train loss:2.29956917063599\n",
      "train loss:2.300526878440089\n",
      "train loss:2.2929433177006255\n",
      "train loss:2.2958815326262387\n",
      "train loss:2.2964500887374695\n",
      "=== epoch:36, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.315346039879531\n",
      "train loss:2.2998535535640126\n",
      "train loss:2.304189273985227\n",
      "train loss:2.3007427717354982\n",
      "train loss:2.306818466031622\n",
      "train loss:2.3076388400204824\n",
      "train loss:2.3011064637715473\n",
      "train loss:2.306406100175557\n",
      "train loss:2.289653399833143\n",
      "train loss:2.3060143791157577\n",
      "train loss:2.3080237422345586\n",
      "train loss:2.3012967692861137\n",
      "train loss:2.290162934585159\n",
      "train loss:2.2983134555682025\n",
      "train loss:2.2951573023401535\n",
      "train loss:2.3040008097107356\n",
      "train loss:2.300954311872196\n",
      "train loss:2.304432665779343\n",
      "train loss:2.301786730247792\n",
      "train loss:2.2962442951833135\n",
      "train loss:2.3073915944098404\n",
      "train loss:2.28714379669416\n",
      "train loss:2.3093006992793943\n",
      "train loss:2.2976171952127493\n",
      "train loss:2.310193730732002\n",
      "train loss:2.299124355036287\n",
      "train loss:2.303842753179246\n",
      "train loss:2.3050887704141276\n",
      "train loss:2.299035827523457\n",
      "train loss:2.298748253914631\n",
      "train loss:2.301574583112018\n",
      "train loss:2.3045519283077196\n",
      "train loss:2.3047570218390168\n",
      "train loss:2.3020610368628702\n",
      "train loss:2.3064426877189645\n",
      "train loss:2.3036521219534247\n",
      "train loss:2.309735297966038\n",
      "train loss:2.304571199004595\n",
      "train loss:2.3072750112479437\n",
      "train loss:2.295070784733248\n",
      "train loss:2.3084552632857944\n",
      "train loss:2.300025919070623\n",
      "train loss:2.3039372040847867\n",
      "train loss:2.2973206308535232\n",
      "train loss:2.301170677362754\n",
      "train loss:2.3026482734095164\n",
      "train loss:2.2981616093252883\n",
      "train loss:2.302846927264196\n",
      "train loss:2.307729941952605\n",
      "train loss:2.2992636952478716\n",
      "train loss:2.297527823444524\n",
      "train loss:2.2970238731313066\n",
      "train loss:2.3013072343117837\n",
      "train loss:2.2929281800591155\n",
      "train loss:2.299375310399448\n",
      "train loss:2.3033140963918095\n",
      "train loss:2.3019722709005848\n",
      "train loss:2.299664626594789\n",
      "train loss:2.3009215848732234\n",
      "train loss:2.3001630147510475\n",
      "train loss:2.2946353951543275\n",
      "train loss:2.3047404755550964\n",
      "train loss:2.299453731092847\n",
      "train loss:2.3094124093926807\n",
      "train loss:2.3071427566020613\n",
      "train loss:2.298695605830278\n",
      "train loss:2.295586639013752\n",
      "train loss:2.2964789973491646\n",
      "train loss:2.3052046647366256\n",
      "train loss:2.3013485389799713\n",
      "train loss:2.309853125879529\n",
      "train loss:2.29924625300395\n",
      "train loss:2.303723585700681\n",
      "train loss:2.2922953252906075\n",
      "train loss:2.2951667357946537\n",
      "train loss:2.3048229569798426\n",
      "train loss:2.292458452574366\n",
      "train loss:2.3013397390261043\n",
      "train loss:2.309446499567467\n",
      "train loss:2.299145896470763\n",
      "train loss:2.2979029502207995\n",
      "train loss:2.297617847710163\n",
      "train loss:2.2985632188888516\n",
      "train loss:2.303789766092215\n",
      "train loss:2.3010724808230743\n",
      "train loss:2.3009089107886194\n",
      "train loss:2.3044638028484807\n",
      "train loss:2.303757988413887\n",
      "train loss:2.3032235916458665\n",
      "train loss:2.3040583922009397\n",
      "train loss:2.298190160163222\n",
      "train loss:2.3009442351519818\n",
      "train loss:2.3115619769065847\n",
      "train loss:2.2932380831502024\n",
      "train loss:2.296018229002923\n",
      "train loss:2.3037901563748213\n",
      "train loss:2.292936487685817\n",
      "train loss:2.3019629594548934\n",
      "train loss:2.302906292789268\n",
      "train loss:2.294119420405813\n",
      "train loss:2.300671278884472\n",
      "train loss:2.298510654074249\n",
      "train loss:2.29971078507136\n",
      "train loss:2.2998893149296955\n",
      "train loss:2.3017443867677176\n",
      "train loss:2.3038646160724046\n",
      "train loss:2.3049980401183405\n",
      "train loss:2.3001224153062725\n",
      "train loss:2.303007572768345\n",
      "train loss:2.3040915072568606\n",
      "train loss:2.300801473104739\n",
      "train loss:2.2971918995434635\n",
      "train loss:2.300044768632145\n",
      "train loss:2.3081110997745564\n",
      "train loss:2.2942790533401043\n",
      "train loss:2.3013483816219833\n",
      "train loss:2.303758097815986\n",
      "train loss:2.3018069918795727\n",
      "train loss:2.297315883908377\n",
      "train loss:2.3118892204406145\n",
      "train loss:2.296923705910123\n",
      "train loss:2.3029499887931157\n",
      "train loss:2.309374865137723\n",
      "train loss:2.302795304126019\n",
      "train loss:2.3081385230060434\n",
      "train loss:2.2956619675737477\n",
      "train loss:2.3023442106511816\n",
      "train loss:2.3065610601650497\n",
      "train loss:2.29980250577963\n",
      "train loss:2.2976851541317673\n",
      "train loss:2.3027833658534487\n",
      "train loss:2.295111295390952\n",
      "train loss:2.3017418848917415\n",
      "train loss:2.301311667173984\n",
      "train loss:2.3067376273312545\n",
      "train loss:2.3073598752038973\n",
      "train loss:2.305993832353349\n",
      "train loss:2.309776958708657\n",
      "train loss:2.2970476359861225\n",
      "train loss:2.3061556632655886\n",
      "train loss:2.296993071952813\n",
      "train loss:2.3000855777046527\n",
      "train loss:2.3021400789813393\n",
      "train loss:2.3023342176467514\n",
      "train loss:2.3048983662093607\n",
      "train loss:2.2985606254207562\n",
      "train loss:2.312060337583244\n",
      "train loss:2.301481160764453\n",
      "train loss:2.2949634603730455\n",
      "train loss:2.3035766480795195\n",
      "train loss:2.298873566841119\n",
      "train loss:2.2961184928202063\n",
      "train loss:2.2993176955579115\n",
      "train loss:2.310827627372535\n",
      "train loss:2.2995979735687215\n",
      "train loss:2.296981966190936\n",
      "train loss:2.306477763560358\n",
      "train loss:2.301764917924611\n",
      "train loss:2.307308752765772\n",
      "train loss:2.3034721048858957\n",
      "train loss:2.3126899020623917\n",
      "train loss:2.296446284323052\n",
      "train loss:2.2965250769705627\n",
      "train loss:2.30321750539433\n",
      "train loss:2.3001886318693083\n",
      "train loss:2.3040844603021506\n",
      "train loss:2.3019327973359154\n",
      "train loss:2.299999013935273\n",
      "train loss:2.297940735495691\n",
      "train loss:2.306723602972418\n",
      "train loss:2.305328094007586\n",
      "train loss:2.3063520867533245\n",
      "train loss:2.306814707989955\n",
      "train loss:2.300126862120314\n",
      "train loss:2.3049141457699287\n",
      "train loss:2.304911491239358\n",
      "train loss:2.3037614263742223\n",
      "train loss:2.3015792213929718\n",
      "train loss:2.303000598359208\n",
      "train loss:2.2921767108827007\n",
      "train loss:2.2970359969084395\n",
      "train loss:2.294717791685705\n",
      "train loss:2.301121735772086\n",
      "train loss:2.3000936690187794\n",
      "train loss:2.3032363398636377\n",
      "train loss:2.3147491642514075\n",
      "train loss:2.304864720445588\n",
      "train loss:2.316901461359531\n",
      "train loss:2.301280867452032\n",
      "train loss:2.2998310136702735\n",
      "train loss:2.300433421639444\n",
      "train loss:2.301121334872696\n",
      "train loss:2.2975529538964534\n",
      "train loss:2.304178898500691\n",
      "train loss:2.300427469471143\n",
      "train loss:2.3019579512928536\n",
      "train loss:2.2976204355060457\n",
      "train loss:2.3030847296055725\n",
      "train loss:2.3039033301924063\n",
      "train loss:2.3041116347490362\n",
      "=== epoch:37, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3044814971013796\n",
      "train loss:2.3003731223262824\n",
      "train loss:2.308161253422392\n",
      "train loss:2.308364493833254\n",
      "train loss:2.3014552255811\n",
      "train loss:2.291896116472995\n",
      "train loss:2.295375435848905\n",
      "train loss:2.306971876616907\n",
      "train loss:2.297838091976567\n",
      "train loss:2.305330166013509\n",
      "train loss:2.3047309043106106\n",
      "train loss:2.306597085218272\n",
      "train loss:2.304160114281211\n",
      "train loss:2.306490065766427\n",
      "train loss:2.3034295312080153\n",
      "train loss:2.295898847524423\n",
      "train loss:2.2973881341044753\n",
      "train loss:2.30107945540771\n",
      "train loss:2.3045008081353253\n",
      "train loss:2.3067137598937\n",
      "train loss:2.307363784393795\n",
      "train loss:2.301845175485362\n",
      "train loss:2.2988284091695217\n",
      "train loss:2.302704757092686\n",
      "train loss:2.303544923533157\n",
      "train loss:2.3031711364534213\n",
      "train loss:2.295330694099406\n",
      "train loss:2.300768235667553\n",
      "train loss:2.3033435237233615\n",
      "train loss:2.2987417646539936\n",
      "train loss:2.300864458688219\n",
      "train loss:2.3007620623712546\n",
      "train loss:2.2964691068807492\n",
      "train loss:2.3054008145260507\n",
      "train loss:2.298877017829662\n",
      "train loss:2.299748592346414\n",
      "train loss:2.2977023823373166\n",
      "train loss:2.307203279874557\n",
      "train loss:2.302815023467706\n",
      "train loss:2.301941670714557\n",
      "train loss:2.301800982405499\n",
      "train loss:2.2907602546697454\n",
      "train loss:2.3013082074805715\n",
      "train loss:2.3093163608651825\n",
      "train loss:2.3046396111731515\n",
      "train loss:2.297774936808836\n",
      "train loss:2.293803053817521\n",
      "train loss:2.3031087993456256\n",
      "train loss:2.300607467710034\n",
      "train loss:2.2891611364686413\n",
      "train loss:2.3051759336667716\n",
      "train loss:2.3011942622233974\n",
      "train loss:2.299959242065318\n",
      "train loss:2.2979884067450134\n",
      "train loss:2.300055559813325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301917650075036\n",
      "train loss:2.3015567201090894\n",
      "train loss:2.305326338901165\n",
      "train loss:2.3014180221839378\n",
      "train loss:2.3042726414995545\n",
      "train loss:2.3031438942552995\n",
      "train loss:2.303876087304869\n",
      "train loss:2.3025411799695474\n",
      "train loss:2.3022521002130443\n",
      "train loss:2.3012805357477593\n",
      "train loss:2.298923443237036\n",
      "train loss:2.2974984806426115\n",
      "train loss:2.297560499538222\n",
      "train loss:2.305041723776268\n",
      "train loss:2.303471616824145\n",
      "train loss:2.299105531595853\n",
      "train loss:2.3051997690976633\n",
      "train loss:2.3089020822821262\n",
      "train loss:2.3027202147012518\n",
      "train loss:2.3014303022833293\n",
      "train loss:2.3038693286508374\n",
      "train loss:2.3011260201968105\n",
      "train loss:2.2949872345112436\n",
      "train loss:2.2901662592102707\n",
      "train loss:2.292003989782482\n",
      "train loss:2.302927961931753\n",
      "train loss:2.3032610619192058\n",
      "train loss:2.307828961956918\n",
      "train loss:2.2963352695928854\n",
      "train loss:2.3083657857834416\n",
      "train loss:2.3037174108184404\n",
      "train loss:2.30938444858717\n",
      "train loss:2.3141555746152536\n",
      "train loss:2.300761488826544\n",
      "train loss:2.293899394529114\n",
      "train loss:2.298549653339642\n",
      "train loss:2.3001920793940704\n",
      "train loss:2.3003033192059195\n",
      "train loss:2.29994517266113\n",
      "train loss:2.3046018909132897\n",
      "train loss:2.303373065830517\n",
      "train loss:2.3073664616183214\n",
      "train loss:2.303357760471661\n",
      "train loss:2.302907957001256\n",
      "train loss:2.3024052051520343\n",
      "train loss:2.3025983953799813\n",
      "train loss:2.3063661744377573\n",
      "train loss:2.3099428133732753\n",
      "train loss:2.299023501571747\n",
      "train loss:2.3046514853892472\n",
      "train loss:2.300617782276312\n",
      "train loss:2.302265823701165\n",
      "train loss:2.302554529828929\n",
      "train loss:2.3031701746895585\n",
      "train loss:2.3068911229829356\n",
      "train loss:2.2996553530493657\n",
      "train loss:2.3030673999713756\n",
      "train loss:2.3075276039830035\n",
      "train loss:2.304271320165251\n",
      "train loss:2.3002241386162625\n",
      "train loss:2.3009613743320187\n",
      "train loss:2.2954782050224076\n",
      "train loss:2.293552380365915\n",
      "train loss:2.307988333311864\n",
      "train loss:2.2999368001028704\n",
      "train loss:2.2903910379551604\n",
      "train loss:2.3029075659650324\n",
      "train loss:2.3066002457868766\n",
      "train loss:2.3008041510779105\n",
      "train loss:2.2949244299268887\n",
      "train loss:2.2935730211806136\n",
      "train loss:2.2920204625875895\n",
      "train loss:2.294145484603827\n",
      "train loss:2.2968301842067023\n",
      "train loss:2.310699221199209\n",
      "train loss:2.3026250458282966\n",
      "train loss:2.2972688538456336\n",
      "train loss:2.3092693056006133\n",
      "train loss:2.295982530820645\n",
      "train loss:2.2945855725395155\n",
      "train loss:2.306746206992419\n",
      "train loss:2.2971042768591965\n",
      "train loss:2.302812792001583\n",
      "train loss:2.302499364210368\n",
      "train loss:2.3039301656361015\n",
      "train loss:2.305856854210583\n",
      "train loss:2.2988292266264243\n",
      "train loss:2.3080508504315875\n",
      "train loss:2.3051399921555307\n",
      "train loss:2.3031945315374984\n",
      "train loss:2.305563771789222\n",
      "train loss:2.31044896689206\n",
      "train loss:2.301280918263584\n",
      "train loss:2.2906291556997385\n",
      "train loss:2.3003477249003645\n",
      "train loss:2.2973167621560764\n",
      "train loss:2.2970077859157594\n",
      "train loss:2.3055862027234713\n",
      "train loss:2.301703250436752\n",
      "train loss:2.2906812092541298\n",
      "train loss:2.3001888381766316\n",
      "train loss:2.2974327886460615\n",
      "train loss:2.3020336276815634\n",
      "train loss:2.3070947388329093\n",
      "train loss:2.309432818624947\n",
      "train loss:2.299662819397267\n",
      "train loss:2.2960422010701698\n",
      "train loss:2.3057971829671247\n",
      "train loss:2.3138576592888427\n",
      "train loss:2.2936052438987176\n",
      "train loss:2.308026320250814\n",
      "train loss:2.2982630529896944\n",
      "train loss:2.3015350442956146\n",
      "train loss:2.2993023556292407\n",
      "train loss:2.300142481004078\n",
      "train loss:2.307748640566055\n",
      "train loss:2.3054613345334642\n",
      "train loss:2.301735367155536\n",
      "train loss:2.306155817701873\n",
      "train loss:2.3001359943082957\n",
      "train loss:2.3019998558310233\n",
      "train loss:2.2981653402612876\n",
      "train loss:2.3034644856462374\n",
      "train loss:2.2981752962847355\n",
      "train loss:2.304568464183216\n",
      "train loss:2.3031422383961515\n",
      "train loss:2.3075640926876906\n",
      "train loss:2.301436029549725\n",
      "train loss:2.2954490678435686\n",
      "train loss:2.2992506102275123\n",
      "train loss:2.303231106836924\n",
      "train loss:2.3003878101161828\n",
      "train loss:2.298222970775999\n",
      "train loss:2.297051138395518\n",
      "train loss:2.3015863223749857\n",
      "train loss:2.3056851757485806\n",
      "train loss:2.296865330015791\n",
      "train loss:2.297158723064899\n",
      "train loss:2.3005765889722114\n",
      "train loss:2.30274862192773\n",
      "train loss:2.3135557965739624\n",
      "train loss:2.3044219863079225\n",
      "train loss:2.3048119428650558\n",
      "train loss:2.3062311983233394\n",
      "train loss:2.293187291563401\n",
      "=== epoch:38, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3097170334406805\n",
      "train loss:2.312640567021664\n",
      "train loss:2.2999006297146036\n",
      "train loss:2.3028455608117278\n",
      "train loss:2.2935090879897335\n",
      "train loss:2.3012092593213023\n",
      "train loss:2.3023368775144495\n",
      "train loss:2.300091959716726\n",
      "train loss:2.297254975033841\n",
      "train loss:2.3110181477056626\n",
      "train loss:2.30038850068328\n",
      "train loss:2.3019044229772643\n",
      "train loss:2.300651307473164\n",
      "train loss:2.3024182907352317\n",
      "train loss:2.297188230620685\n",
      "train loss:2.300862476675049\n",
      "train loss:2.302117891636887\n",
      "train loss:2.307285545251919\n",
      "train loss:2.3003409015763814\n",
      "train loss:2.2966437896618053\n",
      "train loss:2.308361349206359\n",
      "train loss:2.301227874068993\n",
      "train loss:2.294997374025538\n",
      "train loss:2.3043493356887073\n",
      "train loss:2.3019098529095334\n",
      "train loss:2.3068695153852707\n",
      "train loss:2.3032985872335794\n",
      "train loss:2.3059225133096817\n",
      "train loss:2.302957716550749\n",
      "train loss:2.2986149094319837\n",
      "train loss:2.293096839729945\n",
      "train loss:2.306141932117429\n",
      "train loss:2.2861918193946056\n",
      "train loss:2.2914767008595054\n",
      "train loss:2.298890954436314\n",
      "train loss:2.302906787426018\n",
      "train loss:2.29889992218125\n",
      "train loss:2.293968983778341\n",
      "train loss:2.2988903766307254\n",
      "train loss:2.3001160046202154\n",
      "train loss:2.3052543952590883\n",
      "train loss:2.2955832992212506\n",
      "train loss:2.297832758745248\n",
      "train loss:2.303596573762824\n",
      "train loss:2.296691228072961\n",
      "train loss:2.293634966934218\n",
      "train loss:2.298365082240105\n",
      "train loss:2.305989067086532\n",
      "train loss:2.29313831948829\n",
      "train loss:2.2953901016284233\n",
      "train loss:2.2974326045427764\n",
      "train loss:2.3082097324325894\n",
      "train loss:2.304553068239428\n",
      "train loss:2.3015394690761766\n",
      "train loss:2.307264597805592\n",
      "train loss:2.298394377202815\n",
      "train loss:2.301793306832968\n",
      "train loss:2.2973412305308103\n",
      "train loss:2.302864598738741\n",
      "train loss:2.299712355343918\n",
      "train loss:2.303218358332894\n",
      "train loss:2.306915983444197\n",
      "train loss:2.301516957923064\n",
      "train loss:2.3045882083187657\n",
      "train loss:2.2956737730104413\n",
      "train loss:2.3102284242104387\n",
      "train loss:2.3044132933544654\n",
      "train loss:2.2935689564521677\n",
      "train loss:2.30189610987712\n",
      "train loss:2.3035372244649586\n",
      "train loss:2.301336033744914\n",
      "train loss:2.304527090418881\n",
      "train loss:2.3114755191347087\n",
      "train loss:2.2838558786344456\n",
      "train loss:2.29783588319597\n",
      "train loss:2.304282567157382\n",
      "train loss:2.2988765376026166\n",
      "train loss:2.302943920680962\n",
      "train loss:2.3042518239198335\n",
      "train loss:2.299969866984702\n",
      "train loss:2.293723651906237\n",
      "train loss:2.2996897006310753\n",
      "train loss:2.309439527097927\n",
      "train loss:2.3048134279851205\n",
      "train loss:2.2973303494649486\n",
      "train loss:2.2984417472805383\n",
      "train loss:2.2897083295725627\n",
      "train loss:2.2962770155923895\n",
      "train loss:2.3007445044409676\n",
      "train loss:2.29304830787022\n",
      "train loss:2.299400054013751\n",
      "train loss:2.2865741928122825\n",
      "train loss:2.301165616897514\n",
      "train loss:2.3014698004436513\n",
      "train loss:2.2998955209687963\n",
      "train loss:2.3062576971186712\n",
      "train loss:2.3062768449937487\n",
      "train loss:2.2967258941239335\n",
      "train loss:2.2968468073840174\n",
      "train loss:2.293238076691638\n",
      "train loss:2.3001939039159804\n",
      "train loss:2.303029467187532\n",
      "train loss:2.297784702572839\n",
      "train loss:2.2993860236399066\n",
      "train loss:2.301367338534949\n",
      "train loss:2.2950337088949215\n",
      "train loss:2.305706810172925\n",
      "train loss:2.2960767570227727\n",
      "train loss:2.310235057864012\n",
      "train loss:2.3005170829085806\n",
      "train loss:2.3017343209071024\n",
      "train loss:2.3007053881233483\n",
      "train loss:2.2959878192049143\n",
      "train loss:2.2978074906854196\n",
      "train loss:2.290930822932151\n",
      "train loss:2.303135450421352\n",
      "train loss:2.305632192411945\n",
      "train loss:2.2999221662579092\n",
      "train loss:2.2993274727087267\n",
      "train loss:2.288580416043232\n",
      "train loss:2.2982963315744844\n",
      "train loss:2.3057885187429674\n",
      "train loss:2.291763304431068\n",
      "train loss:2.300088711120901\n",
      "train loss:2.3057611175396224\n",
      "train loss:2.3072148160859705\n",
      "train loss:2.3042121913561906\n",
      "train loss:2.3043737024268025\n",
      "train loss:2.3082186937410993\n",
      "train loss:2.2975146017978862\n",
      "train loss:2.302236835968181\n",
      "train loss:2.301705004909863\n",
      "train loss:2.3030007373648713\n",
      "train loss:2.3004260673379218\n",
      "train loss:2.312465386059797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299197565297558\n",
      "train loss:2.2905443161244974\n",
      "train loss:2.3114786872678823\n",
      "train loss:2.3022987019367087\n",
      "train loss:2.2936314827059556\n",
      "train loss:2.305352177214029\n",
      "train loss:2.29720373692913\n",
      "train loss:2.3026156310779706\n",
      "train loss:2.294904025369333\n",
      "train loss:2.3036679117554066\n",
      "train loss:2.299659612455877\n",
      "train loss:2.302414870084502\n",
      "train loss:2.3032469667764106\n",
      "train loss:2.295115836639237\n",
      "train loss:2.3048679591708394\n",
      "train loss:2.294365596156066\n",
      "train loss:2.308136475863582\n",
      "train loss:2.297676520791289\n",
      "train loss:2.2950934847677886\n",
      "train loss:2.3043198297002037\n",
      "train loss:2.3057419684983294\n",
      "train loss:2.3099218340308014\n",
      "train loss:2.300306786349668\n",
      "train loss:2.3036817039561384\n",
      "train loss:2.3053892615301512\n",
      "train loss:2.3073373086960816\n",
      "train loss:2.301280622099662\n",
      "train loss:2.302531225530262\n",
      "train loss:2.306842180414322\n",
      "train loss:2.300732350134384\n",
      "train loss:2.295360048719748\n",
      "train loss:2.294632466468587\n",
      "train loss:2.3104264999618573\n",
      "train loss:2.2992339109366258\n",
      "train loss:2.2982292733281957\n",
      "train loss:2.297439748703112\n",
      "train loss:2.2980124593225923\n",
      "train loss:2.3049512028157615\n",
      "train loss:2.2984244684807016\n",
      "train loss:2.2997618929985615\n",
      "train loss:2.3021952480849657\n",
      "train loss:2.30418792092741\n",
      "train loss:2.3058200190930433\n",
      "train loss:2.305814917190695\n",
      "train loss:2.306585305693895\n",
      "train loss:2.301855577552878\n",
      "train loss:2.302200899223016\n",
      "train loss:2.3010047668286036\n",
      "train loss:2.2902126790562454\n",
      "train loss:2.303946318609707\n",
      "train loss:2.299628519391503\n",
      "train loss:2.303838733093334\n",
      "train loss:2.30384111224338\n",
      "train loss:2.295994334092911\n",
      "train loss:2.3032254435003954\n",
      "train loss:2.2950733015350657\n",
      "train loss:2.3002999824467407\n",
      "train loss:2.2979391919098\n",
      "train loss:2.3037346283997766\n",
      "train loss:2.295645157036747\n",
      "train loss:2.3001940879749565\n",
      "train loss:2.2979823852918573\n",
      "train loss:2.305797679826071\n",
      "train loss:2.3067531009502917\n",
      "train loss:2.308106865654061\n",
      "=== epoch:39, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301411322286742\n",
      "train loss:2.301619185071505\n",
      "train loss:2.2987878063276357\n",
      "train loss:2.3091193830528116\n",
      "train loss:2.3034101825133613\n",
      "train loss:2.3006173144177886\n",
      "train loss:2.2991666940172757\n",
      "train loss:2.302908554793593\n",
      "train loss:2.296437904533667\n",
      "train loss:2.2962030105184223\n",
      "train loss:2.299636393059563\n",
      "train loss:2.309998430387655\n",
      "train loss:2.2977227143443617\n",
      "train loss:2.3015350753496553\n",
      "train loss:2.306923614745558\n",
      "train loss:2.298511083085588\n",
      "train loss:2.295193625623618\n",
      "train loss:2.2955330266322953\n",
      "train loss:2.2985019544733376\n",
      "train loss:2.298750059303801\n",
      "train loss:2.3065297703963528\n",
      "train loss:2.301308689412373\n",
      "train loss:2.3009212853014116\n",
      "train loss:2.302356073518891\n",
      "train loss:2.310421365342072\n",
      "train loss:2.29695910488393\n",
      "train loss:2.3117455659706523\n",
      "train loss:2.2986428949755946\n",
      "train loss:2.3085073110307985\n",
      "train loss:2.2978326458996277\n",
      "train loss:2.306616020092381\n",
      "train loss:2.306550910409491\n",
      "train loss:2.297526375805916\n",
      "train loss:2.308448975951191\n",
      "train loss:2.3033530666053057\n",
      "train loss:2.308234083078025\n",
      "train loss:2.305887342695108\n",
      "train loss:2.2989862273901336\n",
      "train loss:2.2973182610051985\n",
      "train loss:2.3012869353792147\n",
      "train loss:2.3077124294632743\n",
      "train loss:2.307636351442554\n",
      "train loss:2.301509635585697\n",
      "train loss:2.302860032462875\n",
      "train loss:2.3024917967910388\n",
      "train loss:2.3007762533918634\n",
      "train loss:2.302535089886489\n",
      "train loss:2.3048623576087572\n",
      "train loss:2.3030256030963514\n",
      "train loss:2.3056843794898256\n",
      "train loss:2.2986880949551205\n",
      "train loss:2.294311201073853\n",
      "train loss:2.3041513118753407\n",
      "train loss:2.3000205070664954\n",
      "train loss:2.297899498810958\n",
      "train loss:2.2964093524410423\n",
      "train loss:2.308209232203336\n",
      "train loss:2.3085703409661176\n",
      "train loss:2.3000444033869996\n",
      "train loss:2.300509557174583\n",
      "train loss:2.298289525008183\n",
      "train loss:2.2915051716599915\n",
      "train loss:2.302448009198588\n",
      "train loss:2.291961142381075\n",
      "train loss:2.2958767285521415\n",
      "train loss:2.311486321289042\n",
      "train loss:2.2923221438331667\n",
      "train loss:2.296634729722903\n",
      "train loss:2.296530416399588\n",
      "train loss:2.2973811453316046\n",
      "train loss:2.3002230572858013\n",
      "train loss:2.3012020287180577\n",
      "train loss:2.291446166179569\n",
      "train loss:2.300818048882758\n",
      "train loss:2.3044398818874265\n",
      "train loss:2.301967876096353\n",
      "train loss:2.299246317589824\n",
      "train loss:2.3055517889980517\n",
      "train loss:2.2983389596517605\n",
      "train loss:2.2921894598253263\n",
      "train loss:2.3079620914860124\n",
      "train loss:2.2982928023776674\n",
      "train loss:2.2953583934609094\n",
      "train loss:2.2955720225580163\n",
      "train loss:2.3061477626241977\n",
      "train loss:2.309155593231451\n",
      "train loss:2.299901912682928\n",
      "train loss:2.30208473816413\n",
      "train loss:2.3033285823803973\n",
      "train loss:2.299788745764825\n",
      "train loss:2.301837680166144\n",
      "train loss:2.2979534786960327\n",
      "train loss:2.3069583570887113\n",
      "train loss:2.2992224532704975\n",
      "train loss:2.297233822209051\n",
      "train loss:2.2906639660639114\n",
      "train loss:2.2998594266329575\n",
      "train loss:2.3020550696090636\n",
      "train loss:2.308385494646292\n",
      "train loss:2.296082136940627\n",
      "train loss:2.2957039419375858\n",
      "train loss:2.2956381854644494\n",
      "train loss:2.292153810591467\n",
      "train loss:2.296129449574985\n",
      "train loss:2.3039865365594125\n",
      "train loss:2.3043883560707297\n",
      "train loss:2.30187019368403\n",
      "train loss:2.298667829672436\n",
      "train loss:2.303677212232592\n",
      "train loss:2.3060075138956466\n",
      "train loss:2.299483731740782\n",
      "train loss:2.303191071873376\n",
      "train loss:2.308581416145959\n",
      "train loss:2.301976502318147\n",
      "train loss:2.2986646006819416\n",
      "train loss:2.3112003269922763\n",
      "train loss:2.30791759218203\n",
      "train loss:2.2955670035298636\n",
      "train loss:2.2972926819208057\n",
      "train loss:2.3093427250740053\n",
      "train loss:2.298261573606475\n",
      "train loss:2.2920419439329898\n",
      "train loss:2.303375327884388\n",
      "train loss:2.2999950877124955\n",
      "train loss:2.304602104774159\n",
      "train loss:2.305065238864377\n",
      "train loss:2.2954189722922007\n",
      "train loss:2.3050629764784993\n",
      "train loss:2.2962444641427036\n",
      "train loss:2.289919195213221\n",
      "train loss:2.3002181790390916\n",
      "train loss:2.3007002786916053\n",
      "train loss:2.2959326737121093\n",
      "train loss:2.3056048597196077\n",
      "train loss:2.3005707036204663\n",
      "train loss:2.303531694818724\n",
      "train loss:2.2950945223482577\n",
      "train loss:2.3039885528775885\n",
      "train loss:2.294282700498259\n",
      "train loss:2.304769528030668\n",
      "train loss:2.297387757821451\n",
      "train loss:2.298558031097243\n",
      "train loss:2.2968595917344303\n",
      "train loss:2.2983458188499224\n",
      "train loss:2.30463470089932\n",
      "train loss:2.297324662122464\n",
      "train loss:2.2977757545172572\n",
      "train loss:2.3000314702857914\n",
      "train loss:2.294662479356246\n",
      "train loss:2.296536307045501\n",
      "train loss:2.2976005489717326\n",
      "train loss:2.3055245286678487\n",
      "train loss:2.2988123099749362\n",
      "train loss:2.301660794778048\n",
      "train loss:2.305607146998935\n",
      "train loss:2.3041627789152828\n",
      "train loss:2.2945594188010277\n",
      "train loss:2.2961594297405763\n",
      "train loss:2.3042088278301427\n",
      "train loss:2.300413773088854\n",
      "train loss:2.30486490817638\n",
      "train loss:2.3033222963413817\n",
      "train loss:2.298817566543321\n",
      "train loss:2.3039047831328716\n",
      "train loss:2.295713814088268\n",
      "train loss:2.3045703035995886\n",
      "train loss:2.295087078849274\n",
      "train loss:2.2946779019875128\n",
      "train loss:2.3046580575103\n",
      "train loss:2.300549319635846\n",
      "train loss:2.298002024051087\n",
      "train loss:2.3049628123432204\n",
      "train loss:2.2951055285436763\n",
      "train loss:2.302339703001115\n",
      "train loss:2.2943819687888296\n",
      "train loss:2.2932105897421033\n",
      "train loss:2.3004796846421307\n",
      "train loss:2.3053013515148497\n",
      "train loss:2.297094906247493\n",
      "train loss:2.296851658898005\n",
      "train loss:2.2966599090848003\n",
      "train loss:2.2988389748443767\n",
      "train loss:2.300165275904206\n",
      "train loss:2.298267486801614\n",
      "train loss:2.3020083383320866\n",
      "train loss:2.3015758203421526\n",
      "train loss:2.3000535286800234\n",
      "train loss:2.2966795306263954\n",
      "train loss:2.3026870163877216\n",
      "train loss:2.303444351910209\n",
      "train loss:2.301068335262458\n",
      "train loss:2.2941778477967993\n",
      "train loss:2.2940014127298194\n",
      "train loss:2.294423013445411\n",
      "train loss:2.3008995176229172\n",
      "train loss:2.301431214975066\n",
      "train loss:2.2970472023894644\n",
      "train loss:2.304322518493178\n",
      "train loss:2.306024417109988\n",
      "train loss:2.303865526108901\n",
      "=== epoch:40, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308938634975848\n",
      "train loss:2.2942835866031794\n",
      "train loss:2.310842074390338\n",
      "train loss:2.3031823371879163\n",
      "train loss:2.2982553744281744\n",
      "train loss:2.289474488787039\n",
      "train loss:2.3079114382395045\n",
      "train loss:2.2950834574218604\n",
      "train loss:2.298715113559271\n",
      "train loss:2.296984138731089\n",
      "train loss:2.3056232698401624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300569645111435\n",
      "train loss:2.307210727988884\n",
      "train loss:2.303981717575991\n",
      "train loss:2.303886440679251\n",
      "train loss:2.2976822000567276\n",
      "train loss:2.2993858333222224\n",
      "train loss:2.298390579272616\n",
      "train loss:2.2981918404119486\n",
      "train loss:2.295398125392313\n",
      "train loss:2.300652487069133\n",
      "train loss:2.2965922864963906\n",
      "train loss:2.303306535848236\n",
      "train loss:2.308578490095316\n",
      "train loss:2.3040943694391345\n",
      "train loss:2.2994630913640925\n",
      "train loss:2.301664148787254\n",
      "train loss:2.300423003590175\n",
      "train loss:2.3050938694235943\n",
      "train loss:2.3013280611427254\n",
      "train loss:2.2977885440260124\n",
      "train loss:2.3033357211579433\n",
      "train loss:2.3093828574077624\n",
      "train loss:2.298997895361667\n",
      "train loss:2.2946286169900376\n",
      "train loss:2.3061192399717725\n",
      "train loss:2.297091244196499\n",
      "train loss:2.30403824795435\n",
      "train loss:2.3069698133732133\n",
      "train loss:2.3011378566753025\n",
      "train loss:2.30096519210818\n",
      "train loss:2.300534157272487\n",
      "train loss:2.3013168548603455\n",
      "train loss:2.294727425127491\n",
      "train loss:2.2933896324351353\n",
      "train loss:2.2991216774067706\n",
      "train loss:2.2999745856951885\n",
      "train loss:2.3084450959658387\n",
      "train loss:2.3015599958138973\n",
      "train loss:2.306691710618261\n",
      "train loss:2.2973941763707564\n",
      "train loss:2.296837450312143\n",
      "train loss:2.2934121010130593\n",
      "train loss:2.298570557845443\n",
      "train loss:2.2916769704036875\n",
      "train loss:2.303039874350639\n",
      "train loss:2.300486984550281\n",
      "train loss:2.302967924547147\n",
      "train loss:2.3091958258446055\n",
      "train loss:2.2962976832316406\n",
      "train loss:2.307125443534859\n",
      "train loss:2.2990582422925137\n",
      "train loss:2.2964475558381627\n",
      "train loss:2.29884358511644\n",
      "train loss:2.292666857366823\n",
      "train loss:2.296678753466845\n",
      "train loss:2.30329679389709\n",
      "train loss:2.299270704281831\n",
      "train loss:2.301578992768565\n",
      "train loss:2.3048349027196906\n",
      "train loss:2.3057015752568395\n",
      "train loss:2.3121966789551247\n",
      "train loss:2.3010170820082867\n",
      "train loss:2.2930099358263303\n",
      "train loss:2.304388216543318\n",
      "train loss:2.3020415725361874\n",
      "train loss:2.306688368933313\n",
      "train loss:2.309609491469565\n",
      "train loss:2.3056236796323026\n",
      "train loss:2.3064083340498382\n",
      "train loss:2.314844755904979\n",
      "train loss:2.306213688973981\n",
      "train loss:2.29910502449489\n",
      "train loss:2.303974633114387\n",
      "train loss:2.302882348879901\n",
      "train loss:2.3065943692939843\n",
      "train loss:2.3061568653804767\n",
      "train loss:2.292986128907695\n",
      "train loss:2.2958650044051674\n",
      "train loss:2.3001948422217002\n",
      "train loss:2.298590556648078\n",
      "train loss:2.3039074926953385\n",
      "train loss:2.305282971158017\n",
      "train loss:2.2970664621306605\n",
      "train loss:2.2929107113352316\n",
      "train loss:2.301084435565486\n",
      "train loss:2.300287975164651\n",
      "train loss:2.297741721069742\n",
      "train loss:2.299691554563267\n",
      "train loss:2.30205467061584\n",
      "train loss:2.3042613305374617\n",
      "train loss:2.3015807513336\n",
      "train loss:2.3037844240299914\n",
      "train loss:2.302847977789545\n",
      "train loss:2.298694521423008\n",
      "train loss:2.301043216697926\n",
      "train loss:2.3023730520261627\n",
      "train loss:2.2954338613258143\n",
      "train loss:2.301461416009287\n",
      "train loss:2.292867398053806\n",
      "train loss:2.3106028168267825\n",
      "train loss:2.3036483832521126\n",
      "train loss:2.2966397240030503\n",
      "train loss:2.3021372249344254\n",
      "train loss:2.296756807254594\n",
      "train loss:2.292886503398888\n",
      "train loss:2.3041301515441286\n",
      "train loss:2.309381133142437\n",
      "train loss:2.2935804331441076\n",
      "train loss:2.3066297376252574\n",
      "train loss:2.30039865166685\n",
      "train loss:2.3054547604672684\n",
      "train loss:2.3054084994938178\n",
      "train loss:2.2972160606433487\n",
      "train loss:2.298706325922629\n",
      "train loss:2.300408437525039\n",
      "train loss:2.295740173940953\n",
      "train loss:2.3026687168344626\n",
      "train loss:2.3022079311304577\n",
      "train loss:2.303187660692166\n",
      "train loss:2.306280073426436\n",
      "train loss:2.300346839428235\n",
      "train loss:2.3000821758629506\n",
      "train loss:2.298349826884337\n",
      "train loss:2.306364708624932\n",
      "train loss:2.3031425511475296\n",
      "train loss:2.306337060921175\n",
      "train loss:2.291948658662226\n",
      "train loss:2.308709297026373\n",
      "train loss:2.299130138177903\n",
      "train loss:2.301356271776842\n",
      "train loss:2.3067797244581145\n",
      "train loss:2.293815586495949\n",
      "train loss:2.3051642529607275\n",
      "train loss:2.3009586833313644\n",
      "train loss:2.2967118219432576\n",
      "train loss:2.2967267788243575\n",
      "train loss:2.296420736945211\n",
      "train loss:2.3015885654865196\n",
      "train loss:2.3028202332110888\n",
      "train loss:2.3007982890671586\n",
      "train loss:2.3014521886132324\n",
      "train loss:2.296800979985089\n",
      "train loss:2.3007912116275464\n",
      "train loss:2.3005321209994474\n",
      "train loss:2.3060941207768164\n",
      "train loss:2.3062007446573816\n",
      "train loss:2.301054748771654\n",
      "train loss:2.304295429598117\n",
      "train loss:2.3080672675938523\n",
      "train loss:2.299704886999662\n",
      "train loss:2.2999024278645828\n",
      "train loss:2.301581052137109\n",
      "train loss:2.305247848640012\n",
      "train loss:2.3024146726095727\n",
      "train loss:2.307268690125319\n",
      "train loss:2.308360675116388\n",
      "train loss:2.300676390600261\n",
      "train loss:2.2987093110844787\n",
      "train loss:2.3027245148770876\n",
      "train loss:2.2967607324981425\n",
      "train loss:2.301121472413797\n",
      "train loss:2.306041527852111\n",
      "train loss:2.294533083157746\n",
      "train loss:2.3026442080252036\n",
      "train loss:2.308379637182052\n",
      "train loss:2.2853530517405964\n",
      "train loss:2.2960333058656763\n",
      "train loss:2.302460475923173\n",
      "train loss:2.301260831572083\n",
      "train loss:2.305025645452454\n",
      "train loss:2.3052049360361213\n",
      "train loss:2.299642722535619\n",
      "train loss:2.303815494807453\n",
      "train loss:2.299433033821179\n",
      "train loss:2.310516806521799\n",
      "train loss:2.3054452320595313\n",
      "train loss:2.2942420891795234\n",
      "train loss:2.2998508901819967\n",
      "train loss:2.3079993955924154\n",
      "train loss:2.305580632485246\n",
      "train loss:2.3005819672028536\n",
      "train loss:2.287749049163346\n",
      "train loss:2.3082671359763705\n",
      "train loss:2.3095228867303583\n",
      "train loss:2.302440352390944\n",
      "train loss:2.301501362874982\n",
      "train loss:2.294014302684306\n",
      "train loss:2.300412858204964\n",
      "train loss:2.2981831903268803\n",
      "=== epoch:41, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3087917787322376\n",
      "train loss:2.295133411630669\n",
      "train loss:2.3062621063460456\n",
      "train loss:2.301266136233201\n",
      "train loss:2.295813899199277\n",
      "train loss:2.308791047983429\n",
      "train loss:2.307150078506095\n",
      "train loss:2.2946308460009908\n",
      "train loss:2.297082157526326\n",
      "train loss:2.3102380298986933\n",
      "train loss:2.3032386118289594\n",
      "train loss:2.3112774393958397\n",
      "train loss:2.2940953200295784\n",
      "train loss:2.2937029672309395\n",
      "train loss:2.3073388850486407\n",
      "train loss:2.30998502307165\n",
      "train loss:2.3055307445796864\n",
      "train loss:2.3020343037060274\n",
      "train loss:2.3038001894555262\n",
      "train loss:2.2989283362900483\n",
      "train loss:2.298012731746249\n",
      "train loss:2.304820088039671\n",
      "train loss:2.2962532463488285\n",
      "train loss:2.30721427297423\n",
      "train loss:2.298091102692418\n",
      "train loss:2.303851050355311\n",
      "train loss:2.2976484719226336\n",
      "train loss:2.291853714343817\n",
      "train loss:2.2998669700565038\n",
      "train loss:2.3039478968935976\n",
      "train loss:2.3031142286689885\n",
      "train loss:2.3054315154547944\n",
      "train loss:2.2897056834386365\n",
      "train loss:2.2945429730482982\n",
      "train loss:2.2951074985176954\n",
      "train loss:2.2967980373630303\n",
      "train loss:2.302249740154092\n",
      "train loss:2.3042743206360616\n",
      "train loss:2.3063667456424284\n",
      "train loss:2.2974675466476913\n",
      "train loss:2.310446466670892\n",
      "train loss:2.291342974803619\n",
      "train loss:2.3057023595081265\n",
      "train loss:2.3083197305241723\n",
      "train loss:2.307903335884688\n",
      "train loss:2.306419344561185\n",
      "train loss:2.3009454569220833\n",
      "train loss:2.2990914693881304\n",
      "train loss:2.2995853259587804\n",
      "train loss:2.3031731118552035\n",
      "train loss:2.3002036934112957\n",
      "train loss:2.3005877877198166\n",
      "train loss:2.303236818833368\n",
      "train loss:2.2958921813188193\n",
      "train loss:2.3002760594397396\n",
      "train loss:2.30978878229594\n",
      "train loss:2.307618646652612\n",
      "train loss:2.3025806698501974\n",
      "train loss:2.297934377368608\n",
      "train loss:2.288414017692547\n",
      "train loss:2.3001446598716626\n",
      "train loss:2.30493159959244\n",
      "train loss:2.3103150219371216\n",
      "train loss:2.311433319650878\n",
      "train loss:2.300477161683274\n",
      "train loss:2.2981290154118708\n",
      "train loss:2.2993167542822976\n",
      "train loss:2.3055035949857325\n",
      "train loss:2.3010583537460634\n",
      "train loss:2.3032688768157996\n",
      "train loss:2.3013580366875526\n",
      "train loss:2.304638762288628\n",
      "train loss:2.2987161692736797\n",
      "train loss:2.3019481155009736\n",
      "train loss:2.308212121105584\n",
      "train loss:2.298126279023289\n",
      "train loss:2.3020529954734994\n",
      "train loss:2.308041175037062\n",
      "train loss:2.2975003194769394\n",
      "train loss:2.297184309810862\n",
      "train loss:2.3015253356128644\n",
      "train loss:2.299028716299847\n",
      "train loss:2.2993006877308706\n",
      "train loss:2.3049309367292516\n",
      "train loss:2.3066285104265494\n",
      "train loss:2.299585659108235\n",
      "train loss:2.308166924574977\n",
      "train loss:2.297415266505107\n",
      "train loss:2.307572672892731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306670099433444\n",
      "train loss:2.3019038254286017\n",
      "train loss:2.2976321532401234\n",
      "train loss:2.303981947097955\n",
      "train loss:2.2964011735502234\n",
      "train loss:2.299247254657408\n",
      "train loss:2.3015728035650818\n",
      "train loss:2.3045620427438727\n",
      "train loss:2.2985961708473317\n",
      "train loss:2.3017248068472007\n",
      "train loss:2.2910941995716607\n",
      "train loss:2.2976214347796198\n",
      "train loss:2.307068580098696\n",
      "train loss:2.3036358242299895\n",
      "train loss:2.301384841226713\n",
      "train loss:2.3047772989245785\n",
      "train loss:2.307211648985166\n",
      "train loss:2.310443932693309\n",
      "train loss:2.2995640985986086\n",
      "train loss:2.309328393576286\n",
      "train loss:2.291681272232069\n",
      "train loss:2.299230142178067\n",
      "train loss:2.2989117866708733\n",
      "train loss:2.2991723903972794\n",
      "train loss:2.3071186712923617\n",
      "train loss:2.295415779473291\n",
      "train loss:2.2932310251491264\n",
      "train loss:2.307386226187617\n",
      "train loss:2.308560942252517\n",
      "train loss:2.3002099492745938\n",
      "train loss:2.3007672086380344\n",
      "train loss:2.3018271758482713\n",
      "train loss:2.3091291222672914\n",
      "train loss:2.304876406062406\n",
      "train loss:2.3102277713756862\n",
      "train loss:2.298637552570371\n",
      "train loss:2.3061996811073247\n",
      "train loss:2.2968911504006377\n",
      "train loss:2.300628321871268\n",
      "train loss:2.30124591068361\n",
      "train loss:2.2984653793357612\n",
      "train loss:2.3024488298761523\n",
      "train loss:2.2997140219174663\n",
      "train loss:2.306117147475418\n",
      "train loss:2.303709991268359\n",
      "train loss:2.3051439217081344\n",
      "train loss:2.299305122963907\n",
      "train loss:2.3026760378362483\n",
      "train loss:2.2981264604973197\n",
      "train loss:2.2992454566734675\n",
      "train loss:2.300294022909723\n",
      "train loss:2.299135702629356\n",
      "train loss:2.301636942280097\n",
      "train loss:2.3027068352632476\n",
      "train loss:2.3003136445303047\n",
      "train loss:2.300754950882187\n",
      "train loss:2.3050744734579314\n",
      "train loss:2.299062667889858\n",
      "train loss:2.2975955500344476\n",
      "train loss:2.2970695801527423\n",
      "train loss:2.3099941246933082\n",
      "train loss:2.306438867479274\n",
      "train loss:2.2956731932964605\n",
      "train loss:2.303866729379535\n",
      "train loss:2.291386202154328\n",
      "train loss:2.304170932364917\n",
      "train loss:2.3010876114539354\n",
      "train loss:2.308522994053222\n",
      "train loss:2.3045722020525288\n",
      "train loss:2.307537501426874\n",
      "train loss:2.292960607111715\n",
      "train loss:2.3021341033790206\n",
      "train loss:2.298788147612456\n",
      "train loss:2.2974978482657606\n",
      "train loss:2.2966801084938235\n",
      "train loss:2.3059731006996054\n",
      "train loss:2.296515654583044\n",
      "train loss:2.3019229309002123\n",
      "train loss:2.308100764300981\n",
      "train loss:2.301883167996506\n",
      "train loss:2.296338730836746\n",
      "train loss:2.295298728118114\n",
      "train loss:2.3005195136642738\n",
      "train loss:2.3000385340924954\n",
      "train loss:2.30614162094682\n",
      "train loss:2.3106382197345594\n",
      "train loss:2.2991086975857598\n",
      "train loss:2.3018893122102897\n",
      "train loss:2.300064712490698\n",
      "train loss:2.3006636857115823\n",
      "train loss:2.3081950891950815\n",
      "train loss:2.3046409105958063\n",
      "train loss:2.305945294253092\n",
      "train loss:2.3037541067421987\n",
      "train loss:2.3099604839696215\n",
      "train loss:2.2963240487135645\n",
      "train loss:2.297029047323472\n",
      "train loss:2.3046806712589447\n",
      "train loss:2.3000979181064776\n",
      "train loss:2.29696696416314\n",
      "train loss:2.303396478025149\n",
      "train loss:2.2940497823711405\n",
      "train loss:2.304411522943587\n",
      "train loss:2.303093033740503\n",
      "train loss:2.3004828979898293\n",
      "train loss:2.3011601265116024\n",
      "train loss:2.296015362178215\n",
      "train loss:2.2967820555833436\n",
      "train loss:2.2987945167939805\n",
      "train loss:2.305264606210123\n",
      "train loss:2.300041012926067\n",
      "=== epoch:42, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3010522687590726\n",
      "train loss:2.2983404214035676\n",
      "train loss:2.314787171169843\n",
      "train loss:2.3068410390815295\n",
      "train loss:2.2921642962873907\n",
      "train loss:2.3006784513838388\n",
      "train loss:2.3050199239943243\n",
      "train loss:2.304305781029446\n",
      "train loss:2.2963489279882214\n",
      "train loss:2.3050315858924666\n",
      "train loss:2.3089596894617466\n",
      "train loss:2.291145267633879\n",
      "train loss:2.3062577402825397\n",
      "train loss:2.3007160758950813\n",
      "train loss:2.2919879931082883\n",
      "train loss:2.2980963067440516\n",
      "train loss:2.2979565922432075\n",
      "train loss:2.3003312438973134\n",
      "train loss:2.303624993774456\n",
      "train loss:2.3035636322256985\n",
      "train loss:2.3046005117211723\n",
      "train loss:2.302480760283304\n",
      "train loss:2.305812666199605\n",
      "train loss:2.3038165088860607\n",
      "train loss:2.297126221709833\n",
      "train loss:2.3083464521912282\n",
      "train loss:2.2945911189379666\n",
      "train loss:2.2982171802814997\n",
      "train loss:2.305110033189573\n",
      "train loss:2.304915099183684\n",
      "train loss:2.2896524718178006\n",
      "train loss:2.302995775853942\n",
      "train loss:2.30876723413479\n",
      "train loss:2.3027535645122876\n",
      "train loss:2.3052489962080527\n",
      "train loss:2.2959262008093564\n",
      "train loss:2.3027438423111883\n",
      "train loss:2.300323502297422\n",
      "train loss:2.2897746030807657\n",
      "train loss:2.296287505336168\n",
      "train loss:2.3088428390432996\n",
      "train loss:2.309793970312834\n",
      "train loss:2.2945363284689604\n",
      "train loss:2.307631806138524\n",
      "train loss:2.2926560997231107\n",
      "train loss:2.3037997609036234\n",
      "train loss:2.3026658415942634\n",
      "train loss:2.295900513690285\n",
      "train loss:2.2992121127845007\n",
      "train loss:2.2853810035831295\n",
      "train loss:2.301426084207573\n",
      "train loss:2.3027816289297207\n",
      "train loss:2.2985900020799\n",
      "train loss:2.3004672205775845\n",
      "train loss:2.304295804100256\n",
      "train loss:2.297356108116728\n",
      "train loss:2.2962908942706903\n",
      "train loss:2.304104961387967\n",
      "train loss:2.305310462776937\n",
      "train loss:2.303558220701899\n",
      "train loss:2.3012385672828977\n",
      "train loss:2.3015311913144103\n",
      "train loss:2.2996742660404093\n",
      "train loss:2.295035496766547\n",
      "train loss:2.300256347585026\n",
      "train loss:2.3037470054467284\n",
      "train loss:2.3009929323158187\n",
      "train loss:2.303888678672087\n",
      "train loss:2.2921913831933836\n",
      "train loss:2.3059504339544032\n",
      "train loss:2.303726498427649\n",
      "train loss:2.300887918104045\n",
      "train loss:2.297049566164324\n",
      "train loss:2.30938400765194\n",
      "train loss:2.298546398541358\n",
      "train loss:2.297878595743696\n",
      "train loss:2.2995630660879116\n",
      "train loss:2.2946634588856756\n",
      "train loss:2.299067420529808\n",
      "train loss:2.2988909271188334\n",
      "train loss:2.301834167642782\n",
      "train loss:2.30555578577936\n",
      "train loss:2.3069906810868956\n",
      "train loss:2.3049749087056366\n",
      "train loss:2.3033778926069317\n",
      "train loss:2.3075290071209014\n",
      "train loss:2.3097555965019123\n",
      "train loss:2.2961779415044075\n",
      "train loss:2.300020737934354\n",
      "train loss:2.296545845076485\n",
      "train loss:2.296888573249002\n",
      "train loss:2.2989418240358814\n",
      "train loss:2.2952990922665326\n",
      "train loss:2.309503053679027\n",
      "train loss:2.298816711900283\n",
      "train loss:2.3005984802707284\n",
      "train loss:2.303663026378409\n",
      "train loss:2.3062969461150886\n",
      "train loss:2.3065392115985297\n",
      "train loss:2.2967036939014656\n",
      "train loss:2.301708824025944\n",
      "train loss:2.2980537784355892\n",
      "train loss:2.2951808070319\n",
      "train loss:2.3052814914685795\n",
      "train loss:2.29440751678741\n",
      "train loss:2.302761941838134\n",
      "train loss:2.2938958548480852\n",
      "train loss:2.298269097741561\n",
      "train loss:2.292455647932685\n",
      "train loss:2.3058323367348073\n",
      "train loss:2.3031824750523304\n",
      "train loss:2.298098195972443\n",
      "train loss:2.305485199807693\n",
      "train loss:2.2997677735142217\n",
      "train loss:2.311079792290558\n",
      "train loss:2.2995198346323145\n",
      "train loss:2.310883868567179\n",
      "train loss:2.304386503163277\n",
      "train loss:2.3085608773765287\n",
      "train loss:2.307268690112371\n",
      "train loss:2.3048256983556823\n",
      "train loss:2.3047002068224964\n",
      "train loss:2.300417432187217\n",
      "train loss:2.3109978931762094\n",
      "train loss:2.3019971755131214\n",
      "train loss:2.302255548038657\n",
      "train loss:2.3044033580934786\n",
      "train loss:2.289809585439685\n",
      "train loss:2.294389513400487\n",
      "train loss:2.299040892589824\n",
      "train loss:2.2995712694216635\n",
      "train loss:2.3029768298351527\n",
      "train loss:2.2983614995221466\n",
      "train loss:2.293372116358829\n",
      "train loss:2.3020193474596025\n",
      "train loss:2.3007035128868925\n",
      "train loss:2.3119302593104156\n",
      "train loss:2.3079672977990096\n",
      "train loss:2.298470820890731\n",
      "train loss:2.2982958801047944\n",
      "train loss:2.297501544086041\n",
      "train loss:2.3097250485113636\n",
      "train loss:2.296755229734185\n",
      "train loss:2.2957704713776863\n",
      "train loss:2.2944850036937425\n",
      "train loss:2.301635330811131\n",
      "train loss:2.302789617169715\n",
      "train loss:2.304181355793204\n",
      "train loss:2.2997244707760833\n",
      "train loss:2.299851882155821\n",
      "train loss:2.3093681846138656\n",
      "train loss:2.303313493611928\n",
      "train loss:2.3042697136000925\n",
      "train loss:2.299385460612229\n",
      "train loss:2.305911065110133\n",
      "train loss:2.302928627620321\n",
      "train loss:2.2994795891754447\n",
      "train loss:2.3005844497421344\n",
      "train loss:2.3034854171491608\n",
      "train loss:2.298883943087375\n",
      "train loss:2.30356366280619\n",
      "train loss:2.309550579156222\n",
      "train loss:2.305855353665703\n",
      "train loss:2.297769331874798\n",
      "train loss:2.3031514260951265\n",
      "train loss:2.3004323776657065\n",
      "train loss:2.3017226256235706\n",
      "train loss:2.3033663595592744\n",
      "train loss:2.299454350173399\n",
      "train loss:2.299182428752129\n",
      "train loss:2.3030618583858162\n",
      "train loss:2.2958881742351718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2969013455663942\n",
      "train loss:2.295152787471767\n",
      "train loss:2.2974358948587628\n",
      "train loss:2.3025193935883936\n",
      "train loss:2.3053326388803446\n",
      "train loss:2.302177705096635\n",
      "train loss:2.3050291175068787\n",
      "train loss:2.3058874737095945\n",
      "train loss:2.2986058010414725\n",
      "train loss:2.294701712418056\n",
      "train loss:2.300928331352748\n",
      "train loss:2.2966369064217838\n",
      "train loss:2.2977590612147627\n",
      "train loss:2.3061911546840883\n",
      "train loss:2.300932745415204\n",
      "train loss:2.3011878471170886\n",
      "train loss:2.295684363624489\n",
      "train loss:2.302939941460938\n",
      "train loss:2.3024518151394635\n",
      "train loss:2.2928468315956354\n",
      "train loss:2.3105642785049576\n",
      "train loss:2.3023173946581434\n",
      "train loss:2.2964175408678122\n",
      "train loss:2.302668583355055\n",
      "train loss:2.3040318094822183\n",
      "train loss:2.3046819289893996\n",
      "train loss:2.298602404597767\n",
      "train loss:2.3008183726969174\n",
      "=== epoch:43, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2934995938999134\n",
      "train loss:2.2996739518536793\n",
      "train loss:2.2948652138429186\n",
      "train loss:2.30132176457674\n",
      "train loss:2.301681491518696\n",
      "train loss:2.3098830012331053\n",
      "train loss:2.2930804820028774\n",
      "train loss:2.302649045297926\n",
      "train loss:2.2959569362419114\n",
      "train loss:2.3018517778811916\n",
      "train loss:2.3036211321570694\n",
      "train loss:2.30636917233891\n",
      "train loss:2.3005616279681984\n",
      "train loss:2.3055979799304733\n",
      "train loss:2.3073743152222295\n",
      "train loss:2.3010781738376105\n",
      "train loss:2.3064237165132018\n",
      "train loss:2.3082621437978945\n",
      "train loss:2.2997897754006575\n",
      "train loss:2.299971088344183\n",
      "train loss:2.29740455061812\n",
      "train loss:2.2999098940107343\n",
      "train loss:2.302254849555143\n",
      "train loss:2.3021489219245064\n",
      "train loss:2.299719987273897\n",
      "train loss:2.2919755123099357\n",
      "train loss:2.3136763968793157\n",
      "train loss:2.298090701588354\n",
      "train loss:2.3028477208886535\n",
      "train loss:2.294938692065239\n",
      "train loss:2.3019498133137346\n",
      "train loss:2.308533124092842\n",
      "train loss:2.303945804556276\n",
      "train loss:2.290862166841388\n",
      "train loss:2.2920726867598007\n",
      "train loss:2.2951860724530335\n",
      "train loss:2.303514948843878\n",
      "train loss:2.2996529763443956\n",
      "train loss:2.3041620740314164\n",
      "train loss:2.2975051128128747\n",
      "train loss:2.305244148794314\n",
      "train loss:2.3035180557027455\n",
      "train loss:2.2994642114031634\n",
      "train loss:2.30512799138115\n",
      "train loss:2.2982356185854926\n",
      "train loss:2.30544131027569\n",
      "train loss:2.3035798236572647\n",
      "train loss:2.309221214690958\n",
      "train loss:2.3018911931821915\n",
      "train loss:2.298764454744107\n",
      "train loss:2.2985128141030833\n",
      "train loss:2.3003449729312195\n",
      "train loss:2.2954569519788453\n",
      "train loss:2.301212540139409\n",
      "train loss:2.299820536277275\n",
      "train loss:2.301025526251826\n",
      "train loss:2.2958839789298526\n",
      "train loss:2.2997723679677007\n",
      "train loss:2.298161735198467\n",
      "train loss:2.3039363204928787\n",
      "train loss:2.301639898157805\n",
      "train loss:2.3100372780280547\n",
      "train loss:2.3069798954631784\n",
      "train loss:2.3031206509308437\n",
      "train loss:2.2994484282228793\n",
      "train loss:2.2961267499168097\n",
      "train loss:2.2997351654950218\n",
      "train loss:2.2944172098007667\n",
      "train loss:2.2976221804351273\n",
      "train loss:2.302510808661468\n",
      "train loss:2.300438455169953\n",
      "train loss:2.305289981095621\n",
      "train loss:2.3067654028765157\n",
      "train loss:2.2925868913413874\n",
      "train loss:2.2979373714609808\n",
      "train loss:2.301741654988463\n",
      "train loss:2.296189200759203\n",
      "train loss:2.299007088440456\n",
      "train loss:2.3008411299636684\n",
      "train loss:2.2947760184575094\n",
      "train loss:2.300325751524935\n",
      "train loss:2.2975469325887157\n",
      "train loss:2.298271319765629\n",
      "train loss:2.2975764081576626\n",
      "train loss:2.2939284558035666\n",
      "train loss:2.3053440750579886\n",
      "train loss:2.300273723131965\n",
      "train loss:2.295091248050836\n",
      "train loss:2.298455454527879\n",
      "train loss:2.306466009128939\n",
      "train loss:2.2914047997765907\n",
      "train loss:2.3017629680065324\n",
      "train loss:2.302447956592241\n",
      "train loss:2.2977149592768034\n",
      "train loss:2.304676372217371\n",
      "train loss:2.304646276119327\n",
      "train loss:2.306072785350871\n",
      "train loss:2.295817830691768\n",
      "train loss:2.297787077072012\n",
      "train loss:2.3057851942737084\n",
      "train loss:2.303963023188049\n",
      "train loss:2.2987885680005324\n",
      "train loss:2.30192533754671\n",
      "train loss:2.2992498350408748\n",
      "train loss:2.301929514455964\n",
      "train loss:2.3047717782009562\n",
      "train loss:2.2994773230574563\n",
      "train loss:2.2965426094143844\n",
      "train loss:2.2969835779920023\n",
      "train loss:2.301384265917865\n",
      "train loss:2.304773710810027\n",
      "train loss:2.3039095476311444\n",
      "train loss:2.3060196770893606\n",
      "train loss:2.2999279569828155\n",
      "train loss:2.3088460679268525\n",
      "train loss:2.3035939089289648\n",
      "train loss:2.302316997501213\n",
      "train loss:2.3050457107856834\n",
      "train loss:2.3071295848559523\n",
      "train loss:2.303121338847019\n",
      "train loss:2.308923243797417\n",
      "train loss:2.296023074192236\n",
      "train loss:2.3042539912888236\n",
      "train loss:2.3048321034595616\n",
      "train loss:2.2947080722995077\n",
      "train loss:2.2885130806998135\n",
      "train loss:2.301857927839462\n",
      "train loss:2.296948623741264\n",
      "train loss:2.2900984035341345\n",
      "train loss:2.2988759280933886\n",
      "train loss:2.304032675277169\n",
      "train loss:2.2977488270495083\n",
      "train loss:2.3019719349355654\n",
      "train loss:2.3083800138052815\n",
      "train loss:2.3021140356033833\n",
      "train loss:2.298392872596821\n",
      "train loss:2.3002521764748654\n",
      "train loss:2.2924880054646986\n",
      "train loss:2.3104724030156025\n",
      "train loss:2.2978726587940246\n",
      "train loss:2.2986848191587455\n",
      "train loss:2.3001784228285045\n",
      "train loss:2.2955538981903723\n",
      "train loss:2.3046573707526767\n",
      "train loss:2.301687372279278\n",
      "train loss:2.301026244658536\n",
      "train loss:2.3018602282586937\n",
      "train loss:2.296497568997563\n",
      "train loss:2.309027240429798\n",
      "train loss:2.297998386319127\n",
      "train loss:2.30059159834592\n",
      "train loss:2.29955346321165\n",
      "train loss:2.3017587158939827\n",
      "train loss:2.2982730852682107\n",
      "train loss:2.292573362738967\n",
      "train loss:2.294916381342997\n",
      "train loss:2.301621941980904\n",
      "train loss:2.29725297956909\n",
      "train loss:2.2981051204424476\n",
      "train loss:2.297963176986565\n",
      "train loss:2.305571547402325\n",
      "train loss:2.29899023367262\n",
      "train loss:2.289392952136386\n",
      "train loss:2.2964253353226933\n",
      "train loss:2.3110856755828255\n",
      "train loss:2.3064588867149194\n",
      "train loss:2.2994569283647257\n",
      "train loss:2.3027836969511934\n",
      "train loss:2.3050190857232375\n",
      "train loss:2.2983340345528065\n",
      "train loss:2.3065432419078378\n",
      "train loss:2.296702123930015\n",
      "train loss:2.303535650963371\n",
      "train loss:2.3033969882476573\n",
      "train loss:2.291241607149243\n",
      "train loss:2.298735164106326\n",
      "train loss:2.290496669350258\n",
      "train loss:2.298880459007463\n",
      "train loss:2.2969400672830087\n",
      "train loss:2.293779677399157\n",
      "train loss:2.3057475338056914\n",
      "train loss:2.297872023736693\n",
      "train loss:2.300581212540023\n",
      "train loss:2.297933609855135\n",
      "train loss:2.307306293429243\n",
      "train loss:2.296864718904319\n",
      "train loss:2.3002260876112146\n",
      "train loss:2.3048126459104097\n",
      "train loss:2.306511963569251\n",
      "train loss:2.2980166429769437\n",
      "train loss:2.2935196534576145\n",
      "train loss:2.2954922950946064\n",
      "train loss:2.3010785833169356\n",
      "train loss:2.2979725951303083\n",
      "train loss:2.3069802322284265\n",
      "train loss:2.299721435723384\n",
      "train loss:2.294954116644274\n",
      "train loss:2.303723601977034\n",
      "train loss:2.308145844969165\n",
      "train loss:2.3014630666743754\n",
      "=== epoch:44, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3077463779973675\n",
      "train loss:2.301586948131684\n",
      "train loss:2.2927249168090147\n",
      "train loss:2.2931360210512852\n",
      "train loss:2.303929333346138\n",
      "train loss:2.3069983917399837\n",
      "train loss:2.293309859016339\n",
      "train loss:2.302513201095692\n",
      "train loss:2.2947278641509996\n",
      "train loss:2.3052984846947573\n",
      "train loss:2.2996899758366802\n",
      "train loss:2.3031042152529535\n",
      "train loss:2.3170257776123537\n",
      "train loss:2.303007173686573\n",
      "train loss:2.298256710774844\n",
      "train loss:2.286164208845751\n",
      "train loss:2.30700801401121\n",
      "train loss:2.3017728214346977\n",
      "train loss:2.308482738286802\n",
      "train loss:2.304179215472145\n",
      "train loss:2.2954480389232685\n",
      "train loss:2.306237047472608\n",
      "train loss:2.2993279645496076\n",
      "train loss:2.2994471228948115\n",
      "train loss:2.3021060017473256\n",
      "train loss:2.2994801283838955\n",
      "train loss:2.3056868894338316\n",
      "train loss:2.3047926981905533\n",
      "train loss:2.3096068576110604\n",
      "train loss:2.3045771033683367\n",
      "train loss:2.3001854542588744\n",
      "train loss:2.3146307819920917\n",
      "train loss:2.3099595963239836\n",
      "train loss:2.3050357802685557\n",
      "train loss:2.298305597267815\n",
      "train loss:2.292504433986899\n",
      "train loss:2.302322531361582\n",
      "train loss:2.3015958754791424\n",
      "train loss:2.3008317274297574\n",
      "train loss:2.2968100687808026\n",
      "train loss:2.2958163720915925\n",
      "train loss:2.2915208480472873\n",
      "train loss:2.301445175245335\n",
      "train loss:2.308504426550457\n",
      "train loss:2.309431350014507\n",
      "train loss:2.2955328321934743\n",
      "train loss:2.292959497349101\n",
      "train loss:2.3038813870497896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2968250801359025\n",
      "train loss:2.309528229468457\n",
      "train loss:2.300048426524904\n",
      "train loss:2.2979212990314637\n",
      "train loss:2.305347407130691\n",
      "train loss:2.3044438686665334\n",
      "train loss:2.297964919003533\n",
      "train loss:2.298817847611062\n",
      "train loss:2.297397306333744\n",
      "train loss:2.290254767153445\n",
      "train loss:2.3016370916836775\n",
      "train loss:2.3126308986228694\n",
      "train loss:2.2964194404752396\n",
      "train loss:2.2992962409344924\n",
      "train loss:2.2863372202471455\n",
      "train loss:2.30409848640361\n",
      "train loss:2.2968272250974584\n",
      "train loss:2.3016596539933794\n",
      "train loss:2.309607687922553\n",
      "train loss:2.2973039441538647\n",
      "train loss:2.299301889488101\n",
      "train loss:2.3016208425682696\n",
      "train loss:2.3070470524512605\n",
      "train loss:2.307737805234429\n",
      "train loss:2.299933246984615\n",
      "train loss:2.300314317822392\n",
      "train loss:2.294770232804231\n",
      "train loss:2.300495805097518\n",
      "train loss:2.300535938332414\n",
      "train loss:2.3046937990379925\n",
      "train loss:2.2970277471107186\n",
      "train loss:2.291990236053483\n",
      "train loss:2.3002458345122276\n",
      "train loss:2.3060424105050514\n",
      "train loss:2.3033450696359807\n",
      "train loss:2.3132318865628854\n",
      "train loss:2.2955562748400795\n",
      "train loss:2.30499020261067\n",
      "train loss:2.2971181026978877\n",
      "train loss:2.3103079568440616\n",
      "train loss:2.2973279941072042\n",
      "train loss:2.3010179860901347\n",
      "train loss:2.3047891687748843\n",
      "train loss:2.3072982010121756\n",
      "train loss:2.2925751848533467\n",
      "train loss:2.2980706534069015\n",
      "train loss:2.3028106592799604\n",
      "train loss:2.30248583182747\n",
      "train loss:2.2983643264744096\n",
      "train loss:2.2999986015044063\n",
      "train loss:2.2986396291203373\n",
      "train loss:2.3067058953786503\n",
      "train loss:2.3050337634619713\n",
      "train loss:2.3090648450248836\n",
      "train loss:2.2989646011191334\n",
      "train loss:2.3031057275304314\n",
      "train loss:2.3009425145377387\n",
      "train loss:2.3020788305157986\n",
      "train loss:2.299346149392305\n",
      "train loss:2.311545293600441\n",
      "train loss:2.304898311231347\n",
      "train loss:2.288955866797676\n",
      "train loss:2.3028606647853547\n",
      "train loss:2.2948599073720626\n",
      "train loss:2.306986203667645\n",
      "train loss:2.3081137337749227\n",
      "train loss:2.308941087003659\n",
      "train loss:2.303443402353727\n",
      "train loss:2.301743589681733\n",
      "train loss:2.317551138812106\n",
      "train loss:2.2937656244033016\n",
      "train loss:2.298822458164319\n",
      "train loss:2.308529703844276\n",
      "train loss:2.3021659520671296\n",
      "train loss:2.3087495280212385\n",
      "train loss:2.2990351329065843\n",
      "train loss:2.297340342673339\n",
      "train loss:2.299303005882008\n",
      "train loss:2.304792673503847\n",
      "train loss:2.3011497580635867\n",
      "train loss:2.297248661037121\n",
      "train loss:2.3022840943954823\n",
      "train loss:2.297528398341065\n",
      "train loss:2.309156571030318\n",
      "train loss:2.3021927518975027\n",
      "train loss:2.2986436893615294\n",
      "train loss:2.304942829634062\n",
      "train loss:2.302779103742251\n",
      "train loss:2.3007988380461604\n",
      "train loss:2.302783131996415\n",
      "train loss:2.2970828416809836\n",
      "train loss:2.309134397309494\n",
      "train loss:2.297388315331834\n",
      "train loss:2.3046405423210605\n",
      "train loss:2.3062233693364615\n",
      "train loss:2.301393331893594\n",
      "train loss:2.2917694888117714\n",
      "train loss:2.2989544359417096\n",
      "train loss:2.30269105145781\n",
      "train loss:2.3006721042047777\n",
      "train loss:2.300326087935435\n",
      "train loss:2.302725349225205\n",
      "train loss:2.303299010329462\n",
      "train loss:2.293780055542365\n",
      "train loss:2.3057705389044147\n",
      "train loss:2.3056486006340324\n",
      "train loss:2.3089801874536824\n",
      "train loss:2.3062626708813156\n",
      "train loss:2.298427721875622\n",
      "train loss:2.2990433529177907\n",
      "train loss:2.307497746993958\n",
      "train loss:2.303014390026132\n",
      "train loss:2.302112917632257\n",
      "train loss:2.303349065821062\n",
      "train loss:2.30281433523948\n",
      "train loss:2.311191788817454\n",
      "train loss:2.2960495142031667\n",
      "train loss:2.303280105197914\n",
      "train loss:2.2976626715593547\n",
      "train loss:2.302045649904104\n",
      "train loss:2.298630646674184\n",
      "train loss:2.304494203280621\n",
      "train loss:2.3083954375640054\n",
      "train loss:2.31305885940527\n",
      "train loss:2.3106039640518965\n",
      "train loss:2.297395204363573\n",
      "train loss:2.3035757420644076\n",
      "train loss:2.3010230132418497\n",
      "train loss:2.301661103629801\n",
      "train loss:2.296055601919963\n",
      "train loss:2.3094413268072\n",
      "train loss:2.2999055807616235\n",
      "train loss:2.301694645779024\n",
      "train loss:2.295525424104559\n",
      "train loss:2.2959962855571012\n",
      "train loss:2.30507585275134\n",
      "train loss:2.3053738786975746\n",
      "train loss:2.2974399854428387\n",
      "train loss:2.305919615067185\n",
      "train loss:2.3089470407190316\n",
      "train loss:2.300183412923205\n",
      "train loss:2.303627093190582\n",
      "train loss:2.2950198576375835\n",
      "train loss:2.303560824792748\n",
      "train loss:2.298320809667307\n",
      "train loss:2.3049645565464596\n",
      "train loss:2.3004222665787277\n",
      "train loss:2.3058167324697467\n",
      "train loss:2.3050501821357035\n",
      "train loss:2.296395266317136\n",
      "train loss:2.291526337607392\n",
      "train loss:2.2939971694395176\n",
      "=== epoch:45, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.309838403386166\n",
      "train loss:2.3085265349619934\n",
      "train loss:2.3135637550898203\n",
      "train loss:2.3023398275937885\n",
      "train loss:2.306906342631197\n",
      "train loss:2.3045274287987785\n",
      "train loss:2.2964196560833448\n",
      "train loss:2.2940176702850956\n",
      "train loss:2.3006498851979167\n",
      "train loss:2.295913780178576\n",
      "train loss:2.3024213414306254\n",
      "train loss:2.3053773870141785\n",
      "train loss:2.2956401464264307\n",
      "train loss:2.3023330968120796\n",
      "train loss:2.294753485162838\n",
      "train loss:2.3051988414472446\n",
      "train loss:2.295399909720124\n",
      "train loss:2.3051930669415364\n",
      "train loss:2.3027770193873756\n",
      "train loss:2.297498176864835\n",
      "train loss:2.298743574447396\n",
      "train loss:2.297580723129728\n",
      "train loss:2.308407269259648\n",
      "train loss:2.3046932155455946\n",
      "train loss:2.3020560861645816\n",
      "train loss:2.291270855571866\n",
      "train loss:2.3014279534876825\n",
      "train loss:2.2950378317270967\n",
      "train loss:2.3031259222689364\n",
      "train loss:2.297143415990187\n",
      "train loss:2.3143236470185253\n",
      "train loss:2.2948843120701885\n",
      "train loss:2.301733217514562\n",
      "train loss:2.301553990442545\n",
      "train loss:2.3023064703177947\n",
      "train loss:2.295369018617426\n",
      "train loss:2.2959940983520104\n",
      "train loss:2.3003319607022195\n",
      "train loss:2.2933302787363106\n",
      "train loss:2.297839541130367\n",
      "train loss:2.2932567614833843\n",
      "train loss:2.3056328310602914\n",
      "train loss:2.2986821045209447\n",
      "train loss:2.2994926672526086\n",
      "train loss:2.301258191997324\n",
      "train loss:2.2985447690225165\n",
      "train loss:2.3067641841211532\n",
      "train loss:2.3080698307179337\n",
      "train loss:2.3080117449359325\n",
      "train loss:2.301547045652044\n",
      "train loss:2.3003826420023157\n",
      "train loss:2.304487344975471\n",
      "train loss:2.3168796493316153\n",
      "train loss:2.302326361011228\n",
      "train loss:2.3093147883454037\n",
      "train loss:2.300606915286726\n",
      "train loss:2.301706639509005\n",
      "train loss:2.306690769641859\n",
      "train loss:2.3014922885086033\n",
      "train loss:2.303636979719787\n",
      "train loss:2.3058544583894744\n",
      "train loss:2.297521626361335\n",
      "train loss:2.3048299201246896\n",
      "train loss:2.2941541409498405\n",
      "train loss:2.298378485441723\n",
      "train loss:2.309146720063696\n",
      "train loss:2.299338538169785\n",
      "train loss:2.29834464906687\n",
      "train loss:2.3102962360062698\n",
      "train loss:2.2985171710215746\n",
      "train loss:2.2949287673135457\n",
      "train loss:2.288676355940788\n",
      "train loss:2.3007530264640343\n",
      "train loss:2.29904499134641\n",
      "train loss:2.2963203516806994\n",
      "train loss:2.2899715292132634\n",
      "train loss:2.2924356617662642\n",
      "train loss:2.3065042011533397\n",
      "train loss:2.302560436561551\n",
      "train loss:2.2954947737324596\n",
      "train loss:2.2998509574392787\n",
      "train loss:2.3050642541950555\n",
      "train loss:2.3130801396069756\n",
      "train loss:2.3040764988448243\n",
      "train loss:2.2970280100885963\n",
      "train loss:2.3023980964580715\n",
      "train loss:2.301767613435406\n",
      "train loss:2.2908998709497417\n",
      "train loss:2.306901452705826\n",
      "train loss:2.2924656407176456\n",
      "train loss:2.298645596234229\n",
      "train loss:2.3017563363903686\n",
      "train loss:2.3041405368730143\n",
      "train loss:2.2981315784847816\n",
      "train loss:2.3056924572809177\n",
      "train loss:2.2960743006027275\n",
      "train loss:2.3053859591436447\n",
      "train loss:2.297805937307947\n",
      "train loss:2.302752895499486\n",
      "train loss:2.3038469478618606\n",
      "train loss:2.295082797632866\n",
      "train loss:2.3041583874488656\n",
      "train loss:2.301795994198545\n",
      "train loss:2.313304845712825\n",
      "train loss:2.2984923999336404\n",
      "train loss:2.297941632416174\n",
      "train loss:2.3053705712711414\n",
      "train loss:2.2969341401624366\n",
      "train loss:2.2949980987631076\n",
      "train loss:2.3053518482362003\n",
      "train loss:2.303882914208719\n",
      "train loss:2.3051015182858476\n",
      "train loss:2.3096566989807377\n",
      "train loss:2.29383121108777\n",
      "train loss:2.3079264814120486\n",
      "train loss:2.2898150703565667\n",
      "train loss:2.3097210645119293\n",
      "train loss:2.2987064068482987\n",
      "train loss:2.3060738615480254\n",
      "train loss:2.3051055843193424\n",
      "train loss:2.298975404577725\n",
      "train loss:2.3029679228446325\n",
      "train loss:2.299404109058248\n",
      "train loss:2.2964144838263834\n",
      "train loss:2.3028363638828915\n",
      "train loss:2.292792409779755\n",
      "train loss:2.296616997990998\n",
      "train loss:2.3111830139567764\n",
      "train loss:2.302411032316228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3008867246702005\n",
      "train loss:2.309147995549848\n",
      "train loss:2.3009888505829084\n",
      "train loss:2.300196723356674\n",
      "train loss:2.3062640258841567\n",
      "train loss:2.3031383705029485\n",
      "train loss:2.304213051622592\n",
      "train loss:2.3080632265222087\n",
      "train loss:2.3038580106245115\n",
      "train loss:2.294914341203654\n",
      "train loss:2.304427901161804\n",
      "train loss:2.3053629200973007\n",
      "train loss:2.303422109347981\n",
      "train loss:2.3068858488581907\n",
      "train loss:2.312018911238368\n",
      "train loss:2.3051336176743726\n",
      "train loss:2.309094770435565\n",
      "train loss:2.2926926624691126\n",
      "train loss:2.3014157082563074\n",
      "train loss:2.304519356354829\n",
      "train loss:2.3096039512314785\n",
      "train loss:2.3017454738143424\n",
      "train loss:2.3080151666071926\n",
      "train loss:2.304968009598831\n",
      "train loss:2.3027834029852157\n",
      "train loss:2.2941465213642043\n",
      "train loss:2.3098713239666306\n",
      "train loss:2.3030120841306276\n",
      "train loss:2.298870782779422\n",
      "train loss:2.3065542747484757\n",
      "train loss:2.3049860965803197\n",
      "train loss:2.29577874988549\n",
      "train loss:2.297949509430225\n",
      "train loss:2.3023131088389683\n",
      "train loss:2.306599676801006\n",
      "train loss:2.299112612702521\n",
      "train loss:2.2953869379646634\n",
      "train loss:2.295985247136959\n",
      "train loss:2.3041454031170554\n",
      "train loss:2.3028024667440445\n",
      "train loss:2.3009635327250204\n",
      "train loss:2.2971227866859945\n",
      "train loss:2.322852872886805\n",
      "train loss:2.2924410788492917\n",
      "train loss:2.312142352532764\n",
      "train loss:2.3056054230502343\n",
      "train loss:2.3022676588013025\n",
      "train loss:2.3031313987197737\n",
      "train loss:2.297110708349296\n",
      "train loss:2.300615652146417\n",
      "train loss:2.2999116970404603\n",
      "train loss:2.309817757489839\n",
      "train loss:2.3096088184053087\n",
      "train loss:2.304453454839624\n",
      "train loss:2.304155937989445\n",
      "train loss:2.2931239291793037\n",
      "train loss:2.2926371534145558\n",
      "train loss:2.296522887644464\n",
      "train loss:2.304803461743107\n",
      "train loss:2.3020672994820806\n",
      "train loss:2.2958225158458836\n",
      "train loss:2.3015288384569823\n",
      "train loss:2.300056197980542\n",
      "train loss:2.3042521772504356\n",
      "train loss:2.293664309421221\n",
      "train loss:2.3019847744183783\n",
      "train loss:2.3003834643842076\n",
      "train loss:2.3008672442109237\n",
      "train loss:2.3030797109443406\n",
      "train loss:2.296827569416802\n",
      "train loss:2.292196771155877\n",
      "=== epoch:46, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3004067462649536\n",
      "train loss:2.3061475749205043\n",
      "train loss:2.2949796673444878\n",
      "train loss:2.304891865273451\n",
      "train loss:2.2987867347134574\n",
      "train loss:2.3005513693368878\n",
      "train loss:2.3043282922037926\n",
      "train loss:2.3022474718488404\n",
      "train loss:2.2978789680912897\n",
      "train loss:2.2976455139892034\n",
      "train loss:2.296638281434108\n",
      "train loss:2.2936143814266114\n",
      "train loss:2.305128836323706\n",
      "train loss:2.3019198528210274\n",
      "train loss:2.301758886351555\n",
      "train loss:2.2976810025488708\n",
      "train loss:2.304883033792482\n",
      "train loss:2.297968139291342\n",
      "train loss:2.3006757200898154\n",
      "train loss:2.2983884022718493\n",
      "train loss:2.3036018913844307\n",
      "train loss:2.303508464898628\n",
      "train loss:2.2983372815486978\n",
      "train loss:2.3047144231342713\n",
      "train loss:2.2924983707593687\n",
      "train loss:2.295194744881109\n",
      "train loss:2.309927867978972\n",
      "train loss:2.299490563723284\n",
      "train loss:2.304136601483798\n",
      "train loss:2.2991237092549914\n",
      "train loss:2.303712840685384\n",
      "train loss:2.295360744702976\n",
      "train loss:2.3009490619095883\n",
      "train loss:2.2994160938599637\n",
      "train loss:2.3029779641428485\n",
      "train loss:2.299086927447595\n",
      "train loss:2.306484996228255\n",
      "train loss:2.303315801296244\n",
      "train loss:2.2954805328912617\n",
      "train loss:2.295098020547216\n",
      "train loss:2.2980991896940908\n",
      "train loss:2.294116767399962\n",
      "train loss:2.304578698161632\n",
      "train loss:2.2950320463727976\n",
      "train loss:2.297737794807833\n",
      "train loss:2.30879683487529\n",
      "train loss:2.308863102605726\n",
      "train loss:2.3025521962583726\n",
      "train loss:2.3062226064576024\n",
      "train loss:2.303638891603115\n",
      "train loss:2.298742753758862\n",
      "train loss:2.30086776408056\n",
      "train loss:2.3047241777738945\n",
      "train loss:2.302929323644079\n",
      "train loss:2.3001570767175012\n",
      "train loss:2.3090419883916993\n",
      "train loss:2.2926372242136948\n",
      "train loss:2.299838252336582\n",
      "train loss:2.2918351813727416\n",
      "train loss:2.3085062888496792\n",
      "train loss:2.300739203924046\n",
      "train loss:2.3024843737861627\n",
      "train loss:2.299202360024336\n",
      "train loss:2.304421241287526\n",
      "train loss:2.2986052370319032\n",
      "train loss:2.3087336433833228\n",
      "train loss:2.302118674068701\n",
      "train loss:2.3003185034018685\n",
      "train loss:2.3034751639890754\n",
      "train loss:2.308833276060948\n",
      "train loss:2.2974235829640137\n",
      "train loss:2.304189066664405\n",
      "train loss:2.3068667567992525\n",
      "train loss:2.3120394201319603\n",
      "train loss:2.303879896340583\n",
      "train loss:2.2931654106165036\n",
      "train loss:2.307376177824722\n",
      "train loss:2.3014480371397172\n",
      "train loss:2.3085408754855954\n",
      "train loss:2.2946082059088813\n",
      "train loss:2.3072983012133146\n",
      "train loss:2.309457721326444\n",
      "train loss:2.3000714940274833\n",
      "train loss:2.3060238813857468\n",
      "train loss:2.2958721849671258\n",
      "train loss:2.297462431303306\n",
      "train loss:2.2989430467618037\n",
      "train loss:2.294532150991396\n",
      "train loss:2.3041772676175536\n",
      "train loss:2.307735995604835\n",
      "train loss:2.3023788923584387\n",
      "train loss:2.302430791706827\n",
      "train loss:2.3019699155635442\n",
      "train loss:2.3030440365076608\n",
      "train loss:2.301330012118471\n",
      "train loss:2.302858792103667\n",
      "train loss:2.305126640830927\n",
      "train loss:2.2979027430712833\n",
      "train loss:2.301425327026523\n",
      "train loss:2.303367355082692\n",
      "train loss:2.3057461756589483\n",
      "train loss:2.3072266759914233\n",
      "train loss:2.2988416687795428\n",
      "train loss:2.3023419468730344\n",
      "train loss:2.296092660470695\n",
      "train loss:2.3038479972957395\n",
      "train loss:2.308933283541643\n",
      "train loss:2.306299501398313\n",
      "train loss:2.2949947402156052\n",
      "train loss:2.298169934202468\n",
      "train loss:2.297945838268933\n",
      "train loss:2.306253509264384\n",
      "train loss:2.3090436637273477\n",
      "train loss:2.2995004925291878\n",
      "train loss:2.3018489566977562\n",
      "train loss:2.310483088790235\n",
      "train loss:2.304543353504642\n",
      "train loss:2.301791590158704\n",
      "train loss:2.2977094717394233\n",
      "train loss:2.3025543541996254\n",
      "train loss:2.3004003611149737\n",
      "train loss:2.298964554343409\n",
      "train loss:2.2953550679968435\n",
      "train loss:2.2996017321670794\n",
      "train loss:2.2917935019067572\n",
      "train loss:2.299102925554857\n",
      "train loss:2.3025719944320984\n",
      "train loss:2.3066596687650516\n",
      "train loss:2.300895268609568\n",
      "train loss:2.303959343436188\n",
      "train loss:2.305299466355266\n",
      "train loss:2.3135416062664613\n",
      "train loss:2.298080256626847\n",
      "train loss:2.29858605929185\n",
      "train loss:2.2926471178892913\n",
      "train loss:2.300007224411473\n",
      "train loss:2.3050670570232286\n",
      "train loss:2.3037893937849936\n",
      "train loss:2.304490559846689\n",
      "train loss:2.3010915273432566\n",
      "train loss:2.3004646833854467\n",
      "train loss:2.30318082472529\n",
      "train loss:2.3008039924475345\n",
      "train loss:2.2954991803482474\n",
      "train loss:2.299098250901311\n",
      "train loss:2.30146838839438\n",
      "train loss:2.3078836912937994\n",
      "train loss:2.2978589220745507\n",
      "train loss:2.294285808349832\n",
      "train loss:2.2938007742440996\n",
      "train loss:2.3055408384991303\n",
      "train loss:2.3069701514758347\n",
      "train loss:2.2970300932339947\n",
      "train loss:2.308541910256329\n",
      "train loss:2.303821696050104\n",
      "train loss:2.3086964987520546\n",
      "train loss:2.298419400264695\n",
      "train loss:2.298198023435422\n",
      "train loss:2.301826041237393\n",
      "train loss:2.2979952739088056\n",
      "train loss:2.3042299478488313\n",
      "train loss:2.3028930675806754\n",
      "train loss:2.2987852726275646\n",
      "train loss:2.299186011942314\n",
      "train loss:2.2961390290217887\n",
      "train loss:2.2883767460423003\n",
      "train loss:2.2865424943070507\n",
      "train loss:2.3010796520390797\n",
      "train loss:2.2979122404787\n",
      "train loss:2.2962907275534397\n",
      "train loss:2.2970902040784815\n",
      "train loss:2.3095110984511265\n",
      "train loss:2.290623497008552\n",
      "train loss:2.305706636574236\n",
      "train loss:2.302054608173162\n",
      "train loss:2.3047631407021907\n",
      "train loss:2.2999643111485506\n",
      "train loss:2.304970181569209\n",
      "train loss:2.307981678052153\n",
      "train loss:2.2951570613225614\n",
      "train loss:2.3021607244731372\n",
      "train loss:2.295665200854008\n",
      "train loss:2.304940219603962\n",
      "train loss:2.3044570288414477\n",
      "train loss:2.307041298611602\n",
      "train loss:2.296978473466154\n",
      "train loss:2.3004085628692525\n",
      "train loss:2.302558270120765\n",
      "train loss:2.2925285535215214\n",
      "train loss:2.3007304247191427\n",
      "train loss:2.305906364460015\n",
      "train loss:2.299843021069095\n",
      "train loss:2.2983572028530705\n",
      "train loss:2.297582877987236\n",
      "train loss:2.3018508014172343\n",
      "train loss:2.2951762877435913\n",
      "train loss:2.2929377130801645\n",
      "train loss:2.2900746814444717\n",
      "train loss:2.301055851630096\n",
      "train loss:2.312781377158993\n",
      "=== epoch:47, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2961923032289384\n",
      "train loss:2.3058029847621944\n",
      "train loss:2.2968825026026893\n",
      "train loss:2.302801168043536\n",
      "train loss:2.301913147571054\n",
      "train loss:2.3059406403101166\n",
      "train loss:2.3015572998424525\n",
      "train loss:2.304844361239437\n",
      "train loss:2.304873473412066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2957093140712255\n",
      "train loss:2.2948701645651948\n",
      "train loss:2.299907748812976\n",
      "train loss:2.299751915735208\n",
      "train loss:2.2980603678820364\n",
      "train loss:2.3102503392333205\n",
      "train loss:2.2996036153274044\n",
      "train loss:2.310241438598967\n",
      "train loss:2.305960896734338\n",
      "train loss:2.3110451245260366\n",
      "train loss:2.3050539648354875\n",
      "train loss:2.301716425733866\n",
      "train loss:2.303059814419573\n",
      "train loss:2.2903477368097303\n",
      "train loss:2.309469529881611\n",
      "train loss:2.3045739551974993\n",
      "train loss:2.301687266833592\n",
      "train loss:2.299007530108311\n",
      "train loss:2.301701197767143\n",
      "train loss:2.3026528910121984\n",
      "train loss:2.298760232670708\n",
      "train loss:2.301619725892621\n",
      "train loss:2.2968063231591413\n",
      "train loss:2.305148820835996\n",
      "train loss:2.2946264390845017\n",
      "train loss:2.307360058446852\n",
      "train loss:2.302880233339074\n",
      "train loss:2.3001637872814458\n",
      "train loss:2.300970286166134\n",
      "train loss:2.301708214247088\n",
      "train loss:2.297445830114855\n",
      "train loss:2.301940274823065\n",
      "train loss:2.3092427292489695\n",
      "train loss:2.3055002037168775\n",
      "train loss:2.2980411319900846\n",
      "train loss:2.305233719026835\n",
      "train loss:2.2970866585199\n",
      "train loss:2.31024554594327\n",
      "train loss:2.3021872253653166\n",
      "train loss:2.3003664633878036\n",
      "train loss:2.2963061648934744\n",
      "train loss:2.2963199886463572\n",
      "train loss:2.305765196657055\n",
      "train loss:2.308001486920062\n",
      "train loss:2.29781354840606\n",
      "train loss:2.3064473653239865\n",
      "train loss:2.3011598326992724\n",
      "train loss:2.3008460556325003\n",
      "train loss:2.2956845596253195\n",
      "train loss:2.3138886216457295\n",
      "train loss:2.3038171940254712\n",
      "train loss:2.2982615076264303\n",
      "train loss:2.3048280831292276\n",
      "train loss:2.293231101381369\n",
      "train loss:2.3074306033132017\n",
      "train loss:2.294020730570741\n",
      "train loss:2.291190524339841\n",
      "train loss:2.3001098469678496\n",
      "train loss:2.301672599285775\n",
      "train loss:2.3021905159741167\n",
      "train loss:2.3058899429485593\n",
      "train loss:2.2941955127353757\n",
      "train loss:2.308720502610212\n",
      "train loss:2.2999271336703844\n",
      "train loss:2.2913404037207976\n",
      "train loss:2.3105500148986104\n",
      "train loss:2.292144840692164\n",
      "train loss:2.3101237662176906\n",
      "train loss:2.2934293408975974\n",
      "train loss:2.3102730880576607\n",
      "train loss:2.2942026556236925\n",
      "train loss:2.299501600404388\n",
      "train loss:2.3024922715555936\n",
      "train loss:2.3050526733659895\n",
      "train loss:2.306031735274935\n",
      "train loss:2.307032167514463\n",
      "train loss:2.301817004468864\n",
      "train loss:2.3043244409425134\n",
      "train loss:2.3144010774427666\n",
      "train loss:2.3053327001036683\n",
      "train loss:2.3042043700767216\n",
      "train loss:2.3016708110748807\n",
      "train loss:2.3000698262019568\n",
      "train loss:2.3048242987857677\n",
      "train loss:2.3079937305934606\n",
      "train loss:2.301913730919969\n",
      "train loss:2.3036325506199478\n",
      "train loss:2.299347059400307\n",
      "train loss:2.311829729810946\n",
      "train loss:2.299603596843349\n",
      "train loss:2.305529638611261\n",
      "train loss:2.306157217249466\n",
      "train loss:2.2970672843027273\n",
      "train loss:2.2977375328337604\n",
      "train loss:2.3014922809051104\n",
      "train loss:2.3032313554462176\n",
      "train loss:2.30596601277071\n",
      "train loss:2.3065341683082186\n",
      "train loss:2.300665575901715\n",
      "train loss:2.29972459023777\n",
      "train loss:2.3031865842192296\n",
      "train loss:2.304441717910787\n",
      "train loss:2.3058091579783\n",
      "train loss:2.297497607645234\n",
      "train loss:2.3027334764807432\n",
      "train loss:2.3036288529346027\n",
      "train loss:2.303210881266635\n",
      "train loss:2.311650069224607\n",
      "train loss:2.298442277186471\n",
      "train loss:2.2991282701316336\n",
      "train loss:2.3035422098629823\n",
      "train loss:2.3101001261435363\n",
      "train loss:2.3007548409195233\n",
      "train loss:2.30445258377797\n",
      "train loss:2.2931628574834946\n",
      "train loss:2.298740335665998\n",
      "train loss:2.2994870232154994\n",
      "train loss:2.2999419564660193\n",
      "train loss:2.295527587002876\n",
      "train loss:2.2922035561054455\n",
      "train loss:2.3040743379933475\n",
      "train loss:2.2985324479656293\n",
      "train loss:2.3170544817210565\n",
      "train loss:2.3024320005259584\n",
      "train loss:2.3032644063915715\n",
      "train loss:2.3072316225473384\n",
      "train loss:2.3048121902150536\n",
      "train loss:2.302826362539739\n",
      "train loss:2.3049661539384196\n",
      "train loss:2.303500879428612\n",
      "train loss:2.3065245251711195\n",
      "train loss:2.2991178600018536\n",
      "train loss:2.3013737570523487\n",
      "train loss:2.3038824188306934\n",
      "train loss:2.2993053463080524\n",
      "train loss:2.293257880804214\n",
      "train loss:2.3071556227106096\n",
      "train loss:2.3017922909064987\n",
      "train loss:2.3057620622499724\n",
      "train loss:2.2958694030915185\n",
      "train loss:2.2969380041722887\n",
      "train loss:2.302411897113729\n",
      "train loss:2.3088385291354747\n",
      "train loss:2.301885098148787\n",
      "train loss:2.30434893507462\n",
      "train loss:2.3069437110218174\n",
      "train loss:2.2980313507829693\n",
      "train loss:2.3023541131349097\n",
      "train loss:2.304787595945189\n",
      "train loss:2.293750266228937\n",
      "train loss:2.2962424602406033\n",
      "train loss:2.2985839316503918\n",
      "train loss:2.299710791231245\n",
      "train loss:2.302649161630933\n",
      "train loss:2.2998974530306087\n",
      "train loss:2.3006424636611738\n",
      "train loss:2.299313069877227\n",
      "train loss:2.3037404997933137\n",
      "train loss:2.3037751612034434\n",
      "train loss:2.286732475865773\n",
      "train loss:2.308753430549778\n",
      "train loss:2.294721842002576\n",
      "train loss:2.2961302130869106\n",
      "train loss:2.306434768558766\n",
      "train loss:2.295840361351865\n",
      "train loss:2.3094440576115165\n",
      "train loss:2.2992231814044417\n",
      "train loss:2.299238534941905\n",
      "train loss:2.305323888917496\n",
      "train loss:2.304819396413569\n",
      "train loss:2.3092784148327525\n",
      "train loss:2.3050924773782966\n",
      "train loss:2.297802544331689\n",
      "train loss:2.2972616903536145\n",
      "train loss:2.29408282061046\n",
      "train loss:2.3052893762461686\n",
      "train loss:2.2920094520596512\n",
      "train loss:2.3023331057494114\n",
      "train loss:2.29564323068313\n",
      "train loss:2.302421123324631\n",
      "train loss:2.298531600056804\n",
      "train loss:2.298187678279893\n",
      "train loss:2.299551232140664\n",
      "train loss:2.2985030317073787\n",
      "train loss:2.3091896507387046\n",
      "train loss:2.301922395161284\n",
      "train loss:2.29467959519482\n",
      "train loss:2.3001986079203562\n",
      "train loss:2.298541703438768\n",
      "train loss:2.311987953769439\n",
      "train loss:2.30408356307277\n",
      "=== epoch:48, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.306383132507025\n",
      "train loss:2.309936954651131\n",
      "train loss:2.3116903390081722\n",
      "train loss:2.305283798692916\n",
      "train loss:2.3027578910966096\n",
      "train loss:2.3083324023367293\n",
      "train loss:2.289932966328329\n",
      "train loss:2.2951282350645403\n",
      "train loss:2.3001750545688195\n",
      "train loss:2.303706590211541\n",
      "train loss:2.3054084685647815\n",
      "train loss:2.297702038567541\n",
      "train loss:2.2934905008935154\n",
      "train loss:2.307320310032403\n",
      "train loss:2.2985327299126608\n",
      "train loss:2.3028501832550443\n",
      "train loss:2.29846345386801\n",
      "train loss:2.302880437487442\n",
      "train loss:2.3025927491053606\n",
      "train loss:2.2978666320433625\n",
      "train loss:2.298063889958015\n",
      "train loss:2.296372071523854\n",
      "train loss:2.3071085277531433\n",
      "train loss:2.3070115652990513\n",
      "train loss:2.299122625495169\n",
      "train loss:2.3003662375103264\n",
      "train loss:2.298436868812639\n",
      "train loss:2.3093833395736776\n",
      "train loss:2.2979350812231116\n",
      "train loss:2.293319429429565\n",
      "train loss:2.2975282662097025\n",
      "train loss:2.301503552980606\n",
      "train loss:2.3042934675676983\n",
      "train loss:2.304753412470476\n",
      "train loss:2.293769680881169\n",
      "train loss:2.2968631002752056\n",
      "train loss:2.297753259668175\n",
      "train loss:2.307523168761022\n",
      "train loss:2.3059708574729534\n",
      "train loss:2.3021070264308703\n",
      "train loss:2.3143317137527766\n",
      "train loss:2.2993265192252457\n",
      "train loss:2.299930478512151\n",
      "train loss:2.3029818782924036\n",
      "train loss:2.308072480078051\n",
      "train loss:2.2994386569860623\n",
      "train loss:2.2921668349917046\n",
      "train loss:2.300884754499633\n",
      "train loss:2.304114700926784\n",
      "train loss:2.3064911538755712\n",
      "train loss:2.3108610773472735\n",
      "train loss:2.2987290962026306\n",
      "train loss:2.299504821053655\n",
      "train loss:2.292416467736891\n",
      "train loss:2.301453890176353\n",
      "train loss:2.3033706803916485\n",
      "train loss:2.2945612192310842\n",
      "train loss:2.295487407890199\n",
      "train loss:2.3015242112665715\n",
      "train loss:2.298678840385741\n",
      "train loss:2.3039607172832395\n",
      "train loss:2.313551162248338\n",
      "train loss:2.3078476438934774\n",
      "train loss:2.297641038226786\n",
      "train loss:2.2987111952964847\n",
      "train loss:2.297280796637984\n",
      "train loss:2.3066733146346543\n",
      "train loss:2.2994676673167564\n",
      "train loss:2.2952134408516622\n",
      "train loss:2.2905420708110107\n",
      "train loss:2.2991317631029644\n",
      "train loss:2.3008793821955775\n",
      "train loss:2.2924621194256\n",
      "train loss:2.303704686768685\n",
      "train loss:2.298597859063438\n",
      "train loss:2.294487677937055\n",
      "train loss:2.2960444082078344\n",
      "train loss:2.3087738333549006\n",
      "train loss:2.304471590490099\n",
      "train loss:2.3036673515956996\n",
      "train loss:2.30391572745015\n",
      "train loss:2.3062401470635785\n",
      "train loss:2.304697487248975\n",
      "train loss:2.2974522771883983\n",
      "train loss:2.3050128576353885\n",
      "train loss:2.29993274137097\n",
      "train loss:2.2958204124558095\n",
      "train loss:2.3054438751019477\n",
      "train loss:2.2922828342377235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.305320428068317\n",
      "train loss:2.301054123620936\n",
      "train loss:2.3012635494234774\n",
      "train loss:2.303840586276294\n",
      "train loss:2.2974727806862822\n",
      "train loss:2.298888251755289\n",
      "train loss:2.2954663727606714\n",
      "train loss:2.301957863507075\n",
      "train loss:2.2973480166481473\n",
      "train loss:2.296574983938203\n",
      "train loss:2.29795235288719\n",
      "train loss:2.2970857955165984\n",
      "train loss:2.3000200917453832\n",
      "train loss:2.3065329102603287\n",
      "train loss:2.2937469245690334\n",
      "train loss:2.297848810823645\n",
      "train loss:2.30158263693401\n",
      "train loss:2.2902244578426614\n",
      "train loss:2.296420564775912\n",
      "train loss:2.3058693519039455\n",
      "train loss:2.2982941299662083\n",
      "train loss:2.3001501709382643\n",
      "train loss:2.29404489433588\n",
      "train loss:2.303681553145454\n",
      "train loss:2.3011942064491118\n",
      "train loss:2.304014206518316\n",
      "train loss:2.305967280320283\n",
      "train loss:2.302417741773652\n",
      "train loss:2.3043081253647983\n",
      "train loss:2.2948627956156065\n",
      "train loss:2.3060500935569315\n",
      "train loss:2.296742314992191\n",
      "train loss:2.30160059944037\n",
      "train loss:2.3044913128100912\n",
      "train loss:2.3054835573242225\n",
      "train loss:2.3009949266041656\n",
      "train loss:2.2945218143638217\n",
      "train loss:2.297402189515773\n",
      "train loss:2.308258697897841\n",
      "train loss:2.303406348970867\n",
      "train loss:2.3001760372957163\n",
      "train loss:2.302900196785029\n",
      "train loss:2.2994562035446364\n",
      "train loss:2.3051555332536546\n",
      "train loss:2.300531061945463\n",
      "train loss:2.303509616901527\n",
      "train loss:2.293797938355457\n",
      "train loss:2.301576222476638\n",
      "train loss:2.3078997869583886\n",
      "train loss:2.300716279615706\n",
      "train loss:2.301818661205065\n",
      "train loss:2.306340383778743\n",
      "train loss:2.2996656369978288\n",
      "train loss:2.3031981572447795\n",
      "train loss:2.2992351645731675\n",
      "train loss:2.3065787501229176\n",
      "train loss:2.2968779239563077\n",
      "train loss:2.3104644272165578\n",
      "train loss:2.296578647260375\n",
      "train loss:2.301063490036261\n",
      "train loss:2.3064402495274474\n",
      "train loss:2.288873867793549\n",
      "train loss:2.301194448591288\n",
      "train loss:2.301123056857525\n",
      "train loss:2.308173115322024\n",
      "train loss:2.305884938274155\n",
      "train loss:2.295392686182013\n",
      "train loss:2.2982729678753406\n",
      "train loss:2.312288476082857\n",
      "train loss:2.2989850264210157\n",
      "train loss:2.2983322080044677\n",
      "train loss:2.301742651652277\n",
      "train loss:2.3095998658843757\n",
      "train loss:2.3079944261890244\n",
      "train loss:2.3039699534320697\n",
      "train loss:2.29931476125325\n",
      "train loss:2.3016779063162947\n",
      "train loss:2.3033755788002215\n",
      "train loss:2.3042726678691823\n",
      "train loss:2.2924389972766304\n",
      "train loss:2.2992009853885826\n",
      "train loss:2.307662974282081\n",
      "train loss:2.3037981714330993\n",
      "train loss:2.295770723040207\n",
      "train loss:2.3030950824238303\n",
      "train loss:2.295679039651221\n",
      "train loss:2.296456653375027\n",
      "train loss:2.3028131573403314\n",
      "train loss:2.299871888152173\n",
      "train loss:2.291790471064776\n",
      "train loss:2.302103519660241\n",
      "train loss:2.301589374721381\n",
      "train loss:2.300929099337355\n",
      "train loss:2.2932674631766745\n",
      "train loss:2.2930902917598828\n",
      "train loss:2.2986095793896273\n",
      "train loss:2.3035344438467136\n",
      "train loss:2.2925055071038845\n",
      "train loss:2.3137094096539212\n",
      "train loss:2.3046706354862905\n",
      "train loss:2.2994435165611242\n",
      "train loss:2.3064161780484373\n",
      "train loss:2.305537002637762\n",
      "train loss:2.3022863668109275\n",
      "train loss:2.3043477446359204\n",
      "train loss:2.2984472184033455\n",
      "train loss:2.3138768347507086\n",
      "train loss:2.309746240282045\n",
      "train loss:2.2949127638619395\n",
      "train loss:2.3002077640706866\n",
      "train loss:2.2988625785173515\n",
      "=== epoch:49, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2995206790084985\n",
      "train loss:2.2942159168831453\n",
      "train loss:2.293921062268554\n",
      "train loss:2.302445615869703\n",
      "train loss:2.305967399966174\n",
      "train loss:2.305848447529049\n",
      "train loss:2.2976387969426755\n",
      "train loss:2.307107042485963\n",
      "train loss:2.2965211688198663\n",
      "train loss:2.308450581074312\n",
      "train loss:2.302177999621789\n",
      "train loss:2.301898281990388\n",
      "train loss:2.299600287905202\n",
      "train loss:2.3021846070468794\n",
      "train loss:2.3026746914238725\n",
      "train loss:2.304780061969142\n",
      "train loss:2.2958960936330874\n",
      "train loss:2.300958928408978\n",
      "train loss:2.3039347623912505\n",
      "train loss:2.2941412588101193\n",
      "train loss:2.306302518133068\n",
      "train loss:2.300897221800195\n",
      "train loss:2.301235316057417\n",
      "train loss:2.3053491203565426\n",
      "train loss:2.3089280140566197\n",
      "train loss:2.296033272286276\n",
      "train loss:2.2972252082213425\n",
      "train loss:2.3034528000316903\n",
      "train loss:2.3028329298271215\n",
      "train loss:2.294022340078356\n",
      "train loss:2.2960940887709613\n",
      "train loss:2.3061931436078007\n",
      "train loss:2.3030506214744344\n",
      "train loss:2.2974402850237556\n",
      "train loss:2.3011981681711613\n",
      "train loss:2.3008087960497883\n",
      "train loss:2.306616276736597\n",
      "train loss:2.2967324378799567\n",
      "train loss:2.298066461214612\n",
      "train loss:2.3085019819423818\n",
      "train loss:2.3063663345419214\n",
      "train loss:2.3034453971594515\n",
      "train loss:2.2935655189001802\n",
      "train loss:2.2968824986840484\n",
      "train loss:2.2995699923565365\n",
      "train loss:2.298689617668765\n",
      "train loss:2.3003506960023796\n",
      "train loss:2.3055938169751578\n",
      "train loss:2.302159506389354\n",
      "train loss:2.3033240807314734\n",
      "train loss:2.30113701790271\n",
      "train loss:2.3050761509149313\n",
      "train loss:2.3048511210994866\n",
      "train loss:2.3010020856921125\n",
      "train loss:2.3049650620500954\n",
      "train loss:2.3012716164701605\n",
      "train loss:2.2967511406213625\n",
      "train loss:2.3030827009414874\n",
      "train loss:2.302344186402838\n",
      "train loss:2.3026201621257787\n",
      "train loss:2.3000436787677687\n",
      "train loss:2.299747391665417\n",
      "train loss:2.3019262201503525\n",
      "train loss:2.304114793841317\n",
      "train loss:2.302162236150429\n",
      "train loss:2.297560441379551\n",
      "train loss:2.2935701491722926\n",
      "train loss:2.302669177111891\n",
      "train loss:2.3019949132419817\n",
      "train loss:2.296497526240726\n",
      "train loss:2.2998008015382605\n",
      "train loss:2.308408680987862\n",
      "train loss:2.3012834624042426\n",
      "train loss:2.296467186367459\n",
      "train loss:2.294957936753625\n",
      "train loss:2.3099371410635583\n",
      "train loss:2.2954587134886286\n",
      "train loss:2.297787438286526\n",
      "train loss:2.3093126457214126\n",
      "train loss:2.303366690865357\n",
      "train loss:2.297721983313413\n",
      "train loss:2.2939091552086914\n",
      "train loss:2.30679925368074\n",
      "train loss:2.292411630827566\n",
      "train loss:2.3042504593626942\n",
      "train loss:2.297461103560891\n",
      "train loss:2.3054999315139804\n",
      "train loss:2.3059989211395076\n",
      "train loss:2.3006360390927103\n",
      "train loss:2.294851437244549\n",
      "train loss:2.30728264440617\n",
      "train loss:2.3068560142932375\n",
      "train loss:2.3110571978408236\n",
      "train loss:2.3049637295823056\n",
      "train loss:2.2951739815516263\n",
      "train loss:2.2975946689376525\n",
      "train loss:2.295618218643892\n",
      "train loss:2.2960529908951104\n",
      "train loss:2.3046444129442203\n",
      "train loss:2.293767580698139\n",
      "train loss:2.304120896506246\n",
      "train loss:2.293172609830735\n",
      "train loss:2.304007244166318\n",
      "train loss:2.303128989564069\n",
      "train loss:2.29817907314498\n",
      "train loss:2.3054761817577902\n",
      "train loss:2.2948505508327455\n",
      "train loss:2.3040059654847966\n",
      "train loss:2.301882335405357\n",
      "train loss:2.308730731961105\n",
      "train loss:2.3053381807101845\n",
      "train loss:2.3089071717817062\n",
      "train loss:2.301925918276739\n",
      "train loss:2.2978025317029536\n",
      "train loss:2.3018583062447324\n",
      "train loss:2.3039559617331102\n",
      "train loss:2.2982974568144625\n",
      "train loss:2.309975684340743\n",
      "train loss:2.301022802679174\n",
      "train loss:2.300769372297132\n",
      "train loss:2.305062608646645\n",
      "train loss:2.2994595824133905\n",
      "train loss:2.2964299465588693\n",
      "train loss:2.304886052341596\n",
      "train loss:2.2994995320445475\n",
      "train loss:2.2911265586830427\n",
      "train loss:2.314937638895047\n",
      "train loss:2.3008528254513143\n",
      "train loss:2.2942763809619247\n",
      "train loss:2.3030837197064087\n",
      "train loss:2.3035110737094424\n",
      "train loss:2.285403490760731\n",
      "train loss:2.2974358513015516\n",
      "train loss:2.2996131132524393\n",
      "train loss:2.29703050168535\n",
      "train loss:2.305356633782953\n",
      "train loss:2.293975361179499\n",
      "train loss:2.304979925357839\n",
      "train loss:2.2972520724963763\n",
      "train loss:2.3043107853204923\n",
      "train loss:2.291600803695739\n",
      "train loss:2.296291376491033\n",
      "train loss:2.3021188748044223\n",
      "train loss:2.3067527802887207\n",
      "train loss:2.308503275011289\n",
      "train loss:2.2999485532974195\n",
      "train loss:2.296675972599304\n",
      "train loss:2.304897162750352\n",
      "train loss:2.302269374108154\n",
      "train loss:2.299284275589122\n",
      "train loss:2.3065996102447266\n",
      "train loss:2.3019175549288047\n",
      "train loss:2.305443597120369\n",
      "train loss:2.3102785540620507\n",
      "train loss:2.3095348209061894\n",
      "train loss:2.3011455102874407\n",
      "train loss:2.30208296962461\n",
      "train loss:2.301468372533654\n",
      "train loss:2.307480481038652\n",
      "train loss:2.3040341636913855\n",
      "train loss:2.2972655655697904\n",
      "train loss:2.299390370554774\n",
      "train loss:2.3016224499814544\n",
      "train loss:2.308290420765856\n",
      "train loss:2.296794824671412\n",
      "train loss:2.3155804485088254\n",
      "train loss:2.3051694286706437\n",
      "train loss:2.301989086982683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3014928490571505\n",
      "train loss:2.3058214788154507\n",
      "train loss:2.3116914764270846\n",
      "train loss:2.30253203042585\n",
      "train loss:2.3002721300261504\n",
      "train loss:2.3015242278793058\n",
      "train loss:2.2944878841726086\n",
      "train loss:2.305676960601118\n",
      "train loss:2.297736972940652\n",
      "train loss:2.305700342211338\n",
      "train loss:2.2934864858968913\n",
      "train loss:2.3071145095317096\n",
      "train loss:2.295546147930992\n",
      "train loss:2.299108554347801\n",
      "train loss:2.305049463972373\n",
      "train loss:2.2996482996821173\n",
      "train loss:2.297313230016765\n",
      "train loss:2.3036357173139512\n",
      "train loss:2.2981387760964305\n",
      "train loss:2.2979031811419124\n",
      "train loss:2.2948641251015682\n",
      "train loss:2.3099703754344416\n",
      "train loss:2.3008274296731126\n",
      "train loss:2.288560641507756\n",
      "train loss:2.299989573927172\n",
      "train loss:2.3048811292307696\n",
      "train loss:2.3009440702477417\n",
      "train loss:2.3044826398042786\n",
      "train loss:2.301418621566549\n",
      "train loss:2.297850945648777\n",
      "train loss:2.3005967493126582\n",
      "train loss:2.306621663226603\n",
      "=== epoch:50, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3056397531494297\n",
      "train loss:2.297461653271256\n",
      "train loss:2.297227643758916\n",
      "train loss:2.3006277900176144\n",
      "train loss:2.302706891005268\n",
      "train loss:2.3147622513403796\n",
      "train loss:2.3018401890057034\n",
      "train loss:2.306825023221311\n",
      "train loss:2.3003691239162882\n",
      "train loss:2.301825301009036\n",
      "train loss:2.301184670547541\n",
      "train loss:2.301225407096142\n",
      "train loss:2.3024260621308157\n",
      "train loss:2.3039006334502954\n",
      "train loss:2.304519122909419\n",
      "train loss:2.294321682817429\n",
      "train loss:2.3139925038583242\n",
      "train loss:2.302845197711847\n",
      "train loss:2.3003970949079187\n",
      "train loss:2.3018864461836426\n",
      "train loss:2.304012401219188\n",
      "train loss:2.300589943394577\n",
      "train loss:2.302318327984959\n",
      "train loss:2.2963197990209103\n",
      "train loss:2.2990054933444304\n",
      "train loss:2.301455376114271\n",
      "train loss:2.3065328859085223\n",
      "train loss:2.3007796702230507\n",
      "train loss:2.308346336664765\n",
      "train loss:2.3075237156369535\n",
      "train loss:2.306763418559339\n",
      "train loss:2.3080513507670397\n",
      "train loss:2.3037601875045155\n",
      "train loss:2.295193424865958\n",
      "train loss:2.296088133909309\n",
      "train loss:2.3049090137297963\n",
      "train loss:2.292441805855959\n",
      "train loss:2.299727285112597\n",
      "train loss:2.305496618138513\n",
      "train loss:2.29779803412917\n",
      "train loss:2.296190352561543\n",
      "train loss:2.2972012334367276\n",
      "train loss:2.30174298258935\n",
      "train loss:2.3061882729542478\n",
      "train loss:2.295029783930543\n",
      "train loss:2.3019763112860208\n",
      "train loss:2.3063711147273285\n",
      "train loss:2.3133811335074634\n",
      "train loss:2.298807962703002\n",
      "train loss:2.2991663133432523\n",
      "train loss:2.310515363293244\n",
      "train loss:2.3067624162079055\n",
      "train loss:2.3083999626098124\n",
      "train loss:2.291427157128127\n",
      "train loss:2.3008922139782015\n",
      "train loss:2.2908685664737165\n",
      "train loss:2.3099389397090313\n",
      "train loss:2.2987017972590453\n",
      "train loss:2.3003732770510568\n",
      "train loss:2.2997528415395947\n",
      "train loss:2.290431790883973\n",
      "train loss:2.294707820095495\n",
      "train loss:2.3096169282629964\n",
      "train loss:2.2966058876858217\n",
      "train loss:2.3013908848527365\n",
      "train loss:2.296395631801382\n",
      "train loss:2.3030694223373596\n",
      "train loss:2.3065105687384326\n",
      "train loss:2.300405905717653\n",
      "train loss:2.299702163873699\n",
      "train loss:2.3029451945575805\n",
      "train loss:2.3112663081338978\n",
      "train loss:2.3107893927791356\n",
      "train loss:2.306713067465501\n",
      "train loss:2.3004277127390305\n",
      "train loss:2.3110838158756124\n",
      "train loss:2.3004376412378074\n",
      "train loss:2.3016310907885207\n",
      "train loss:2.3084561929807257\n",
      "train loss:2.302977493670894\n",
      "train loss:2.3044990743662086\n",
      "train loss:2.3078024671371273\n",
      "train loss:2.2998276995209386\n",
      "train loss:2.3044780160611866\n",
      "train loss:2.2986714478605847\n",
      "train loss:2.2959561280053165\n",
      "train loss:2.3002384755845213\n",
      "train loss:2.299898190545737\n",
      "train loss:2.301497062715501\n",
      "train loss:2.3011372227179465\n",
      "train loss:2.300250942974912\n",
      "train loss:2.301606851900208\n",
      "train loss:2.3074057268175565\n",
      "train loss:2.3042825369219426\n",
      "train loss:2.3005994150549967\n",
      "train loss:2.2983807307321653\n",
      "train loss:2.303984768713269\n",
      "train loss:2.3036987763224066\n",
      "train loss:2.308998507593211\n",
      "train loss:2.303108885387533\n",
      "train loss:2.310981277220332\n",
      "train loss:2.3042132658135923\n",
      "train loss:2.3019968661950543\n",
      "train loss:2.3012065785843454\n",
      "train loss:2.3026408389266013\n",
      "train loss:2.303817852920735\n",
      "train loss:2.3057241499283645\n",
      "train loss:2.3033373552679968\n",
      "train loss:2.310017720042053\n",
      "train loss:2.2958864593043558\n",
      "train loss:2.303974219966632\n",
      "train loss:2.297769792136647\n",
      "train loss:2.3004323356864558\n",
      "train loss:2.303480517206098\n",
      "train loss:2.288893884511141\n",
      "train loss:2.3015878598563773\n",
      "train loss:2.303439327043372\n",
      "train loss:2.302595932927233\n",
      "train loss:2.2957102843596915\n",
      "train loss:2.302289861335946\n",
      "train loss:2.2966265215586064\n",
      "train loss:2.3032386942510614\n",
      "train loss:2.300429879866776\n",
      "train loss:2.295152102847013\n",
      "train loss:2.2894990244562274\n",
      "train loss:2.3064350023280693\n",
      "train loss:2.299031710101708\n",
      "train loss:2.3011827177175035\n",
      "train loss:2.304955062562009\n",
      "train loss:2.2949370852113247\n",
      "train loss:2.298440666542608\n",
      "train loss:2.2908022772457994\n",
      "train loss:2.307780065245658\n",
      "train loss:2.296683574364355\n",
      "train loss:2.3026716959287405\n",
      "train loss:2.3017671917436417\n",
      "train loss:2.290463456699252\n",
      "train loss:2.2987120566514543\n",
      "train loss:2.3033060692687983\n",
      "train loss:2.3030062913173395\n",
      "train loss:2.301819982237357\n",
      "train loss:2.3069985247751896\n",
      "train loss:2.3021370746343495\n",
      "train loss:2.2859091042995985\n",
      "train loss:2.309321108682136\n",
      "train loss:2.3025542863655493\n",
      "train loss:2.312943496571077\n",
      "train loss:2.3022520930575037\n",
      "train loss:2.305508646981575\n",
      "train loss:2.293804962331141\n",
      "train loss:2.3021715779874503\n",
      "train loss:2.303691343191326\n",
      "train loss:2.304460382198858\n",
      "train loss:2.3040887022967516\n",
      "train loss:2.3008387785181443\n",
      "train loss:2.304480971305717\n",
      "train loss:2.2960842781070205\n",
      "train loss:2.300119033849918\n",
      "train loss:2.302347799706654\n",
      "train loss:2.3076815985316275\n",
      "train loss:2.297448565940422\n",
      "train loss:2.30176359029984\n",
      "train loss:2.305166102206478\n",
      "train loss:2.300909191946128\n",
      "train loss:2.306655939342619\n",
      "train loss:2.300512428906064\n",
      "train loss:2.3042054855627097\n",
      "train loss:2.3001778639143535\n",
      "train loss:2.3116490620254657\n",
      "train loss:2.3026796558965543\n",
      "train loss:2.3072091192510666\n",
      "train loss:2.301954123278084\n",
      "train loss:2.305584603136872\n",
      "train loss:2.3059579330276514\n",
      "train loss:2.304514522532322\n",
      "train loss:2.302587152722713\n",
      "train loss:2.3047970988600586\n",
      "train loss:2.3039791203714834\n",
      "train loss:2.3024727202978763\n",
      "train loss:2.305029774509682\n",
      "train loss:2.2993797276778167\n",
      "train loss:2.30563030794749\n",
      "train loss:2.3007729669811083\n",
      "train loss:2.3021729968089093\n",
      "train loss:2.2987685935357076\n",
      "train loss:2.298406465155911\n",
      "train loss:2.303221177483829\n",
      "train loss:2.2963216997866707\n",
      "train loss:2.298228059468666\n",
      "train loss:2.308733166995554\n",
      "train loss:2.3021850232616052\n",
      "train loss:2.30602147328312\n",
      "train loss:2.3034494027136487\n",
      "train loss:2.3033927255825826\n",
      "train loss:2.3102498747275257\n",
      "train loss:2.3007898527178114\n",
      "train loss:2.3034722578383504\n",
      "train loss:2.3068940331460985\n",
      "train loss:2.303641247765337\n",
      "train loss:2.2995033232101685\n",
      "=== epoch:51, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.303640321448005\n",
      "train loss:2.297166901939096\n",
      "train loss:2.3077511296844704\n",
      "train loss:2.3072052325296024\n",
      "train loss:2.300081363585702\n",
      "train loss:2.299942569216981\n",
      "train loss:2.29879678919514\n",
      "train loss:2.297883715741335\n",
      "train loss:2.3049166123699667\n",
      "train loss:2.3069275988425155\n",
      "train loss:2.2935188381569658\n",
      "train loss:2.3028341919147177\n",
      "train loss:2.3010934276919794\n",
      "train loss:2.303934305520079\n",
      "train loss:2.3003393936917513\n",
      "train loss:2.296918430135338\n",
      "train loss:2.290304936024553\n",
      "train loss:2.3064548512001326\n",
      "train loss:2.2964779900433316\n",
      "train loss:2.297248765947334\n",
      "train loss:2.2969897781908326\n",
      "train loss:2.3084970321777973\n",
      "train loss:2.302722587729446\n",
      "train loss:2.310593236491787\n",
      "train loss:2.296637631112011\n",
      "train loss:2.294954005738627\n",
      "train loss:2.3056892840632406\n",
      "train loss:2.2918934772698107\n",
      "train loss:2.3059131658778\n",
      "train loss:2.2918798509359983\n",
      "train loss:2.2946319372521096\n",
      "train loss:2.306058805626211\n",
      "train loss:2.307144559204697\n",
      "train loss:2.3045194179393445\n",
      "train loss:2.296699610144151\n",
      "train loss:2.299218764732108\n",
      "train loss:2.3077916531598612\n",
      "train loss:2.2997379570278853\n",
      "train loss:2.302271567572311\n",
      "train loss:2.307262636174327\n",
      "train loss:2.307508735190776\n",
      "train loss:2.2939945932676675\n",
      "train loss:2.3049068748706243\n",
      "train loss:2.302445865365342\n",
      "train loss:2.2950736982013535\n",
      "train loss:2.307564536388879\n",
      "train loss:2.303957033357028\n",
      "train loss:2.298139126688676\n",
      "train loss:2.2966431074672653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306212016892649\n",
      "train loss:2.2936984703777186\n",
      "train loss:2.3059950008563392\n",
      "train loss:2.3024383395287624\n",
      "train loss:2.3026103458349922\n",
      "train loss:2.2949286795578905\n",
      "train loss:2.2963257108250863\n",
      "train loss:2.29623978276754\n",
      "train loss:2.299286227734682\n",
      "train loss:2.303548075061893\n",
      "train loss:2.297676509272565\n",
      "train loss:2.29655322179533\n",
      "train loss:2.3030359782082632\n",
      "train loss:2.3049154238829157\n",
      "train loss:2.3028700832675306\n",
      "train loss:2.284000038227485\n",
      "train loss:2.2964324396558706\n",
      "train loss:2.3027675044174343\n",
      "train loss:2.2974199854527306\n",
      "train loss:2.2979289483746292\n",
      "train loss:2.3022395446934487\n",
      "train loss:2.3085250881125843\n",
      "train loss:2.287899732008169\n",
      "train loss:2.2927464351786675\n",
      "train loss:2.3011098551136766\n",
      "train loss:2.2907292166789395\n",
      "train loss:2.288162377109886\n",
      "train loss:2.30040235196384\n",
      "train loss:2.3010081719743454\n",
      "train loss:2.299441568541339\n",
      "train loss:2.3008991410917234\n",
      "train loss:2.3051080565684114\n",
      "train loss:2.3037964570330987\n",
      "train loss:2.2997079512257974\n",
      "train loss:2.3088338939207547\n",
      "train loss:2.311145344342253\n",
      "train loss:2.297729274301683\n",
      "train loss:2.302570445499809\n",
      "train loss:2.309350756819966\n",
      "train loss:2.306846351239845\n",
      "train loss:2.3023674507449954\n",
      "train loss:2.302840771224986\n",
      "train loss:2.302759593756592\n",
      "train loss:2.305026166225244\n",
      "train loss:2.303687146644562\n",
      "train loss:2.303748311343875\n",
      "train loss:2.2998562308244828\n",
      "train loss:2.3006852847722286\n",
      "train loss:2.296464320300311\n",
      "train loss:2.3047814329190226\n",
      "train loss:2.3018654577604374\n",
      "train loss:2.304175494258579\n",
      "train loss:2.299851404294533\n",
      "train loss:2.299059299058831\n",
      "train loss:2.2960033927302783\n",
      "train loss:2.3023012339038837\n",
      "train loss:2.304495190067354\n",
      "train loss:2.3029069153594577\n",
      "train loss:2.2917760400354035\n",
      "train loss:2.3056787206822627\n",
      "train loss:2.2998375753618046\n",
      "train loss:2.3021727544405164\n",
      "train loss:2.297269494887476\n",
      "train loss:2.2966943237436293\n",
      "train loss:2.3041491760509767\n",
      "train loss:2.3031144967092207\n",
      "train loss:2.2969001718695874\n",
      "train loss:2.295055086141768\n",
      "train loss:2.3062277943604244\n",
      "train loss:2.30656358696853\n",
      "train loss:2.2979386582095414\n",
      "train loss:2.3095057133688996\n",
      "train loss:2.298409352405903\n",
      "train loss:2.2995757570461697\n",
      "train loss:2.2986550624183097\n",
      "train loss:2.3057736071666963\n",
      "train loss:2.295979030487082\n",
      "train loss:2.2966414191441618\n",
      "train loss:2.3032804845261787\n",
      "train loss:2.295640626227662\n",
      "train loss:2.2969246783447184\n",
      "train loss:2.298927303639055\n",
      "train loss:2.2964378761287065\n",
      "train loss:2.2926989388678205\n",
      "train loss:2.2911230119390336\n",
      "train loss:2.296016826720721\n",
      "train loss:2.297220913317719\n",
      "train loss:2.300654361244741\n",
      "train loss:2.298053448582817\n",
      "train loss:2.305739847237158\n",
      "train loss:2.3042996264654554\n",
      "train loss:2.289574276794604\n",
      "train loss:2.303184871492754\n",
      "train loss:2.292474010351063\n",
      "train loss:2.303938288400629\n",
      "train loss:2.3044599786985467\n",
      "train loss:2.2977419287277003\n",
      "train loss:2.30671507604362\n",
      "train loss:2.3052073727615023\n",
      "train loss:2.3063827105545007\n",
      "train loss:2.296485949070428\n",
      "train loss:2.3044680589667625\n",
      "train loss:2.293423289394909\n",
      "train loss:2.285237110628325\n",
      "train loss:2.3010388750500708\n",
      "train loss:2.3053595551632795\n",
      "train loss:2.302184259935604\n",
      "train loss:2.3043609777222023\n",
      "train loss:2.3037934154861155\n",
      "train loss:2.2958553951534943\n",
      "train loss:2.3026241715190934\n",
      "train loss:2.3026401793244378\n",
      "train loss:2.303703051402276\n",
      "train loss:2.2971780347605497\n",
      "train loss:2.3056995635103643\n",
      "train loss:2.2929277191144966\n",
      "train loss:2.303165927533285\n",
      "train loss:2.299930786039342\n",
      "train loss:2.3004159151955736\n",
      "train loss:2.306909389651108\n",
      "train loss:2.301786894213735\n",
      "train loss:2.291680548197779\n",
      "train loss:2.304037989603938\n",
      "train loss:2.296302136458088\n",
      "train loss:2.3002972349659725\n",
      "train loss:2.3007689227544956\n",
      "train loss:2.3059415926788387\n",
      "train loss:2.2990430051262547\n",
      "train loss:2.298986231401549\n",
      "train loss:2.3025750473966906\n",
      "train loss:2.2981827456429684\n",
      "train loss:2.3026839813472764\n",
      "train loss:2.3024160951489945\n",
      "train loss:2.296573370838533\n",
      "train loss:2.3013649012904565\n",
      "train loss:2.288622391522355\n",
      "train loss:2.3018684401168534\n",
      "train loss:2.297908718296711\n",
      "train loss:2.3046871797954074\n",
      "train loss:2.3005494788895997\n",
      "train loss:2.3060238122544092\n",
      "train loss:2.3072106348143295\n",
      "train loss:2.3095252197560243\n",
      "train loss:2.301196809514901\n",
      "train loss:2.308418898670271\n",
      "train loss:2.3059098760434016\n",
      "train loss:2.300589266099577\n",
      "train loss:2.303764232284738\n",
      "train loss:2.303814151172847\n",
      "train loss:2.3148961199491134\n",
      "train loss:2.2990929063230636\n",
      "=== epoch:52, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.300419980639013\n",
      "train loss:2.299289344070389\n",
      "train loss:2.296973300687972\n",
      "train loss:2.3051489840697665\n",
      "train loss:2.3034252654251843\n",
      "train loss:2.3029707354503266\n",
      "train loss:2.2988497604348335\n",
      "train loss:2.3053745401539394\n",
      "train loss:2.295220127778496\n",
      "train loss:2.2932942424124896\n",
      "train loss:2.305025892975843\n",
      "train loss:2.3090673490194322\n",
      "train loss:2.303498138904023\n",
      "train loss:2.3049244770825443\n",
      "train loss:2.3021165526565457\n",
      "train loss:2.2977169290377137\n",
      "train loss:2.3028738008199996\n",
      "train loss:2.2996698384681955\n",
      "train loss:2.306475312910163\n",
      "train loss:2.298991676157936\n",
      "train loss:2.3026473488343373\n",
      "train loss:2.298925779625948\n",
      "train loss:2.306220686594285\n",
      "train loss:2.2960388100081977\n",
      "train loss:2.303136149092397\n",
      "train loss:2.3016732542581035\n",
      "train loss:2.298378852493578\n",
      "train loss:2.3050104406546454\n",
      "train loss:2.3019217665030665\n",
      "train loss:2.302992745438379\n",
      "train loss:2.298091781555848\n",
      "train loss:2.3021299151636674\n",
      "train loss:2.302763976221181\n",
      "train loss:2.3069699692601504\n",
      "train loss:2.3009313024880838\n",
      "train loss:2.3043106901474952\n",
      "train loss:2.3022566332698062\n",
      "train loss:2.3008337570809254\n",
      "train loss:2.303143510497204\n",
      "train loss:2.301773330754085\n",
      "train loss:2.30384565846869\n",
      "train loss:2.304533043640868\n",
      "train loss:2.3002374947720226\n",
      "train loss:2.3005972811028106\n",
      "train loss:2.2949414025164163\n",
      "train loss:2.2984517173311128\n",
      "train loss:2.304159969462189\n",
      "train loss:2.2969395112851463\n",
      "train loss:2.301766576620821\n",
      "train loss:2.30252810274019\n",
      "train loss:2.3062161371245935\n",
      "train loss:2.2999589009933574\n",
      "train loss:2.307720931256043\n",
      "train loss:2.308490574013623\n",
      "train loss:2.297206262365931\n",
      "train loss:2.3047052736266065\n",
      "train loss:2.3004892289377525\n",
      "train loss:2.3000219183685737\n",
      "train loss:2.2968438514397853\n",
      "train loss:2.3058427556033814\n",
      "train loss:2.305545146935117\n",
      "train loss:2.304203939574495\n",
      "train loss:2.301687562100751\n",
      "train loss:2.304629928357339\n",
      "train loss:2.3003917532518674\n",
      "train loss:2.30028599134814\n",
      "train loss:2.305444284103701\n",
      "train loss:2.303165620355874\n",
      "train loss:2.3003078849803225\n",
      "train loss:2.3001227513011195\n",
      "train loss:2.3036766066803502\n",
      "train loss:2.301193333348723\n",
      "train loss:2.3028505314340664\n",
      "train loss:2.306784638083398\n",
      "train loss:2.3019555837469974\n",
      "train loss:2.3046293683014376\n",
      "train loss:2.3037264926750822\n",
      "train loss:2.3058992049747657\n",
      "train loss:2.291719930832403\n",
      "train loss:2.3036466809092913\n",
      "train loss:2.3072162908873635\n",
      "train loss:2.297187837413075\n",
      "train loss:2.2998660365573107\n",
      "train loss:2.296854730533957\n",
      "train loss:2.306873494655547\n",
      "train loss:2.3054418031241055\n",
      "train loss:2.302865924941642\n",
      "train loss:2.3025115502780302\n",
      "train loss:2.301995180842084\n",
      "train loss:2.2968798151063896\n",
      "train loss:2.298825879278132\n",
      "train loss:2.3002109196448877\n",
      "train loss:2.3077178157376332\n",
      "train loss:2.3038950591011544\n",
      "train loss:2.3066080344981486\n",
      "train loss:2.303563644135654\n",
      "train loss:2.304336730153569\n",
      "train loss:2.2944523880502627\n",
      "train loss:2.3002329257104743\n",
      "train loss:2.3033616283248604\n",
      "train loss:2.2937914789449327\n",
      "train loss:2.296679685939876\n",
      "train loss:2.3033266769278398\n",
      "train loss:2.3083137620763594\n",
      "train loss:2.3034291344233737\n",
      "train loss:2.299923981861745\n",
      "train loss:2.3004605223416568\n",
      "train loss:2.30434024250783\n",
      "train loss:2.3050957547418567\n",
      "train loss:2.3048093039699733\n",
      "train loss:2.3067410689002736\n",
      "train loss:2.3084259038673927\n",
      "train loss:2.304418759072584\n",
      "train loss:2.29811685401853\n",
      "train loss:2.2981619714992867\n",
      "train loss:2.296727126436184\n",
      "train loss:2.2953979052053177\n",
      "train loss:2.310327708942205\n",
      "train loss:2.29685550650421\n",
      "train loss:2.2969598152509514\n",
      "train loss:2.2980899789433655\n",
      "train loss:2.287968717223291\n",
      "train loss:2.2995823363551646\n",
      "train loss:2.298400980722349\n",
      "train loss:2.2978216669883205\n",
      "train loss:2.300523839235856\n",
      "train loss:2.3024322141443023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2901694735255003\n",
      "train loss:2.3023379330734364\n",
      "train loss:2.3009528966668413\n",
      "train loss:2.3042928508645484\n",
      "train loss:2.291817735562785\n",
      "train loss:2.3046727333301065\n",
      "train loss:2.298407836020384\n",
      "train loss:2.3009803049668234\n",
      "train loss:2.3000860417590476\n",
      "train loss:2.2929695841534\n",
      "train loss:2.301133683184509\n",
      "train loss:2.3030752376680943\n",
      "train loss:2.3003744523030467\n",
      "train loss:2.3107318142119535\n",
      "train loss:2.2967584148064333\n",
      "train loss:2.298202023723695\n",
      "train loss:2.298415115631889\n",
      "train loss:2.3020761178736295\n",
      "train loss:2.298338162699263\n",
      "train loss:2.30807866193936\n",
      "train loss:2.300247634020935\n",
      "train loss:2.2958476550376634\n",
      "train loss:2.3042822906385325\n",
      "train loss:2.2989589139833893\n",
      "train loss:2.3038050430996795\n",
      "train loss:2.3044127074659597\n",
      "train loss:2.299708818086781\n",
      "train loss:2.288302468941042\n",
      "train loss:2.3069028989002422\n",
      "train loss:2.298407855672995\n",
      "train loss:2.30271392313916\n",
      "train loss:2.2968488484005998\n",
      "train loss:2.3151122957701924\n",
      "train loss:2.2895737276848194\n",
      "train loss:2.3058756730141274\n",
      "train loss:2.294047658246834\n",
      "train loss:2.3093261059039762\n",
      "train loss:2.3049221131556252\n",
      "train loss:2.2975418157951624\n",
      "train loss:2.300960372799321\n",
      "train loss:2.2984528127979935\n",
      "train loss:2.290706575890733\n",
      "train loss:2.3094405969477236\n",
      "train loss:2.3014896220262924\n",
      "train loss:2.3016489683818753\n",
      "train loss:2.3053690574126517\n",
      "train loss:2.3082856532409832\n",
      "train loss:2.2931900878324223\n",
      "train loss:2.307047702180641\n",
      "train loss:2.289563130302524\n",
      "train loss:2.3002950164237173\n",
      "train loss:2.294508785831335\n",
      "train loss:2.295840995951603\n",
      "train loss:2.2988422807471545\n",
      "train loss:2.30142439371781\n",
      "train loss:2.3069301352249436\n",
      "train loss:2.3052081650959764\n",
      "train loss:2.301565887269234\n",
      "train loss:2.296773478172122\n",
      "train loss:2.304469842685493\n",
      "train loss:2.2889226721637486\n",
      "train loss:2.300895483894256\n",
      "train loss:2.3057236083230026\n",
      "train loss:2.299727430374089\n",
      "train loss:2.2927732338613644\n",
      "train loss:2.3046734756946003\n",
      "train loss:2.3024384827517728\n",
      "train loss:2.312222062046842\n",
      "train loss:2.300141543738248\n",
      "train loss:2.3131813899146927\n",
      "train loss:2.2983244590295238\n",
      "train loss:2.307495469173895\n",
      "train loss:2.2981337413528045\n",
      "=== epoch:53, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.304970394697372\n",
      "train loss:2.3027649040520672\n",
      "train loss:2.315817602462816\n",
      "train loss:2.296757461280406\n",
      "train loss:2.2968298422419338\n",
      "train loss:2.305654424130263\n",
      "train loss:2.3031554121445863\n",
      "train loss:2.2976516349138953\n",
      "train loss:2.296200183983431\n",
      "train loss:2.3045279367610743\n",
      "train loss:2.2988245124970303\n",
      "train loss:2.3017968135668365\n",
      "train loss:2.298047820168788\n",
      "train loss:2.295574439914338\n",
      "train loss:2.310750248806745\n",
      "train loss:2.3067422832110878\n",
      "train loss:2.2916988615766285\n",
      "train loss:2.3000891459006247\n",
      "train loss:2.3015515472303347\n",
      "train loss:2.299571608701635\n",
      "train loss:2.302763758205691\n",
      "train loss:2.306768490827104\n",
      "train loss:2.2971612514382254\n",
      "train loss:2.3100960229099097\n",
      "train loss:2.30206904614368\n",
      "train loss:2.296584850253491\n",
      "train loss:2.3020782212226747\n",
      "train loss:2.2973481973230965\n",
      "train loss:2.3005384118468446\n",
      "train loss:2.300169536837485\n",
      "train loss:2.300882318144176\n",
      "train loss:2.3036827003516462\n",
      "train loss:2.3059135386781646\n",
      "train loss:2.303461192036829\n",
      "train loss:2.3025189535624433\n",
      "train loss:2.3030429214421306\n",
      "train loss:2.3043737355290013\n",
      "train loss:2.3024563625877534\n",
      "train loss:2.301485452854144\n",
      "train loss:2.2996123414274945\n",
      "train loss:2.301620883353855\n",
      "train loss:2.306697880586364\n",
      "train loss:2.30519226599081\n",
      "train loss:2.300753004852624\n",
      "train loss:2.297949094230493\n",
      "train loss:2.301265056873877\n",
      "train loss:2.2983162885475283\n",
      "train loss:2.2925117193970532\n",
      "train loss:2.305255583619131\n",
      "train loss:2.2998442388487597\n",
      "train loss:2.306146713349074\n",
      "train loss:2.3002554662606256\n",
      "train loss:2.297250668405925\n",
      "train loss:2.302795805504165\n",
      "train loss:2.3085373152063537\n",
      "train loss:2.3008630234902485\n",
      "train loss:2.298343202916094\n",
      "train loss:2.3049285943715816\n",
      "train loss:2.3009906429779554\n",
      "train loss:2.3009653740072964\n",
      "train loss:2.297875353077605\n",
      "train loss:2.298091487568603\n",
      "train loss:2.2936060439133743\n",
      "train loss:2.3121936842539244\n",
      "train loss:2.303022932724786\n",
      "train loss:2.3042061434035292\n",
      "train loss:2.30289405453107\n",
      "train loss:2.301106765983553\n",
      "train loss:2.2979089234831473\n",
      "train loss:2.299462238831412\n",
      "train loss:2.3056254262376097\n",
      "train loss:2.2978749247794314\n",
      "train loss:2.2981461206212335\n",
      "train loss:2.306349884626289\n",
      "train loss:2.3045708186242435\n",
      "train loss:2.2997625570166487\n",
      "train loss:2.300516851217163\n",
      "train loss:2.308992740711594\n",
      "train loss:2.2978177731321843\n",
      "train loss:2.2976428173912393\n",
      "train loss:2.29711047705047\n",
      "train loss:2.3068722023688566\n",
      "train loss:2.3017256781035824\n",
      "train loss:2.3082071288677724\n",
      "train loss:2.300214225455984\n",
      "train loss:2.3073652313108424\n",
      "train loss:2.3037307226537855\n",
      "train loss:2.3028283095493673\n",
      "train loss:2.3017971858874833\n",
      "train loss:2.3025509385562195\n",
      "train loss:2.293816679794054\n",
      "train loss:2.3094895206350596\n",
      "train loss:2.2986863238009922\n",
      "train loss:2.2951726483011567\n",
      "train loss:2.295958748903261\n",
      "train loss:2.3053318246521903\n",
      "train loss:2.2965387483562436\n",
      "train loss:2.302652890068916\n",
      "train loss:2.3020116453161665\n",
      "train loss:2.2995704969552184\n",
      "train loss:2.3002945813000233\n",
      "train loss:2.305750590527975\n",
      "train loss:2.301561545266811\n",
      "train loss:2.299454643882262\n",
      "train loss:2.300483318511437\n",
      "train loss:2.3043685152915643\n",
      "train loss:2.3036938346157134\n",
      "train loss:2.3086647562726745\n",
      "train loss:2.308694922460496\n",
      "train loss:2.299485973336008\n",
      "train loss:2.301980073983498\n",
      "train loss:2.3025857803900314\n",
      "train loss:2.2949377023246633\n",
      "train loss:2.302363334928161\n",
      "train loss:2.2950193646785544\n",
      "train loss:2.295880649215018\n",
      "train loss:2.30488405560419\n",
      "train loss:2.294571132310739\n",
      "train loss:2.3005337910735673\n",
      "train loss:2.29755978670519\n",
      "train loss:2.3056865446281427\n",
      "train loss:2.296918309600076\n",
      "train loss:2.3003715877058926\n",
      "train loss:2.304108638929088\n",
      "train loss:2.3001374389220848\n",
      "train loss:2.299718916068514\n",
      "train loss:2.3112641730776633\n",
      "train loss:2.300893094000477\n",
      "train loss:2.305154265139053\n",
      "train loss:2.305013353287944\n",
      "train loss:2.301467780341\n",
      "train loss:2.2952100417210555\n",
      "train loss:2.304498972104333\n",
      "train loss:2.3057062473946397\n",
      "train loss:2.304437548703594\n",
      "train loss:2.2964945915609047\n",
      "train loss:2.3026740708325044\n",
      "train loss:2.3045425813586826\n",
      "train loss:2.2980942746568784\n",
      "train loss:2.3050814886692996\n",
      "train loss:2.293750626087619\n",
      "train loss:2.3053961053592467\n",
      "train loss:2.3000513141084826\n",
      "train loss:2.2962872322521846\n",
      "train loss:2.29994464285098\n",
      "train loss:2.309438421411223\n",
      "train loss:2.299078183506119\n",
      "train loss:2.2986150699460923\n",
      "train loss:2.2968761414258334\n",
      "train loss:2.302911328397887\n",
      "train loss:2.297851030213645\n",
      "train loss:2.3037179731438875\n",
      "train loss:2.292134429925742\n",
      "train loss:2.298747228522595\n",
      "train loss:2.307100139368436\n",
      "train loss:2.2980353212356532\n",
      "train loss:2.28759408409108\n",
      "train loss:2.2942877671241435\n",
      "train loss:2.3089785737152817\n",
      "train loss:2.299775129146979\n",
      "train loss:2.301680690215639\n",
      "train loss:2.304661515694622\n",
      "train loss:2.2938083979267723\n",
      "train loss:2.304263500456409\n",
      "train loss:2.3059725802928512\n",
      "train loss:2.3055796020723656\n",
      "train loss:2.2923851771689354\n",
      "train loss:2.295809771596797\n",
      "train loss:2.299778703014176\n",
      "train loss:2.304172089136178\n",
      "train loss:2.301194394136691\n",
      "train loss:2.3064103213042704\n",
      "train loss:2.301433898449893\n",
      "train loss:2.294161031519255\n",
      "train loss:2.300197930377525\n",
      "train loss:2.294100532578596\n",
      "train loss:2.301259733534729\n",
      "train loss:2.3041880125448566\n",
      "train loss:2.2996055429675177\n",
      "train loss:2.302300735981071\n",
      "train loss:2.302640549065613\n",
      "train loss:2.306948557013167\n",
      "train loss:2.298768183502766\n",
      "train loss:2.300123179353071\n",
      "train loss:2.3023386725421586\n",
      "train loss:2.298218131244187\n",
      "train loss:2.30215002134186\n",
      "train loss:2.297906119349299\n",
      "train loss:2.2993837292070034\n",
      "train loss:2.2939998171537375\n",
      "train loss:2.304868897547053\n",
      "train loss:2.30483153633412\n",
      "train loss:2.3032828347048824\n",
      "train loss:2.2969969153735446\n",
      "train loss:2.302571063461566\n",
      "train loss:2.308674529855145\n",
      "train loss:2.308180358587953\n",
      "train loss:2.288489416777096\n",
      "train loss:2.3085169258227096\n",
      "train loss:2.3002464872553983\n",
      "=== epoch:54, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308388936138723\n",
      "train loss:2.2949701112587273\n",
      "train loss:2.3097723016168925\n",
      "train loss:2.3040218390468916\n",
      "train loss:2.299472676626076\n",
      "train loss:2.299407392671611\n",
      "train loss:2.3055447106395874\n",
      "train loss:2.308031034434047\n",
      "train loss:2.298523463887142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.303640654769156\n",
      "train loss:2.303143378277493\n",
      "train loss:2.3031918435269425\n",
      "train loss:2.2966328382495806\n",
      "train loss:2.305531204115561\n",
      "train loss:2.3039358298623984\n",
      "train loss:2.296283846569784\n",
      "train loss:2.3026058542302597\n",
      "train loss:2.301136903519023\n",
      "train loss:2.305608510983841\n",
      "train loss:2.3091811963513824\n",
      "train loss:2.2966446651247443\n",
      "train loss:2.303737062669936\n",
      "train loss:2.3008613022207616\n",
      "train loss:2.300208293413292\n",
      "train loss:2.297738269839232\n",
      "train loss:2.3004915011023304\n",
      "train loss:2.2977661612413387\n",
      "train loss:2.294235830275639\n",
      "train loss:2.301029137661553\n",
      "train loss:2.2947075884706374\n",
      "train loss:2.2943316539372898\n",
      "train loss:2.308168003787143\n",
      "train loss:2.307372181758487\n",
      "train loss:2.302380929659591\n",
      "train loss:2.300304709794001\n",
      "train loss:2.305155364520579\n",
      "train loss:2.3029351237179356\n",
      "train loss:2.306379588864672\n",
      "train loss:2.2997329265798236\n",
      "train loss:2.3005340031881647\n",
      "train loss:2.296580723035662\n",
      "train loss:2.2997054277825733\n",
      "train loss:2.302775987177831\n",
      "train loss:2.3000113383513794\n",
      "train loss:2.296102144029925\n",
      "train loss:2.2966749959422113\n",
      "train loss:2.2971326415755606\n",
      "train loss:2.297098151270243\n",
      "train loss:2.307917862829269\n",
      "train loss:2.2981468742126427\n",
      "train loss:2.297604538076745\n",
      "train loss:2.308257946530475\n",
      "train loss:2.3010516707326842\n",
      "train loss:2.3013630062958255\n",
      "train loss:2.299256946834406\n",
      "train loss:2.287213547433013\n",
      "train loss:2.3034029849688014\n",
      "train loss:2.3060523851789276\n",
      "train loss:2.301980573017385\n",
      "train loss:2.301762996309639\n",
      "train loss:2.2991310048988183\n",
      "train loss:2.2958851975346115\n",
      "train loss:2.2985777318793685\n",
      "train loss:2.304969748733919\n",
      "train loss:2.301648855350847\n",
      "train loss:2.302640219398008\n",
      "train loss:2.311580582150515\n",
      "train loss:2.304610442317431\n",
      "train loss:2.302839133062846\n",
      "train loss:2.2930439439003725\n",
      "train loss:2.2953500972025633\n",
      "train loss:2.2941894194330215\n",
      "train loss:2.2923946577333036\n",
      "train loss:2.2978462455884414\n",
      "train loss:2.2950955471241326\n",
      "train loss:2.299249153936588\n",
      "train loss:2.298818203953884\n",
      "train loss:2.293407125539681\n",
      "train loss:2.298511314853803\n",
      "train loss:2.3084980297590105\n",
      "train loss:2.3033987528863085\n",
      "train loss:2.307930956966721\n",
      "train loss:2.309068678134517\n",
      "train loss:2.3033819699488993\n",
      "train loss:2.306624861231161\n",
      "train loss:2.3034499844735614\n",
      "train loss:2.3014227795671762\n",
      "train loss:2.2961600922707066\n",
      "train loss:2.2971317395581847\n",
      "train loss:2.291768940747239\n",
      "train loss:2.3003890918323626\n",
      "train loss:2.3046589812693514\n",
      "train loss:2.2943984345052324\n",
      "train loss:2.294704026425545\n",
      "train loss:2.3045650400849187\n",
      "train loss:2.30111399963575\n",
      "train loss:2.3024478970820077\n",
      "train loss:2.3048921413633128\n",
      "train loss:2.2979543872639328\n",
      "train loss:2.3064907963597103\n",
      "train loss:2.3134922943693317\n",
      "train loss:2.2951704135432327\n",
      "train loss:2.2999384900445805\n",
      "train loss:2.3024441339165587\n",
      "train loss:2.3076296707064805\n",
      "train loss:2.2985749849408745\n",
      "train loss:2.3040912456699396\n",
      "train loss:2.302509296639005\n",
      "train loss:2.2965225080190406\n",
      "train loss:2.3002029734966727\n",
      "train loss:2.298224151499615\n",
      "train loss:2.298543886534184\n",
      "train loss:2.305494653674137\n",
      "train loss:2.296289583841442\n",
      "train loss:2.303839740307246\n",
      "train loss:2.297475724459997\n",
      "train loss:2.3067979272767833\n",
      "train loss:2.3087846130004728\n",
      "train loss:2.3047591152938716\n",
      "train loss:2.300304445579549\n",
      "train loss:2.298592250223099\n",
      "train loss:2.292956664269422\n",
      "train loss:2.3053809676685275\n",
      "train loss:2.2994705833543674\n",
      "train loss:2.3051701484062987\n",
      "train loss:2.3032926441630446\n",
      "train loss:2.299992525682001\n",
      "train loss:2.285079800949942\n",
      "train loss:2.2960589456146145\n",
      "train loss:2.296862295823515\n",
      "train loss:2.2999519810790447\n",
      "train loss:2.296418600488837\n",
      "train loss:2.3052583120619\n",
      "train loss:2.305263712111719\n",
      "train loss:2.2991107646520534\n",
      "train loss:2.305769897854274\n",
      "train loss:2.3050971599643453\n",
      "train loss:2.300781474305736\n",
      "train loss:2.308724740674831\n",
      "train loss:2.3003782482158077\n",
      "train loss:2.2926733966228827\n",
      "train loss:2.292978943887805\n",
      "train loss:2.3068918676250374\n",
      "train loss:2.302468968293326\n",
      "train loss:2.2950408802857516\n",
      "train loss:2.307647420760036\n",
      "train loss:2.3002215527875505\n",
      "train loss:2.3026160085006024\n",
      "train loss:2.297171494441046\n",
      "train loss:2.3003076298420755\n",
      "train loss:2.2960408544073623\n",
      "train loss:2.3000795746773806\n",
      "train loss:2.310491372124921\n",
      "train loss:2.306340890334148\n",
      "train loss:2.2970180366250337\n",
      "train loss:2.3000504577279575\n",
      "train loss:2.301076614457816\n",
      "train loss:2.295539293880949\n",
      "train loss:2.315148038937888\n",
      "train loss:2.302100252801554\n",
      "train loss:2.305429349060824\n",
      "train loss:2.3003707205058403\n",
      "train loss:2.3059047896889635\n",
      "train loss:2.296832776686128\n",
      "train loss:2.3058472126089096\n",
      "train loss:2.3035269491059425\n",
      "train loss:2.304697495778739\n",
      "train loss:2.302888568271576\n",
      "train loss:2.304127113331966\n",
      "train loss:2.2938413491734546\n",
      "train loss:2.308857064429155\n",
      "train loss:2.3036627058826564\n",
      "train loss:2.2983014998977995\n",
      "train loss:2.301539900384423\n",
      "train loss:2.3033031074320895\n",
      "train loss:2.3046783321449062\n",
      "train loss:2.2986698298047474\n",
      "train loss:2.2982220669024804\n",
      "train loss:2.295080928046252\n",
      "train loss:2.3006211752281036\n",
      "train loss:2.2986636542285064\n",
      "train loss:2.2980239187550757\n",
      "train loss:2.294790544183674\n",
      "train loss:2.308532534429135\n",
      "train loss:2.30480632102038\n",
      "train loss:2.3036138900904377\n",
      "train loss:2.3065456730213514\n",
      "train loss:2.301471631828825\n",
      "train loss:2.3011721708728152\n",
      "train loss:2.3006891695008473\n",
      "train loss:2.2992519835023084\n",
      "train loss:2.304928158784046\n",
      "train loss:2.3015299662768394\n",
      "train loss:2.296544975190184\n",
      "train loss:2.3043420641049073\n",
      "train loss:2.3023158662217624\n",
      "train loss:2.297925496283837\n",
      "train loss:2.3031005501074455\n",
      "train loss:2.302531483078183\n",
      "train loss:2.3049375633265345\n",
      "=== epoch:55, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2974169937524587\n",
      "train loss:2.2988985359965364\n",
      "train loss:2.2967696306405974\n",
      "train loss:2.293063479275728\n",
      "train loss:2.295344587369558\n",
      "train loss:2.3035780263003875\n",
      "train loss:2.3000203456801547\n",
      "train loss:2.3018695715126416\n",
      "train loss:2.3001119337942026\n",
      "train loss:2.2981534950116735\n",
      "train loss:2.303717735160055\n",
      "train loss:2.3058816105907556\n",
      "train loss:2.2993676722736724\n",
      "train loss:2.3017190570120003\n",
      "train loss:2.303602217264897\n",
      "train loss:2.2968159648269557\n",
      "train loss:2.2898606459370097\n",
      "train loss:2.3020546636759978\n",
      "train loss:2.296557153570909\n",
      "train loss:2.2967263627206638\n",
      "train loss:2.3050911068017297\n",
      "train loss:2.2933632473372465\n",
      "train loss:2.300523453529075\n",
      "train loss:2.3002992685468158\n",
      "train loss:2.2961288490801453\n",
      "train loss:2.2951360228619375\n",
      "train loss:2.2998460591536185\n",
      "train loss:2.2968444216058597\n",
      "train loss:2.307395358045332\n",
      "train loss:2.302830806711174\n",
      "train loss:2.3042675387370664\n",
      "train loss:2.2998335042173883\n",
      "train loss:2.3083355830630627\n",
      "train loss:2.3075827972794123\n",
      "train loss:2.3010046660009325\n",
      "train loss:2.294255759228556\n",
      "train loss:2.301990656182251\n",
      "train loss:2.302639782447332\n",
      "train loss:2.3025829377024025\n",
      "train loss:2.3012177245418517\n",
      "train loss:2.2966294603478046\n",
      "train loss:2.2924270116222467\n",
      "train loss:2.292217996793596\n",
      "train loss:2.2982896342081416\n",
      "train loss:2.2882301150508013\n",
      "train loss:2.306477245433905\n",
      "train loss:2.2951925779308398\n",
      "train loss:2.3159467177147692\n",
      "train loss:2.2992172738983654\n",
      "train loss:2.299540762454289\n",
      "train loss:2.304706166435562\n",
      "train loss:2.3034604413651536\n",
      "train loss:2.3054602276003746\n",
      "train loss:2.3024621213011085\n",
      "train loss:2.2986162785906297\n",
      "train loss:2.307502443870153\n",
      "train loss:2.309503480584673\n",
      "train loss:2.303272245979895\n",
      "train loss:2.298703034887144\n",
      "train loss:2.291238257659545\n",
      "train loss:2.3106199730653514\n",
      "train loss:2.2992750777470263\n",
      "train loss:2.2994201057964614\n",
      "train loss:2.3020067288120205\n",
      "train loss:2.30653959627997\n",
      "train loss:2.3090606174216086\n",
      "train loss:2.299741276503779\n",
      "train loss:2.3025384725112357\n",
      "train loss:2.2985957865567013\n",
      "train loss:2.295533440511387\n",
      "train loss:2.2986473226423314\n",
      "train loss:2.30676361779707\n",
      "train loss:2.30305573654694\n",
      "train loss:2.3020270575970265\n",
      "train loss:2.2979058249815374\n",
      "train loss:2.3044737288863604\n",
      "train loss:2.3106681344159723\n",
      "train loss:2.3029857107724645\n",
      "train loss:2.3013267790825456\n",
      "train loss:2.305723133750882\n",
      "train loss:2.2911216169390465\n",
      "train loss:2.307027389689732\n",
      "train loss:2.301940728166076\n",
      "train loss:2.299408587833077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3022503666306204\n",
      "train loss:2.3034102963541967\n",
      "train loss:2.298469238653865\n",
      "train loss:2.3039477158596045\n",
      "train loss:2.2959269245928082\n",
      "train loss:2.2974676394766576\n",
      "train loss:2.306628664521722\n",
      "train loss:2.292327422410269\n",
      "train loss:2.3009640402606393\n",
      "train loss:2.294756275605966\n",
      "train loss:2.3029599932047327\n",
      "train loss:2.300475019269068\n",
      "train loss:2.305703450056537\n",
      "train loss:2.3055484123080543\n",
      "train loss:2.3040211391118968\n",
      "train loss:2.29944517813375\n",
      "train loss:2.297746397987517\n",
      "train loss:2.3032931899349096\n",
      "train loss:2.298739674515074\n",
      "train loss:2.3060541473856513\n",
      "train loss:2.30183282886182\n",
      "train loss:2.299902468639384\n",
      "train loss:2.298272495658315\n",
      "train loss:2.2971930718454106\n",
      "train loss:2.3032179465990352\n",
      "train loss:2.2919851798378237\n",
      "train loss:2.304477253187235\n",
      "train loss:2.2945853512580165\n",
      "train loss:2.3093342602248805\n",
      "train loss:2.299596311445928\n",
      "train loss:2.299037055074964\n",
      "train loss:2.310650521735512\n",
      "train loss:2.307584127345874\n",
      "train loss:2.300860451754101\n",
      "train loss:2.297771568461057\n",
      "train loss:2.2974883759063998\n",
      "train loss:2.2935545766602976\n",
      "train loss:2.3003933065763222\n",
      "train loss:2.301251259903381\n",
      "train loss:2.2987827010994284\n",
      "train loss:2.307433050149431\n",
      "train loss:2.3008145890368468\n",
      "train loss:2.2968002975922515\n",
      "train loss:2.296707692017815\n",
      "train loss:2.3004132601863017\n",
      "train loss:2.3009366537976574\n",
      "train loss:2.3074617793165837\n",
      "train loss:2.3035086088749157\n",
      "train loss:2.303771703540051\n",
      "train loss:2.288621600176659\n",
      "train loss:2.3054921217201434\n",
      "train loss:2.304503436555061\n",
      "train loss:2.300674158485516\n",
      "train loss:2.3018755351934663\n",
      "train loss:2.2925010440123246\n",
      "train loss:2.297098640941767\n",
      "train loss:2.2993438792490104\n",
      "train loss:2.303806144408207\n",
      "train loss:2.2997960439041534\n",
      "train loss:2.2932794736629103\n",
      "train loss:2.3109669317817936\n",
      "train loss:2.295416840005898\n",
      "train loss:2.3070658505211994\n",
      "train loss:2.3016782191307685\n",
      "train loss:2.3031861127041426\n",
      "train loss:2.3050888493035426\n",
      "train loss:2.296243394637087\n",
      "train loss:2.298156552786158\n",
      "train loss:2.300817023263535\n",
      "train loss:2.3078957600946124\n",
      "train loss:2.3090442750008267\n",
      "train loss:2.3059356003697893\n",
      "train loss:2.2990174045222447\n",
      "train loss:2.308821919980875\n",
      "train loss:2.3015606189084648\n",
      "train loss:2.3061617918297532\n",
      "train loss:2.304089287412282\n",
      "train loss:2.3046520077724684\n",
      "train loss:2.29653672984576\n",
      "train loss:2.2972914458072786\n",
      "train loss:2.2981292849533768\n",
      "train loss:2.2976165078134065\n",
      "train loss:2.3020187194370387\n",
      "train loss:2.306601210709863\n",
      "train loss:2.303655680480914\n",
      "train loss:2.303578365723333\n",
      "train loss:2.304045976421172\n",
      "train loss:2.300958172694203\n",
      "train loss:2.3001706431974482\n",
      "train loss:2.304978331544145\n",
      "train loss:2.296682949003194\n",
      "train loss:2.3061204874013197\n",
      "train loss:2.295073476744857\n",
      "train loss:2.3011161488093688\n",
      "train loss:2.302439281204828\n",
      "train loss:2.301315359038422\n",
      "train loss:2.299308273399161\n",
      "train loss:2.297495059573896\n",
      "train loss:2.2990874095425684\n",
      "train loss:2.30378756201947\n",
      "train loss:2.309553196125715\n",
      "train loss:2.2960326293044893\n",
      "train loss:2.304630607126618\n",
      "train loss:2.3062447895097953\n",
      "train loss:2.302373938712026\n",
      "train loss:2.3010822504027098\n",
      "train loss:2.3124989694912306\n",
      "train loss:2.3090381011637686\n",
      "train loss:2.305245830754331\n",
      "train loss:2.312925285099695\n",
      "train loss:2.301906851041548\n",
      "train loss:2.30381987890782\n",
      "train loss:2.2996016483548543\n",
      "train loss:2.301626211421521\n",
      "train loss:2.2963657716811317\n",
      "train loss:2.301645004459771\n",
      "=== epoch:56, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308838352210464\n",
      "train loss:2.2924384604643953\n",
      "train loss:2.303944613211797\n",
      "train loss:2.3016040904144526\n",
      "train loss:2.2957460189128107\n",
      "train loss:2.2973380321923482\n",
      "train loss:2.2986035918578325\n",
      "train loss:2.297561844888542\n",
      "train loss:2.3024789479559957\n",
      "train loss:2.308412500148203\n",
      "train loss:2.299552975016887\n",
      "train loss:2.2955098284208715\n",
      "train loss:2.2985557785342956\n",
      "train loss:2.301177006998137\n",
      "train loss:2.3033792068580388\n",
      "train loss:2.2979321298843365\n",
      "train loss:2.3047483128067645\n",
      "train loss:2.3058891490315276\n",
      "train loss:2.2963058645697285\n",
      "train loss:2.3052585204120737\n",
      "train loss:2.301887449146265\n",
      "train loss:2.2962959574132236\n",
      "train loss:2.3097622914674756\n",
      "train loss:2.3023484340850313\n",
      "train loss:2.301223342799024\n",
      "train loss:2.3026097971327144\n",
      "train loss:2.2983729566050237\n",
      "train loss:2.305776630192918\n",
      "train loss:2.2936810050366447\n",
      "train loss:2.3000849643111216\n",
      "train loss:2.2965461019467175\n",
      "train loss:2.3128193635023626\n",
      "train loss:2.3117298277006233\n",
      "train loss:2.2970476917883165\n",
      "train loss:2.2948729250267896\n",
      "train loss:2.303312868990625\n",
      "train loss:2.300013091746964\n",
      "train loss:2.2982375409698172\n",
      "train loss:2.297097170599899\n",
      "train loss:2.2970624063823366\n",
      "train loss:2.3027397809758043\n",
      "train loss:2.3033666391451693\n",
      "train loss:2.3034777995582565\n",
      "train loss:2.306645217154419\n",
      "train loss:2.2979043764873603\n",
      "train loss:2.304567746300089\n",
      "train loss:2.310385811292216\n",
      "train loss:2.298787547261572\n",
      "train loss:2.3005762362148214\n",
      "train loss:2.307064264909472\n",
      "train loss:2.300744274957967\n",
      "train loss:2.3021657221773233\n",
      "train loss:2.3032386458797123\n",
      "train loss:2.3038831397460533\n",
      "train loss:2.301308416110172\n",
      "train loss:2.3049336890618286\n",
      "train loss:2.301076398207715\n",
      "train loss:2.302307341310127\n",
      "train loss:2.308690774353622\n",
      "train loss:2.299200058035406\n",
      "train loss:2.2989561589764467\n",
      "train loss:2.305816152335686\n",
      "train loss:2.2944622808485353\n",
      "train loss:2.3036717401233506\n",
      "train loss:2.295746675807224\n",
      "train loss:2.3064425694619484\n",
      "train loss:2.2978219878476334\n",
      "train loss:2.30275491255862\n",
      "train loss:2.3013357650928516\n",
      "train loss:2.3020703923478467\n",
      "train loss:2.2988900897213678\n",
      "train loss:2.3053029793064357\n",
      "train loss:2.302207959387436\n",
      "train loss:2.3116910163819404\n",
      "train loss:2.3032808104117537\n",
      "train loss:2.3024671371783034\n",
      "train loss:2.295601267142878\n",
      "train loss:2.315761742417486\n",
      "train loss:2.2967991886835657\n",
      "train loss:2.3039377257385665\n",
      "train loss:2.298652835261343\n",
      "train loss:2.3067885754562334\n",
      "train loss:2.3023594624970407\n",
      "train loss:2.297834710377643\n",
      "train loss:2.2961390105188175\n",
      "train loss:2.3071862623016695\n",
      "train loss:2.307415213854428\n",
      "train loss:2.3013388959948498\n",
      "train loss:2.3038256403774526\n",
      "train loss:2.3015740876154793\n",
      "train loss:2.3027119837480496\n",
      "train loss:2.309581857963999\n",
      "train loss:2.3083761984224282\n",
      "train loss:2.297976806247737\n",
      "train loss:2.306112578002376\n",
      "train loss:2.299530361743191\n",
      "train loss:2.2994574284190183\n",
      "train loss:2.297175217688844\n",
      "train loss:2.3050659446459996\n",
      "train loss:2.301760474385933\n",
      "train loss:2.300481913093896\n",
      "train loss:2.303812440722355\n",
      "train loss:2.2978486296526226\n",
      "train loss:2.2995173241655085\n",
      "train loss:2.3077362187872503\n",
      "train loss:2.296972583376636\n",
      "train loss:2.3086143352377535\n",
      "train loss:2.3044800285473976\n",
      "train loss:2.3109203909563614\n",
      "train loss:2.298749559130459\n",
      "train loss:2.303283317418492\n",
      "train loss:2.299105758310638\n",
      "train loss:2.300144880462844\n",
      "train loss:2.300527810496847\n",
      "train loss:2.307323164227028\n",
      "train loss:2.301116403043459\n",
      "train loss:2.294960111938906\n",
      "train loss:2.304615221321379\n",
      "train loss:2.2955910081602933\n",
      "train loss:2.304819367545154\n",
      "train loss:2.3054498679283135\n",
      "train loss:2.2965940518036767\n",
      "train loss:2.3091445700132915\n",
      "train loss:2.3066990489308252\n",
      "train loss:2.3039045474267335\n",
      "train loss:2.2989483061296396\n",
      "train loss:2.2991230440234776\n",
      "train loss:2.3083702706860243\n",
      "train loss:2.2996638766406505\n",
      "train loss:2.3039632487083948\n",
      "train loss:2.300197256389155\n",
      "train loss:2.3018864524357303\n",
      "train loss:2.3023034730303324\n",
      "train loss:2.299683655922937\n",
      "train loss:2.2989769109018856\n",
      "train loss:2.3011003999016575\n",
      "train loss:2.2951115162466036\n",
      "train loss:2.311131951213556\n",
      "train loss:2.299176070510745\n",
      "train loss:2.3002772910904197\n",
      "train loss:2.3097890554561133\n",
      "train loss:2.2950314611872695\n",
      "train loss:2.2959925559234464\n",
      "train loss:2.301372293318698\n",
      "train loss:2.2981511637892216\n",
      "train loss:2.294542865383446\n",
      "train loss:2.3035414117788813\n",
      "train loss:2.2979897890303937\n",
      "train loss:2.299519667917245\n",
      "train loss:2.297626553491872\n",
      "train loss:2.3019687356267893\n",
      "train loss:2.309511236703584\n",
      "train loss:2.3006936460502643\n",
      "train loss:2.304149004177436\n",
      "train loss:2.300063105813514\n",
      "train loss:2.298054106850827\n",
      "train loss:2.2941445510240586\n",
      "train loss:2.3057244414216034\n",
      "train loss:2.2992406628344817\n",
      "train loss:2.3007203628405866\n",
      "train loss:2.3037158402034326\n",
      "train loss:2.2999958972252177\n",
      "train loss:2.3078697956103853\n",
      "train loss:2.2994926938191407\n",
      "train loss:2.303932581292107\n",
      "train loss:2.2906517288254338\n",
      "train loss:2.295778540102118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.297348617203397\n",
      "train loss:2.3036898255157054\n",
      "train loss:2.3053131706392467\n",
      "train loss:2.2938254588245\n",
      "train loss:2.3050144113910953\n",
      "train loss:2.298104009541181\n",
      "train loss:2.303639069185558\n",
      "train loss:2.299003861927994\n",
      "train loss:2.307520315269691\n",
      "train loss:2.296823062660129\n",
      "train loss:2.306516000455613\n",
      "train loss:2.3098900852043776\n",
      "train loss:2.307843222127351\n",
      "train loss:2.2982913364555086\n",
      "train loss:2.3045581153217087\n",
      "train loss:2.304444041184944\n",
      "train loss:2.2973583879915127\n",
      "train loss:2.2948680941779\n",
      "train loss:2.305332966661559\n",
      "train loss:2.3010380094833\n",
      "train loss:2.2975745250921342\n",
      "train loss:2.3047218646272114\n",
      "train loss:2.301447458316829\n",
      "train loss:2.303138839158274\n",
      "train loss:2.3106927308640235\n",
      "train loss:2.3069984237056094\n",
      "train loss:2.29622502355736\n",
      "train loss:2.3000603948497913\n",
      "train loss:2.301996433558089\n",
      "train loss:2.2948588246804347\n",
      "train loss:2.2995444768742805\n",
      "train loss:2.301074821295632\n",
      "train loss:2.2927231896993816\n",
      "=== epoch:57, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.309519980803361\n",
      "train loss:2.3027560883761287\n",
      "train loss:2.294092566462261\n",
      "train loss:2.303262089963122\n",
      "train loss:2.300419383394503\n",
      "train loss:2.3008565909866374\n",
      "train loss:2.3045673800429616\n",
      "train loss:2.3079755429584687\n",
      "train loss:2.2900110358068173\n",
      "train loss:2.299887497362071\n",
      "train loss:2.3063865127693006\n",
      "train loss:2.2984478296784485\n",
      "train loss:2.3013912881645817\n",
      "train loss:2.3051077530487856\n",
      "train loss:2.307892504166976\n",
      "train loss:2.293694677638961\n",
      "train loss:2.296801689158368\n",
      "train loss:2.2950567483020072\n",
      "train loss:2.299532824812282\n",
      "train loss:2.28599832199942\n",
      "train loss:2.301195857320546\n",
      "train loss:2.3042639670430236\n",
      "train loss:2.2921055997870505\n",
      "train loss:2.2995926030054017\n",
      "train loss:2.3020562573805257\n",
      "train loss:2.294672627662835\n",
      "train loss:2.2984537301374597\n",
      "train loss:2.2921803540595977\n",
      "train loss:2.303902039534491\n",
      "train loss:2.3195875814147078\n",
      "train loss:2.312088236796442\n",
      "train loss:2.3010173322923015\n",
      "train loss:2.2979111253039597\n",
      "train loss:2.2961652651106865\n",
      "train loss:2.298437537592287\n",
      "train loss:2.2983162371972754\n",
      "train loss:2.300595590866301\n",
      "train loss:2.3004486396532693\n",
      "train loss:2.3114601306463887\n",
      "train loss:2.3064037093036402\n",
      "train loss:2.3008137719742128\n",
      "train loss:2.2948522488133443\n",
      "train loss:2.300039359561632\n",
      "train loss:2.3033087232670257\n",
      "train loss:2.3024393755680377\n",
      "train loss:2.295510984092386\n",
      "train loss:2.299408342957781\n",
      "train loss:2.302405357791611\n",
      "train loss:2.3089125440833764\n",
      "train loss:2.294489046430549\n",
      "train loss:2.3005241222748674\n",
      "train loss:2.3103153755560077\n",
      "train loss:2.302441334011147\n",
      "train loss:2.30017270773017\n",
      "train loss:2.293863766287209\n",
      "train loss:2.295411913903302\n",
      "train loss:2.298559350761038\n",
      "train loss:2.3024205245956653\n",
      "train loss:2.300091909713398\n",
      "train loss:2.2998231291072497\n",
      "train loss:2.2994345590974294\n",
      "train loss:2.295588625638722\n",
      "train loss:2.3081492237534853\n",
      "train loss:2.3042194747450995\n",
      "train loss:2.3067632197639503\n",
      "train loss:2.301964451837535\n",
      "train loss:2.296454967281261\n",
      "train loss:2.2976927456151914\n",
      "train loss:2.294135460374312\n",
      "train loss:2.306124882094792\n",
      "train loss:2.2941391242762275\n",
      "train loss:2.304671627186089\n",
      "train loss:2.303892048953363\n",
      "train loss:2.3085936335020905\n",
      "train loss:2.301847242561911\n",
      "train loss:2.305972113146677\n",
      "train loss:2.286560238342239\n",
      "train loss:2.302527631477235\n",
      "train loss:2.305502293689682\n",
      "train loss:2.302186704981255\n",
      "train loss:2.2995811330609004\n",
      "train loss:2.298069306157844\n",
      "train loss:2.301479973386827\n",
      "train loss:2.2998459891012146\n",
      "train loss:2.294143048458241\n",
      "train loss:2.3001666457228955\n",
      "train loss:2.293105999718336\n",
      "train loss:2.298910216908885\n",
      "train loss:2.305679089163618\n",
      "train loss:2.3065367163878028\n",
      "train loss:2.300598980078751\n",
      "train loss:2.3081078445411154\n",
      "train loss:2.2938596443719717\n",
      "train loss:2.3033618381675924\n",
      "train loss:2.301912715158209\n",
      "train loss:2.3032521365313676\n",
      "train loss:2.29501678334863\n",
      "train loss:2.3004614764698044\n",
      "train loss:2.2926017615875254\n",
      "train loss:2.305582614531769\n",
      "train loss:2.2996771209492786\n",
      "train loss:2.30020772396222\n",
      "train loss:2.3051242625172725\n",
      "train loss:2.293460475523709\n",
      "train loss:2.298083888024823\n",
      "train loss:2.3055801028290666\n",
      "train loss:2.296161364443366\n",
      "train loss:2.3016486755926633\n",
      "train loss:2.296505862336401\n",
      "train loss:2.3001848489488506\n",
      "train loss:2.29781648447383\n",
      "train loss:2.3007753749777904\n",
      "train loss:2.3035930666588107\n",
      "train loss:2.305286851925498\n",
      "train loss:2.3117844131169183\n",
      "train loss:2.298883199236686\n",
      "train loss:2.3048345689860033\n",
      "train loss:2.307864124847853\n",
      "train loss:2.301819385541549\n",
      "train loss:2.303337430821882\n",
      "train loss:2.3004956517523145\n",
      "train loss:2.3012451709747013\n",
      "train loss:2.3035793778736102\n",
      "train loss:2.293798367740356\n",
      "train loss:2.2994459197138695\n",
      "train loss:2.3095179703039763\n",
      "train loss:2.3033185667975475\n",
      "train loss:2.2977988315608333\n",
      "train loss:2.301624977572289\n",
      "train loss:2.298301473090739\n",
      "train loss:2.308111648952675\n",
      "train loss:2.2975720077214614\n",
      "train loss:2.2957001942886692\n",
      "train loss:2.2991518249326686\n",
      "train loss:2.2921730134830147\n",
      "train loss:2.2998234946036478\n",
      "train loss:2.3090664749971275\n",
      "train loss:2.292295304794331\n",
      "train loss:2.3030000960211505\n",
      "train loss:2.2956684067742024\n",
      "train loss:2.305601109743404\n",
      "train loss:2.301396465833541\n",
      "train loss:2.302964790060494\n",
      "train loss:2.3037983292145388\n",
      "train loss:2.296937795584358\n",
      "train loss:2.299043434324697\n",
      "train loss:2.30062848452728\n",
      "train loss:2.300645814157645\n",
      "train loss:2.3039525674367565\n",
      "train loss:2.3020451096841024\n",
      "train loss:2.297388215172199\n",
      "train loss:2.301788653146064\n",
      "train loss:2.2999888187394535\n",
      "train loss:2.299992482530321\n",
      "train loss:2.3057170873222987\n",
      "train loss:2.3011101941785808\n",
      "train loss:2.289876817835014\n",
      "train loss:2.306659269505645\n",
      "train loss:2.3069777170436434\n",
      "train loss:2.3029197111357314\n",
      "train loss:2.299899443129208\n",
      "train loss:2.297569328164598\n",
      "train loss:2.30161929788671\n",
      "train loss:2.300148744322402\n",
      "train loss:2.3003292975890828\n",
      "train loss:2.299057108487317\n",
      "train loss:2.2852686448962665\n",
      "train loss:2.3040205441707187\n",
      "train loss:2.3056207946667984\n",
      "train loss:2.302542728130724\n",
      "train loss:2.306689328948853\n",
      "train loss:2.3006764689714747\n",
      "train loss:2.299925051906782\n",
      "train loss:2.3003968315642784\n",
      "train loss:2.30422234187336\n",
      "train loss:2.305122591688139\n",
      "train loss:2.301906992876609\n",
      "train loss:2.3068534064521677\n",
      "train loss:2.305679784760039\n",
      "train loss:2.3014185617310274\n",
      "train loss:2.3016032188336673\n",
      "train loss:2.2996768874076734\n",
      "train loss:2.305513656283458\n",
      "train loss:2.3034234579911126\n",
      "train loss:2.2998411536909478\n",
      "train loss:2.3002650605312827\n",
      "train loss:2.3092875506387456\n",
      "train loss:2.296631676998962\n",
      "train loss:2.3042412519272792\n",
      "train loss:2.2968144713878975\n",
      "train loss:2.292594105591006\n",
      "train loss:2.3002275397457836\n",
      "train loss:2.2985941969279606\n",
      "train loss:2.2926366488428456\n",
      "train loss:2.2930629379081062\n",
      "train loss:2.299988376853225\n",
      "train loss:2.307444907271558\n",
      "train loss:2.3035099110768114\n",
      "train loss:2.3035636834047954\n",
      "train loss:2.2967361689669836\n",
      "=== epoch:58, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2949173582321665\n",
      "train loss:2.296454718034386\n",
      "train loss:2.300783554670376\n",
      "train loss:2.3043665356818663\n",
      "train loss:2.2988733495085585\n",
      "train loss:2.3011936272948414\n",
      "train loss:2.308761115544696\n",
      "train loss:2.294634155271263\n",
      "train loss:2.3038037581174278\n",
      "train loss:2.2987809614156416\n",
      "train loss:2.2985576386992985\n",
      "train loss:2.294847164408863\n",
      "train loss:2.3115679261355377\n",
      "train loss:2.2967091919360136\n",
      "train loss:2.298648263867367\n",
      "train loss:2.302878210444652\n",
      "train loss:2.2973873374315033\n",
      "train loss:2.3074300182876017\n",
      "train loss:2.2979643944567276\n",
      "train loss:2.2994103621656783\n",
      "train loss:2.299658286878674\n",
      "train loss:2.3039805209021647\n",
      "train loss:2.3000453988478116\n",
      "train loss:2.3047306177146076\n",
      "train loss:2.304300089660633\n",
      "train loss:2.2968841923290215\n",
      "train loss:2.3040965310596406\n",
      "train loss:2.2951833086823314\n",
      "train loss:2.3015547341593083\n",
      "train loss:2.3002044352025304\n",
      "train loss:2.304639633095456\n",
      "train loss:2.293380238565108\n",
      "train loss:2.2956210611153574\n",
      "train loss:2.3051538389182613\n",
      "train loss:2.3007688746206734\n",
      "train loss:2.3090770947362516\n",
      "train loss:2.296475764294816\n",
      "train loss:2.306134486613279\n",
      "train loss:2.3074801519320585\n",
      "train loss:2.304687260254495\n",
      "train loss:2.2955326727388092\n",
      "train loss:2.304078874582749\n",
      "train loss:2.3031839284172104\n",
      "train loss:2.306326214196461\n",
      "train loss:2.299486310581264\n",
      "train loss:2.288902107891156\n",
      "train loss:2.297943280105209\n",
      "train loss:2.3060700676127626\n",
      "train loss:2.2998885904409114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3031896534517085\n",
      "train loss:2.2962272610593937\n",
      "train loss:2.306577218631187\n",
      "train loss:2.300587998115288\n",
      "train loss:2.301872743873926\n",
      "train loss:2.2967869733706014\n",
      "train loss:2.2994331461249504\n",
      "train loss:2.2998484134140265\n",
      "train loss:2.303192914413409\n",
      "train loss:2.296993788255302\n",
      "train loss:2.3043824738915664\n",
      "train loss:2.28919987672286\n",
      "train loss:2.2987620977254557\n",
      "train loss:2.309167954504158\n",
      "train loss:2.3023922811725464\n",
      "train loss:2.3003363822258662\n",
      "train loss:2.301367677237312\n",
      "train loss:2.3069741699797293\n",
      "train loss:2.308682666584978\n",
      "train loss:2.303788252587558\n",
      "train loss:2.301783521400105\n",
      "train loss:2.30926241999133\n",
      "train loss:2.3009700320914397\n",
      "train loss:2.3006148654230496\n",
      "train loss:2.300549472682523\n",
      "train loss:2.3025280303153957\n",
      "train loss:2.298559673096221\n",
      "train loss:2.3075282950591864\n",
      "train loss:2.3061193664921356\n",
      "train loss:2.303124092640402\n",
      "train loss:2.3074470516324723\n",
      "train loss:2.3057422226940982\n",
      "train loss:2.295704082663736\n",
      "train loss:2.302727715648668\n",
      "train loss:2.3061846321093737\n",
      "train loss:2.303130810391222\n",
      "train loss:2.3009081163386575\n",
      "train loss:2.2989835454874346\n",
      "train loss:2.312452635306269\n",
      "train loss:2.30989469742036\n",
      "train loss:2.2985242618143165\n",
      "train loss:2.3074141058009214\n",
      "train loss:2.3030577431606005\n",
      "train loss:2.3066733425127364\n",
      "train loss:2.292123790103326\n",
      "train loss:2.3075100652679277\n",
      "train loss:2.3028431747381592\n",
      "train loss:2.3077720736261997\n",
      "train loss:2.306152773233568\n",
      "train loss:2.29520395819876\n",
      "train loss:2.301718679823852\n",
      "train loss:2.3001957509574273\n",
      "train loss:2.308016014656423\n",
      "train loss:2.301600768888857\n",
      "train loss:2.2999686087080153\n",
      "train loss:2.3044513219637244\n",
      "train loss:2.3011931098896428\n",
      "train loss:2.2974785673700975\n",
      "train loss:2.3068335540279135\n",
      "train loss:2.3019317260393293\n",
      "train loss:2.3058333559497073\n",
      "train loss:2.3026480836285814\n",
      "train loss:2.298431216900252\n",
      "train loss:2.295195978112271\n",
      "train loss:2.2989362524634385\n",
      "train loss:2.301175719030345\n",
      "train loss:2.2937329617042423\n",
      "train loss:2.3028122429909454\n",
      "train loss:2.308638530208516\n",
      "train loss:2.304332572474889\n",
      "train loss:2.2984163784540876\n",
      "train loss:2.296405190656495\n",
      "train loss:2.3022115876429075\n",
      "train loss:2.304572554745801\n",
      "train loss:2.2936974750328925\n",
      "train loss:2.304568494982324\n",
      "train loss:2.3032573377820347\n",
      "train loss:2.3067449102642965\n",
      "train loss:2.2913900854717046\n",
      "train loss:2.298362726407838\n",
      "train loss:2.3052232708311124\n",
      "train loss:2.302839128201077\n",
      "train loss:2.305098810469894\n",
      "train loss:2.3016782643584053\n",
      "train loss:2.299315622236224\n",
      "train loss:2.3038950207968\n",
      "train loss:2.3064371693823005\n",
      "train loss:2.298505065789661\n",
      "train loss:2.3051786717092417\n",
      "train loss:2.3028196313693896\n",
      "train loss:2.3026572944264267\n",
      "train loss:2.302315728845464\n",
      "train loss:2.2975935585122857\n",
      "train loss:2.301711797843096\n",
      "train loss:2.2978905138764714\n",
      "train loss:2.291251230990784\n",
      "train loss:2.2966276004582373\n",
      "train loss:2.305168551639361\n",
      "train loss:2.3130407520573764\n",
      "train loss:2.2970659308154753\n",
      "train loss:2.3068226014716324\n",
      "train loss:2.3016754618219477\n",
      "train loss:2.2963121247934066\n",
      "train loss:2.305647402647483\n",
      "train loss:2.301346484286721\n",
      "train loss:2.3001835817039957\n",
      "train loss:2.30095715772643\n",
      "train loss:2.3028644729189933\n",
      "train loss:2.298002916767271\n",
      "train loss:2.304667062889172\n",
      "train loss:2.298763744437039\n",
      "train loss:2.3049590515657035\n",
      "train loss:2.30496107887318\n",
      "train loss:2.297796225411811\n",
      "train loss:2.3065880571484927\n",
      "train loss:2.295928332645877\n",
      "train loss:2.3089719736010825\n",
      "train loss:2.301479950538387\n",
      "train loss:2.2965179887186227\n",
      "train loss:2.3030651010569656\n",
      "train loss:2.304403833326296\n",
      "train loss:2.307934829334272\n",
      "train loss:2.297998031102876\n",
      "train loss:2.29772959466936\n",
      "train loss:2.306672434759876\n",
      "train loss:2.3013716026996067\n",
      "train loss:2.3039224865754617\n",
      "train loss:2.299002388403732\n",
      "train loss:2.3012170930895537\n",
      "train loss:2.303938585904564\n",
      "train loss:2.300067249782314\n",
      "train loss:2.3000739114631004\n",
      "train loss:2.2985888544848203\n",
      "train loss:2.299182172527955\n",
      "train loss:2.302889140471938\n",
      "train loss:2.298415725954134\n",
      "train loss:2.304229636155265\n",
      "train loss:2.297030176929462\n",
      "train loss:2.300837527452869\n",
      "train loss:2.3015696969923005\n",
      "train loss:2.310462052776228\n",
      "train loss:2.3125531299534985\n",
      "train loss:2.3018573680362353\n",
      "train loss:2.317376201165511\n",
      "train loss:2.3011519888160703\n",
      "train loss:2.2979526894314115\n",
      "train loss:2.2940686940852095\n",
      "train loss:2.3043563239430163\n",
      "train loss:2.3087091666177275\n",
      "train loss:2.3000722635121438\n",
      "train loss:2.297103009191806\n",
      "=== epoch:59, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.300099021542692\n",
      "train loss:2.3061410454476543\n",
      "train loss:2.2971087919206328\n",
      "train loss:2.2979186355392764\n",
      "train loss:2.309047993771049\n",
      "train loss:2.3076143009496577\n",
      "train loss:2.2970794247619466\n",
      "train loss:2.31081804872514\n",
      "train loss:2.2994619695143417\n",
      "train loss:2.3013571053358826\n",
      "train loss:2.3028352577670423\n",
      "train loss:2.3034404233251626\n",
      "train loss:2.294306726251462\n",
      "train loss:2.303993125925626\n",
      "train loss:2.303956389241059\n",
      "train loss:2.294823654911438\n",
      "train loss:2.2962004397646854\n",
      "train loss:2.3027587631727195\n",
      "train loss:2.302446955764499\n",
      "train loss:2.293757232455473\n",
      "train loss:2.296730654501285\n",
      "train loss:2.3005237381824797\n",
      "train loss:2.2999364181325714\n",
      "train loss:2.304874122737816\n",
      "train loss:2.2988513529903556\n",
      "train loss:2.305239730607043\n",
      "train loss:2.2992585028137302\n",
      "train loss:2.300147506707162\n",
      "train loss:2.303550165463821\n",
      "train loss:2.3095127531743036\n",
      "train loss:2.3035813154523206\n",
      "train loss:2.298575733130809\n",
      "train loss:2.2929054592203575\n",
      "train loss:2.3014812571433505\n",
      "train loss:2.3000806512206924\n",
      "train loss:2.2970079553248026\n",
      "train loss:2.3075072481657553\n",
      "train loss:2.303829422182824\n",
      "train loss:2.301499915505936\n",
      "train loss:2.297660076517824\n",
      "train loss:2.3021782154644863\n",
      "train loss:2.3000928531527896\n",
      "train loss:2.3163339203650173\n",
      "train loss:2.3033658092340707\n",
      "train loss:2.3082896966573485\n",
      "train loss:2.303917562292772\n",
      "train loss:2.3097772752672703\n",
      "train loss:2.301953827341973\n",
      "train loss:2.2994838730653053\n",
      "train loss:2.297230371096645\n",
      "train loss:2.3008946934926073\n",
      "train loss:2.3055588790335912\n",
      "train loss:2.2968396360792696\n",
      "train loss:2.3018993626341007\n",
      "train loss:2.3027990144279014\n",
      "train loss:2.3036693171260847\n",
      "train loss:2.3055861039671575\n",
      "train loss:2.3032003045365723\n",
      "train loss:2.300617621021637\n",
      "train loss:2.29343629626221\n",
      "train loss:2.30076467946121\n",
      "train loss:2.3046666480982485\n",
      "train loss:2.3011795378863638\n",
      "train loss:2.3024436297300523\n",
      "train loss:2.2960605028141763\n",
      "train loss:2.303844871588775\n",
      "train loss:2.3061268036007156\n",
      "train loss:2.296673820131548\n",
      "train loss:2.298308164827962\n",
      "train loss:2.3177940913350796\n",
      "train loss:2.29528419893684\n",
      "train loss:2.2929652050368943\n",
      "train loss:2.2974086935464193\n",
      "train loss:2.300113480761214\n",
      "train loss:2.2962765795600624\n",
      "train loss:2.3062459029008515\n",
      "train loss:2.2989261281876874\n",
      "train loss:2.301331865579494\n",
      "train loss:2.2990482523443245\n",
      "train loss:2.3006911434977764\n",
      "train loss:2.3026364258740126\n",
      "train loss:2.307205865646575\n",
      "train loss:2.3008222645953693\n",
      "train loss:2.3038015418743685\n",
      "train loss:2.2977793593146205\n",
      "train loss:2.298465544624918\n",
      "train loss:2.2936740636628055\n",
      "train loss:2.2985775261356602\n",
      "train loss:2.3094406511477703\n",
      "train loss:2.29910496332223\n",
      "train loss:2.3029596542089235\n",
      "train loss:2.2993887536544575\n",
      "train loss:2.304666955059364\n",
      "train loss:2.2946361514886044\n",
      "train loss:2.298839754347014\n",
      "train loss:2.301408841112682\n",
      "train loss:2.3014651425697252\n",
      "train loss:2.3013925820987895\n",
      "train loss:2.3037975582710715\n",
      "train loss:2.303635199076761\n",
      "train loss:2.2978123772841306\n",
      "train loss:2.304764535450639\n",
      "train loss:2.29632738586247\n",
      "train loss:2.2978233955829825\n",
      "train loss:2.3014857033295657\n",
      "train loss:2.3019259208975775\n",
      "train loss:2.300817085041211\n",
      "train loss:2.2993455469315442\n",
      "train loss:2.3038349286017694\n",
      "train loss:2.3017965431325673\n",
      "train loss:2.2988334462099074\n",
      "train loss:2.3033367900853383\n",
      "train loss:2.3023819990423475\n",
      "train loss:2.301988437149885\n",
      "train loss:2.2954837917191155\n",
      "train loss:2.3005804304797435\n",
      "train loss:2.3000204113897778\n",
      "train loss:2.2973860032655926\n",
      "train loss:2.3033969699695187\n",
      "train loss:2.3096176809424693\n",
      "train loss:2.30711994494194\n",
      "train loss:2.311364186974015\n",
      "train loss:2.296735276949192\n",
      "train loss:2.2927962807280147\n",
      "train loss:2.299220025114378\n",
      "train loss:2.2996480316089105\n",
      "train loss:2.304597101288125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3024149262716604\n",
      "train loss:2.3079954589242058\n",
      "train loss:2.301248417910144\n",
      "train loss:2.2959428625279807\n",
      "train loss:2.2967859156540116\n",
      "train loss:2.2975123626150302\n",
      "train loss:2.3016851553594293\n",
      "train loss:2.3088006261827196\n",
      "train loss:2.3095143298324277\n",
      "train loss:2.299631184589804\n",
      "train loss:2.298502878556317\n",
      "train loss:2.3045745227821266\n",
      "train loss:2.303786363169665\n",
      "train loss:2.298890760182031\n",
      "train loss:2.311981737480502\n",
      "train loss:2.2993390478761784\n",
      "train loss:2.297506328018076\n",
      "train loss:2.305760074596378\n",
      "train loss:2.292578333033962\n",
      "train loss:2.3016466047772672\n",
      "train loss:2.2942172410085093\n",
      "train loss:2.295666931029572\n",
      "train loss:2.306462394389056\n",
      "train loss:2.3015826364049885\n",
      "train loss:2.2994408252060348\n",
      "train loss:2.293393474238187\n",
      "train loss:2.306953393866635\n",
      "train loss:2.3028138476564326\n",
      "train loss:2.2996553717820514\n",
      "train loss:2.302617673400198\n",
      "train loss:2.311299507727208\n",
      "train loss:2.304468466351965\n",
      "train loss:2.3012947942395683\n",
      "train loss:2.3020997706149657\n",
      "train loss:2.2959070306965326\n",
      "train loss:2.3056982070287724\n",
      "train loss:2.3003937608267107\n",
      "train loss:2.300668843107723\n",
      "train loss:2.296856871656804\n",
      "train loss:2.302484441628959\n",
      "train loss:2.2933798038361153\n",
      "train loss:2.3004749007697645\n",
      "train loss:2.303348415476154\n",
      "train loss:2.3028629391888833\n",
      "train loss:2.2962431310929943\n",
      "train loss:2.3012650630706872\n",
      "train loss:2.306192767288897\n",
      "train loss:2.310892454546234\n",
      "train loss:2.2988062643313243\n",
      "train loss:2.2943342929744044\n",
      "train loss:2.2990019826711605\n",
      "train loss:2.2991215643498517\n",
      "train loss:2.292277014125438\n",
      "train loss:2.2961212296209648\n",
      "train loss:2.30111650278448\n",
      "train loss:2.2986748808758177\n",
      "train loss:2.3018085707558895\n",
      "train loss:2.3006199906021503\n",
      "train loss:2.3042192924551967\n",
      "train loss:2.295368743068502\n",
      "train loss:2.296198505465143\n",
      "train loss:2.2952284860831016\n",
      "train loss:2.3021535637723076\n",
      "train loss:2.2968109596982234\n",
      "train loss:2.2973328402991444\n",
      "train loss:2.2967063518793642\n",
      "train loss:2.301433085461717\n",
      "train loss:2.300642744248286\n",
      "train loss:2.2991352696337737\n",
      "train loss:2.3091724599779186\n",
      "train loss:2.305426312745125\n",
      "train loss:2.311715801818106\n",
      "train loss:2.3060801444557426\n",
      "=== epoch:60, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301268116646755\n",
      "train loss:2.2999060306035553\n",
      "train loss:2.295989886656767\n",
      "train loss:2.298109454310937\n",
      "train loss:2.29612045410933\n",
      "train loss:2.2974008783929816\n",
      "train loss:2.303386108793018\n",
      "train loss:2.311112539283839\n",
      "train loss:2.3069254633973237\n",
      "train loss:2.307661649405743\n",
      "train loss:2.293911791598133\n",
      "train loss:2.2880122919969894\n",
      "train loss:2.3001886977649195\n",
      "train loss:2.2993047588867124\n",
      "train loss:2.306588353276337\n",
      "train loss:2.301399247154088\n",
      "train loss:2.307852089377819\n",
      "train loss:2.303307446840787\n",
      "train loss:2.3022112941981194\n",
      "train loss:2.3030873425631135\n",
      "train loss:2.2957246176350243\n",
      "train loss:2.2969202840451737\n",
      "train loss:2.2971929194589698\n",
      "train loss:2.305601566807492\n",
      "train loss:2.2986562696002384\n",
      "train loss:2.29906466975152\n",
      "train loss:2.2979164488958808\n",
      "train loss:2.295639579582931\n",
      "train loss:2.299752939201996\n",
      "train loss:2.3037172025326687\n",
      "train loss:2.299820765620393\n",
      "train loss:2.310064555372622\n",
      "train loss:2.3029875150261705\n",
      "train loss:2.308654281221656\n",
      "train loss:2.2937882847931\n",
      "train loss:2.3079585026223026\n",
      "train loss:2.2997476210509946\n",
      "train loss:2.2993114778092414\n",
      "train loss:2.296578327642808\n",
      "train loss:2.304531912032703\n",
      "train loss:2.2924024183694764\n",
      "train loss:2.294826392666811\n",
      "train loss:2.2966473434693775\n",
      "train loss:2.305166179257449\n",
      "train loss:2.3073820866362555\n",
      "train loss:2.3072250925261684\n",
      "train loss:2.309217239632455\n",
      "train loss:2.2955563905707947\n",
      "train loss:2.3073669721026278\n",
      "train loss:2.303959373180569\n",
      "train loss:2.30172165479303\n",
      "train loss:2.3118598036830296\n",
      "train loss:2.2950575532515427\n",
      "train loss:2.309517314950543\n",
      "train loss:2.301750911836468\n",
      "train loss:2.3038734699017196\n",
      "train loss:2.3010647116055876\n",
      "train loss:2.297361659144223\n",
      "train loss:2.303778979259574\n",
      "train loss:2.2971554168367296\n",
      "train loss:2.2942839892055007\n",
      "train loss:2.302233592708429\n",
      "train loss:2.2968753822609167\n",
      "train loss:2.3061271040871767\n",
      "train loss:2.3004644362201216\n",
      "train loss:2.2976662340599616\n",
      "train loss:2.3033300747100998\n",
      "train loss:2.30220081930038\n",
      "train loss:2.295100538396409\n",
      "train loss:2.3022781372684777\n",
      "train loss:2.2970018124049028\n",
      "train loss:2.3003125100198876\n",
      "train loss:2.302905376445771\n",
      "train loss:2.2994484447318264\n",
      "train loss:2.2953482266861056\n",
      "train loss:2.297411033400852\n",
      "train loss:2.3102629275985387\n",
      "train loss:2.2932570748968795\n",
      "train loss:2.3076811744451\n",
      "train loss:2.304716185553856\n",
      "train loss:2.307678937303416\n",
      "train loss:2.305015221911828\n",
      "train loss:2.3053503082424456\n",
      "train loss:2.305492297139309\n",
      "train loss:2.297796622929434\n",
      "train loss:2.309130461421107\n",
      "train loss:2.3073285903116005\n",
      "train loss:2.3115884212026065\n",
      "train loss:2.3068170398315098\n",
      "train loss:2.305193573819881\n",
      "train loss:2.3013607589346536\n",
      "train loss:2.2983066120000797\n",
      "train loss:2.2926949695744985\n",
      "train loss:2.2922334163196223\n",
      "train loss:2.30085841814121\n",
      "train loss:2.30473976305962\n",
      "train loss:2.2947049842985106\n",
      "train loss:2.302720906171649\n",
      "train loss:2.295642601118272\n",
      "train loss:2.3001486036206407\n",
      "train loss:2.299592417761412\n",
      "train loss:2.2992041977067985\n",
      "train loss:2.304159203978794\n",
      "train loss:2.3014095995076085\n",
      "train loss:2.2984657724110584\n",
      "train loss:2.3057609442077664\n",
      "train loss:2.295318015173549\n",
      "train loss:2.3044996702095553\n",
      "train loss:2.301832168087928\n",
      "train loss:2.3056909954929963\n",
      "train loss:2.2977129729401353\n",
      "train loss:2.299916086960456\n",
      "train loss:2.3025832211393604\n",
      "train loss:2.2988800382777153\n",
      "train loss:2.3011803985902675\n",
      "train loss:2.3019725783002056\n",
      "train loss:2.2997725786943737\n",
      "train loss:2.297533077520313\n",
      "train loss:2.3034268557286297\n",
      "train loss:2.3052157089123906\n",
      "train loss:2.298782565080076\n",
      "train loss:2.2986037427376482\n",
      "train loss:2.3053995392306756\n",
      "train loss:2.301700834317153\n",
      "train loss:2.3017363848583927\n",
      "train loss:2.2976024443025582\n",
      "train loss:2.2955171121947777\n",
      "train loss:2.298672374639202\n",
      "train loss:2.3008055522601376\n",
      "train loss:2.30302621987517\n",
      "train loss:2.298251650079758\n",
      "train loss:2.294047254079805\n",
      "train loss:2.303565870039138\n",
      "train loss:2.292446912612542\n",
      "train loss:2.2991802440459113\n",
      "train loss:2.306402206369015\n",
      "train loss:2.3032187061927907\n",
      "train loss:2.2884949498934275\n",
      "train loss:2.3029870288829075\n",
      "train loss:2.302955503804534\n",
      "train loss:2.2915783025153003\n",
      "train loss:2.300217356666029\n",
      "train loss:2.301342187069178\n",
      "train loss:2.29995742967113\n",
      "train loss:2.307314320305328\n",
      "train loss:2.3040210220737047\n",
      "train loss:2.306857804218765\n",
      "train loss:2.297864904264022\n",
      "train loss:2.308586266915238\n",
      "train loss:2.3008390582394447\n",
      "train loss:2.2992473172670698\n",
      "train loss:2.294582308354216\n",
      "train loss:2.2979957639189146\n",
      "train loss:2.2989484177242145\n",
      "train loss:2.3033261006322454\n",
      "train loss:2.303079056507219\n",
      "train loss:2.297811345344307\n",
      "train loss:2.3014675560965108\n",
      "train loss:2.309810600883406\n",
      "train loss:2.3001109897691285\n",
      "train loss:2.3052464304306075\n",
      "train loss:2.3045094134764335\n",
      "train loss:2.301933624807933\n",
      "train loss:2.3101372532276927\n",
      "train loss:2.3036638997689445\n",
      "train loss:2.311095443429846\n",
      "train loss:2.2981195658271454\n",
      "train loss:2.309465084937125\n",
      "train loss:2.2979226902173147\n",
      "train loss:2.3113723231899868\n",
      "train loss:2.3041170203643166\n",
      "train loss:2.3046294069034756\n",
      "train loss:2.3078087146977486\n",
      "train loss:2.2969066628437442\n",
      "train loss:2.311701157289711\n",
      "train loss:2.2944558178309427\n",
      "train loss:2.296066078695702\n",
      "train loss:2.300295823265683\n",
      "train loss:2.2998356817003645\n",
      "train loss:2.3055469386326735\n",
      "train loss:2.2888722490828424\n",
      "train loss:2.3040441411806927\n",
      "train loss:2.2984766210023504\n",
      "train loss:2.2982564823977754\n",
      "train loss:2.2992192406742697\n",
      "train loss:2.3059300869046275\n",
      "train loss:2.2962331204272544\n",
      "train loss:2.298587666864552\n",
      "train loss:2.3018468130290604\n",
      "train loss:2.3003754567618553\n",
      "train loss:2.3033949239966742\n",
      "train loss:2.302723118792699\n",
      "train loss:2.2986593680863177\n",
      "train loss:2.305124537143354\n",
      "train loss:2.3059629533531876\n",
      "train loss:2.3104643876077966\n",
      "train loss:2.3157275877400534\n",
      "train loss:2.3125314634147127\n",
      "train loss:2.3027410648760296\n",
      "train loss:2.2968588008178523\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.2 # 드롭아웃 비율\n",
    "weight_decay_lambda = 0.1 # 가중치 감쇠\n",
    "\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio,\n",
    "                             weight_decay_lambda = weight_decay_lambda)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2463573",
   "metadata": {},
   "source": [
    "데이터 개수가 적어서인지 가중치 감쇠와 드롭 아웃을 동시에 적용하면 효과적인 성능 수치가 나오지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f024e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
