{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e48f49",
   "metadata": {},
   "source": [
    "## 바른 학습을 위해\n",
    "- 오버피팅 방지하는 다양한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bb94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eaef7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6743c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad2a4b",
   "metadata": {},
   "source": [
    "epoch마다 train acc와 test acc 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5402ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.09, test acc:0.1032\n",
      "epoch:1, train acc:0.09666666666666666, test acc:0.1038\n",
      "epoch:2, train acc:0.09333333333333334, test acc:0.1052\n",
      "epoch:3, train acc:0.10333333333333333, test acc:0.11\n",
      "epoch:4, train acc:0.10333333333333333, test acc:0.1136\n",
      "epoch:5, train acc:0.11, test acc:0.1154\n",
      "epoch:6, train acc:0.11666666666666667, test acc:0.121\n",
      "epoch:7, train acc:0.15333333333333332, test acc:0.1285\n",
      "epoch:8, train acc:0.15666666666666668, test acc:0.1361\n",
      "epoch:9, train acc:0.17, test acc:0.1402\n",
      "epoch:10, train acc:0.2, test acc:0.1572\n",
      "epoch:11, train acc:0.21333333333333335, test acc:0.165\n",
      "epoch:12, train acc:0.25666666666666665, test acc:0.1903\n",
      "epoch:13, train acc:0.2866666666666667, test acc:0.2089\n",
      "epoch:14, train acc:0.31666666666666665, test acc:0.2305\n",
      "epoch:15, train acc:0.33666666666666667, test acc:0.2481\n",
      "epoch:16, train acc:0.3433333333333333, test acc:0.2492\n",
      "epoch:17, train acc:0.36666666666666664, test acc:0.2649\n",
      "epoch:18, train acc:0.3933333333333333, test acc:0.29\n",
      "epoch:19, train acc:0.41, test acc:0.2936\n",
      "epoch:20, train acc:0.42, test acc:0.3055\n",
      "epoch:21, train acc:0.45, test acc:0.3339\n",
      "epoch:22, train acc:0.47, test acc:0.3494\n",
      "epoch:23, train acc:0.49, test acc:0.3659\n",
      "epoch:24, train acc:0.5033333333333333, test acc:0.381\n",
      "epoch:25, train acc:0.5133333333333333, test acc:0.3917\n",
      "epoch:26, train acc:0.51, test acc:0.3864\n",
      "epoch:27, train acc:0.5333333333333333, test acc:0.4087\n",
      "epoch:28, train acc:0.54, test acc:0.4313\n",
      "epoch:29, train acc:0.58, test acc:0.4578\n",
      "epoch:30, train acc:0.6033333333333334, test acc:0.4705\n",
      "epoch:31, train acc:0.63, test acc:0.49\n",
      "epoch:32, train acc:0.61, test acc:0.4857\n",
      "epoch:33, train acc:0.6333333333333333, test acc:0.5029\n",
      "epoch:34, train acc:0.6533333333333333, test acc:0.5119\n",
      "epoch:35, train acc:0.6533333333333333, test acc:0.5203\n",
      "epoch:36, train acc:0.64, test acc:0.5153\n",
      "epoch:37, train acc:0.6466666666666666, test acc:0.5246\n",
      "epoch:38, train acc:0.6366666666666667, test acc:0.524\n",
      "epoch:39, train acc:0.64, test acc:0.5231\n",
      "epoch:40, train acc:0.6533333333333333, test acc:0.5281\n",
      "epoch:41, train acc:0.65, test acc:0.521\n",
      "epoch:42, train acc:0.66, test acc:0.5451\n",
      "epoch:43, train acc:0.6733333333333333, test acc:0.5437\n",
      "epoch:44, train acc:0.68, test acc:0.5538\n",
      "epoch:45, train acc:0.6966666666666667, test acc:0.5625\n",
      "epoch:46, train acc:0.7, test acc:0.5566\n",
      "epoch:47, train acc:0.67, test acc:0.5598\n",
      "epoch:48, train acc:0.6833333333333333, test acc:0.5588\n",
      "epoch:49, train acc:0.7133333333333334, test acc:0.5658\n",
      "epoch:50, train acc:0.7, test acc:0.5683\n",
      "epoch:51, train acc:0.7233333333333334, test acc:0.5884\n",
      "epoch:52, train acc:0.7366666666666667, test acc:0.5923\n",
      "epoch:53, train acc:0.7333333333333333, test acc:0.5907\n",
      "epoch:54, train acc:0.7333333333333333, test acc:0.5887\n",
      "epoch:55, train acc:0.7433333333333333, test acc:0.5882\n",
      "epoch:56, train acc:0.75, test acc:0.5955\n",
      "epoch:57, train acc:0.7533333333333333, test acc:0.5918\n",
      "epoch:58, train acc:0.7533333333333333, test acc:0.6037\n",
      "epoch:59, train acc:0.76, test acc:0.598\n",
      "epoch:60, train acc:0.7633333333333333, test acc:0.6029\n",
      "epoch:61, train acc:0.7633333333333333, test acc:0.6028\n",
      "epoch:62, train acc:0.7666666666666667, test acc:0.6113\n",
      "epoch:63, train acc:0.77, test acc:0.6057\n",
      "epoch:64, train acc:0.76, test acc:0.6063\n",
      "epoch:65, train acc:0.7633333333333333, test acc:0.6145\n",
      "epoch:66, train acc:0.7666666666666667, test acc:0.6256\n",
      "epoch:67, train acc:0.7566666666666667, test acc:0.6171\n",
      "epoch:68, train acc:0.7733333333333333, test acc:0.6226\n",
      "epoch:69, train acc:0.7633333333333333, test acc:0.6221\n",
      "epoch:70, train acc:0.7633333333333333, test acc:0.607\n",
      "epoch:71, train acc:0.7766666666666666, test acc:0.6215\n",
      "epoch:72, train acc:0.7366666666666667, test acc:0.5866\n",
      "epoch:73, train acc:0.7633333333333333, test acc:0.6115\n",
      "epoch:74, train acc:0.77, test acc:0.6154\n",
      "epoch:75, train acc:0.7766666666666666, test acc:0.6166\n",
      "epoch:76, train acc:0.76, test acc:0.6041\n",
      "epoch:77, train acc:0.7566666666666667, test acc:0.6088\n",
      "epoch:78, train acc:0.7533333333333333, test acc:0.6057\n",
      "epoch:79, train acc:0.7366666666666667, test acc:0.5942\n",
      "epoch:80, train acc:0.78, test acc:0.6295\n",
      "epoch:81, train acc:0.7566666666666667, test acc:0.6334\n",
      "epoch:82, train acc:0.7533333333333333, test acc:0.6104\n",
      "epoch:83, train acc:0.7633333333333333, test acc:0.6198\n",
      "epoch:84, train acc:0.78, test acc:0.6275\n",
      "epoch:85, train acc:0.77, test acc:0.6434\n",
      "epoch:86, train acc:0.7666666666666667, test acc:0.6388\n",
      "epoch:87, train acc:0.7766666666666666, test acc:0.63\n",
      "epoch:88, train acc:0.8, test acc:0.6362\n",
      "epoch:89, train acc:0.8033333333333333, test acc:0.6375\n",
      "epoch:90, train acc:0.8, test acc:0.6257\n",
      "epoch:91, train acc:0.8, test acc:0.65\n",
      "epoch:92, train acc:0.8233333333333334, test acc:0.6515\n",
      "epoch:93, train acc:0.7933333333333333, test acc:0.6451\n",
      "epoch:94, train acc:0.8033333333333333, test acc:0.6619\n",
      "epoch:95, train acc:0.82, test acc:0.6536\n",
      "epoch:96, train acc:0.8066666666666666, test acc:0.6611\n",
      "epoch:97, train acc:0.8266666666666667, test acc:0.659\n",
      "epoch:98, train acc:0.8333333333333334, test acc:0.6707\n",
      "epoch:99, train acc:0.8333333333333334, test acc:0.668\n",
      "epoch:100, train acc:0.8066666666666666, test acc:0.6427\n",
      "epoch:101, train acc:0.8266666666666667, test acc:0.6593\n",
      "epoch:102, train acc:0.8166666666666667, test acc:0.6568\n",
      "epoch:103, train acc:0.8233333333333334, test acc:0.6626\n",
      "epoch:104, train acc:0.83, test acc:0.6713\n",
      "epoch:105, train acc:0.82, test acc:0.6539\n",
      "epoch:106, train acc:0.84, test acc:0.6664\n",
      "epoch:107, train acc:0.84, test acc:0.6651\n",
      "epoch:108, train acc:0.83, test acc:0.6728\n",
      "epoch:109, train acc:0.8233333333333334, test acc:0.6611\n",
      "epoch:110, train acc:0.8166666666666667, test acc:0.6614\n",
      "epoch:111, train acc:0.8266666666666667, test acc:0.661\n",
      "epoch:112, train acc:0.8433333333333334, test acc:0.6724\n",
      "epoch:113, train acc:0.8233333333333334, test acc:0.6681\n",
      "epoch:114, train acc:0.8533333333333334, test acc:0.6844\n",
      "epoch:115, train acc:0.85, test acc:0.6868\n",
      "epoch:116, train acc:0.8533333333333334, test acc:0.6767\n",
      "epoch:117, train acc:0.8466666666666667, test acc:0.6758\n",
      "epoch:118, train acc:0.8366666666666667, test acc:0.6752\n",
      "epoch:119, train acc:0.8466666666666667, test acc:0.6855\n",
      "epoch:120, train acc:0.8333333333333334, test acc:0.6906\n",
      "epoch:121, train acc:0.85, test acc:0.6885\n",
      "epoch:122, train acc:0.8, test acc:0.6599\n",
      "epoch:123, train acc:0.84, test acc:0.6825\n",
      "epoch:124, train acc:0.8233333333333334, test acc:0.6722\n",
      "epoch:125, train acc:0.83, test acc:0.6813\n",
      "epoch:126, train acc:0.85, test acc:0.6846\n",
      "epoch:127, train acc:0.8533333333333334, test acc:0.6916\n",
      "epoch:128, train acc:0.8466666666666667, test acc:0.6972\n",
      "epoch:129, train acc:0.85, test acc:0.6963\n",
      "epoch:130, train acc:0.86, test acc:0.6975\n",
      "epoch:131, train acc:0.8466666666666667, test acc:0.6941\n",
      "epoch:132, train acc:0.86, test acc:0.6985\n",
      "epoch:133, train acc:0.85, test acc:0.6934\n",
      "epoch:134, train acc:0.8366666666666667, test acc:0.7008\n",
      "epoch:135, train acc:0.8433333333333334, test acc:0.6916\n",
      "epoch:136, train acc:0.8566666666666667, test acc:0.6962\n",
      "epoch:137, train acc:0.8633333333333333, test acc:0.7053\n",
      "epoch:138, train acc:0.8433333333333334, test acc:0.7001\n",
      "epoch:139, train acc:0.8533333333333334, test acc:0.701\n",
      "epoch:140, train acc:0.8566666666666667, test acc:0.7044\n",
      "epoch:141, train acc:0.8633333333333333, test acc:0.7082\n",
      "epoch:142, train acc:0.8566666666666667, test acc:0.7009\n",
      "epoch:143, train acc:0.84, test acc:0.6959\n",
      "epoch:144, train acc:0.8533333333333334, test acc:0.7009\n",
      "epoch:145, train acc:0.85, test acc:0.6982\n",
      "epoch:146, train acc:0.85, test acc:0.7013\n",
      "epoch:147, train acc:0.8533333333333334, test acc:0.6952\n",
      "epoch:148, train acc:0.8566666666666667, test acc:0.7073\n",
      "epoch:149, train acc:0.8433333333333334, test acc:0.7042\n",
      "epoch:150, train acc:0.8566666666666667, test acc:0.7012\n",
      "epoch:151, train acc:0.8633333333333333, test acc:0.7064\n",
      "epoch:152, train acc:0.8466666666666667, test acc:0.7064\n",
      "epoch:153, train acc:0.85, test acc:0.7067\n",
      "epoch:154, train acc:0.8433333333333334, test acc:0.7016\n",
      "epoch:155, train acc:0.85, test acc:0.7091\n",
      "epoch:156, train acc:0.8566666666666667, test acc:0.6986\n",
      "epoch:157, train acc:0.85, test acc:0.6979\n",
      "epoch:158, train acc:0.8633333333333333, test acc:0.7031\n",
      "epoch:159, train acc:0.8666666666666667, test acc:0.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:160, train acc:0.86, test acc:0.7175\n",
      "epoch:161, train acc:0.86, test acc:0.7119\n",
      "epoch:162, train acc:0.85, test acc:0.7002\n",
      "epoch:163, train acc:0.8433333333333334, test acc:0.6957\n",
      "epoch:164, train acc:0.8533333333333334, test acc:0.7033\n",
      "epoch:165, train acc:0.8566666666666667, test acc:0.7083\n",
      "epoch:166, train acc:0.8466666666666667, test acc:0.7058\n",
      "epoch:167, train acc:0.8566666666666667, test acc:0.7111\n",
      "epoch:168, train acc:0.8533333333333334, test acc:0.7062\n",
      "epoch:169, train acc:0.85, test acc:0.7024\n",
      "epoch:170, train acc:0.8533333333333334, test acc:0.7052\n",
      "epoch:171, train acc:0.84, test acc:0.7006\n",
      "epoch:172, train acc:0.85, test acc:0.708\n",
      "epoch:173, train acc:0.8466666666666667, test acc:0.7057\n",
      "epoch:174, train acc:0.8533333333333334, test acc:0.7133\n",
      "epoch:175, train acc:0.8433333333333334, test acc:0.702\n",
      "epoch:176, train acc:0.8533333333333334, test acc:0.7088\n",
      "epoch:177, train acc:0.8533333333333334, test acc:0.7119\n",
      "epoch:178, train acc:0.8566666666666667, test acc:0.7089\n",
      "epoch:179, train acc:0.86, test acc:0.7106\n",
      "epoch:180, train acc:0.8466666666666667, test acc:0.7037\n",
      "epoch:181, train acc:0.84, test acc:0.7016\n",
      "epoch:182, train acc:0.8633333333333333, test acc:0.708\n",
      "epoch:183, train acc:0.8566666666666667, test acc:0.7131\n",
      "epoch:184, train acc:0.8566666666666667, test acc:0.7079\n",
      "epoch:185, train acc:0.87, test acc:0.7209\n",
      "epoch:186, train acc:0.8566666666666667, test acc:0.7096\n",
      "epoch:187, train acc:0.85, test acc:0.7054\n",
      "epoch:188, train acc:0.8466666666666667, test acc:0.7036\n",
      "epoch:189, train acc:0.85, test acc:0.706\n",
      "epoch:190, train acc:0.8566666666666667, test acc:0.7041\n",
      "epoch:191, train acc:0.8466666666666667, test acc:0.7059\n",
      "epoch:192, train acc:0.8566666666666667, test acc:0.7055\n",
      "epoch:193, train acc:0.8533333333333334, test acc:0.7004\n",
      "epoch:194, train acc:0.85, test acc:0.704\n",
      "epoch:195, train acc:0.8666666666666667, test acc:0.7041\n",
      "epoch:196, train acc:0.8666666666666667, test acc:0.7093\n",
      "epoch:197, train acc:0.87, test acc:0.7136\n",
      "epoch:198, train acc:0.86, test acc:0.7095\n",
      "epoch:199, train acc:0.8433333333333334, test acc:0.6981\n",
      "epoch:200, train acc:0.8533333333333334, test acc:0.7117\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75feb17",
   "metadata": {},
   "source": [
    "그래프로 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103ae786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59009d1",
   "metadata": {},
   "source": [
    "### 가중치 감소로 오버피팅 억제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db74104c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5419a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c0401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bee16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c9a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7657ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.10333333333333333, test acc:0.0976\n",
      "epoch:1, train acc:0.11, test acc:0.1004\n",
      "epoch:2, train acc:0.12666666666666668, test acc:0.1083\n",
      "epoch:3, train acc:0.14333333333333334, test acc:0.1145\n",
      "epoch:4, train acc:0.14333333333333334, test acc:0.1208\n",
      "epoch:5, train acc:0.16333333333333333, test acc:0.1288\n",
      "epoch:6, train acc:0.18666666666666668, test acc:0.1333\n",
      "epoch:7, train acc:0.2, test acc:0.1484\n",
      "epoch:8, train acc:0.22, test acc:0.1549\n",
      "epoch:9, train acc:0.21666666666666667, test acc:0.1745\n",
      "epoch:10, train acc:0.23333333333333334, test acc:0.1817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-df2a3bf0be5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtest_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\multi_layer_net.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\multi_layer_net.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5333c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1aaef8",
   "metadata": {},
   "source": [
    "- 가중치 감쇠 0.1을 주지 않으면 train acc가 0.99 넘게 오버피팅이 발생\n",
    "- 가중치 감쇠 0.2 적용 시, 학습이 제대로 되지 않는 모습 적절한 가중치 감쇠 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb4e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "weight_decay_lambda = 0.2 # weight decay를 사용하지 않을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20623aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6f7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c56402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.11666666666666667, test acc:0.1014\n",
      "epoch:1, train acc:0.12333333333333334, test acc:0.1047\n",
      "epoch:2, train acc:0.13666666666666666, test acc:0.1114\n",
      "epoch:3, train acc:0.14666666666666667, test acc:0.1208\n",
      "epoch:4, train acc:0.17666666666666667, test acc:0.1482\n",
      "epoch:5, train acc:0.2, test acc:0.1734\n",
      "epoch:6, train acc:0.23, test acc:0.1849\n",
      "epoch:7, train acc:0.25, test acc:0.193\n",
      "epoch:8, train acc:0.27, test acc:0.2128\n",
      "epoch:9, train acc:0.31333333333333335, test acc:0.2432\n",
      "epoch:10, train acc:0.3566666666666667, test acc:0.2786\n",
      "epoch:11, train acc:0.36666666666666664, test acc:0.2865\n",
      "epoch:12, train acc:0.42333333333333334, test acc:0.3194\n",
      "epoch:13, train acc:0.4166666666666667, test acc:0.3182\n",
      "epoch:14, train acc:0.42333333333333334, test acc:0.3364\n",
      "epoch:15, train acc:0.46, test acc:0.3526\n",
      "epoch:16, train acc:0.4766666666666667, test acc:0.3656\n",
      "epoch:17, train acc:0.4866666666666667, test acc:0.3824\n",
      "epoch:18, train acc:0.49333333333333335, test acc:0.3924\n",
      "epoch:19, train acc:0.47333333333333333, test acc:0.3843\n",
      "epoch:20, train acc:0.48333333333333334, test acc:0.3907\n",
      "epoch:21, train acc:0.49, test acc:0.397\n",
      "epoch:22, train acc:0.48, test acc:0.3956\n",
      "epoch:23, train acc:0.5, test acc:0.3947\n",
      "epoch:24, train acc:0.5033333333333333, test acc:0.404\n",
      "epoch:25, train acc:0.51, test acc:0.4001\n",
      "epoch:26, train acc:0.5166666666666667, test acc:0.4135\n",
      "epoch:27, train acc:0.52, test acc:0.4218\n",
      "epoch:28, train acc:0.5233333333333333, test acc:0.4293\n",
      "epoch:29, train acc:0.5233333333333333, test acc:0.4183\n",
      "epoch:30, train acc:0.52, test acc:0.4335\n",
      "epoch:31, train acc:0.5266666666666666, test acc:0.4187\n",
      "epoch:32, train acc:0.5166666666666667, test acc:0.4138\n",
      "epoch:33, train acc:0.53, test acc:0.4326\n",
      "epoch:34, train acc:0.5233333333333333, test acc:0.4227\n",
      "epoch:35, train acc:0.54, test acc:0.4365\n",
      "epoch:36, train acc:0.53, test acc:0.4301\n",
      "epoch:37, train acc:0.5333333333333333, test acc:0.4318\n",
      "epoch:38, train acc:0.5466666666666666, test acc:0.4429\n",
      "epoch:39, train acc:0.55, test acc:0.4466\n",
      "epoch:40, train acc:0.54, test acc:0.4494\n",
      "epoch:41, train acc:0.5533333333333333, test acc:0.4517\n",
      "epoch:42, train acc:0.5366666666666666, test acc:0.4376\n",
      "epoch:43, train acc:0.5366666666666666, test acc:0.4398\n",
      "epoch:44, train acc:0.5333333333333333, test acc:0.4353\n",
      "epoch:45, train acc:0.5433333333333333, test acc:0.4387\n",
      "epoch:46, train acc:0.5366666666666666, test acc:0.4399\n",
      "epoch:47, train acc:0.5366666666666666, test acc:0.435\n",
      "epoch:48, train acc:0.5433333333333333, test acc:0.4298\n",
      "epoch:49, train acc:0.5366666666666666, test acc:0.4307\n",
      "epoch:50, train acc:0.54, test acc:0.4341\n",
      "epoch:51, train acc:0.5433333333333333, test acc:0.445\n",
      "epoch:52, train acc:0.55, test acc:0.4397\n",
      "epoch:53, train acc:0.5533333333333333, test acc:0.4564\n",
      "epoch:54, train acc:0.55, test acc:0.4507\n",
      "epoch:55, train acc:0.5466666666666666, test acc:0.4527\n",
      "epoch:56, train acc:0.5466666666666666, test acc:0.4525\n",
      "epoch:57, train acc:0.5466666666666666, test acc:0.4605\n",
      "epoch:58, train acc:0.55, test acc:0.4588\n",
      "epoch:59, train acc:0.5533333333333333, test acc:0.4637\n",
      "epoch:60, train acc:0.5533333333333333, test acc:0.4624\n",
      "epoch:61, train acc:0.56, test acc:0.456\n",
      "epoch:62, train acc:0.5466666666666666, test acc:0.4524\n",
      "epoch:63, train acc:0.54, test acc:0.442\n",
      "epoch:64, train acc:0.5333333333333333, test acc:0.4359\n",
      "epoch:65, train acc:0.5366666666666666, test acc:0.4421\n",
      "epoch:66, train acc:0.5333333333333333, test acc:0.44\n",
      "epoch:67, train acc:0.5266666666666666, test acc:0.4394\n",
      "epoch:68, train acc:0.5233333333333333, test acc:0.4329\n",
      "epoch:69, train acc:0.5366666666666666, test acc:0.4358\n",
      "epoch:70, train acc:0.5333333333333333, test acc:0.4379\n",
      "epoch:71, train acc:0.5266666666666666, test acc:0.431\n",
      "epoch:72, train acc:0.5266666666666666, test acc:0.4308\n",
      "epoch:73, train acc:0.5266666666666666, test acc:0.4261\n",
      "epoch:74, train acc:0.53, test acc:0.4302\n",
      "epoch:75, train acc:0.5266666666666666, test acc:0.4309\n",
      "epoch:76, train acc:0.5166666666666667, test acc:0.425\n",
      "epoch:77, train acc:0.52, test acc:0.4311\n",
      "epoch:78, train acc:0.52, test acc:0.4319\n",
      "epoch:79, train acc:0.52, test acc:0.4292\n",
      "epoch:80, train acc:0.52, test acc:0.4334\n",
      "epoch:81, train acc:0.52, test acc:0.4311\n",
      "epoch:82, train acc:0.5166666666666667, test acc:0.4292\n",
      "epoch:83, train acc:0.52, test acc:0.4266\n",
      "epoch:84, train acc:0.5133333333333333, test acc:0.4152\n",
      "epoch:85, train acc:0.5033333333333333, test acc:0.4192\n",
      "epoch:86, train acc:0.5166666666666667, test acc:0.421\n",
      "epoch:87, train acc:0.5133333333333333, test acc:0.4163\n",
      "epoch:88, train acc:0.5133333333333333, test acc:0.4214\n",
      "epoch:89, train acc:0.5133333333333333, test acc:0.4136\n",
      "epoch:90, train acc:0.5, test acc:0.4047\n",
      "epoch:91, train acc:0.5, test acc:0.4103\n",
      "epoch:92, train acc:0.49333333333333335, test acc:0.4065\n",
      "epoch:93, train acc:0.49333333333333335, test acc:0.4047\n",
      "epoch:94, train acc:0.4866666666666667, test acc:0.4014\n",
      "epoch:95, train acc:0.48, test acc:0.3975\n",
      "epoch:96, train acc:0.48333333333333334, test acc:0.3985\n",
      "epoch:97, train acc:0.4766666666666667, test acc:0.3931\n",
      "epoch:98, train acc:0.47333333333333333, test acc:0.3929\n",
      "epoch:99, train acc:0.4666666666666667, test acc:0.3896\n",
      "epoch:100, train acc:0.4633333333333333, test acc:0.3881\n",
      "epoch:101, train acc:0.4666666666666667, test acc:0.3886\n",
      "epoch:102, train acc:0.47333333333333333, test acc:0.3912\n",
      "epoch:103, train acc:0.4666666666666667, test acc:0.389\n",
      "epoch:104, train acc:0.4633333333333333, test acc:0.3893\n",
      "epoch:105, train acc:0.4633333333333333, test acc:0.3826\n",
      "epoch:106, train acc:0.4633333333333333, test acc:0.3895\n",
      "epoch:107, train acc:0.4533333333333333, test acc:0.3782\n",
      "epoch:108, train acc:0.44333333333333336, test acc:0.375\n",
      "epoch:109, train acc:0.45, test acc:0.3752\n",
      "epoch:110, train acc:0.43666666666666665, test acc:0.3736\n",
      "epoch:111, train acc:0.44, test acc:0.3781\n",
      "epoch:112, train acc:0.44, test acc:0.3784\n",
      "epoch:113, train acc:0.44, test acc:0.3753\n",
      "epoch:114, train acc:0.44, test acc:0.3736\n",
      "epoch:115, train acc:0.43666666666666665, test acc:0.3666\n",
      "epoch:116, train acc:0.43, test acc:0.368\n",
      "epoch:117, train acc:0.44333333333333336, test acc:0.3705\n",
      "epoch:118, train acc:0.4266666666666667, test acc:0.3661\n",
      "epoch:119, train acc:0.42333333333333334, test acc:0.3613\n",
      "epoch:120, train acc:0.42333333333333334, test acc:0.3626\n",
      "epoch:121, train acc:0.42, test acc:0.3615\n",
      "epoch:122, train acc:0.42333333333333334, test acc:0.3625\n",
      "epoch:123, train acc:0.43666666666666665, test acc:0.3673\n",
      "epoch:124, train acc:0.43666666666666665, test acc:0.3673\n",
      "epoch:125, train acc:0.44333333333333336, test acc:0.3696\n",
      "epoch:126, train acc:0.44666666666666666, test acc:0.3727\n",
      "epoch:127, train acc:0.44333333333333336, test acc:0.3779\n",
      "epoch:128, train acc:0.42, test acc:0.3641\n",
      "epoch:129, train acc:0.4066666666666667, test acc:0.3587\n",
      "epoch:130, train acc:0.4166666666666667, test acc:0.3618\n",
      "epoch:131, train acc:0.42333333333333334, test acc:0.3624\n",
      "epoch:132, train acc:0.4066666666666667, test acc:0.3514\n",
      "epoch:133, train acc:0.4033333333333333, test acc:0.3468\n",
      "epoch:134, train acc:0.4033333333333333, test acc:0.3464\n",
      "epoch:135, train acc:0.4066666666666667, test acc:0.351\n",
      "epoch:136, train acc:0.41, test acc:0.3502\n",
      "epoch:137, train acc:0.4066666666666667, test acc:0.3484\n",
      "epoch:138, train acc:0.41333333333333333, test acc:0.3509\n",
      "epoch:139, train acc:0.4, test acc:0.3484\n",
      "epoch:140, train acc:0.41333333333333333, test acc:0.3511\n",
      "epoch:141, train acc:0.42, test acc:0.3536\n",
      "epoch:142, train acc:0.4033333333333333, test acc:0.3452\n",
      "epoch:143, train acc:0.41, test acc:0.3468\n",
      "epoch:144, train acc:0.3933333333333333, test acc:0.3378\n",
      "epoch:145, train acc:0.39, test acc:0.3329\n",
      "epoch:146, train acc:0.38666666666666666, test acc:0.3349\n",
      "epoch:147, train acc:0.38666666666666666, test acc:0.3387\n",
      "epoch:148, train acc:0.38666666666666666, test acc:0.3411\n",
      "epoch:149, train acc:0.3933333333333333, test acc:0.339\n",
      "epoch:150, train acc:0.3933333333333333, test acc:0.3382\n",
      "epoch:151, train acc:0.38666666666666666, test acc:0.3376\n",
      "epoch:152, train acc:0.39666666666666667, test acc:0.3377\n",
      "epoch:153, train acc:0.4033333333333333, test acc:0.3374\n",
      "epoch:154, train acc:0.4, test acc:0.3362\n",
      "epoch:155, train acc:0.39, test acc:0.3328\n",
      "epoch:156, train acc:0.38666666666666666, test acc:0.3309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:157, train acc:0.3933333333333333, test acc:0.3313\n",
      "epoch:158, train acc:0.3933333333333333, test acc:0.3265\n",
      "epoch:159, train acc:0.38666666666666666, test acc:0.3222\n",
      "epoch:160, train acc:0.37666666666666665, test acc:0.3194\n",
      "epoch:161, train acc:0.37666666666666665, test acc:0.3168\n",
      "epoch:162, train acc:0.38, test acc:0.3179\n",
      "epoch:163, train acc:0.37666666666666665, test acc:0.3168\n",
      "epoch:164, train acc:0.37666666666666665, test acc:0.3149\n",
      "epoch:165, train acc:0.36666666666666664, test acc:0.3121\n",
      "epoch:166, train acc:0.36, test acc:0.3056\n",
      "epoch:167, train acc:0.35333333333333333, test acc:0.2993\n",
      "epoch:168, train acc:0.3433333333333333, test acc:0.2942\n",
      "epoch:169, train acc:0.33666666666666667, test acc:0.2907\n",
      "epoch:170, train acc:0.32666666666666666, test acc:0.2834\n",
      "epoch:171, train acc:0.33, test acc:0.2809\n",
      "epoch:172, train acc:0.32666666666666666, test acc:0.28\n",
      "epoch:173, train acc:0.32666666666666666, test acc:0.2793\n",
      "epoch:174, train acc:0.32666666666666666, test acc:0.2771\n",
      "epoch:175, train acc:0.32666666666666666, test acc:0.2743\n",
      "epoch:176, train acc:0.31666666666666665, test acc:0.2751\n",
      "epoch:177, train acc:0.31, test acc:0.2691\n",
      "epoch:178, train acc:0.31, test acc:0.2667\n",
      "epoch:179, train acc:0.30333333333333334, test acc:0.2647\n",
      "epoch:180, train acc:0.30333333333333334, test acc:0.2637\n",
      "epoch:181, train acc:0.3, test acc:0.2629\n",
      "epoch:182, train acc:0.3, test acc:0.2628\n",
      "epoch:183, train acc:0.3, test acc:0.2608\n",
      "epoch:184, train acc:0.3, test acc:0.2606\n",
      "epoch:185, train acc:0.3, test acc:0.26\n",
      "epoch:186, train acc:0.3, test acc:0.2628\n",
      "epoch:187, train acc:0.3, test acc:0.2618\n",
      "epoch:188, train acc:0.3, test acc:0.2599\n",
      "epoch:189, train acc:0.3, test acc:0.2584\n",
      "epoch:190, train acc:0.3, test acc:0.2584\n",
      "epoch:191, train acc:0.3, test acc:0.2581\n",
      "epoch:192, train acc:0.29, test acc:0.2532\n",
      "epoch:193, train acc:0.28, test acc:0.2487\n",
      "epoch:194, train acc:0.28, test acc:0.2475\n",
      "epoch:195, train acc:0.27666666666666667, test acc:0.2448\n",
      "epoch:196, train acc:0.2733333333333333, test acc:0.2428\n",
      "epoch:197, train acc:0.27, test acc:0.2415\n",
      "epoch:198, train acc:0.27, test acc:0.2402\n",
      "epoch:199, train acc:0.27, test acc:0.2408\n",
      "epoch:200, train acc:0.27, test acc:0.2415\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1e3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49d370",
   "metadata": {},
   "source": [
    "### 드롭아웃으로 오버피팅 억제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddff94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6586c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 줄이기\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76b0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306466211116495\n",
      "=== epoch:1, train acc:0.08333333333333333, test acc:0.0935 ===\n",
      "train loss:2.3233742007586047\n",
      "train loss:2.316996688370058\n",
      "train loss:2.3128754499657806\n",
      "=== epoch:2, train acc:0.1, test acc:0.101 ===\n",
      "train loss:2.305261699158031\n",
      "train loss:2.323233781629496\n",
      "train loss:2.322380959380987\n",
      "=== epoch:3, train acc:0.10666666666666667, test acc:0.1037 ===\n",
      "train loss:2.3299841311425937\n",
      "train loss:2.2944871777306837\n",
      "train loss:2.313756615900901\n",
      "=== epoch:4, train acc:0.10333333333333333, test acc:0.1087 ===\n",
      "train loss:2.3115197446062776\n",
      "train loss:2.3146947128784494\n",
      "train loss:2.2915479274684913\n",
      "=== epoch:5, train acc:0.12, test acc:0.1127 ===\n",
      "train loss:2.3060189595239247\n",
      "train loss:2.3060533806881014\n",
      "train loss:2.3087166566969675\n",
      "=== epoch:6, train acc:0.11, test acc:0.1165 ===\n",
      "train loss:2.300769070790303\n",
      "train loss:2.307209865920115\n",
      "train loss:2.290991967723541\n",
      "=== epoch:7, train acc:0.11666666666666667, test acc:0.1241 ===\n",
      "train loss:2.3042449149116804\n",
      "train loss:2.2975776964477137\n",
      "train loss:2.2885401175155495\n",
      "=== epoch:8, train acc:0.13, test acc:0.1316 ===\n",
      "train loss:2.290009505624647\n",
      "train loss:2.3114718620468526\n",
      "train loss:2.314299966573357\n",
      "=== epoch:9, train acc:0.12333333333333334, test acc:0.1372 ===\n",
      "train loss:2.300997411163083\n",
      "train loss:2.292089951101843\n",
      "train loss:2.2936505064186385\n",
      "=== epoch:10, train acc:0.13333333333333333, test acc:0.1406 ===\n",
      "train loss:2.3057349982420634\n",
      "train loss:2.294437959087648\n",
      "train loss:2.2972333948046346\n",
      "=== epoch:11, train acc:0.14, test acc:0.146 ===\n",
      "train loss:2.2966845916733263\n",
      "train loss:2.302595062438771\n",
      "train loss:2.2936272236303665\n",
      "=== epoch:12, train acc:0.15, test acc:0.1514 ===\n",
      "train loss:2.287574277852396\n",
      "train loss:2.2908719939764004\n",
      "train loss:2.2864183601058214\n",
      "=== epoch:13, train acc:0.16, test acc:0.1565 ===\n",
      "train loss:2.2900563264228655\n",
      "train loss:2.279374210136909\n",
      "train loss:2.2798698285287617\n",
      "=== epoch:14, train acc:0.15666666666666668, test acc:0.1615 ===\n",
      "train loss:2.2927065558190676\n",
      "train loss:2.2836143283649046\n",
      "train loss:2.2726964474231117\n",
      "=== epoch:15, train acc:0.17, test acc:0.17 ===\n",
      "train loss:2.2960013969785003\n",
      "train loss:2.2689028758724774\n",
      "train loss:2.3006900325557735\n",
      "=== epoch:16, train acc:0.17, test acc:0.1749 ===\n",
      "train loss:2.275545627425918\n",
      "train loss:2.300626775246902\n",
      "train loss:2.2834397872269903\n",
      "=== epoch:17, train acc:0.18333333333333332, test acc:0.1792 ===\n",
      "train loss:2.2854016707761486\n",
      "train loss:2.275003031339694\n",
      "train loss:2.264771086151391\n",
      "=== epoch:18, train acc:0.19333333333333333, test acc:0.188 ===\n",
      "train loss:2.2621317308720457\n",
      "train loss:2.269633891726682\n",
      "train loss:2.2817180115804274\n",
      "=== epoch:19, train acc:0.19333333333333333, test acc:0.1881 ===\n",
      "train loss:2.2827966059700158\n",
      "train loss:2.277116157370144\n",
      "train loss:2.2743481096972133\n",
      "=== epoch:20, train acc:0.19666666666666666, test acc:0.1909 ===\n",
      "train loss:2.271351861406633\n",
      "train loss:2.274971025352863\n",
      "train loss:2.282493765244773\n",
      "=== epoch:21, train acc:0.19666666666666666, test acc:0.1911 ===\n",
      "train loss:2.2847516496402545\n",
      "train loss:2.281258419481394\n",
      "train loss:2.2808844123978633\n",
      "=== epoch:22, train acc:0.20333333333333334, test acc:0.1955 ===\n",
      "train loss:2.291271840559267\n",
      "train loss:2.2796015335111512\n",
      "train loss:2.2634768516493193\n",
      "=== epoch:23, train acc:0.21333333333333335, test acc:0.2009 ===\n",
      "train loss:2.2665197128286847\n",
      "train loss:2.2768140916321418\n",
      "train loss:2.2785730658557726\n",
      "=== epoch:24, train acc:0.22333333333333333, test acc:0.2104 ===\n",
      "train loss:2.2663376392810712\n",
      "train loss:2.274713623840891\n",
      "train loss:2.2633045335824757\n",
      "=== epoch:25, train acc:0.23333333333333334, test acc:0.2194 ===\n",
      "train loss:2.258350522315239\n",
      "train loss:2.263467401701561\n",
      "train loss:2.263810696225091\n",
      "=== epoch:26, train acc:0.24333333333333335, test acc:0.2248 ===\n",
      "train loss:2.2719670864073733\n",
      "train loss:2.258505320526888\n",
      "train loss:2.2721435310239184\n",
      "=== epoch:27, train acc:0.25, test acc:0.2254 ===\n",
      "train loss:2.2667403604002296\n",
      "train loss:2.255703762010085\n",
      "train loss:2.2550799902834817\n",
      "=== epoch:28, train acc:0.25333333333333335, test acc:0.2319 ===\n",
      "train loss:2.2628350101872896\n",
      "train loss:2.2541026848274903\n",
      "train loss:2.2513549666058568\n",
      "=== epoch:29, train acc:0.26666666666666666, test acc:0.2345 ===\n",
      "train loss:2.27104335020848\n",
      "train loss:2.266373564037674\n",
      "train loss:2.2463008280965533\n",
      "=== epoch:30, train acc:0.2733333333333333, test acc:0.2378 ===\n",
      "train loss:2.234608790240035\n",
      "train loss:2.257359700564871\n",
      "train loss:2.2516675582899257\n",
      "=== epoch:31, train acc:0.27, test acc:0.2433 ===\n",
      "train loss:2.2460809665510375\n",
      "train loss:2.2692647383740514\n",
      "train loss:2.2580551559271207\n",
      "=== epoch:32, train acc:0.2833333333333333, test acc:0.2473 ===\n",
      "train loss:2.2514674039957336\n",
      "train loss:2.259232688923554\n",
      "train loss:2.254856259730866\n",
      "=== epoch:33, train acc:0.2833333333333333, test acc:0.2519 ===\n",
      "train loss:2.2359951467459744\n",
      "train loss:2.2411223544755603\n",
      "train loss:2.246183040606921\n",
      "=== epoch:34, train acc:0.2966666666666667, test acc:0.2551 ===\n",
      "train loss:2.2355342739651434\n",
      "train loss:2.2583691397117778\n",
      "train loss:2.2462930817236413\n",
      "=== epoch:35, train acc:0.3, test acc:0.2542 ===\n",
      "train loss:2.250624086878304\n",
      "train loss:2.2509002890509837\n",
      "train loss:2.238299275075596\n",
      "=== epoch:36, train acc:0.30666666666666664, test acc:0.2541 ===\n",
      "train loss:2.2469496470793904\n",
      "train loss:2.2270729149146855\n",
      "train loss:2.249170334568686\n",
      "=== epoch:37, train acc:0.31666666666666665, test acc:0.2617 ===\n",
      "train loss:2.2151523810020137\n",
      "train loss:2.2349687067928143\n",
      "train loss:2.222176044617786\n",
      "=== epoch:38, train acc:0.31666666666666665, test acc:0.2601 ===\n",
      "train loss:2.2353180610683046\n",
      "train loss:2.2392839645559044\n",
      "train loss:2.2554847349947846\n",
      "=== epoch:39, train acc:0.32666666666666666, test acc:0.2612 ===\n",
      "train loss:2.2522851644798494\n",
      "train loss:2.2691303186269622\n",
      "train loss:2.2120275118514803\n",
      "=== epoch:40, train acc:0.32666666666666666, test acc:0.2614 ===\n",
      "train loss:2.233681913580767\n",
      "train loss:2.2379408299392383\n",
      "train loss:2.237635800680383\n",
      "=== epoch:41, train acc:0.3233333333333333, test acc:0.2604 ===\n",
      "train loss:2.2311098030616296\n",
      "train loss:2.204024102053522\n",
      "train loss:2.229517581285359\n",
      "=== epoch:42, train acc:0.31666666666666665, test acc:0.2592 ===\n",
      "train loss:2.2480636673351673\n",
      "train loss:2.2324528887559136\n",
      "train loss:2.2086472348960715\n",
      "=== epoch:43, train acc:0.31666666666666665, test acc:0.2613 ===\n",
      "train loss:2.2221036284932363\n",
      "train loss:2.2226573237758105\n",
      "train loss:2.228207205098986\n",
      "=== epoch:44, train acc:0.31333333333333335, test acc:0.2637 ===\n",
      "train loss:2.222286467570423\n",
      "train loss:2.228216764631159\n",
      "train loss:2.2027883032968463\n",
      "=== epoch:45, train acc:0.31333333333333335, test acc:0.2593 ===\n",
      "train loss:2.175825519634632\n",
      "train loss:2.2138095574150434\n",
      "train loss:2.224458255014681\n",
      "=== epoch:46, train acc:0.31333333333333335, test acc:0.2562 ===\n",
      "train loss:2.20016953170136\n",
      "train loss:2.2305340690599884\n",
      "train loss:2.2050653626787793\n",
      "=== epoch:47, train acc:0.30666666666666664, test acc:0.2535 ===\n",
      "train loss:2.204181707905084\n",
      "train loss:2.200520706809454\n",
      "train loss:2.2086306634284476\n",
      "=== epoch:48, train acc:0.30666666666666664, test acc:0.2517 ===\n",
      "train loss:2.2189813096507076\n",
      "train loss:2.207840655563053\n",
      "train loss:2.1910574040759934\n",
      "=== epoch:49, train acc:0.30666666666666664, test acc:0.248 ===\n",
      "train loss:2.2289168518644864\n",
      "train loss:2.194211712716603\n",
      "train loss:2.2117735733704755\n",
      "=== epoch:50, train acc:0.29333333333333333, test acc:0.2455 ===\n",
      "train loss:2.2140305935525157\n",
      "train loss:2.231060525386386\n",
      "train loss:2.1683812080913203\n",
      "=== epoch:51, train acc:0.2866666666666667, test acc:0.2424 ===\n",
      "train loss:2.1715043750530034\n",
      "train loss:2.2179230949048576\n",
      "train loss:2.223687022218727\n",
      "=== epoch:52, train acc:0.2866666666666667, test acc:0.2369 ===\n",
      "train loss:2.1866634306045283\n",
      "train loss:2.1606003196243586\n",
      "train loss:2.216471425981635\n",
      "=== epoch:53, train acc:0.2866666666666667, test acc:0.2339 ===\n",
      "train loss:2.17530738761391\n",
      "train loss:2.163543264998564\n",
      "train loss:2.1955676651925735\n",
      "=== epoch:54, train acc:0.29, test acc:0.2362 ===\n",
      "train loss:2.2113123911587937\n",
      "train loss:2.1864020673574216\n",
      "train loss:2.1699184315435724\n",
      "=== epoch:55, train acc:0.29333333333333333, test acc:0.2363 ===\n",
      "train loss:2.1638485031866854\n",
      "train loss:2.1991380107183787\n",
      "train loss:2.1637002537235914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:56, train acc:0.29333333333333333, test acc:0.2372 ===\n",
      "train loss:2.2043457542969236\n",
      "train loss:2.190724808082194\n",
      "train loss:2.12176917575993\n",
      "=== epoch:57, train acc:0.29, test acc:0.235 ===\n",
      "train loss:2.201563117509279\n",
      "train loss:2.21408780497264\n",
      "train loss:2.156440709260063\n",
      "=== epoch:58, train acc:0.2966666666666667, test acc:0.236 ===\n",
      "train loss:2.185494282222665\n",
      "train loss:2.179938471891265\n",
      "train loss:2.1669958492015713\n",
      "=== epoch:59, train acc:0.30333333333333334, test acc:0.2393 ===\n",
      "train loss:2.155080301619888\n",
      "train loss:2.214972568723867\n",
      "train loss:2.1924286573298772\n",
      "=== epoch:60, train acc:0.30333333333333334, test acc:0.2384 ===\n",
      "train loss:2.1954490380953584\n",
      "train loss:2.1917199165158703\n",
      "train loss:2.135285917841273\n",
      "=== epoch:61, train acc:0.3, test acc:0.2344 ===\n",
      "train loss:2.1374873671940895\n",
      "train loss:2.1712117660836183\n",
      "train loss:2.157337407301946\n",
      "=== epoch:62, train acc:0.30333333333333334, test acc:0.2358 ===\n",
      "train loss:2.197214408911791\n",
      "train loss:2.1340088662450527\n",
      "train loss:2.1963347648446616\n",
      "=== epoch:63, train acc:0.31, test acc:0.239 ===\n",
      "train loss:2.0961473835332782\n",
      "train loss:2.1556404134038156\n",
      "train loss:2.145186422866775\n",
      "=== epoch:64, train acc:0.30333333333333334, test acc:0.2352 ===\n",
      "train loss:2.1047583066942135\n",
      "train loss:2.17416321067493\n",
      "train loss:2.136464455604462\n",
      "=== epoch:65, train acc:0.3, test acc:0.2329 ===\n",
      "train loss:2.095199340756926\n",
      "train loss:2.1280060004313297\n",
      "train loss:2.1508010494399574\n",
      "=== epoch:66, train acc:0.3, test acc:0.2286 ===\n",
      "train loss:2.1491211067169003\n",
      "train loss:2.188961098552264\n",
      "train loss:2.1609381744049823\n",
      "=== epoch:67, train acc:0.30333333333333334, test acc:0.2331 ===\n",
      "train loss:2.1405163138459025\n",
      "train loss:2.1293217140319594\n",
      "train loss:2.159784360535679\n",
      "=== epoch:68, train acc:0.30333333333333334, test acc:0.2295 ===\n",
      "train loss:2.1665215432176983\n",
      "train loss:2.1190703258067587\n",
      "train loss:2.1568460648894527\n",
      "=== epoch:69, train acc:0.3, test acc:0.2273 ===\n",
      "train loss:2.170126839609187\n",
      "train loss:2.1709183254058817\n",
      "train loss:2.1750460640745963\n",
      "=== epoch:70, train acc:0.30333333333333334, test acc:0.2313 ===\n",
      "train loss:2.1516825187335904\n",
      "train loss:2.175807002980665\n",
      "train loss:2.1746239355833823\n",
      "=== epoch:71, train acc:0.31, test acc:0.2362 ===\n",
      "train loss:2.077103026731032\n",
      "train loss:2.1143349405387286\n",
      "train loss:2.1176134605231867\n",
      "=== epoch:72, train acc:0.30333333333333334, test acc:0.2345 ===\n",
      "train loss:2.169600097198854\n",
      "train loss:2.0909819749760112\n",
      "train loss:2.1027689477333427\n",
      "=== epoch:73, train acc:0.30666666666666664, test acc:0.2368 ===\n",
      "train loss:2.1212013433462173\n",
      "train loss:2.109703888278283\n",
      "train loss:2.1571999518097926\n",
      "=== epoch:74, train acc:0.30666666666666664, test acc:0.2378 ===\n",
      "train loss:2.109575903841557\n",
      "train loss:2.091084240127404\n",
      "train loss:2.1726588023881166\n",
      "=== epoch:75, train acc:0.30666666666666664, test acc:0.2383 ===\n",
      "train loss:2.1165126104410485\n",
      "train loss:2.1022525852333307\n",
      "train loss:2.0140884658957856\n",
      "=== epoch:76, train acc:0.30333333333333334, test acc:0.2353 ===\n",
      "train loss:2.120465052456174\n",
      "train loss:2.1057812096584296\n",
      "train loss:2.073566312069658\n",
      "=== epoch:77, train acc:0.30666666666666664, test acc:0.2387 ===\n",
      "train loss:2.0063600918365263\n",
      "train loss:2.0455780146046676\n",
      "train loss:2.0772572522998973\n",
      "=== epoch:78, train acc:0.30333333333333334, test acc:0.2325 ===\n",
      "train loss:2.143493080130799\n",
      "train loss:2.068131661541089\n",
      "train loss:2.082874160344243\n",
      "=== epoch:79, train acc:0.30333333333333334, test acc:0.2348 ===\n",
      "train loss:2.044735241403992\n",
      "train loss:2.0835912799630263\n",
      "train loss:2.0685884237235816\n",
      "=== epoch:80, train acc:0.3, test acc:0.2346 ===\n",
      "train loss:2.140197848367549\n",
      "train loss:2.0900989694716725\n",
      "train loss:2.079859770449529\n",
      "=== epoch:81, train acc:0.30333333333333334, test acc:0.237 ===\n",
      "train loss:2.07826003382277\n",
      "train loss:2.1109006022836727\n",
      "train loss:2.085782782184177\n",
      "=== epoch:82, train acc:0.30333333333333334, test acc:0.2385 ===\n",
      "train loss:2.050714121979553\n",
      "train loss:2.017120493526961\n",
      "train loss:2.068569892746531\n",
      "=== epoch:83, train acc:0.30666666666666664, test acc:0.2391 ===\n",
      "train loss:2.154741115688295\n",
      "train loss:2.102921123146962\n",
      "train loss:2.127026817473995\n",
      "=== epoch:84, train acc:0.31333333333333335, test acc:0.2468 ===\n",
      "train loss:2.1174355933169964\n",
      "train loss:2.058312749914067\n",
      "train loss:2.0838782531105857\n",
      "=== epoch:85, train acc:0.31333333333333335, test acc:0.2508 ===\n",
      "train loss:1.9999144715841848\n",
      "train loss:2.094235247661783\n",
      "train loss:2.0710289838161295\n",
      "=== epoch:86, train acc:0.31666666666666665, test acc:0.2508 ===\n",
      "train loss:1.962951165302697\n",
      "train loss:2.1029572331143274\n",
      "train loss:2.100354090592684\n",
      "=== epoch:87, train acc:0.31333333333333335, test acc:0.25 ===\n",
      "train loss:2.0949216334010505\n",
      "train loss:2.001015997987105\n",
      "train loss:2.066105628699365\n",
      "=== epoch:88, train acc:0.31666666666666665, test acc:0.2509 ===\n",
      "train loss:2.0981840242166054\n",
      "train loss:2.053092753418431\n",
      "train loss:2.0666996677843845\n",
      "=== epoch:89, train acc:0.32, test acc:0.2545 ===\n",
      "train loss:2.123406989792541\n",
      "train loss:2.087528664847112\n",
      "train loss:2.108928030580263\n",
      "=== epoch:90, train acc:0.3333333333333333, test acc:0.2613 ===\n",
      "train loss:2.080345918222377\n",
      "train loss:2.0285288320436736\n",
      "train loss:1.9975793759875915\n",
      "=== epoch:91, train acc:0.3333333333333333, test acc:0.2633 ===\n",
      "train loss:2.0010705444051404\n",
      "train loss:1.9825810895798133\n",
      "train loss:2.0740051988891444\n",
      "=== epoch:92, train acc:0.33666666666666667, test acc:0.2616 ===\n",
      "train loss:2.0799947947013546\n",
      "train loss:1.9554917851969507\n",
      "train loss:1.9620829559855135\n",
      "=== epoch:93, train acc:0.3333333333333333, test acc:0.257 ===\n",
      "train loss:2.045447353978871\n",
      "train loss:2.0266142699301266\n",
      "train loss:2.013073431991896\n",
      "=== epoch:94, train acc:0.33666666666666667, test acc:0.2589 ===\n",
      "train loss:1.9700995200417593\n",
      "train loss:2.0597891491057245\n",
      "train loss:1.924463444088442\n",
      "=== epoch:95, train acc:0.33666666666666667, test acc:0.2597 ===\n",
      "train loss:2.0126721395479774\n",
      "train loss:1.9742468024440378\n",
      "train loss:2.0370932172848124\n",
      "=== epoch:96, train acc:0.3333333333333333, test acc:0.2637 ===\n",
      "train loss:2.047839689931791\n",
      "train loss:2.055010692565265\n",
      "train loss:2.031704936162639\n",
      "=== epoch:97, train acc:0.32666666666666666, test acc:0.2689 ===\n",
      "train loss:2.086728812914129\n",
      "train loss:2.025600659233238\n",
      "train loss:1.9228989213061882\n",
      "=== epoch:98, train acc:0.32666666666666666, test acc:0.272 ===\n",
      "train loss:1.980111611418458\n",
      "train loss:2.0427352839722053\n",
      "train loss:2.0122394285697367\n",
      "=== epoch:99, train acc:0.3466666666666667, test acc:0.2774 ===\n",
      "train loss:1.9710721370176754\n",
      "train loss:2.0520410722583526\n",
      "train loss:1.9252601091984871\n",
      "=== epoch:100, train acc:0.3466666666666667, test acc:0.2771 ===\n",
      "train loss:2.096549575065842\n",
      "train loss:2.058671294047095\n",
      "train loss:2.009575307889618\n",
      "=== epoch:101, train acc:0.35333333333333333, test acc:0.2788 ===\n",
      "train loss:2.0216313617497037\n",
      "train loss:2.069756165033512\n",
      "train loss:2.0958592559973273\n",
      "=== epoch:102, train acc:0.35333333333333333, test acc:0.2832 ===\n",
      "train loss:1.9759118986026982\n",
      "train loss:1.928232361361231\n",
      "train loss:1.9306104123263919\n",
      "=== epoch:103, train acc:0.3566666666666667, test acc:0.2815 ===\n",
      "train loss:1.9732486877805235\n",
      "train loss:2.026683973009633\n",
      "train loss:1.994379439942019\n",
      "=== epoch:104, train acc:0.3566666666666667, test acc:0.2834 ===\n",
      "train loss:1.9596710308120233\n",
      "train loss:2.0712337546525355\n",
      "train loss:2.048992533824373\n",
      "=== epoch:105, train acc:0.36, test acc:0.2878 ===\n",
      "train loss:2.0596536875919393\n",
      "train loss:1.9104685928129987\n",
      "train loss:2.066678671525226\n",
      "=== epoch:106, train acc:0.36, test acc:0.2918 ===\n",
      "train loss:1.9754689404095338\n",
      "train loss:1.9205523961415818\n",
      "train loss:1.9298552677457301\n",
      "=== epoch:107, train acc:0.36333333333333334, test acc:0.2899 ===\n",
      "train loss:1.9511642140415142\n",
      "train loss:1.8787648961853531\n",
      "train loss:1.9350564932672016\n",
      "=== epoch:108, train acc:0.36, test acc:0.2905 ===\n",
      "train loss:2.0457107021910885\n",
      "train loss:1.9253613193892403\n",
      "train loss:2.044716693778173\n",
      "=== epoch:109, train acc:0.36666666666666664, test acc:0.2977 ===\n",
      "train loss:2.001470676211955\n",
      "train loss:1.992995753346786\n",
      "train loss:2.0207822040190675\n",
      "=== epoch:110, train acc:0.37, test acc:0.2997 ===\n",
      "train loss:2.013799685900152\n",
      "train loss:1.9469390074474737\n",
      "train loss:1.8848027541557408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:111, train acc:0.37333333333333335, test acc:0.303 ===\n",
      "train loss:1.863452083852164\n",
      "train loss:1.937497807885072\n",
      "train loss:1.969950842592458\n",
      "=== epoch:112, train acc:0.37, test acc:0.3027 ===\n",
      "train loss:1.9049242890656959\n",
      "train loss:2.017450237110186\n",
      "train loss:1.835137259118409\n",
      "=== epoch:113, train acc:0.37333333333333335, test acc:0.306 ===\n",
      "train loss:1.9738056651486022\n",
      "train loss:1.9669826390060792\n",
      "train loss:1.9638809475322234\n",
      "=== epoch:114, train acc:0.37333333333333335, test acc:0.3074 ===\n",
      "train loss:1.947992724339209\n",
      "train loss:1.9648766440084484\n",
      "train loss:1.9905953751590344\n",
      "=== epoch:115, train acc:0.38666666666666666, test acc:0.3137 ===\n",
      "train loss:2.073229472587781\n",
      "train loss:2.001055140157013\n",
      "train loss:1.883459914052046\n",
      "=== epoch:116, train acc:0.4, test acc:0.3175 ===\n",
      "train loss:1.9715764168907208\n",
      "train loss:1.9532246361397336\n",
      "train loss:2.0137251617989946\n",
      "=== epoch:117, train acc:0.4033333333333333, test acc:0.3217 ===\n",
      "train loss:2.0072060775902303\n",
      "train loss:1.9603636658612413\n",
      "train loss:1.9550114175511046\n",
      "=== epoch:118, train acc:0.4066666666666667, test acc:0.3219 ===\n",
      "train loss:1.885017062953431\n",
      "train loss:1.8895416826579001\n",
      "train loss:1.8947465154142729\n",
      "=== epoch:119, train acc:0.4033333333333333, test acc:0.3201 ===\n",
      "train loss:1.8552356140450899\n",
      "train loss:1.9365921897213685\n",
      "train loss:1.8968552182221823\n",
      "=== epoch:120, train acc:0.4, test acc:0.3223 ===\n",
      "train loss:1.9660937032613084\n",
      "train loss:1.9179496054361116\n",
      "train loss:1.8679196626831647\n",
      "=== epoch:121, train acc:0.4033333333333333, test acc:0.3222 ===\n",
      "train loss:1.9181336613500322\n",
      "train loss:1.9588848723547656\n",
      "train loss:1.9112120510320258\n",
      "=== epoch:122, train acc:0.4066666666666667, test acc:0.3279 ===\n",
      "train loss:1.9428524536329028\n",
      "train loss:1.9360273467166624\n",
      "train loss:1.8310146635423836\n",
      "=== epoch:123, train acc:0.4066666666666667, test acc:0.3302 ===\n",
      "train loss:1.8845972365136319\n",
      "train loss:1.9136446423616598\n",
      "train loss:1.9513628266037728\n",
      "=== epoch:124, train acc:0.4066666666666667, test acc:0.3289 ===\n",
      "train loss:1.8999139258945368\n",
      "train loss:1.7436191233528284\n",
      "train loss:1.9633620079528364\n",
      "=== epoch:125, train acc:0.4, test acc:0.3241 ===\n",
      "train loss:1.8078206443892393\n",
      "train loss:1.9204161737519514\n",
      "train loss:1.9692494248375538\n",
      "=== epoch:126, train acc:0.41, test acc:0.3274 ===\n",
      "train loss:1.8495257007477273\n",
      "train loss:1.8848171274290086\n",
      "train loss:1.896807106938679\n",
      "=== epoch:127, train acc:0.4166666666666667, test acc:0.3328 ===\n",
      "train loss:1.9463139324575893\n",
      "train loss:1.9467062871570346\n",
      "train loss:1.9488738095153086\n",
      "=== epoch:128, train acc:0.42, test acc:0.3368 ===\n",
      "train loss:1.8875726606137162\n",
      "train loss:1.882605702478759\n",
      "train loss:1.9098870956035958\n",
      "=== epoch:129, train acc:0.42333333333333334, test acc:0.338 ===\n",
      "train loss:1.9276721882692482\n",
      "train loss:1.8804383636124242\n",
      "train loss:1.877575203711257\n",
      "=== epoch:130, train acc:0.42333333333333334, test acc:0.3379 ===\n",
      "train loss:1.874869470499332\n",
      "train loss:1.8347752151398244\n",
      "train loss:1.9046259965815606\n",
      "=== epoch:131, train acc:0.43, test acc:0.3438 ===\n",
      "train loss:1.9562914691765356\n",
      "train loss:1.7830735662921955\n",
      "train loss:1.8085722992757807\n",
      "=== epoch:132, train acc:0.43, test acc:0.3433 ===\n",
      "train loss:1.8431579923732528\n",
      "train loss:1.820205641233764\n",
      "train loss:1.8824685962172387\n",
      "=== epoch:133, train acc:0.44, test acc:0.3501 ===\n",
      "train loss:1.9471758210465586\n",
      "train loss:1.9387082708468153\n",
      "train loss:1.8295844230772655\n",
      "=== epoch:134, train acc:0.43666666666666665, test acc:0.352 ===\n",
      "train loss:1.8863842683703818\n",
      "train loss:1.8979563979446197\n",
      "train loss:1.8874920252782115\n",
      "=== epoch:135, train acc:0.43666666666666665, test acc:0.3539 ===\n",
      "train loss:1.766198065415861\n",
      "train loss:1.790087729946218\n",
      "train loss:1.8359418005865962\n",
      "=== epoch:136, train acc:0.43666666666666665, test acc:0.3508 ===\n",
      "train loss:1.8247117369754406\n",
      "train loss:1.8385637125601533\n",
      "train loss:1.9072738092688164\n",
      "=== epoch:137, train acc:0.44, test acc:0.3547 ===\n",
      "train loss:1.9173286519184223\n",
      "train loss:1.8598588161658984\n",
      "train loss:1.6957436621442066\n",
      "=== epoch:138, train acc:0.44, test acc:0.3578 ===\n",
      "train loss:1.8419253994828595\n",
      "train loss:1.8563795237200855\n",
      "train loss:1.8685046302136465\n",
      "=== epoch:139, train acc:0.43666666666666665, test acc:0.3596 ===\n",
      "train loss:1.9092898101685765\n",
      "train loss:1.8378940258212757\n",
      "train loss:1.7307121659583504\n",
      "=== epoch:140, train acc:0.45, test acc:0.362 ===\n",
      "train loss:1.921209739286273\n",
      "train loss:1.9430290350368924\n",
      "train loss:1.9762455307716535\n",
      "=== epoch:141, train acc:0.4633333333333333, test acc:0.3679 ===\n",
      "train loss:1.7770123139522243\n",
      "train loss:1.818934408089961\n",
      "train loss:1.8679358866506193\n",
      "=== epoch:142, train acc:0.45666666666666667, test acc:0.3659 ===\n",
      "train loss:1.8589132858490516\n",
      "train loss:1.8521994076542447\n",
      "train loss:1.8526514095098443\n",
      "=== epoch:143, train acc:0.4533333333333333, test acc:0.3654 ===\n",
      "train loss:1.809842574348896\n",
      "train loss:1.828976228192824\n",
      "train loss:1.8875240369376975\n",
      "=== epoch:144, train acc:0.45666666666666667, test acc:0.3689 ===\n",
      "train loss:1.8277376801492873\n",
      "train loss:1.7947614781598957\n",
      "train loss:1.8396395809696298\n",
      "=== epoch:145, train acc:0.4533333333333333, test acc:0.3716 ===\n",
      "train loss:1.7804115223304375\n",
      "train loss:1.7556824123598886\n",
      "train loss:1.7931502216594848\n",
      "=== epoch:146, train acc:0.4533333333333333, test acc:0.3727 ===\n",
      "train loss:1.6574393178431628\n",
      "train loss:1.7929381225118413\n",
      "train loss:1.7315551723651157\n",
      "=== epoch:147, train acc:0.4533333333333333, test acc:0.3705 ===\n",
      "train loss:1.8290707046627848\n",
      "train loss:1.8197239568243662\n",
      "train loss:1.8403142870625413\n",
      "=== epoch:148, train acc:0.47333333333333333, test acc:0.3795 ===\n",
      "train loss:1.9267036905764983\n",
      "train loss:1.8015016369917185\n",
      "train loss:1.7847627306552893\n",
      "=== epoch:149, train acc:0.49, test acc:0.3848 ===\n",
      "train loss:1.7521190638221582\n",
      "train loss:1.8567626135548054\n",
      "train loss:1.9187238467023007\n",
      "=== epoch:150, train acc:0.48, test acc:0.3838 ===\n",
      "train loss:1.7634433806132994\n",
      "train loss:1.7059626972168829\n",
      "train loss:2.0104631015999987\n",
      "=== epoch:151, train acc:0.4866666666666667, test acc:0.3825 ===\n",
      "train loss:1.7830327053299195\n",
      "train loss:1.8379623375874352\n",
      "train loss:1.8028815046991016\n",
      "=== epoch:152, train acc:0.48333333333333334, test acc:0.3855 ===\n",
      "train loss:1.8307731171941615\n",
      "train loss:1.7890442329647467\n",
      "train loss:1.7971813290147596\n",
      "=== epoch:153, train acc:0.49333333333333335, test acc:0.3912 ===\n",
      "train loss:1.9061444168470063\n",
      "train loss:1.854118395072082\n",
      "train loss:1.8826491128521545\n",
      "=== epoch:154, train acc:0.5033333333333333, test acc:0.3989 ===\n",
      "train loss:1.7834515315176218\n",
      "train loss:1.7522203012017477\n",
      "train loss:1.6638197349589163\n",
      "=== epoch:155, train acc:0.5066666666666667, test acc:0.3955 ===\n",
      "train loss:1.7276052791568994\n",
      "train loss:1.7313195758271933\n",
      "train loss:1.642404802938783\n",
      "=== epoch:156, train acc:0.4866666666666667, test acc:0.3898 ===\n",
      "train loss:1.7132303900702632\n",
      "train loss:1.7194705578706464\n",
      "train loss:1.7743966388969232\n",
      "=== epoch:157, train acc:0.49333333333333335, test acc:0.3885 ===\n",
      "train loss:1.774510671833936\n",
      "train loss:1.7432010411035117\n",
      "train loss:1.8711055844826194\n",
      "=== epoch:158, train acc:0.49666666666666665, test acc:0.3916 ===\n",
      "train loss:1.7005424321709806\n",
      "train loss:1.569724105866936\n",
      "train loss:1.7408298147956731\n",
      "=== epoch:159, train acc:0.49333333333333335, test acc:0.3893 ===\n",
      "train loss:1.660870391042236\n",
      "train loss:1.8229695404279482\n",
      "train loss:1.7283972647915748\n",
      "=== epoch:160, train acc:0.49333333333333335, test acc:0.3905 ===\n",
      "train loss:1.726467484046831\n",
      "train loss:1.8038296539317051\n",
      "train loss:1.687605147499169\n",
      "=== epoch:161, train acc:0.49333333333333335, test acc:0.3912 ===\n",
      "train loss:1.8259191479980126\n",
      "train loss:1.7226172390316723\n",
      "train loss:1.6660484387227086\n",
      "=== epoch:162, train acc:0.49333333333333335, test acc:0.3938 ===\n",
      "train loss:1.7125585680419582\n",
      "train loss:1.8129335080652489\n",
      "train loss:1.8801586078450847\n",
      "=== epoch:163, train acc:0.49666666666666665, test acc:0.396 ===\n",
      "train loss:1.7921393489950452\n",
      "train loss:1.7789450955522776\n",
      "train loss:1.7122385311070107\n",
      "=== epoch:164, train acc:0.49666666666666665, test acc:0.3963 ===\n",
      "train loss:1.8206450777678997\n",
      "train loss:1.811054917373853\n",
      "train loss:1.8078561997829983\n",
      "=== epoch:165, train acc:0.5033333333333333, test acc:0.4006 ===\n",
      "train loss:1.7246025573148578\n",
      "train loss:1.691201594482689\n",
      "train loss:1.7323912438207782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:166, train acc:0.49666666666666665, test acc:0.4022 ===\n",
      "train loss:1.601684135433723\n",
      "train loss:1.7534902978161329\n",
      "train loss:1.6689916438129972\n",
      "=== epoch:167, train acc:0.49666666666666665, test acc:0.4021 ===\n",
      "train loss:1.767247388982888\n",
      "train loss:1.7652262820605011\n",
      "train loss:1.7727762141998011\n",
      "=== epoch:168, train acc:0.51, test acc:0.4072 ===\n",
      "train loss:1.6497015887287703\n",
      "train loss:1.8147428397102647\n",
      "train loss:1.7310005145524383\n",
      "=== epoch:169, train acc:0.51, test acc:0.4066 ===\n",
      "train loss:1.7079196307432756\n",
      "train loss:1.7113215901791798\n",
      "train loss:1.7317891399700982\n",
      "=== epoch:170, train acc:0.5133333333333333, test acc:0.4084 ===\n",
      "train loss:1.790786745746264\n",
      "train loss:1.8151818308201715\n",
      "train loss:1.7186703009842716\n",
      "=== epoch:171, train acc:0.5233333333333333, test acc:0.4118 ===\n",
      "train loss:1.6778716516976533\n",
      "train loss:1.768381609365936\n",
      "train loss:1.6891317476807217\n",
      "=== epoch:172, train acc:0.53, test acc:0.4156 ===\n",
      "train loss:1.6391650514771285\n",
      "train loss:1.6751290743088896\n",
      "train loss:1.7667181471304871\n",
      "=== epoch:173, train acc:0.5366666666666666, test acc:0.4183 ===\n",
      "train loss:1.6907334101748888\n",
      "train loss:1.7454665775065237\n",
      "train loss:1.7378241656069577\n",
      "=== epoch:174, train acc:0.5366666666666666, test acc:0.4185 ===\n",
      "train loss:1.6731617141719428\n",
      "train loss:1.7197805229276355\n",
      "train loss:1.7751625898201149\n",
      "=== epoch:175, train acc:0.54, test acc:0.4192 ===\n",
      "train loss:1.741912228683132\n",
      "train loss:1.6844273424469034\n",
      "train loss:1.6351793154968979\n",
      "=== epoch:176, train acc:0.5366666666666666, test acc:0.4218 ===\n",
      "train loss:1.6902212963028616\n",
      "train loss:1.57468276361576\n",
      "train loss:1.707432025341697\n",
      "=== epoch:177, train acc:0.54, test acc:0.421 ===\n",
      "train loss:1.6783240534001047\n",
      "train loss:1.7392929856980783\n",
      "train loss:1.5724173987671386\n",
      "=== epoch:178, train acc:0.55, test acc:0.4223 ===\n",
      "train loss:1.7247349605023148\n",
      "train loss:1.6294788574945491\n",
      "train loss:1.5912112384159725\n",
      "=== epoch:179, train acc:0.55, test acc:0.4232 ===\n",
      "train loss:1.7310804614135156\n",
      "train loss:1.627958516241037\n",
      "train loss:1.7001622392434694\n",
      "=== epoch:180, train acc:0.54, test acc:0.4225 ===\n",
      "train loss:1.6144641944228466\n",
      "train loss:1.7285393986956397\n",
      "train loss:1.7787172363641854\n",
      "=== epoch:181, train acc:0.55, test acc:0.4255 ===\n",
      "train loss:1.6105142779313482\n",
      "train loss:1.6235072318149764\n",
      "train loss:1.7167728002644445\n",
      "=== epoch:182, train acc:0.5566666666666666, test acc:0.4295 ===\n",
      "train loss:1.5179784772185088\n",
      "train loss:1.5677420088595801\n",
      "train loss:1.7249818738592742\n",
      "=== epoch:183, train acc:0.5566666666666666, test acc:0.4296 ===\n",
      "train loss:1.7401551689627706\n",
      "train loss:1.6368073193421253\n",
      "train loss:1.5484931385768923\n",
      "=== epoch:184, train acc:0.5466666666666666, test acc:0.4311 ===\n",
      "train loss:1.5974244107887225\n",
      "train loss:1.695956168950966\n",
      "train loss:1.6288922333343112\n",
      "=== epoch:185, train acc:0.5533333333333333, test acc:0.4291 ===\n",
      "train loss:1.6604919701883278\n",
      "train loss:1.6273283864287966\n",
      "train loss:1.7403766261338998\n",
      "=== epoch:186, train acc:0.55, test acc:0.4328 ===\n",
      "train loss:1.54806900916699\n",
      "train loss:1.5925704868591493\n",
      "train loss:1.6814110990387412\n",
      "=== epoch:187, train acc:0.5466666666666666, test acc:0.4285 ===\n",
      "train loss:1.5863674724521801\n",
      "train loss:1.6666648080751756\n",
      "train loss:1.6084668661606312\n",
      "=== epoch:188, train acc:0.54, test acc:0.4298 ===\n",
      "train loss:1.7393499320306212\n",
      "train loss:1.5584883253748152\n",
      "train loss:1.5069289880666847\n",
      "=== epoch:189, train acc:0.55, test acc:0.4347 ===\n",
      "train loss:1.5427575357779955\n",
      "train loss:1.5336068870055564\n",
      "train loss:1.6504807799034722\n",
      "=== epoch:190, train acc:0.54, test acc:0.4346 ===\n",
      "train loss:1.540493425598388\n",
      "train loss:1.7589093052630633\n",
      "train loss:1.6502430734953624\n",
      "=== epoch:191, train acc:0.5433333333333333, test acc:0.4381 ===\n",
      "train loss:1.59962158800939\n",
      "train loss:1.601770200825981\n",
      "train loss:1.616589464461352\n",
      "=== epoch:192, train acc:0.56, test acc:0.441 ===\n",
      "train loss:1.5612813083260255\n",
      "train loss:1.51013095548976\n",
      "train loss:1.575434308080463\n",
      "=== epoch:193, train acc:0.56, test acc:0.4398 ===\n",
      "train loss:1.7201677629291967\n",
      "train loss:1.477165709742033\n",
      "train loss:1.6411379247501543\n",
      "=== epoch:194, train acc:0.5633333333333334, test acc:0.4414 ===\n",
      "train loss:1.618538203603042\n",
      "train loss:1.4916871314289881\n",
      "train loss:1.4881884191481232\n",
      "=== epoch:195, train acc:0.5566666666666666, test acc:0.4412 ===\n",
      "train loss:1.6018101084317837\n",
      "train loss:1.5023306979212088\n",
      "train loss:1.6234176269373717\n",
      "=== epoch:196, train acc:0.5666666666666667, test acc:0.4415 ===\n",
      "train loss:1.5761382275843647\n",
      "train loss:1.5327480943091152\n",
      "train loss:1.7252882538933654\n",
      "=== epoch:197, train acc:0.56, test acc:0.4439 ===\n",
      "train loss:1.5448585464902522\n",
      "train loss:1.5806991800504733\n",
      "train loss:1.4633821282826411\n",
      "=== epoch:198, train acc:0.57, test acc:0.4453 ===\n",
      "train loss:1.6032439963682368\n",
      "train loss:1.54545251606266\n",
      "train loss:1.5548369940912679\n",
      "=== epoch:199, train acc:0.5566666666666666, test acc:0.4434 ===\n",
      "train loss:1.6002185774943707\n",
      "train loss:1.5735079616510104\n",
      "train loss:1.607459703283637\n",
      "=== epoch:200, train acc:0.5533333333333333, test acc:0.4457 ===\n",
      "train loss:1.61106208651671\n",
      "train loss:1.5676701173058332\n",
      "train loss:1.495056265865563\n",
      "=== epoch:201, train acc:0.56, test acc:0.4477 ===\n",
      "train loss:1.4766437532505103\n",
      "train loss:1.5143720234868252\n",
      "train loss:1.4613072581129987\n",
      "=== epoch:202, train acc:0.56, test acc:0.4503 ===\n",
      "train loss:1.640829854478072\n",
      "train loss:1.60761518209723\n",
      "train loss:1.6524062699151592\n",
      "=== epoch:203, train acc:0.56, test acc:0.4527 ===\n",
      "train loss:1.6205358394578697\n",
      "train loss:1.4416077848682733\n",
      "train loss:1.6150049258460817\n",
      "=== epoch:204, train acc:0.5666666666666667, test acc:0.4526 ===\n",
      "train loss:1.501044427598424\n",
      "train loss:1.5741689822923988\n",
      "train loss:1.462139781751987\n",
      "=== epoch:205, train acc:0.5633333333333334, test acc:0.4533 ===\n",
      "train loss:1.5829506523174184\n",
      "train loss:1.5161850357891382\n",
      "train loss:1.4555729439479712\n",
      "=== epoch:206, train acc:0.5666666666666667, test acc:0.4552 ===\n",
      "train loss:1.6241504189463427\n",
      "train loss:1.4581638622630078\n",
      "train loss:1.4668230059280984\n",
      "=== epoch:207, train acc:0.5666666666666667, test acc:0.4572 ===\n",
      "train loss:1.5859037484907652\n",
      "train loss:1.6446975778374386\n",
      "train loss:1.6452889564113904\n",
      "=== epoch:208, train acc:0.58, test acc:0.4626 ===\n",
      "train loss:1.55723459005815\n",
      "train loss:1.5002854717867962\n",
      "train loss:1.5743642967363067\n",
      "=== epoch:209, train acc:0.5733333333333334, test acc:0.463 ===\n",
      "train loss:1.3581567274249275\n",
      "train loss:1.4593024313284875\n",
      "train loss:1.5232856839435454\n",
      "=== epoch:210, train acc:0.5733333333333334, test acc:0.4638 ===\n",
      "train loss:1.3810507484558212\n",
      "train loss:1.3882511512599387\n",
      "train loss:1.4423707856799155\n",
      "=== epoch:211, train acc:0.5733333333333334, test acc:0.4631 ===\n",
      "train loss:1.4943386591016838\n",
      "train loss:1.3958618833646446\n",
      "train loss:1.395149494937089\n",
      "=== epoch:212, train acc:0.5766666666666667, test acc:0.4642 ===\n",
      "train loss:1.4833030088495844\n",
      "train loss:1.6151506146045662\n",
      "train loss:1.6864757179365384\n",
      "=== epoch:213, train acc:0.58, test acc:0.4701 ===\n",
      "train loss:1.4668678557910595\n",
      "train loss:1.3823398601130026\n",
      "train loss:1.3480259270761374\n",
      "=== epoch:214, train acc:0.5866666666666667, test acc:0.4714 ===\n",
      "train loss:1.448968584271033\n",
      "train loss:1.46559123977792\n",
      "train loss:1.4623504609023597\n",
      "=== epoch:215, train acc:0.5966666666666667, test acc:0.4737 ===\n",
      "train loss:1.6216038829220363\n",
      "train loss:1.4391676413420635\n",
      "train loss:1.6171858656024087\n",
      "=== epoch:216, train acc:0.6033333333333334, test acc:0.4741 ===\n",
      "train loss:1.6002258135861576\n",
      "train loss:1.3762247272586554\n",
      "train loss:1.4383409130870746\n",
      "=== epoch:217, train acc:0.5966666666666667, test acc:0.4757 ===\n",
      "train loss:1.3947300279667334\n",
      "train loss:1.3757619387047257\n",
      "train loss:1.375314752198522\n",
      "=== epoch:218, train acc:0.6, test acc:0.4753 ===\n",
      "train loss:1.4759933774602099\n",
      "train loss:1.5022995666229428\n",
      "train loss:1.4622276942701817\n",
      "=== epoch:219, train acc:0.61, test acc:0.4794 ===\n",
      "train loss:1.3669341814983447\n",
      "train loss:1.3843379040169455\n",
      "train loss:1.4609514141464073\n",
      "=== epoch:220, train acc:0.61, test acc:0.4807 ===\n",
      "train loss:1.4469235535656446\n",
      "train loss:1.445915669811161\n",
      "train loss:1.5604840487444522\n",
      "=== epoch:221, train acc:0.6066666666666667, test acc:0.4837 ===\n",
      "train loss:1.5471660017280462\n",
      "train loss:1.467757238039431\n",
      "train loss:1.4438819131921738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.62, test acc:0.4851 ===\n",
      "train loss:1.4183465324123992\n",
      "train loss:1.4545543974882411\n",
      "train loss:1.4115579966448637\n",
      "=== epoch:223, train acc:0.6233333333333333, test acc:0.4878 ===\n",
      "train loss:1.4117753910437292\n",
      "train loss:1.3873947336072212\n",
      "train loss:1.3675283135712375\n",
      "=== epoch:224, train acc:0.6233333333333333, test acc:0.488 ===\n",
      "train loss:1.5479985881799125\n",
      "train loss:1.3541978959952385\n",
      "train loss:1.4034139705617479\n",
      "=== epoch:225, train acc:0.6233333333333333, test acc:0.4891 ===\n",
      "train loss:1.440403018738745\n",
      "train loss:1.4764444387355564\n",
      "train loss:1.4887438322602093\n",
      "=== epoch:226, train acc:0.62, test acc:0.4899 ===\n",
      "train loss:1.4090230323535555\n",
      "train loss:1.562307415010366\n",
      "train loss:1.4496912061393403\n",
      "=== epoch:227, train acc:0.6166666666666667, test acc:0.4912 ===\n",
      "train loss:1.4117036891269574\n",
      "train loss:1.4017382236684954\n",
      "train loss:1.4830368223933297\n",
      "=== epoch:228, train acc:0.62, test acc:0.4902 ===\n",
      "train loss:1.3148115400546394\n",
      "train loss:1.2912160687574583\n",
      "train loss:1.581487201004996\n",
      "=== epoch:229, train acc:0.6133333333333333, test acc:0.4869 ===\n",
      "train loss:1.296942387228282\n",
      "train loss:1.1682508715651234\n",
      "train loss:1.3931844566748233\n",
      "=== epoch:230, train acc:0.6066666666666667, test acc:0.4891 ===\n",
      "train loss:1.3935527557303922\n",
      "train loss:1.3715418668011181\n",
      "train loss:1.4232189725231696\n",
      "=== epoch:231, train acc:0.61, test acc:0.4901 ===\n",
      "train loss:1.2617164697526213\n",
      "train loss:1.3311602078963423\n",
      "train loss:1.275518506189129\n",
      "=== epoch:232, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.434589733930982\n",
      "train loss:1.4096389035882564\n",
      "train loss:1.3917692124003511\n",
      "=== epoch:233, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.2818695302209142\n",
      "train loss:1.4026666009448507\n",
      "train loss:1.43017668191691\n",
      "=== epoch:234, train acc:0.62, test acc:0.4928 ===\n",
      "train loss:1.1710561071587469\n",
      "train loss:1.4822128479521124\n",
      "train loss:1.1993306322183\n",
      "=== epoch:235, train acc:0.61, test acc:0.4966 ===\n",
      "train loss:1.2656868294621497\n",
      "train loss:1.355177579547156\n",
      "train loss:1.1916068126361834\n",
      "=== epoch:236, train acc:0.6166666666666667, test acc:0.4939 ===\n",
      "train loss:1.3140271438601119\n",
      "train loss:1.3003115742701947\n",
      "train loss:1.32219638625986\n",
      "=== epoch:237, train acc:0.6166666666666667, test acc:0.4943 ===\n",
      "train loss:1.1644347289183075\n",
      "train loss:1.4207487909877816\n",
      "train loss:1.3074685913952524\n",
      "=== epoch:238, train acc:0.6166666666666667, test acc:0.4957 ===\n",
      "train loss:1.228960293123101\n",
      "train loss:1.4195609153070061\n",
      "train loss:1.40238838294953\n",
      "=== epoch:239, train acc:0.62, test acc:0.4973 ===\n",
      "train loss:1.3808227177664696\n",
      "train loss:1.3341466980320278\n",
      "train loss:1.4339088323483165\n",
      "=== epoch:240, train acc:0.6233333333333333, test acc:0.5005 ===\n",
      "train loss:1.3125003242997466\n",
      "train loss:1.3263370542515907\n",
      "train loss:1.4818384355857797\n",
      "=== epoch:241, train acc:0.6233333333333333, test acc:0.5043 ===\n",
      "train loss:1.3287529531646056\n",
      "train loss:1.3937851045421972\n",
      "train loss:1.2150911221815253\n",
      "=== epoch:242, train acc:0.6266666666666667, test acc:0.5054 ===\n",
      "train loss:1.3427367787132964\n",
      "train loss:1.3137481052621167\n",
      "train loss:1.4476330692196522\n",
      "=== epoch:243, train acc:0.6333333333333333, test acc:0.5105 ===\n",
      "train loss:1.308608411332156\n",
      "train loss:1.4778569546594766\n",
      "train loss:1.1400750300715272\n",
      "=== epoch:244, train acc:0.6266666666666667, test acc:0.5113 ===\n",
      "train loss:1.3619732985344646\n",
      "train loss:1.2083959315807078\n",
      "train loss:1.288040631833285\n",
      "=== epoch:245, train acc:0.63, test acc:0.5113 ===\n",
      "train loss:1.365655138211914\n",
      "train loss:1.3640579582060959\n",
      "train loss:1.2200629382725374\n",
      "=== epoch:246, train acc:0.6333333333333333, test acc:0.5107 ===\n",
      "train loss:1.3011220142843616\n",
      "train loss:1.310209685042117\n",
      "train loss:1.3672493602550668\n",
      "=== epoch:247, train acc:0.63, test acc:0.5128 ===\n",
      "train loss:1.351409787100981\n",
      "train loss:1.1595067385526727\n",
      "train loss:1.1410123166524082\n",
      "=== epoch:248, train acc:0.6333333333333333, test acc:0.5127 ===\n",
      "train loss:1.275452742568816\n",
      "train loss:1.396810384830021\n",
      "train loss:1.2612874106618572\n",
      "=== epoch:249, train acc:0.63, test acc:0.5145 ===\n",
      "train loss:1.21477018962594\n",
      "train loss:1.1997441465647338\n",
      "train loss:1.1639943701986015\n",
      "=== epoch:250, train acc:0.63, test acc:0.513 ===\n",
      "train loss:1.2743407539486264\n",
      "train loss:1.2497999396875503\n",
      "train loss:1.2040936949321057\n",
      "=== epoch:251, train acc:0.6333333333333333, test acc:0.5139 ===\n",
      "train loss:1.16388560137538\n",
      "train loss:1.219159498633611\n",
      "train loss:1.1364667686978864\n",
      "=== epoch:252, train acc:0.6366666666666667, test acc:0.5157 ===\n",
      "train loss:1.1921830934806392\n",
      "train loss:1.1298404912913282\n",
      "train loss:1.399244499397906\n",
      "=== epoch:253, train acc:0.6366666666666667, test acc:0.5178 ===\n",
      "train loss:1.1775865934841214\n",
      "train loss:1.2647793402124439\n",
      "train loss:1.2155369465695003\n",
      "=== epoch:254, train acc:0.64, test acc:0.518 ===\n",
      "train loss:1.2293331351631485\n",
      "train loss:1.1527264144535403\n",
      "train loss:1.3675531767986207\n",
      "=== epoch:255, train acc:0.64, test acc:0.5169 ===\n",
      "train loss:1.3133567579860903\n",
      "train loss:1.1467386893884572\n",
      "train loss:1.1356097313405282\n",
      "=== epoch:256, train acc:0.6433333333333333, test acc:0.5206 ===\n",
      "train loss:1.1908281649187138\n",
      "train loss:1.1513170784093583\n",
      "train loss:1.3369344993405123\n",
      "=== epoch:257, train acc:0.6466666666666666, test acc:0.5211 ===\n",
      "train loss:1.1115841186825335\n",
      "train loss:1.1910838726466628\n",
      "train loss:1.0521012179608429\n",
      "=== epoch:258, train acc:0.64, test acc:0.5175 ===\n",
      "train loss:1.17451663437281\n",
      "train loss:1.2180602075075082\n",
      "train loss:1.074061808073714\n",
      "=== epoch:259, train acc:0.6366666666666667, test acc:0.519 ===\n",
      "train loss:1.1520708482786135\n",
      "train loss:1.1476955321561184\n",
      "train loss:1.2157996911996296\n",
      "=== epoch:260, train acc:0.64, test acc:0.5217 ===\n",
      "train loss:1.1384795232510094\n",
      "train loss:1.0569084305750323\n",
      "train loss:1.1917819772760796\n",
      "=== epoch:261, train acc:0.65, test acc:0.5244 ===\n",
      "train loss:1.2550672082374927\n",
      "train loss:1.1750098295736886\n",
      "train loss:1.298699452657462\n",
      "=== epoch:262, train acc:0.6566666666666666, test acc:0.526 ===\n",
      "train loss:1.266089851659973\n",
      "train loss:1.1897316838105962\n",
      "train loss:1.241490250420644\n",
      "=== epoch:263, train acc:0.6566666666666666, test acc:0.527 ===\n",
      "train loss:1.1156105448835623\n",
      "train loss:1.1449100297684571\n",
      "train loss:1.2283657088276438\n",
      "=== epoch:264, train acc:0.66, test acc:0.5306 ===\n",
      "train loss:1.127053958045407\n",
      "train loss:1.1464714686856554\n",
      "train loss:1.2510267756488334\n",
      "=== epoch:265, train acc:0.6533333333333333, test acc:0.5313 ===\n",
      "train loss:1.1189348908774295\n",
      "train loss:1.0428306145948951\n",
      "train loss:1.100043088613373\n",
      "=== epoch:266, train acc:0.6566666666666666, test acc:0.5316 ===\n",
      "train loss:1.2035948815665156\n",
      "train loss:1.1714703350430284\n",
      "train loss:1.0719876856855255\n",
      "=== epoch:267, train acc:0.6566666666666666, test acc:0.5308 ===\n",
      "train loss:1.1297963247809804\n",
      "train loss:1.2411578860272308\n",
      "train loss:1.1340313077458084\n",
      "=== epoch:268, train acc:0.66, test acc:0.5345 ===\n",
      "train loss:1.1044595151182683\n",
      "train loss:1.1736621975423216\n",
      "train loss:1.1192580038802544\n",
      "=== epoch:269, train acc:0.6566666666666666, test acc:0.5347 ===\n",
      "train loss:1.134031774340811\n",
      "train loss:1.3073268938631466\n",
      "train loss:1.2590638206633664\n",
      "=== epoch:270, train acc:0.6566666666666666, test acc:0.5388 ===\n",
      "train loss:1.1740987646918501\n",
      "train loss:1.1560711788312081\n",
      "train loss:1.1788198372567735\n",
      "=== epoch:271, train acc:0.66, test acc:0.5398 ===\n",
      "train loss:1.2131893288602051\n",
      "train loss:1.160347351323801\n",
      "train loss:1.1173030820922358\n",
      "=== epoch:272, train acc:0.6633333333333333, test acc:0.542 ===\n",
      "train loss:1.2665271704856738\n",
      "train loss:1.0690772826265005\n",
      "train loss:1.1037228366337366\n",
      "=== epoch:273, train acc:0.6566666666666666, test acc:0.5425 ===\n",
      "train loss:1.0908080048774154\n",
      "train loss:1.05256551064852\n",
      "train loss:1.0974251514359603\n",
      "=== epoch:274, train acc:0.66, test acc:0.5465 ===\n",
      "train loss:1.0753521369933337\n",
      "train loss:1.0542607106786632\n",
      "train loss:1.0924397058510498\n",
      "=== epoch:275, train acc:0.6666666666666666, test acc:0.5498 ===\n",
      "train loss:1.1127191810877772\n",
      "train loss:1.1134829227528247\n",
      "train loss:1.1965425636670135\n",
      "=== epoch:276, train acc:0.66, test acc:0.5462 ===\n",
      "train loss:1.0092791055355754\n",
      "train loss:1.1412261092739895\n",
      "train loss:1.2213100351916082\n",
      "=== epoch:277, train acc:0.67, test acc:0.5462 ===\n",
      "train loss:1.136137435754212\n",
      "train loss:1.0566513610714923\n",
      "train loss:1.018401930575842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.6666666666666666, test acc:0.5486 ===\n",
      "train loss:1.0874193582387206\n",
      "train loss:1.0418094114709664\n",
      "train loss:1.0120216389474441\n",
      "=== epoch:279, train acc:0.6733333333333333, test acc:0.5512 ===\n",
      "train loss:1.0568060210001897\n",
      "train loss:1.1224952015772531\n",
      "train loss:1.1263569077099056\n",
      "=== epoch:280, train acc:0.67, test acc:0.5542 ===\n",
      "train loss:0.98461136874101\n",
      "train loss:1.1920972859060037\n",
      "train loss:1.1785947116287852\n",
      "=== epoch:281, train acc:0.6733333333333333, test acc:0.5558 ===\n",
      "train loss:1.084859618396277\n",
      "train loss:0.9360729176722776\n",
      "train loss:1.181873792830452\n",
      "=== epoch:282, train acc:0.6733333333333333, test acc:0.5555 ===\n",
      "train loss:0.9529161696389034\n",
      "train loss:1.076630310748851\n",
      "train loss:1.08050008463593\n",
      "=== epoch:283, train acc:0.67, test acc:0.5559 ===\n",
      "train loss:1.0963873432347955\n",
      "train loss:1.2305915423633498\n",
      "train loss:1.0654327781361803\n",
      "=== epoch:284, train acc:0.6733333333333333, test acc:0.5549 ===\n",
      "train loss:1.17976571224707\n",
      "train loss:1.0651040003310612\n",
      "train loss:1.07647080193693\n",
      "=== epoch:285, train acc:0.6766666666666666, test acc:0.5545 ===\n",
      "train loss:1.0434379198301322\n",
      "train loss:1.1946393542185163\n",
      "train loss:0.9248372179225274\n",
      "=== epoch:286, train acc:0.6766666666666666, test acc:0.5561 ===\n",
      "train loss:0.9760895067864449\n",
      "train loss:0.9953704347960022\n",
      "train loss:0.9419919315740007\n",
      "=== epoch:287, train acc:0.68, test acc:0.5572 ===\n",
      "train loss:0.9866982249887059\n",
      "train loss:0.9395837930402189\n",
      "train loss:0.9135119066975034\n",
      "=== epoch:288, train acc:0.68, test acc:0.5563 ===\n",
      "train loss:1.0480032276619893\n",
      "train loss:1.1123146916797213\n",
      "train loss:1.019298976649288\n",
      "=== epoch:289, train acc:0.6833333333333333, test acc:0.5581 ===\n",
      "train loss:1.0533770968204135\n",
      "train loss:1.0138834481951786\n",
      "train loss:1.034457727162962\n",
      "=== epoch:290, train acc:0.68, test acc:0.5572 ===\n",
      "train loss:1.1042829130158793\n",
      "train loss:1.0274356821157085\n",
      "train loss:1.0282452686175676\n",
      "=== epoch:291, train acc:0.6833333333333333, test acc:0.5619 ===\n",
      "train loss:1.0538385452053824\n",
      "train loss:0.9440829396652138\n",
      "train loss:1.028482267387899\n",
      "=== epoch:292, train acc:0.6866666666666666, test acc:0.5659 ===\n",
      "train loss:0.9323650884273765\n",
      "train loss:0.9043018828264329\n",
      "train loss:1.1737123619599352\n",
      "=== epoch:293, train acc:0.6966666666666667, test acc:0.5667 ===\n",
      "train loss:1.0046762831328353\n",
      "train loss:1.0105227361607048\n",
      "train loss:0.859192961577333\n",
      "=== epoch:294, train acc:0.6866666666666666, test acc:0.5664 ===\n",
      "train loss:0.9301344261153623\n",
      "train loss:1.0291152875650837\n",
      "train loss:1.0379859787976857\n",
      "=== epoch:295, train acc:0.6966666666666667, test acc:0.5685 ===\n",
      "train loss:0.8921797040558829\n",
      "train loss:0.856398456927686\n",
      "train loss:1.0352314036163812\n",
      "=== epoch:296, train acc:0.6966666666666667, test acc:0.5693 ===\n",
      "train loss:1.0942260367010597\n",
      "train loss:0.9750134344716517\n",
      "train loss:1.0634005020293436\n",
      "=== epoch:297, train acc:0.69, test acc:0.5719 ===\n",
      "train loss:0.9115175532700575\n",
      "train loss:0.9311663700583519\n",
      "train loss:0.9693783969338614\n",
      "=== epoch:298, train acc:0.69, test acc:0.5743 ===\n",
      "train loss:1.0597827325185911\n",
      "train loss:1.0504322188185946\n",
      "train loss:1.0189951686192409\n",
      "=== epoch:299, train acc:0.6966666666666667, test acc:0.5762 ===\n",
      "train loss:0.9447887138246603\n",
      "train loss:1.1564443552982862\n",
      "train loss:0.9787197033465947\n",
      "=== epoch:300, train acc:0.7033333333333334, test acc:0.5766 ===\n",
      "train loss:0.9791444733549995\n",
      "train loss:0.9787897074256374\n",
      "train loss:0.9087327081812776\n",
      "=== epoch:301, train acc:0.71, test acc:0.5751 ===\n",
      "train loss:0.9203411457996056\n",
      "train loss:1.0053928187220311\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5789\n"
     ]
    }
   ],
   "source": [
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ad3f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dccfd",
   "metadata": {},
   "source": [
    "드롭아웃 비율 조정해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a7a3b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3314751181089512\n",
      "=== epoch:1, train acc:0.08666666666666667, test acc:0.1034 ===\n",
      "train loss:2.3393209350684536\n",
      "train loss:2.316973392017008\n",
      "train loss:2.320330647975749\n",
      "=== epoch:2, train acc:0.09, test acc:0.1054 ===\n",
      "train loss:2.3096688153665212\n",
      "train loss:2.315644790181693\n",
      "train loss:2.3051668297623507\n",
      "=== epoch:3, train acc:0.07666666666666666, test acc:0.1104 ===\n",
      "train loss:2.3121373355348935\n",
      "train loss:2.3000943990132647\n",
      "train loss:2.2679346128534514\n",
      "=== epoch:4, train acc:0.08666666666666667, test acc:0.1112 ===\n",
      "train loss:2.2790382893603276\n",
      "train loss:2.2800470053793895\n",
      "train loss:2.3197652621345344\n",
      "=== epoch:5, train acc:0.09666666666666666, test acc:0.1157 ===\n",
      "train loss:2.2760483068444675\n",
      "train loss:2.3182318850256776\n",
      "train loss:2.3084666431057097\n",
      "=== epoch:6, train acc:0.10333333333333333, test acc:0.118 ===\n",
      "train loss:2.2794577794697\n",
      "train loss:2.294928824587181\n",
      "train loss:2.2701767258192604\n",
      "=== epoch:7, train acc:0.10333333333333333, test acc:0.1219 ===\n",
      "train loss:2.2787233305391514\n",
      "train loss:2.275167847259375\n",
      "train loss:2.2492261400537923\n",
      "=== epoch:8, train acc:0.11, test acc:0.1227 ===\n",
      "train loss:2.277677463851423\n",
      "train loss:2.2833027555985894\n",
      "train loss:2.2887755450892313\n",
      "=== epoch:9, train acc:0.12, test acc:0.1281 ===\n",
      "train loss:2.2603139608853904\n",
      "train loss:2.261413481444062\n",
      "train loss:2.2672874530879663\n",
      "=== epoch:10, train acc:0.12666666666666668, test acc:0.1301 ===\n",
      "train loss:2.280975767426994\n",
      "train loss:2.2533497105312517\n",
      "train loss:2.2388683325196403\n",
      "=== epoch:11, train acc:0.13, test acc:0.1341 ===\n",
      "train loss:2.268293509020627\n",
      "train loss:2.270409009696171\n",
      "train loss:2.246787803878088\n",
      "=== epoch:12, train acc:0.13666666666666666, test acc:0.1369 ===\n",
      "train loss:2.2363150216505976\n",
      "train loss:2.261381894991155\n",
      "train loss:2.2690194038559937\n",
      "=== epoch:13, train acc:0.14, test acc:0.1394 ===\n",
      "train loss:2.252182668023545\n",
      "train loss:2.2553891149993\n",
      "train loss:2.247501984269832\n",
      "=== epoch:14, train acc:0.14666666666666667, test acc:0.1431 ===\n",
      "train loss:2.2438956839722706\n",
      "train loss:2.2653975249232023\n",
      "train loss:2.229117508431663\n",
      "=== epoch:15, train acc:0.16333333333333333, test acc:0.1492 ===\n",
      "train loss:2.2395315616997906\n",
      "train loss:2.2371708792981146\n",
      "train loss:2.2347003811846777\n",
      "=== epoch:16, train acc:0.16666666666666666, test acc:0.1571 ===\n",
      "train loss:2.2562851910474335\n",
      "train loss:2.2396253390588696\n",
      "train loss:2.2458841572448267\n",
      "=== epoch:17, train acc:0.17333333333333334, test acc:0.1661 ===\n",
      "train loss:2.246802576636897\n",
      "train loss:2.2317929128132645\n",
      "train loss:2.206902036471575\n",
      "=== epoch:18, train acc:0.18333333333333332, test acc:0.1754 ===\n",
      "train loss:2.221058581037594\n",
      "train loss:2.223989798713319\n",
      "train loss:2.2455424940453867\n",
      "=== epoch:19, train acc:0.20333333333333334, test acc:0.1835 ===\n",
      "train loss:2.2085512827387452\n",
      "train loss:2.2256773192976893\n",
      "train loss:2.219811375773403\n",
      "=== epoch:20, train acc:0.21666666666666667, test acc:0.1886 ===\n",
      "train loss:2.195888435767331\n",
      "train loss:2.199453457946164\n",
      "train loss:2.218299751710297\n",
      "=== epoch:21, train acc:0.23333333333333334, test acc:0.1962 ===\n",
      "train loss:2.228064617897319\n",
      "train loss:2.199407575057315\n",
      "train loss:2.1884990424537647\n",
      "=== epoch:22, train acc:0.23666666666666666, test acc:0.1974 ===\n",
      "train loss:2.211859114420577\n",
      "train loss:2.2065132471685893\n",
      "train loss:2.2085999038001045\n",
      "=== epoch:23, train acc:0.24, test acc:0.2002 ===\n",
      "train loss:2.184738845797837\n",
      "train loss:2.1876451618721613\n",
      "train loss:2.2068063486140925\n",
      "=== epoch:24, train acc:0.26666666666666666, test acc:0.2087 ===\n",
      "train loss:2.221595304021757\n",
      "train loss:2.2100650715804315\n",
      "train loss:2.1752261951599663\n",
      "=== epoch:25, train acc:0.2733333333333333, test acc:0.2149 ===\n",
      "train loss:2.1734652086590063\n",
      "train loss:2.1464117947157875\n",
      "train loss:2.1708208232401667\n",
      "=== epoch:26, train acc:0.28, test acc:0.2187 ===\n",
      "train loss:2.193653062638936\n",
      "train loss:2.168038391438246\n",
      "train loss:2.1663721089979706\n",
      "=== epoch:27, train acc:0.2866666666666667, test acc:0.2221 ===\n",
      "train loss:2.2250697998705022\n",
      "train loss:2.1712327288357742\n",
      "train loss:2.146426847541231\n",
      "=== epoch:28, train acc:0.30333333333333334, test acc:0.2333 ===\n",
      "train loss:2.1945349573367143\n",
      "train loss:2.122321132553878\n",
      "train loss:2.174403691401599\n",
      "=== epoch:29, train acc:0.30666666666666664, test acc:0.2353 ===\n",
      "train loss:2.135877083362867\n",
      "train loss:2.178073662055428\n",
      "train loss:2.218221544813119\n",
      "=== epoch:30, train acc:0.31333333333333335, test acc:0.2481 ===\n",
      "train loss:2.150380079151246\n",
      "train loss:2.1649648168103854\n",
      "train loss:2.1769385106620134\n",
      "=== epoch:31, train acc:0.32, test acc:0.251 ===\n",
      "train loss:2.1495172025238296\n",
      "train loss:2.1722576954473922\n",
      "train loss:2.174520756220784\n",
      "=== epoch:32, train acc:0.3233333333333333, test acc:0.2599 ===\n",
      "train loss:2.1391107017658073\n",
      "train loss:2.160747122613695\n",
      "train loss:2.123087070196292\n",
      "=== epoch:33, train acc:0.3333333333333333, test acc:0.2697 ===\n",
      "train loss:2.060234799256805\n",
      "train loss:2.121065542866403\n",
      "train loss:2.1618339473568837\n",
      "=== epoch:34, train acc:0.33, test acc:0.2723 ===\n",
      "train loss:2.1555695860448956\n",
      "train loss:2.1095991432557732\n",
      "train loss:2.21852315413534\n",
      "=== epoch:35, train acc:0.3433333333333333, test acc:0.2785 ===\n",
      "train loss:2.1248873808312574\n",
      "train loss:2.151492618777807\n",
      "train loss:2.153594834473054\n",
      "=== epoch:36, train acc:0.36, test acc:0.292 ===\n",
      "train loss:2.1137145937162356\n",
      "train loss:2.1496351354735648\n",
      "train loss:2.1219488128280624\n",
      "=== epoch:37, train acc:0.36333333333333334, test acc:0.2972 ===\n",
      "train loss:2.1700131198432375\n",
      "train loss:2.0696233938403408\n",
      "train loss:2.106695903597171\n",
      "=== epoch:38, train acc:0.36333333333333334, test acc:0.2998 ===\n",
      "train loss:2.1397263894684966\n",
      "train loss:2.1065762631417\n",
      "train loss:2.105102455002205\n",
      "=== epoch:39, train acc:0.36666666666666664, test acc:0.3092 ===\n",
      "train loss:2.081202220631357\n",
      "train loss:2.1416842601049604\n",
      "train loss:2.0649587674815\n",
      "=== epoch:40, train acc:0.39, test acc:0.3156 ===\n",
      "train loss:2.102533139653063\n",
      "train loss:2.0798136325846106\n",
      "train loss:2.137988683819812\n",
      "=== epoch:41, train acc:0.39666666666666667, test acc:0.3209 ===\n",
      "train loss:2.120071543900722\n",
      "train loss:2.1069852521884234\n",
      "train loss:2.1204466201187\n",
      "=== epoch:42, train acc:0.41, test acc:0.3281 ===\n",
      "train loss:2.0856271410891734\n",
      "train loss:2.0700122187639582\n",
      "train loss:2.1136004404013047\n",
      "=== epoch:43, train acc:0.41333333333333333, test acc:0.3415 ===\n",
      "train loss:2.0533532237708885\n",
      "train loss:2.086742259944952\n",
      "train loss:2.1344584588534414\n",
      "=== epoch:44, train acc:0.4533333333333333, test acc:0.3478 ===\n",
      "train loss:2.0512321151466897\n",
      "train loss:2.037815799621777\n",
      "train loss:2.049596410459815\n",
      "=== epoch:45, train acc:0.44, test acc:0.3447 ===\n",
      "train loss:2.105469909625949\n",
      "train loss:2.0524171526964223\n",
      "train loss:2.0601417597630385\n",
      "=== epoch:46, train acc:0.43666666666666665, test acc:0.3439 ===\n",
      "train loss:2.0981503857460364\n",
      "train loss:2.06237092889476\n",
      "train loss:2.0598684064174\n",
      "=== epoch:47, train acc:0.4633333333333333, test acc:0.351 ===\n",
      "train loss:2.027399715363854\n",
      "train loss:2.04160080389209\n",
      "train loss:2.01416795277841\n",
      "=== epoch:48, train acc:0.4533333333333333, test acc:0.3458 ===\n",
      "train loss:2.0283666549146924\n",
      "train loss:2.0467030577807934\n",
      "train loss:1.9973881002958476\n",
      "=== epoch:49, train acc:0.46, test acc:0.3523 ===\n",
      "train loss:2.0039743403424044\n",
      "train loss:2.066295705278144\n",
      "train loss:1.991652387384154\n",
      "=== epoch:50, train acc:0.4633333333333333, test acc:0.3658 ===\n",
      "train loss:2.0283537709628963\n",
      "train loss:2.0586031740830815\n",
      "train loss:1.9911143401824245\n",
      "=== epoch:51, train acc:0.4866666666666667, test acc:0.3789 ===\n",
      "train loss:1.9767693710032939\n",
      "train loss:2.0026433829797456\n",
      "train loss:2.0341105874606606\n",
      "=== epoch:52, train acc:0.5, test acc:0.3842 ===\n",
      "train loss:1.9778097228859353\n",
      "train loss:2.0434224045743967\n",
      "train loss:1.9628631575653828\n",
      "=== epoch:53, train acc:0.51, test acc:0.3911 ===\n",
      "train loss:1.9223105633308373\n",
      "train loss:2.017177047819587\n",
      "train loss:2.018043646922051\n",
      "=== epoch:54, train acc:0.5133333333333333, test acc:0.3993 ===\n",
      "train loss:1.9500777434306327\n",
      "train loss:2.006454204994922\n",
      "train loss:1.994693738127192\n",
      "=== epoch:55, train acc:0.53, test acc:0.4055 ===\n",
      "train loss:1.9607747816432832\n",
      "train loss:1.9704989768692176\n",
      "train loss:2.0194607794402977\n",
      "=== epoch:56, train acc:0.5533333333333333, test acc:0.4144 ===\n",
      "train loss:1.9168497112643124\n",
      "train loss:1.9566571207526704\n",
      "train loss:2.023822897081169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.5566666666666666, test acc:0.4157 ===\n",
      "train loss:1.9721562038951685\n",
      "train loss:1.9831237895919558\n",
      "train loss:1.9249488649550446\n",
      "=== epoch:58, train acc:0.5666666666666667, test acc:0.4169 ===\n",
      "train loss:1.9987866740320683\n",
      "train loss:1.929361675205635\n",
      "train loss:2.000839817672491\n",
      "=== epoch:59, train acc:0.5833333333333334, test acc:0.4346 ===\n",
      "train loss:1.973932005282387\n",
      "train loss:1.9374767791678238\n",
      "train loss:1.8406987348124373\n",
      "=== epoch:60, train acc:0.58, test acc:0.439 ===\n",
      "train loss:2.000701086821687\n",
      "train loss:1.8417411838556945\n",
      "train loss:1.8541279760966933\n",
      "=== epoch:61, train acc:0.5833333333333334, test acc:0.4464 ===\n",
      "train loss:1.9125640679186355\n",
      "train loss:1.950086794273136\n",
      "train loss:1.9126243624838575\n",
      "=== epoch:62, train acc:0.5933333333333334, test acc:0.4487 ===\n",
      "train loss:1.9855500172449785\n",
      "train loss:1.8776971627603078\n",
      "train loss:2.0099015276676626\n",
      "=== epoch:63, train acc:0.5966666666666667, test acc:0.4552 ===\n",
      "train loss:1.8876429114634632\n",
      "train loss:1.9001630210457259\n",
      "train loss:1.825306074543248\n",
      "=== epoch:64, train acc:0.6, test acc:0.4582 ===\n",
      "train loss:1.8609838352319994\n",
      "train loss:1.8965170921872896\n",
      "train loss:1.8332325460134584\n",
      "=== epoch:65, train acc:0.6133333333333333, test acc:0.4636 ===\n",
      "train loss:1.7812601088404358\n",
      "train loss:1.8974534385724366\n",
      "train loss:1.8250341488140238\n",
      "=== epoch:66, train acc:0.6133333333333333, test acc:0.4694 ===\n",
      "train loss:1.802202690342329\n",
      "train loss:1.808501577507559\n",
      "train loss:1.878816483128099\n",
      "=== epoch:67, train acc:0.6166666666666667, test acc:0.474 ===\n",
      "train loss:1.821700787325427\n",
      "train loss:1.8692206553303057\n",
      "train loss:1.8018492087461437\n",
      "=== epoch:68, train acc:0.6266666666666667, test acc:0.4753 ===\n",
      "train loss:1.8099422673718566\n",
      "train loss:1.8287160064209425\n",
      "train loss:1.8534570907437875\n",
      "=== epoch:69, train acc:0.6333333333333333, test acc:0.481 ===\n",
      "train loss:1.9452779008658876\n",
      "train loss:1.846584673256376\n",
      "train loss:1.7781442631176925\n",
      "=== epoch:70, train acc:0.6366666666666667, test acc:0.4911 ===\n",
      "train loss:1.749266573857681\n",
      "train loss:1.7657503888548953\n",
      "train loss:1.8162667885491928\n",
      "=== epoch:71, train acc:0.6466666666666666, test acc:0.4915 ===\n",
      "train loss:1.8771804540176507\n",
      "train loss:1.879317527286778\n",
      "train loss:1.8091056653249669\n",
      "=== epoch:72, train acc:0.6433333333333333, test acc:0.4964 ===\n",
      "train loss:1.8065523010572355\n",
      "train loss:1.7321570476260575\n",
      "train loss:1.7861011387392494\n",
      "=== epoch:73, train acc:0.6333333333333333, test acc:0.5047 ===\n",
      "train loss:1.7239072431208913\n",
      "train loss:1.7696910892494098\n",
      "train loss:1.7254181727443705\n",
      "=== epoch:74, train acc:0.6433333333333333, test acc:0.5038 ===\n",
      "train loss:1.6777487734147636\n",
      "train loss:1.7440732669206278\n",
      "train loss:1.753136771168056\n",
      "=== epoch:75, train acc:0.6533333333333333, test acc:0.5086 ===\n",
      "train loss:1.7251700424586658\n",
      "train loss:1.7155686987129248\n",
      "train loss:1.7498576292376302\n",
      "=== epoch:76, train acc:0.6566666666666666, test acc:0.5094 ===\n",
      "train loss:1.6150389995385888\n",
      "train loss:1.604624815695182\n",
      "train loss:1.7372650692117326\n",
      "=== epoch:77, train acc:0.6633333333333333, test acc:0.5102 ===\n",
      "train loss:1.7205789924865034\n",
      "train loss:1.6387920989117066\n",
      "train loss:1.6844148802388539\n",
      "=== epoch:78, train acc:0.6566666666666666, test acc:0.5067 ===\n",
      "train loss:1.7151046456104957\n",
      "train loss:1.5927156125740212\n",
      "train loss:1.6504976501065576\n",
      "=== epoch:79, train acc:0.66, test acc:0.5017 ===\n",
      "train loss:1.5541408636552114\n",
      "train loss:1.693129377879037\n",
      "train loss:1.6471899989793817\n",
      "=== epoch:80, train acc:0.66, test acc:0.5046 ===\n",
      "train loss:1.6782397422711173\n",
      "train loss:1.5655953744846618\n",
      "train loss:1.6412818251659607\n",
      "=== epoch:81, train acc:0.67, test acc:0.5144 ===\n",
      "train loss:1.583055976589565\n",
      "train loss:1.64280547010722\n",
      "train loss:1.582238768312077\n",
      "=== epoch:82, train acc:0.69, test acc:0.5221 ===\n",
      "train loss:1.6145857918527633\n",
      "train loss:1.4797368681641156\n",
      "train loss:1.598329006650105\n",
      "=== epoch:83, train acc:0.6966666666666667, test acc:0.5224 ===\n",
      "train loss:1.531476424333644\n",
      "train loss:1.5890966968520714\n",
      "train loss:1.5083759332574502\n",
      "=== epoch:84, train acc:0.7033333333333334, test acc:0.5188 ===\n",
      "train loss:1.646554812269387\n",
      "train loss:1.573197344738831\n",
      "train loss:1.4946957631631221\n",
      "=== epoch:85, train acc:0.7, test acc:0.5269 ===\n",
      "train loss:1.5390340248408036\n",
      "train loss:1.5975901788257278\n",
      "train loss:1.577103010274842\n",
      "=== epoch:86, train acc:0.6933333333333334, test acc:0.5342 ===\n",
      "train loss:1.5876816173756771\n",
      "train loss:1.4823276214057461\n",
      "train loss:1.5725928988773952\n",
      "=== epoch:87, train acc:0.6966666666666667, test acc:0.5433 ===\n",
      "train loss:1.583194925371978\n",
      "train loss:1.5453211314276305\n",
      "train loss:1.5407893214676998\n",
      "=== epoch:88, train acc:0.69, test acc:0.5462 ===\n",
      "train loss:1.5034946228593393\n",
      "train loss:1.4332620021675857\n",
      "train loss:1.4513092916073738\n",
      "=== epoch:89, train acc:0.68, test acc:0.544 ===\n",
      "train loss:1.5671411136595352\n",
      "train loss:1.4484973370031438\n",
      "train loss:1.531887920799126\n",
      "=== epoch:90, train acc:0.7033333333333334, test acc:0.5516 ===\n",
      "train loss:1.4598795104106914\n",
      "train loss:1.4698309582108129\n",
      "train loss:1.4752815401698731\n",
      "=== epoch:91, train acc:0.7066666666666667, test acc:0.5565 ===\n",
      "train loss:1.5236771420230675\n",
      "train loss:1.3904486915764378\n",
      "train loss:1.387796259429332\n",
      "=== epoch:92, train acc:0.71, test acc:0.5571 ===\n",
      "train loss:1.49151893352701\n",
      "train loss:1.48398431809724\n",
      "train loss:1.396134445444086\n",
      "=== epoch:93, train acc:0.7233333333333334, test acc:0.5674 ===\n",
      "train loss:1.4479403405714448\n",
      "train loss:1.421130218512689\n",
      "train loss:1.2977363382824163\n",
      "=== epoch:94, train acc:0.7166666666666667, test acc:0.5619 ===\n",
      "train loss:1.3541961071159379\n",
      "train loss:1.3084477059209112\n",
      "train loss:1.347213247832475\n",
      "=== epoch:95, train acc:0.7133333333333334, test acc:0.566 ===\n",
      "train loss:1.3757414574287565\n",
      "train loss:1.3353537141323886\n",
      "train loss:1.206984232514094\n",
      "=== epoch:96, train acc:0.73, test acc:0.5717 ===\n",
      "train loss:1.3934018217166064\n",
      "train loss:1.3141243791063182\n",
      "train loss:1.3269712153445077\n",
      "=== epoch:97, train acc:0.7333333333333333, test acc:0.5741 ===\n",
      "train loss:1.2986069588839633\n",
      "train loss:1.3039707982939424\n",
      "train loss:1.2857458088642089\n",
      "=== epoch:98, train acc:0.7366666666666667, test acc:0.5791 ===\n",
      "train loss:1.302838899210085\n",
      "train loss:1.2726860944843055\n",
      "train loss:1.2943622514922333\n",
      "=== epoch:99, train acc:0.74, test acc:0.5816 ===\n",
      "train loss:1.2538384468722823\n",
      "train loss:1.2884114175105827\n",
      "train loss:1.1058264463759209\n",
      "=== epoch:100, train acc:0.74, test acc:0.5808 ===\n",
      "train loss:1.3356074708317138\n",
      "train loss:1.319102653776705\n",
      "train loss:1.2123044335483557\n",
      "=== epoch:101, train acc:0.7466666666666667, test acc:0.5873 ===\n",
      "train loss:1.2101517596473959\n",
      "train loss:1.1305438836691706\n",
      "train loss:1.2671950535205283\n",
      "=== epoch:102, train acc:0.7533333333333333, test acc:0.5877 ===\n",
      "train loss:1.198045376898906\n",
      "train loss:1.1266987941813036\n",
      "train loss:1.1631002218963364\n",
      "=== epoch:103, train acc:0.7533333333333333, test acc:0.5977 ===\n",
      "train loss:1.2670228899092737\n",
      "train loss:1.1490931822932366\n",
      "train loss:1.1578501862072605\n",
      "=== epoch:104, train acc:0.76, test acc:0.6057 ===\n",
      "train loss:1.1396031173192664\n",
      "train loss:1.1901952140212353\n",
      "train loss:1.180812465046181\n",
      "=== epoch:105, train acc:0.7533333333333333, test acc:0.6073 ===\n",
      "train loss:1.038353966018775\n",
      "train loss:1.1298728882292783\n",
      "train loss:1.2219888326418122\n",
      "=== epoch:106, train acc:0.7533333333333333, test acc:0.6075 ===\n",
      "train loss:1.1552436330160267\n",
      "train loss:1.1991368834335177\n",
      "train loss:1.0667593034527518\n",
      "=== epoch:107, train acc:0.7666666666666667, test acc:0.6127 ===\n",
      "train loss:1.1886763077418436\n",
      "train loss:1.1872951322136354\n",
      "train loss:1.123853673580222\n",
      "=== epoch:108, train acc:0.7766666666666666, test acc:0.6172 ===\n",
      "train loss:0.9606848924379296\n",
      "train loss:1.1338854753675953\n",
      "train loss:1.0860508163563356\n",
      "=== epoch:109, train acc:0.7766666666666666, test acc:0.6171 ===\n",
      "train loss:0.9855647560741662\n",
      "train loss:1.0083980332611022\n",
      "train loss:1.1431922653109021\n",
      "=== epoch:110, train acc:0.7766666666666666, test acc:0.6161 ===\n",
      "train loss:1.1642094676987111\n",
      "train loss:1.0589737822781573\n",
      "train loss:0.9355160218592412\n",
      "=== epoch:111, train acc:0.7766666666666666, test acc:0.6184 ===\n",
      "train loss:0.9345701572644071\n",
      "train loss:1.029973054700705\n",
      "train loss:0.9501567832203287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.7766666666666666, test acc:0.6251 ===\n",
      "train loss:1.0890030922829073\n",
      "train loss:1.0035228391521682\n",
      "train loss:0.9110133998514475\n",
      "=== epoch:113, train acc:0.7833333333333333, test acc:0.6251 ===\n",
      "train loss:1.0500320898645155\n",
      "train loss:0.9052904373140686\n",
      "train loss:0.9918238062995709\n",
      "=== epoch:114, train acc:0.7866666666666666, test acc:0.6286 ===\n",
      "train loss:1.0058597085337555\n",
      "train loss:0.9719382211748507\n",
      "train loss:0.8695512975452963\n",
      "=== epoch:115, train acc:0.7933333333333333, test acc:0.6323 ===\n",
      "train loss:0.937137262392272\n",
      "train loss:0.962628665529667\n",
      "train loss:0.9420410394762888\n",
      "=== epoch:116, train acc:0.7933333333333333, test acc:0.6335 ===\n",
      "train loss:0.9139496799634164\n",
      "train loss:0.9841397565505181\n",
      "train loss:0.893294135343492\n",
      "=== epoch:117, train acc:0.8, test acc:0.6352 ===\n",
      "train loss:0.8902889073956183\n",
      "train loss:0.8583793642647641\n",
      "train loss:1.0255823885991873\n",
      "=== epoch:118, train acc:0.7966666666666666, test acc:0.6363 ===\n",
      "train loss:0.8937293306467883\n",
      "train loss:0.8589326076024073\n",
      "train loss:0.9251781822417804\n",
      "=== epoch:119, train acc:0.8, test acc:0.6403 ===\n",
      "train loss:0.9868012514650517\n",
      "train loss:0.9089774311551853\n",
      "train loss:0.8546447973401047\n",
      "=== epoch:120, train acc:0.8, test acc:0.6446 ===\n",
      "train loss:0.7677750521869819\n",
      "train loss:0.9516587560243375\n",
      "train loss:0.9515917566933865\n",
      "=== epoch:121, train acc:0.8, test acc:0.6438 ===\n",
      "train loss:0.8385862322444037\n",
      "train loss:0.8540074776984733\n",
      "train loss:0.8137802976853908\n",
      "=== epoch:122, train acc:0.8033333333333333, test acc:0.6436 ===\n",
      "train loss:0.664438476755531\n",
      "train loss:0.8406219383120593\n",
      "train loss:0.9233091317988462\n",
      "=== epoch:123, train acc:0.8133333333333334, test acc:0.6502 ===\n",
      "train loss:0.8653301131958413\n",
      "train loss:0.7398981747088684\n",
      "train loss:0.7939363333237166\n",
      "=== epoch:124, train acc:0.8133333333333334, test acc:0.6591 ===\n",
      "train loss:0.8329577236590306\n",
      "train loss:0.7460256538505083\n",
      "train loss:0.7750664346852829\n",
      "=== epoch:125, train acc:0.8233333333333334, test acc:0.6555 ===\n",
      "train loss:0.9066516390334103\n",
      "train loss:0.9103161976241505\n",
      "train loss:0.7242966934798888\n",
      "=== epoch:126, train acc:0.8166666666666667, test acc:0.6533 ===\n",
      "train loss:0.655183298740838\n",
      "train loss:0.8244573848848965\n",
      "train loss:0.7658673419762623\n",
      "=== epoch:127, train acc:0.8233333333333334, test acc:0.653 ===\n",
      "train loss:0.7253315240310415\n",
      "train loss:0.7643891388564934\n",
      "train loss:0.7186021468235502\n",
      "=== epoch:128, train acc:0.8233333333333334, test acc:0.651 ===\n",
      "train loss:0.9353225703701961\n",
      "train loss:0.7643161858716813\n",
      "train loss:0.7647478214836607\n",
      "=== epoch:129, train acc:0.8233333333333334, test acc:0.6534 ===\n",
      "train loss:0.8593101863481446\n",
      "train loss:0.8220071228285398\n",
      "train loss:0.7612616348479111\n",
      "=== epoch:130, train acc:0.8233333333333334, test acc:0.6528 ===\n",
      "train loss:0.6731284618116179\n",
      "train loss:0.75747621066513\n",
      "train loss:0.7865882748190804\n",
      "=== epoch:131, train acc:0.8333333333333334, test acc:0.6605 ===\n",
      "train loss:0.8804815892565513\n",
      "train loss:0.6582376854619554\n",
      "train loss:0.6146876938119017\n",
      "=== epoch:132, train acc:0.8266666666666667, test acc:0.6593 ===\n",
      "train loss:0.6428759458350143\n",
      "train loss:0.5389811181601682\n",
      "train loss:0.6710441111262675\n",
      "=== epoch:133, train acc:0.82, test acc:0.6616 ===\n",
      "train loss:0.7753589775185248\n",
      "train loss:0.5540974585985282\n",
      "train loss:0.6100905606442167\n",
      "=== epoch:134, train acc:0.83, test acc:0.6624 ===\n",
      "train loss:0.6568837215973028\n",
      "train loss:0.5749503545177888\n",
      "train loss:0.7711802999364592\n",
      "=== epoch:135, train acc:0.83, test acc:0.6627 ===\n",
      "train loss:0.631701328544024\n",
      "train loss:0.5920103660687862\n",
      "train loss:0.6210008446815514\n",
      "=== epoch:136, train acc:0.8366666666666667, test acc:0.6678 ===\n",
      "train loss:0.6096156673984635\n",
      "train loss:0.5553082004129845\n",
      "train loss:0.6954423651747015\n",
      "=== epoch:137, train acc:0.8333333333333334, test acc:0.6701 ===\n",
      "train loss:0.5866523937480126\n",
      "train loss:0.5545559129211205\n",
      "train loss:0.6213208065731999\n",
      "=== epoch:138, train acc:0.8366666666666667, test acc:0.6689 ===\n",
      "train loss:0.597370173637325\n",
      "train loss:0.6190745369584103\n",
      "train loss:0.6465800224720817\n",
      "=== epoch:139, train acc:0.8333333333333334, test acc:0.6683 ===\n",
      "train loss:0.7405140609090358\n",
      "train loss:0.7002878241326697\n",
      "train loss:0.6139495347868494\n",
      "=== epoch:140, train acc:0.8366666666666667, test acc:0.667 ===\n",
      "train loss:0.583687183445412\n",
      "train loss:0.696203098758028\n",
      "train loss:0.6533735658807979\n",
      "=== epoch:141, train acc:0.8433333333333334, test acc:0.6747 ===\n",
      "train loss:0.5318753995265396\n",
      "train loss:0.5952990020928511\n",
      "train loss:0.6003486752333967\n",
      "=== epoch:142, train acc:0.84, test acc:0.6685 ===\n",
      "train loss:0.7453471611696995\n",
      "train loss:0.5792504629688068\n",
      "train loss:0.4815265194754246\n",
      "=== epoch:143, train acc:0.8433333333333334, test acc:0.669 ===\n",
      "train loss:0.4960983991837597\n",
      "train loss:0.6450299008726295\n",
      "train loss:0.6120536830960532\n",
      "=== epoch:144, train acc:0.85, test acc:0.671 ===\n",
      "train loss:0.635511188230613\n",
      "train loss:0.612550868692479\n",
      "train loss:0.654827725577217\n",
      "=== epoch:145, train acc:0.86, test acc:0.6799 ===\n",
      "train loss:0.5734227550321803\n",
      "train loss:0.5566536525775291\n",
      "train loss:0.6195058031748345\n",
      "=== epoch:146, train acc:0.8533333333333334, test acc:0.6856 ===\n",
      "train loss:0.5795636417577549\n",
      "train loss:0.5491847474410405\n",
      "train loss:0.5006060272592049\n",
      "=== epoch:147, train acc:0.85, test acc:0.6822 ===\n",
      "train loss:0.4670273381804847\n",
      "train loss:0.6964649196917789\n",
      "train loss:0.49819429707268037\n",
      "=== epoch:148, train acc:0.8566666666666667, test acc:0.6857 ===\n",
      "train loss:0.5736392583160608\n",
      "train loss:0.5980144968702749\n",
      "train loss:0.5089115768588964\n",
      "=== epoch:149, train acc:0.86, test acc:0.687 ===\n",
      "train loss:0.5380381073006709\n",
      "train loss:0.5468153296164381\n",
      "train loss:0.5775739038443566\n",
      "=== epoch:150, train acc:0.86, test acc:0.6845 ===\n",
      "train loss:0.3820256749942546\n",
      "train loss:0.5488408458906374\n",
      "train loss:0.7430034375276868\n",
      "=== epoch:151, train acc:0.8633333333333333, test acc:0.6884 ===\n",
      "train loss:0.5253371841253066\n",
      "train loss:0.5025632579771115\n",
      "train loss:0.5921659592700902\n",
      "=== epoch:152, train acc:0.8666666666666667, test acc:0.6885 ===\n",
      "train loss:0.4448185315367615\n",
      "train loss:0.5383812319134497\n",
      "train loss:0.5271138227082361\n",
      "=== epoch:153, train acc:0.8666666666666667, test acc:0.6895 ===\n",
      "train loss:0.5412203450578992\n",
      "train loss:0.5043897266764403\n",
      "train loss:0.5591647827647142\n",
      "=== epoch:154, train acc:0.87, test acc:0.6919 ===\n",
      "train loss:0.5904391798997429\n",
      "train loss:0.482437019664887\n",
      "train loss:0.6700995120264486\n",
      "=== epoch:155, train acc:0.8666666666666667, test acc:0.6944 ===\n",
      "train loss:0.5622785055213826\n",
      "train loss:0.49969750614510017\n",
      "train loss:0.3518296434964632\n",
      "=== epoch:156, train acc:0.8733333333333333, test acc:0.6921 ===\n",
      "train loss:0.37189347743569895\n",
      "train loss:0.5281726486165771\n",
      "train loss:0.4096759554695582\n",
      "=== epoch:157, train acc:0.8766666666666667, test acc:0.6953 ===\n",
      "train loss:0.45112682209069194\n",
      "train loss:0.5003373416232721\n",
      "train loss:0.6269931735539052\n",
      "=== epoch:158, train acc:0.88, test acc:0.7018 ===\n",
      "train loss:0.5093421234381308\n",
      "train loss:0.548252797165669\n",
      "train loss:0.4341344538601853\n",
      "=== epoch:159, train acc:0.8766666666666667, test acc:0.6987 ===\n",
      "train loss:0.5454577473751214\n",
      "train loss:0.47379563701633737\n",
      "train loss:0.38165244526468206\n",
      "=== epoch:160, train acc:0.8766666666666667, test acc:0.6979 ===\n",
      "train loss:0.5106936284105201\n",
      "train loss:0.43318584995531567\n",
      "train loss:0.5089290547455244\n",
      "=== epoch:161, train acc:0.88, test acc:0.6965 ===\n",
      "train loss:0.5852294045164999\n",
      "train loss:0.4019460569380597\n",
      "train loss:0.38220339619535326\n",
      "=== epoch:162, train acc:0.8766666666666667, test acc:0.6969 ===\n",
      "train loss:0.5345931830975238\n",
      "train loss:0.5512533475452516\n",
      "train loss:0.6280666833984223\n",
      "=== epoch:163, train acc:0.8766666666666667, test acc:0.6975 ===\n",
      "train loss:0.6115921319402531\n",
      "train loss:0.5336315865818667\n",
      "train loss:0.41677699563521986\n",
      "=== epoch:164, train acc:0.8766666666666667, test acc:0.7003 ===\n",
      "train loss:0.4615033160516171\n",
      "train loss:0.3995064152900841\n",
      "train loss:0.477573832060774\n",
      "=== epoch:165, train acc:0.8866666666666667, test acc:0.7059 ===\n",
      "train loss:0.4747476521331556\n",
      "train loss:0.398344483766911\n",
      "train loss:0.44723107207283946\n",
      "=== epoch:166, train acc:0.8866666666666667, test acc:0.7075 ===\n",
      "train loss:0.600023633833381\n",
      "train loss:0.3878932894337983\n",
      "train loss:0.3782841041207945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.8766666666666667, test acc:0.7051 ===\n",
      "train loss:0.3562913190380809\n",
      "train loss:0.44151522590917547\n",
      "train loss:0.524248096218429\n",
      "=== epoch:168, train acc:0.8766666666666667, test acc:0.7032 ===\n",
      "train loss:0.4566745931973897\n",
      "train loss:0.5046605454591169\n",
      "train loss:0.327084520129558\n",
      "=== epoch:169, train acc:0.8833333333333333, test acc:0.7055 ===\n",
      "train loss:0.4514508852399375\n",
      "train loss:0.42001646791621206\n",
      "train loss:0.3251089408047702\n",
      "=== epoch:170, train acc:0.88, test acc:0.7028 ===\n",
      "train loss:0.4210808354792249\n",
      "train loss:0.30821991259147974\n",
      "train loss:0.37709539835732925\n",
      "=== epoch:171, train acc:0.8833333333333333, test acc:0.707 ===\n",
      "train loss:0.32386604128962626\n",
      "train loss:0.4318487394349312\n",
      "train loss:0.3518430207159878\n",
      "=== epoch:172, train acc:0.8933333333333333, test acc:0.707 ===\n",
      "train loss:0.448123243288776\n",
      "train loss:0.4003471315834391\n",
      "train loss:0.45217807954375333\n",
      "=== epoch:173, train acc:0.89, test acc:0.7118 ===\n",
      "train loss:0.43224531604201916\n",
      "train loss:0.4203703924971432\n",
      "train loss:0.3860015932392718\n",
      "=== epoch:174, train acc:0.8966666666666666, test acc:0.7153 ===\n",
      "train loss:0.5459943975873413\n",
      "train loss:0.3543154889251571\n",
      "train loss:0.3831171722121461\n",
      "=== epoch:175, train acc:0.89, test acc:0.7156 ===\n",
      "train loss:0.3126486192968705\n",
      "train loss:0.4603282534406752\n",
      "train loss:0.4064853629391348\n",
      "=== epoch:176, train acc:0.8933333333333333, test acc:0.7134 ===\n",
      "train loss:0.40383576518974423\n",
      "train loss:0.4380860464293766\n",
      "train loss:0.37008923644619707\n",
      "=== epoch:177, train acc:0.8966666666666666, test acc:0.7116 ===\n",
      "train loss:0.3878436467348211\n",
      "train loss:0.4664181026233777\n",
      "train loss:0.37726925245063214\n",
      "=== epoch:178, train acc:0.8966666666666666, test acc:0.7096 ===\n",
      "train loss:0.42968407112419854\n",
      "train loss:0.4576789197918136\n",
      "train loss:0.4227078404043052\n",
      "=== epoch:179, train acc:0.89, test acc:0.7109 ===\n",
      "train loss:0.45345822883900977\n",
      "train loss:0.32685542755570596\n",
      "train loss:0.3565319861742166\n",
      "=== epoch:180, train acc:0.8933333333333333, test acc:0.7116 ===\n",
      "train loss:0.3022540159932102\n",
      "train loss:0.41610581616194686\n",
      "train loss:0.40162879181312283\n",
      "=== epoch:181, train acc:0.8866666666666667, test acc:0.7105 ===\n",
      "train loss:0.39894187101326006\n",
      "train loss:0.33697901500890765\n",
      "train loss:0.4030901068489303\n",
      "=== epoch:182, train acc:0.8966666666666666, test acc:0.707 ===\n",
      "train loss:0.35564394814814887\n",
      "train loss:0.3778917596830248\n",
      "train loss:0.30273475090910507\n",
      "=== epoch:183, train acc:0.8933333333333333, test acc:0.7091 ===\n",
      "train loss:0.3548134478595079\n",
      "train loss:0.4113196886649989\n",
      "train loss:0.28332287525691413\n",
      "=== epoch:184, train acc:0.8966666666666666, test acc:0.7176 ===\n",
      "train loss:0.28450973847032957\n",
      "train loss:0.3724887118876268\n",
      "train loss:0.31955864149904006\n",
      "=== epoch:185, train acc:0.8966666666666666, test acc:0.7193 ===\n",
      "train loss:0.41313058614600157\n",
      "train loss:0.2716309309366766\n",
      "train loss:0.3040519905116495\n",
      "=== epoch:186, train acc:0.9066666666666666, test acc:0.7191 ===\n",
      "train loss:0.3121712405732873\n",
      "train loss:0.36398450042886277\n",
      "train loss:0.2888472534179611\n",
      "=== epoch:187, train acc:0.91, test acc:0.7217 ===\n",
      "train loss:0.33943488234734503\n",
      "train loss:0.3744811626080731\n",
      "train loss:0.3367118626215916\n",
      "=== epoch:188, train acc:0.9033333333333333, test acc:0.7172 ===\n",
      "train loss:0.4697912029471013\n",
      "train loss:0.3193703832913469\n",
      "train loss:0.27139530727604844\n",
      "=== epoch:189, train acc:0.9, test acc:0.7217 ===\n",
      "train loss:0.3366488714660835\n",
      "train loss:0.29327754812999607\n",
      "train loss:0.3595112172885447\n",
      "=== epoch:190, train acc:0.9033333333333333, test acc:0.7185 ===\n",
      "train loss:0.2670021693569596\n",
      "train loss:0.31170942372893684\n",
      "train loss:0.39045899921288557\n",
      "=== epoch:191, train acc:0.91, test acc:0.7127 ===\n",
      "train loss:0.2852228240771037\n",
      "train loss:0.35623830718200866\n",
      "train loss:0.36830007965936284\n",
      "=== epoch:192, train acc:0.9133333333333333, test acc:0.7187 ===\n",
      "train loss:0.2692127867165659\n",
      "train loss:0.3574315980656331\n",
      "train loss:0.35017905201351823\n",
      "=== epoch:193, train acc:0.92, test acc:0.7221 ===\n",
      "train loss:0.44907040146009486\n",
      "train loss:0.2961521847204532\n",
      "train loss:0.35125074319213373\n",
      "=== epoch:194, train acc:0.9166666666666666, test acc:0.721 ===\n",
      "train loss:0.26832654241651804\n",
      "train loss:0.3378583089535842\n",
      "train loss:0.30384089927529007\n",
      "=== epoch:195, train acc:0.9166666666666666, test acc:0.7266 ===\n",
      "train loss:0.4334022014571122\n",
      "train loss:0.24117012008338087\n",
      "train loss:0.2904049329039599\n",
      "=== epoch:196, train acc:0.9233333333333333, test acc:0.7321 ===\n",
      "train loss:0.3403738667982195\n",
      "train loss:0.3169310497740884\n",
      "train loss:0.28725919298168173\n",
      "=== epoch:197, train acc:0.9266666666666666, test acc:0.7287 ===\n",
      "train loss:0.26060709053186815\n",
      "train loss:0.22489165512223125\n",
      "train loss:0.21934678396559343\n",
      "=== epoch:198, train acc:0.9133333333333333, test acc:0.7289 ===\n",
      "train loss:0.321155716381516\n",
      "train loss:0.3090857202472624\n",
      "train loss:0.3743670958713718\n",
      "=== epoch:199, train acc:0.92, test acc:0.7293 ===\n",
      "train loss:0.26305019432654314\n",
      "train loss:0.28634888516836626\n",
      "train loss:0.21336471897740933\n",
      "=== epoch:200, train acc:0.92, test acc:0.7266 ===\n",
      "train loss:0.3414233600342316\n",
      "train loss:0.2684553279726481\n",
      "train loss:0.2335385329030648\n",
      "=== epoch:201, train acc:0.9166666666666666, test acc:0.7283 ===\n",
      "train loss:0.23732587505868175\n",
      "train loss:0.31384190946903756\n",
      "train loss:0.331068479748481\n",
      "=== epoch:202, train acc:0.9266666666666666, test acc:0.7281 ===\n",
      "train loss:0.30570878183014033\n",
      "train loss:0.2998062077032113\n",
      "train loss:0.2920046286928063\n",
      "=== epoch:203, train acc:0.9266666666666666, test acc:0.731 ===\n",
      "train loss:0.186265941968274\n",
      "train loss:0.38078322999520803\n",
      "train loss:0.30439914370602916\n",
      "=== epoch:204, train acc:0.9266666666666666, test acc:0.7295 ===\n",
      "train loss:0.26487269570264216\n",
      "train loss:0.21588075236037582\n",
      "train loss:0.3298007696503245\n",
      "=== epoch:205, train acc:0.93, test acc:0.7309 ===\n",
      "train loss:0.341585672000251\n",
      "train loss:0.3460265683327349\n",
      "train loss:0.3245520174746007\n",
      "=== epoch:206, train acc:0.9233333333333333, test acc:0.7291 ===\n",
      "train loss:0.2698935624328511\n",
      "train loss:0.18956865379966475\n",
      "train loss:0.2888209216143328\n",
      "=== epoch:207, train acc:0.9333333333333333, test acc:0.7324 ===\n",
      "train loss:0.2667065591479961\n",
      "train loss:0.32870745878462837\n",
      "train loss:0.28548986665206033\n",
      "=== epoch:208, train acc:0.93, test acc:0.7339 ===\n",
      "train loss:0.27115194681761184\n",
      "train loss:0.2809987303990017\n",
      "train loss:0.25055336639135645\n",
      "=== epoch:209, train acc:0.9333333333333333, test acc:0.7332 ===\n",
      "train loss:0.2934456991058031\n",
      "train loss:0.2521022984076552\n",
      "train loss:0.21353937691636266\n",
      "=== epoch:210, train acc:0.9366666666666666, test acc:0.7336 ===\n",
      "train loss:0.3094416072540507\n",
      "train loss:0.2849192862566676\n",
      "train loss:0.2837667380224702\n",
      "=== epoch:211, train acc:0.94, test acc:0.7347 ===\n",
      "train loss:0.215857022780644\n",
      "train loss:0.16717098096069902\n",
      "train loss:0.2122250497529906\n",
      "=== epoch:212, train acc:0.9366666666666666, test acc:0.7386 ===\n",
      "train loss:0.2119072459095473\n",
      "train loss:0.24393206350913196\n",
      "train loss:0.2649277242861019\n",
      "=== epoch:213, train acc:0.9466666666666667, test acc:0.7395 ===\n",
      "train loss:0.22720114247761602\n",
      "train loss:0.16999372421897926\n",
      "train loss:0.24670954576016935\n",
      "=== epoch:214, train acc:0.9433333333333334, test acc:0.7385 ===\n",
      "train loss:0.3270696266799664\n",
      "train loss:0.19782575450544435\n",
      "train loss:0.2828579825069614\n",
      "=== epoch:215, train acc:0.9433333333333334, test acc:0.7379 ===\n",
      "train loss:0.22077795145132856\n",
      "train loss:0.1630071123983612\n",
      "train loss:0.19849498053099934\n",
      "=== epoch:216, train acc:0.95, test acc:0.7411 ===\n",
      "train loss:0.2866566580119868\n",
      "train loss:0.17916903599450273\n",
      "train loss:0.2086510414270055\n",
      "=== epoch:217, train acc:0.9466666666666667, test acc:0.7391 ===\n",
      "train loss:0.18544509717659952\n",
      "train loss:0.2585688978919371\n",
      "train loss:0.23504890037760426\n",
      "=== epoch:218, train acc:0.9466666666666667, test acc:0.7412 ===\n",
      "train loss:0.24861829868286375\n",
      "train loss:0.29921749962679384\n",
      "train loss:0.19142413552905047\n",
      "=== epoch:219, train acc:0.9433333333333334, test acc:0.7425 ===\n",
      "train loss:0.21463677891002572\n",
      "train loss:0.24167063393019472\n",
      "train loss:0.22609892142928206\n",
      "=== epoch:220, train acc:0.9433333333333334, test acc:0.7433 ===\n",
      "train loss:0.207197912146232\n",
      "train loss:0.2605493077328009\n",
      "train loss:0.27990574356914766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:221, train acc:0.95, test acc:0.7436 ===\n",
      "train loss:0.2271276964277159\n",
      "train loss:0.18655909850646868\n",
      "train loss:0.23835464321562724\n",
      "=== epoch:222, train acc:0.9533333333333334, test acc:0.7459 ===\n",
      "train loss:0.17816765786780828\n",
      "train loss:0.1947044701511259\n",
      "train loss:0.19624099282561439\n",
      "=== epoch:223, train acc:0.96, test acc:0.7466 ===\n",
      "train loss:0.21858868642913012\n",
      "train loss:0.15983837995678646\n",
      "train loss:0.17408571338961681\n",
      "=== epoch:224, train acc:0.96, test acc:0.7467 ===\n",
      "train loss:0.13903908868145978\n",
      "train loss:0.24917512807371453\n",
      "train loss:0.14552743883821867\n",
      "=== epoch:225, train acc:0.96, test acc:0.7426 ===\n",
      "train loss:0.20348761741265448\n",
      "train loss:0.24980619701078915\n",
      "train loss:0.15982454720589184\n",
      "=== epoch:226, train acc:0.95, test acc:0.7421 ===\n",
      "train loss:0.2426850875297005\n",
      "train loss:0.20134773976706338\n",
      "train loss:0.21605980760554658\n",
      "=== epoch:227, train acc:0.95, test acc:0.7449 ===\n",
      "train loss:0.14406161120697403\n",
      "train loss:0.2260123021769342\n",
      "train loss:0.16040793887406374\n",
      "=== epoch:228, train acc:0.9566666666666667, test acc:0.7419 ===\n",
      "train loss:0.20404795969117945\n",
      "train loss:0.2211347277432554\n",
      "train loss:0.2283407767755891\n",
      "=== epoch:229, train acc:0.9533333333333334, test acc:0.7426 ===\n",
      "train loss:0.12842225592803824\n",
      "train loss:0.1384336740668977\n",
      "train loss:0.17997003901065742\n",
      "=== epoch:230, train acc:0.9533333333333334, test acc:0.7454 ===\n",
      "train loss:0.17280480554280683\n",
      "train loss:0.1797225850385656\n",
      "train loss:0.1881366694849984\n",
      "=== epoch:231, train acc:0.9566666666666667, test acc:0.7438 ===\n",
      "train loss:0.17596390026038866\n",
      "train loss:0.1572932564718338\n",
      "train loss:0.13129735060416478\n",
      "=== epoch:232, train acc:0.9566666666666667, test acc:0.7461 ===\n",
      "train loss:0.1649365353816248\n",
      "train loss:0.2519495536288616\n",
      "train loss:0.20561249379468244\n",
      "=== epoch:233, train acc:0.9566666666666667, test acc:0.7433 ===\n",
      "train loss:0.1348551660861858\n",
      "train loss:0.12078777109577464\n",
      "train loss:0.18740363444422797\n",
      "=== epoch:234, train acc:0.9566666666666667, test acc:0.7441 ===\n",
      "train loss:0.16245391011395685\n",
      "train loss:0.17337329368736001\n",
      "train loss:0.1492043987438627\n",
      "=== epoch:235, train acc:0.9566666666666667, test acc:0.7431 ===\n",
      "train loss:0.1598406594548851\n",
      "train loss:0.1949773084017518\n",
      "train loss:0.2074233029899124\n",
      "=== epoch:236, train acc:0.9566666666666667, test acc:0.7452 ===\n",
      "train loss:0.16478627852518385\n",
      "train loss:0.22270913432404138\n",
      "train loss:0.1326949823286314\n",
      "=== epoch:237, train acc:0.9566666666666667, test acc:0.7438 ===\n",
      "train loss:0.11879926820649736\n",
      "train loss:0.2266651256485227\n",
      "train loss:0.18478820218386902\n",
      "=== epoch:238, train acc:0.9666666666666667, test acc:0.7503 ===\n",
      "train loss:0.14092263572917454\n",
      "train loss:0.14055909865203467\n",
      "train loss:0.12868168345487357\n",
      "=== epoch:239, train acc:0.9633333333333334, test acc:0.7493 ===\n",
      "train loss:0.14543491662033345\n",
      "train loss:0.1245118252256099\n",
      "train loss:0.13099742504679512\n",
      "=== epoch:240, train acc:0.9666666666666667, test acc:0.7485 ===\n",
      "train loss:0.22507858713665765\n",
      "train loss:0.11262372498444345\n",
      "train loss:0.19686982393374589\n",
      "=== epoch:241, train acc:0.9666666666666667, test acc:0.7497 ===\n",
      "train loss:0.15912651614139717\n",
      "train loss:0.1931271551304609\n",
      "train loss:0.11830671150812928\n",
      "=== epoch:242, train acc:0.97, test acc:0.7502 ===\n",
      "train loss:0.12460813925039017\n",
      "train loss:0.14293650384844483\n",
      "train loss:0.20496144758756066\n",
      "=== epoch:243, train acc:0.97, test acc:0.7537 ===\n",
      "train loss:0.12480236643254877\n",
      "train loss:0.15920219496364077\n",
      "train loss:0.14402096743764092\n",
      "=== epoch:244, train acc:0.97, test acc:0.7543 ===\n",
      "train loss:0.12690602305992627\n",
      "train loss:0.11127734101412465\n",
      "train loss:0.19674215710914317\n",
      "=== epoch:245, train acc:0.97, test acc:0.7565 ===\n",
      "train loss:0.14945542872186926\n",
      "train loss:0.10953191191498649\n",
      "train loss:0.15705586222046386\n",
      "=== epoch:246, train acc:0.9733333333333334, test acc:0.7535 ===\n",
      "train loss:0.17291065525631816\n",
      "train loss:0.17242549726247142\n",
      "train loss:0.15083641047624063\n",
      "=== epoch:247, train acc:0.9733333333333334, test acc:0.755 ===\n",
      "train loss:0.13778516144522748\n",
      "train loss:0.17427405256833522\n",
      "train loss:0.12345732847027742\n",
      "=== epoch:248, train acc:0.9733333333333334, test acc:0.7552 ===\n",
      "train loss:0.13635269046185683\n",
      "train loss:0.11510984628203862\n",
      "train loss:0.11010371205449537\n",
      "=== epoch:249, train acc:0.97, test acc:0.7551 ===\n",
      "train loss:0.15791512006331263\n",
      "train loss:0.10292316909595246\n",
      "train loss:0.1700790560723622\n",
      "=== epoch:250, train acc:0.97, test acc:0.7532 ===\n",
      "train loss:0.1782573651135888\n",
      "train loss:0.12462168927270399\n",
      "train loss:0.06892099215425056\n",
      "=== epoch:251, train acc:0.97, test acc:0.7534 ===\n",
      "train loss:0.15432864128443544\n",
      "train loss:0.1346954267990259\n",
      "train loss:0.16210396305293778\n",
      "=== epoch:252, train acc:0.9766666666666667, test acc:0.7579 ===\n",
      "train loss:0.11862870585477925\n",
      "train loss:0.1468038854488039\n",
      "train loss:0.12380475263700479\n",
      "=== epoch:253, train acc:0.9766666666666667, test acc:0.7568 ===\n",
      "train loss:0.08002957021298356\n",
      "train loss:0.13825520811445183\n",
      "train loss:0.08213348294838044\n",
      "=== epoch:254, train acc:0.9766666666666667, test acc:0.7579 ===\n",
      "train loss:0.10793703147071698\n",
      "train loss:0.09988498730389353\n",
      "train loss:0.1645720634845032\n",
      "=== epoch:255, train acc:0.9766666666666667, test acc:0.7557 ===\n",
      "train loss:0.09206592108435047\n",
      "train loss:0.11640372286416766\n",
      "train loss:0.08656317687498101\n",
      "=== epoch:256, train acc:0.9733333333333334, test acc:0.7534 ===\n",
      "train loss:0.16658340548616213\n",
      "train loss:0.08684851521658697\n",
      "train loss:0.08923912404838248\n",
      "=== epoch:257, train acc:0.9733333333333334, test acc:0.7536 ===\n",
      "train loss:0.131273893564967\n",
      "train loss:0.12943599311273254\n",
      "train loss:0.08554953130817677\n",
      "=== epoch:258, train acc:0.9766666666666667, test acc:0.7567 ===\n",
      "train loss:0.10879640831780674\n",
      "train loss:0.17626735517307537\n",
      "train loss:0.09640760936562312\n",
      "=== epoch:259, train acc:0.9766666666666667, test acc:0.7576 ===\n",
      "train loss:0.10342089796707159\n",
      "train loss:0.15176265671102765\n",
      "train loss:0.06902548129614045\n",
      "=== epoch:260, train acc:0.9766666666666667, test acc:0.7595 ===\n",
      "train loss:0.11874499020033062\n",
      "train loss:0.1235664252403224\n",
      "train loss:0.08168558061636658\n",
      "=== epoch:261, train acc:0.9766666666666667, test acc:0.7603 ===\n",
      "train loss:0.09124745857012354\n",
      "train loss:0.09966552730040629\n",
      "train loss:0.13329013191781874\n",
      "=== epoch:262, train acc:0.9766666666666667, test acc:0.7585 ===\n",
      "train loss:0.1455517400446086\n",
      "train loss:0.1207856503805544\n",
      "train loss:0.15135454997446832\n",
      "=== epoch:263, train acc:0.9766666666666667, test acc:0.7598 ===\n",
      "train loss:0.1520612810677011\n",
      "train loss:0.10975815964429779\n",
      "train loss:0.1944851611590102\n",
      "=== epoch:264, train acc:0.9766666666666667, test acc:0.7644 ===\n",
      "train loss:0.10016488663849575\n",
      "train loss:0.07586616803597088\n",
      "train loss:0.12738932523647228\n",
      "=== epoch:265, train acc:0.9766666666666667, test acc:0.7649 ===\n",
      "train loss:0.10901311463810304\n",
      "train loss:0.15522507332994395\n",
      "train loss:0.11855235822156113\n",
      "=== epoch:266, train acc:0.9766666666666667, test acc:0.7627 ===\n",
      "train loss:0.13509554050636505\n",
      "train loss:0.07628274437364618\n",
      "train loss:0.12041324627927195\n",
      "=== epoch:267, train acc:0.9766666666666667, test acc:0.7617 ===\n",
      "train loss:0.0685259095360872\n",
      "train loss:0.11263301699009393\n",
      "train loss:0.10333573237607804\n",
      "=== epoch:268, train acc:0.9766666666666667, test acc:0.7609 ===\n",
      "train loss:0.13822977036676407\n",
      "train loss:0.07093680633171029\n",
      "train loss:0.1032184930180701\n",
      "=== epoch:269, train acc:0.9766666666666667, test acc:0.7637 ===\n",
      "train loss:0.08359807002462331\n",
      "train loss:0.07910561430955201\n",
      "train loss:0.12487605410987589\n",
      "=== epoch:270, train acc:0.9766666666666667, test acc:0.7613 ===\n",
      "train loss:0.057277458758252235\n",
      "train loss:0.09741414025397457\n",
      "train loss:0.10448245673023913\n",
      "=== epoch:271, train acc:0.98, test acc:0.7642 ===\n",
      "train loss:0.059630311042018776\n",
      "train loss:0.0980496264157331\n",
      "train loss:0.08981113257648021\n",
      "=== epoch:272, train acc:0.9766666666666667, test acc:0.762 ===\n",
      "train loss:0.09422291623157733\n",
      "train loss:0.10239041534737849\n",
      "train loss:0.09527250939271738\n",
      "=== epoch:273, train acc:0.98, test acc:0.7632 ===\n",
      "train loss:0.10263603634933129\n",
      "train loss:0.05622545055781213\n",
      "train loss:0.09169707841857759\n",
      "=== epoch:274, train acc:0.9766666666666667, test acc:0.7614 ===\n",
      "train loss:0.09144234831367486\n",
      "train loss:0.06575422366214177\n",
      "train loss:0.09836021828767731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:275, train acc:0.9766666666666667, test acc:0.7672 ===\n",
      "train loss:0.06369305613702442\n",
      "train loss:0.05001647875081544\n",
      "train loss:0.11458420227072402\n",
      "=== epoch:276, train acc:0.9766666666666667, test acc:0.7669 ===\n",
      "train loss:0.07236331187954331\n",
      "train loss:0.13737500289632398\n",
      "train loss:0.11724276418711643\n",
      "=== epoch:277, train acc:0.9833333333333333, test acc:0.769 ===\n",
      "train loss:0.09473087589056853\n",
      "train loss:0.09456787329578048\n",
      "train loss:0.09085356604693154\n",
      "=== epoch:278, train acc:0.98, test acc:0.7664 ===\n",
      "train loss:0.08779360451016645\n",
      "train loss:0.058643266613138255\n",
      "train loss:0.13507327073429543\n",
      "=== epoch:279, train acc:0.9833333333333333, test acc:0.7676 ===\n",
      "train loss:0.10608131389104056\n",
      "train loss:0.11740214211224907\n",
      "train loss:0.07551924787853667\n",
      "=== epoch:280, train acc:0.9833333333333333, test acc:0.7666 ===\n",
      "train loss:0.06539642731794604\n",
      "train loss:0.07028589425900271\n",
      "train loss:0.06739836360786086\n",
      "=== epoch:281, train acc:0.9833333333333333, test acc:0.7691 ===\n",
      "train loss:0.1479921432537477\n",
      "train loss:0.10183846721057735\n",
      "train loss:0.0689563993921679\n",
      "=== epoch:282, train acc:0.9833333333333333, test acc:0.7703 ===\n",
      "train loss:0.1139740116313339\n",
      "train loss:0.07102635274196317\n",
      "train loss:0.08850732253045085\n",
      "=== epoch:283, train acc:0.98, test acc:0.768 ===\n",
      "train loss:0.061526133593022996\n",
      "train loss:0.07619890549392742\n",
      "train loss:0.06539386649157233\n",
      "=== epoch:284, train acc:0.9766666666666667, test acc:0.7648 ===\n",
      "train loss:0.06851425170584965\n",
      "train loss:0.07248978989147514\n",
      "train loss:0.08597386254608345\n",
      "=== epoch:285, train acc:0.9833333333333333, test acc:0.7681 ===\n",
      "train loss:0.08164988726462384\n",
      "train loss:0.04439732628230582\n",
      "train loss:0.06687785926796262\n",
      "=== epoch:286, train acc:0.98, test acc:0.7656 ===\n",
      "train loss:0.08412839804033849\n",
      "train loss:0.06532930956179776\n",
      "train loss:0.048708241267070226\n",
      "=== epoch:287, train acc:0.9833333333333333, test acc:0.7664 ===\n",
      "train loss:0.07079366803990389\n",
      "train loss:0.07529241455438564\n",
      "train loss:0.09028574643672442\n",
      "=== epoch:288, train acc:0.9833333333333333, test acc:0.7674 ===\n",
      "train loss:0.07328159022682405\n",
      "train loss:0.06391708914408842\n",
      "train loss:0.10160064482236013\n",
      "=== epoch:289, train acc:0.9833333333333333, test acc:0.767 ===\n",
      "train loss:0.05035522526795684\n",
      "train loss:0.09210709036814192\n",
      "train loss:0.08434478121071884\n",
      "=== epoch:290, train acc:0.99, test acc:0.7676 ===\n",
      "train loss:0.06868255158283809\n",
      "train loss:0.0749864629778736\n",
      "train loss:0.08200683146058958\n",
      "=== epoch:291, train acc:0.99, test acc:0.769 ===\n",
      "train loss:0.05256483315431068\n",
      "train loss:0.09990863306550202\n",
      "train loss:0.08098705603316964\n",
      "=== epoch:292, train acc:0.99, test acc:0.7716 ===\n",
      "train loss:0.10136035205525191\n",
      "train loss:0.047712717802691554\n",
      "train loss:0.0765037857004578\n",
      "=== epoch:293, train acc:0.99, test acc:0.7717 ===\n",
      "train loss:0.10577902894146617\n",
      "train loss:0.06639087016348331\n",
      "train loss:0.1079091991529123\n",
      "=== epoch:294, train acc:0.9833333333333333, test acc:0.7682 ===\n",
      "train loss:0.07196889871490737\n",
      "train loss:0.06809919105575729\n",
      "train loss:0.08185918895577533\n",
      "=== epoch:295, train acc:0.9833333333333333, test acc:0.7699 ===\n",
      "train loss:0.08977636423334893\n",
      "train loss:0.0865834292523031\n",
      "train loss:0.09901197667913182\n",
      "=== epoch:296, train acc:0.99, test acc:0.7722 ===\n",
      "train loss:0.05866753093611084\n",
      "train loss:0.08191150583720364\n",
      "train loss:0.06340839358215056\n",
      "=== epoch:297, train acc:0.99, test acc:0.7709 ===\n",
      "train loss:0.05320435024241235\n",
      "train loss:0.05825204871594305\n",
      "train loss:0.05616506582436518\n",
      "=== epoch:298, train acc:0.99, test acc:0.7735 ===\n",
      "train loss:0.07145344548443719\n",
      "train loss:0.038649999427917754\n",
      "train loss:0.0666227300161927\n",
      "=== epoch:299, train acc:0.99, test acc:0.7751 ===\n",
      "train loss:0.05978505846713872\n",
      "train loss:0.06825186792925869\n",
      "train loss:0.045503955874571254\n",
      "=== epoch:300, train acc:0.99, test acc:0.7744 ===\n",
      "train loss:0.06072517265334643\n",
      "train loss:0.061660115182687186\n",
      "train loss:0.04851051362601647\n",
      "=== epoch:301, train acc:0.99, test acc:0.7726 ===\n",
      "train loss:0.06938849631705087\n",
      "train loss:0.08191049921151389\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7726\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.1\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f325f4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0810cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2959210647840487\n",
      "=== epoch:1, train acc:0.06, test acc:0.0634 ===\n",
      "train loss:2.310403273288428\n",
      "train loss:2.309717111159578\n",
      "train loss:2.301306025939381\n",
      "=== epoch:2, train acc:0.06333333333333334, test acc:0.0656 ===\n",
      "train loss:2.3171689045428097\n",
      "train loss:2.3135769199630234\n",
      "train loss:2.2903067130252794\n",
      "=== epoch:3, train acc:0.06333333333333334, test acc:0.0672 ===\n",
      "train loss:2.301512804903795\n",
      "train loss:2.2937438779694914\n",
      "train loss:2.301702891448014\n",
      "=== epoch:4, train acc:0.06333333333333334, test acc:0.0664 ===\n",
      "train loss:2.303317523288316\n",
      "train loss:2.3028910463682273\n",
      "train loss:2.303676819906938\n",
      "=== epoch:5, train acc:0.06333333333333334, test acc:0.0681 ===\n",
      "train loss:2.302550624868801\n",
      "train loss:2.295723532825746\n",
      "train loss:2.2982014673736275\n",
      "=== epoch:6, train acc:0.06666666666666667, test acc:0.0702 ===\n",
      "train loss:2.313798894624564\n",
      "train loss:2.294495453289609\n",
      "train loss:2.3117153786873157\n",
      "=== epoch:7, train acc:0.06666666666666667, test acc:0.0705 ===\n",
      "train loss:2.303076107896964\n",
      "train loss:2.302335652390609\n",
      "train loss:2.307486675497276\n",
      "=== epoch:8, train acc:0.07333333333333333, test acc:0.0728 ===\n",
      "train loss:2.30207814269406\n",
      "train loss:2.3064332658669406\n",
      "train loss:2.3101012085344426\n",
      "=== epoch:9, train acc:0.07, test acc:0.0718 ===\n",
      "train loss:2.30435211762618\n",
      "train loss:2.2990072976850344\n",
      "train loss:2.3052742729046694\n",
      "=== epoch:10, train acc:0.06333333333333334, test acc:0.0711 ===\n",
      "train loss:2.2964635537156983\n",
      "train loss:2.309586249461594\n",
      "train loss:2.29427422609366\n",
      "=== epoch:11, train acc:0.06333333333333334, test acc:0.07 ===\n",
      "train loss:2.3030987108661143\n",
      "train loss:2.3036821661675106\n",
      "train loss:2.3015939926851767\n",
      "=== epoch:12, train acc:0.06333333333333334, test acc:0.0701 ===\n",
      "train loss:2.3004389880362415\n",
      "train loss:2.3005788082790493\n",
      "train loss:2.2964507657961795\n",
      "=== epoch:13, train acc:0.07, test acc:0.0739 ===\n",
      "train loss:2.305490104836701\n",
      "train loss:2.292263905348486\n",
      "train loss:2.2894203875093293\n",
      "=== epoch:14, train acc:0.07333333333333333, test acc:0.0762 ===\n",
      "train loss:2.3036854860613123\n",
      "train loss:2.3003223066392073\n",
      "train loss:2.302878949206849\n",
      "=== epoch:15, train acc:0.08, test acc:0.0815 ===\n",
      "train loss:2.2952250945305783\n",
      "train loss:2.304812085738685\n",
      "train loss:2.2999992672842655\n",
      "=== epoch:16, train acc:0.07666666666666666, test acc:0.0821 ===\n",
      "train loss:2.296599754782113\n",
      "train loss:2.303393721090042\n",
      "train loss:2.288804220478623\n",
      "=== epoch:17, train acc:0.08666666666666667, test acc:0.0864 ===\n",
      "train loss:2.2943853906477383\n",
      "train loss:2.300221984296144\n",
      "train loss:2.2996628053657315\n",
      "=== epoch:18, train acc:0.08666666666666667, test acc:0.089 ===\n",
      "train loss:2.2967302043868667\n",
      "train loss:2.298903666724759\n",
      "train loss:2.28970980751237\n",
      "=== epoch:19, train acc:0.09, test acc:0.0906 ===\n",
      "train loss:2.30186974712694\n",
      "train loss:2.3012543499990286\n",
      "train loss:2.3047863188571927\n",
      "=== epoch:20, train acc:0.09, test acc:0.0933 ===\n",
      "train loss:2.289581295673129\n",
      "train loss:2.2950694968363075\n",
      "train loss:2.299175786135928\n",
      "=== epoch:21, train acc:0.09, test acc:0.0969 ===\n",
      "train loss:2.3003282821871833\n",
      "train loss:2.293662871844098\n",
      "train loss:2.295323470103624\n",
      "=== epoch:22, train acc:0.09, test acc:0.0977 ===\n",
      "train loss:2.289435466471637\n",
      "train loss:2.2902272178896848\n",
      "train loss:2.2930002767520024\n",
      "=== epoch:23, train acc:0.09, test acc:0.0983 ===\n",
      "train loss:2.294576653668409\n",
      "train loss:2.305874791788363\n",
      "train loss:2.2930476402563063\n",
      "=== epoch:24, train acc:0.09333333333333334, test acc:0.1039 ===\n",
      "train loss:2.2910514740393957\n",
      "train loss:2.2955426330122144\n",
      "train loss:2.300260905676572\n",
      "=== epoch:25, train acc:0.09, test acc:0.1072 ===\n",
      "train loss:2.2850883116624625\n",
      "train loss:2.2924451101494343\n",
      "train loss:2.3039651666017638\n",
      "=== epoch:26, train acc:0.09333333333333334, test acc:0.1075 ===\n",
      "train loss:2.2911860843703558\n",
      "train loss:2.2954109676641146\n",
      "train loss:2.2958004998944683\n",
      "=== epoch:27, train acc:0.09666666666666666, test acc:0.1143 ===\n",
      "train loss:2.291271531641643\n",
      "train loss:2.291802434656523\n",
      "train loss:2.289716194193251\n",
      "=== epoch:28, train acc:0.1, test acc:0.1177 ===\n",
      "train loss:2.3000653084113507\n",
      "train loss:2.290674214837148\n",
      "train loss:2.2975958919526924\n",
      "=== epoch:29, train acc:0.10333333333333333, test acc:0.1185 ===\n",
      "train loss:2.2953266412774678\n",
      "train loss:2.285095269550165\n",
      "train loss:2.3005682648884513\n",
      "=== epoch:30, train acc:0.10333333333333333, test acc:0.1222 ===\n",
      "train loss:2.293549378108912\n",
      "train loss:2.2859070962367887\n",
      "train loss:2.291977607652868\n",
      "=== epoch:31, train acc:0.11333333333333333, test acc:0.1268 ===\n",
      "train loss:2.2943865412156446\n",
      "train loss:2.292379960242773\n",
      "train loss:2.2875323981785325\n",
      "=== epoch:32, train acc:0.10666666666666667, test acc:0.1259 ===\n",
      "train loss:2.2910976758109682\n",
      "train loss:2.2913779789612194\n",
      "train loss:2.2935688722094003\n",
      "=== epoch:33, train acc:0.11666666666666667, test acc:0.1291 ===\n",
      "train loss:2.296315285032957\n",
      "train loss:2.2936384029252315\n",
      "train loss:2.2921460426250264\n",
      "=== epoch:34, train acc:0.13333333333333333, test acc:0.1338 ===\n",
      "train loss:2.284263849794433\n",
      "train loss:2.294642470886613\n",
      "train loss:2.287628961740775\n",
      "=== epoch:35, train acc:0.13333333333333333, test acc:0.1325 ===\n",
      "train loss:2.2936198595116215\n",
      "train loss:2.2897712423991727\n",
      "train loss:2.297125649486114\n",
      "=== epoch:36, train acc:0.14, test acc:0.133 ===\n",
      "train loss:2.293272936046459\n",
      "train loss:2.290121794809491\n",
      "train loss:2.291806638507535\n",
      "=== epoch:37, train acc:0.15333333333333332, test acc:0.1358 ===\n",
      "train loss:2.2916644155232366\n",
      "train loss:2.296997498108446\n",
      "train loss:2.294745314052499\n",
      "=== epoch:38, train acc:0.15666666666666668, test acc:0.1359 ===\n",
      "train loss:2.2900483282606645\n",
      "train loss:2.3013172366155517\n",
      "train loss:2.288466452173415\n",
      "=== epoch:39, train acc:0.15, test acc:0.1381 ===\n",
      "train loss:2.2907890789054868\n",
      "train loss:2.2863209004410523\n",
      "train loss:2.2869806179918664\n",
      "=== epoch:40, train acc:0.14666666666666667, test acc:0.1429 ===\n",
      "train loss:2.2885475653979315\n",
      "train loss:2.2788186724773274\n",
      "train loss:2.286369647788927\n",
      "=== epoch:41, train acc:0.14666666666666667, test acc:0.1464 ===\n",
      "train loss:2.2933854959743654\n",
      "train loss:2.294345612350153\n",
      "train loss:2.284172490038612\n",
      "=== epoch:42, train acc:0.13666666666666666, test acc:0.1443 ===\n",
      "train loss:2.289280287019315\n",
      "train loss:2.28333027964375\n",
      "train loss:2.2939267546056814\n",
      "=== epoch:43, train acc:0.14, test acc:0.1469 ===\n",
      "train loss:2.2937134465982116\n",
      "train loss:2.288760799866408\n",
      "train loss:2.295066850922924\n",
      "=== epoch:44, train acc:0.13666666666666666, test acc:0.147 ===\n",
      "train loss:2.282589897150939\n",
      "train loss:2.2841705024071057\n",
      "train loss:2.2839770478153882\n",
      "=== epoch:45, train acc:0.15, test acc:0.1491 ===\n",
      "train loss:2.2883878952824235\n",
      "train loss:2.286926274089181\n",
      "train loss:2.2863958868436973\n",
      "=== epoch:46, train acc:0.15, test acc:0.1493 ===\n",
      "train loss:2.2793517562067387\n",
      "train loss:2.280008799059533\n",
      "train loss:2.2867080220159695\n",
      "=== epoch:47, train acc:0.15333333333333332, test acc:0.1497 ===\n",
      "train loss:2.2820302573606286\n",
      "train loss:2.2909256911245155\n",
      "train loss:2.284955366260783\n",
      "=== epoch:48, train acc:0.16333333333333333, test acc:0.1504 ===\n",
      "train loss:2.281379042715747\n",
      "train loss:2.2825831507172936\n",
      "train loss:2.290050530499571\n",
      "=== epoch:49, train acc:0.17, test acc:0.1498 ===\n",
      "train loss:2.284204601972079\n",
      "train loss:2.291063715919493\n",
      "train loss:2.2859055695135613\n",
      "=== epoch:50, train acc:0.17, test acc:0.1503 ===\n",
      "train loss:2.2926761101026893\n",
      "train loss:2.2950507815715806\n",
      "train loss:2.2835029076577795\n",
      "=== epoch:51, train acc:0.17, test acc:0.1503 ===\n",
      "train loss:2.2876616951006152\n",
      "train loss:2.2878235275050502\n",
      "train loss:2.279039037359133\n",
      "=== epoch:52, train acc:0.17, test acc:0.1491 ===\n",
      "train loss:2.2940804914554285\n",
      "train loss:2.2828100277925314\n",
      "train loss:2.2887951265254913\n",
      "=== epoch:53, train acc:0.16666666666666666, test acc:0.1481 ===\n",
      "train loss:2.2883364589751904\n",
      "train loss:2.2884825601516634\n",
      "train loss:2.283688880590029\n",
      "=== epoch:54, train acc:0.16666666666666666, test acc:0.1484 ===\n",
      "train loss:2.2814552859954644\n",
      "train loss:2.285823056375803\n",
      "train loss:2.283466369717962\n",
      "=== epoch:55, train acc:0.16666666666666666, test acc:0.1491 ===\n",
      "train loss:2.282524908125053\n",
      "train loss:2.2842539794623398\n",
      "train loss:2.272885175433056\n",
      "=== epoch:56, train acc:0.17, test acc:0.1498 ===\n",
      "train loss:2.2859842567211666\n",
      "train loss:2.2872547932428726\n",
      "train loss:2.283275615392374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.16666666666666666, test acc:0.1487 ===\n",
      "train loss:2.275802165070892\n",
      "train loss:2.2908857977089156\n",
      "train loss:2.282374887941925\n",
      "=== epoch:58, train acc:0.16666666666666666, test acc:0.1504 ===\n",
      "train loss:2.28095827052158\n",
      "train loss:2.2838101412891327\n",
      "train loss:2.2872185115216284\n",
      "=== epoch:59, train acc:0.17, test acc:0.1509 ===\n",
      "train loss:2.2835653945951195\n",
      "train loss:2.2882847893987885\n",
      "train loss:2.2880837724992116\n",
      "=== epoch:60, train acc:0.17, test acc:0.153 ===\n",
      "train loss:2.290100123577364\n",
      "train loss:2.298242782627925\n",
      "train loss:2.282750116603335\n",
      "=== epoch:61, train acc:0.17666666666666667, test acc:0.154 ===\n",
      "train loss:2.273254988776885\n",
      "train loss:2.289447676031024\n",
      "train loss:2.272374007568524\n",
      "=== epoch:62, train acc:0.18333333333333332, test acc:0.155 ===\n",
      "train loss:2.2777473972999065\n",
      "train loss:2.2811134262066592\n",
      "train loss:2.2894144816969946\n",
      "=== epoch:63, train acc:0.18333333333333332, test acc:0.1569 ===\n",
      "train loss:2.27637160495262\n",
      "train loss:2.27866582567482\n",
      "train loss:2.27678398773252\n",
      "=== epoch:64, train acc:0.19666666666666666, test acc:0.1584 ===\n",
      "train loss:2.2774993994406456\n",
      "train loss:2.280471637705402\n",
      "train loss:2.283636597677321\n",
      "=== epoch:65, train acc:0.2, test acc:0.159 ===\n",
      "train loss:2.289207247307384\n",
      "train loss:2.285255839724723\n",
      "train loss:2.282236387623759\n",
      "=== epoch:66, train acc:0.19666666666666666, test acc:0.1617 ===\n",
      "train loss:2.2720872363358513\n",
      "train loss:2.2746569113736697\n",
      "train loss:2.281975228236105\n",
      "=== epoch:67, train acc:0.20333333333333334, test acc:0.1656 ===\n",
      "train loss:2.2717240689195295\n",
      "train loss:2.278765891595342\n",
      "train loss:2.2794019739711078\n",
      "=== epoch:68, train acc:0.20333333333333334, test acc:0.1633 ===\n",
      "train loss:2.293477553833294\n",
      "train loss:2.283057362747065\n",
      "train loss:2.2824473902645686\n",
      "=== epoch:69, train acc:0.21, test acc:0.1672 ===\n",
      "train loss:2.2837795638106018\n",
      "train loss:2.2748566855157613\n",
      "train loss:2.2795773935558303\n",
      "=== epoch:70, train acc:0.21333333333333335, test acc:0.1694 ===\n",
      "train loss:2.2812869492940155\n",
      "train loss:2.2811221239141095\n",
      "train loss:2.27874572763875\n",
      "=== epoch:71, train acc:0.21333333333333335, test acc:0.1715 ===\n",
      "train loss:2.278362954154591\n",
      "train loss:2.2830520008708315\n",
      "train loss:2.29233432011581\n",
      "=== epoch:72, train acc:0.21666666666666667, test acc:0.1728 ===\n",
      "train loss:2.2809680594263755\n",
      "train loss:2.2761231170515583\n",
      "train loss:2.275467196153504\n",
      "=== epoch:73, train acc:0.21, test acc:0.1712 ===\n",
      "train loss:2.2866224676754277\n",
      "train loss:2.2694602090681757\n",
      "train loss:2.2827283786058574\n",
      "=== epoch:74, train acc:0.21666666666666667, test acc:0.1729 ===\n",
      "train loss:2.27273906430341\n",
      "train loss:2.2887703888553674\n",
      "train loss:2.2705999020869316\n",
      "=== epoch:75, train acc:0.22, test acc:0.1736 ===\n",
      "train loss:2.2956883780345065\n",
      "train loss:2.2821136016928865\n",
      "train loss:2.2792109992079075\n",
      "=== epoch:76, train acc:0.21666666666666667, test acc:0.1738 ===\n",
      "train loss:2.2765876481290817\n",
      "train loss:2.2848867853094994\n",
      "train loss:2.2755930292518913\n",
      "=== epoch:77, train acc:0.21666666666666667, test acc:0.174 ===\n",
      "train loss:2.282528839293836\n",
      "train loss:2.27439226845504\n",
      "train loss:2.272922941542467\n",
      "=== epoch:78, train acc:0.22333333333333333, test acc:0.1756 ===\n",
      "train loss:2.2706210445227715\n",
      "train loss:2.2731699309224225\n",
      "train loss:2.274868874624125\n",
      "=== epoch:79, train acc:0.22666666666666666, test acc:0.179 ===\n",
      "train loss:2.280303992826934\n",
      "train loss:2.280280152818405\n",
      "train loss:2.270277813648095\n",
      "=== epoch:80, train acc:0.22333333333333333, test acc:0.1848 ===\n",
      "train loss:2.2749382861851544\n",
      "train loss:2.269614547116435\n",
      "train loss:2.285445537019714\n",
      "=== epoch:81, train acc:0.22333333333333333, test acc:0.1883 ===\n",
      "train loss:2.2891191467532668\n",
      "train loss:2.2786017037323574\n",
      "train loss:2.278592657454243\n",
      "=== epoch:82, train acc:0.23, test acc:0.1883 ===\n",
      "train loss:2.282958681150463\n",
      "train loss:2.2760535352147\n",
      "train loss:2.271023801923548\n",
      "=== epoch:83, train acc:0.22333333333333333, test acc:0.189 ===\n",
      "train loss:2.280495642604137\n",
      "train loss:2.2717797322151223\n",
      "train loss:2.2672037107738343\n",
      "=== epoch:84, train acc:0.22666666666666666, test acc:0.1881 ===\n",
      "train loss:2.2787263180458597\n",
      "train loss:2.274402511819966\n",
      "train loss:2.301009695427286\n",
      "=== epoch:85, train acc:0.23333333333333334, test acc:0.1872 ===\n",
      "train loss:2.277843201181242\n",
      "train loss:2.2823151424739816\n",
      "train loss:2.275751767074566\n",
      "=== epoch:86, train acc:0.23333333333333334, test acc:0.1905 ===\n",
      "train loss:2.2765182742183216\n",
      "train loss:2.287203672057506\n",
      "train loss:2.2771164447220142\n",
      "=== epoch:87, train acc:0.23333333333333334, test acc:0.1943 ===\n",
      "train loss:2.2752833644092005\n",
      "train loss:2.2830499557252955\n",
      "train loss:2.2781236175130495\n",
      "=== epoch:88, train acc:0.23666666666666666, test acc:0.1969 ===\n",
      "train loss:2.2797769062443565\n",
      "train loss:2.2730059673380696\n",
      "train loss:2.278258254249683\n",
      "=== epoch:89, train acc:0.23333333333333334, test acc:0.1929 ===\n",
      "train loss:2.2677608150430952\n",
      "train loss:2.278231121911288\n",
      "train loss:2.2757742047232186\n",
      "=== epoch:90, train acc:0.23333333333333334, test acc:0.1942 ===\n",
      "train loss:2.2570478948557064\n",
      "train loss:2.282109957130385\n",
      "train loss:2.2773050595349345\n",
      "=== epoch:91, train acc:0.22666666666666666, test acc:0.194 ===\n",
      "train loss:2.280825401989618\n",
      "train loss:2.2668567546812053\n",
      "train loss:2.277460740286303\n",
      "=== epoch:92, train acc:0.22666666666666666, test acc:0.193 ===\n",
      "train loss:2.268639544761697\n",
      "train loss:2.2661070873975357\n",
      "train loss:2.2765611373336845\n",
      "=== epoch:93, train acc:0.23, test acc:0.1939 ===\n",
      "train loss:2.2835165592374813\n",
      "train loss:2.2782484874918962\n",
      "train loss:2.2721778995107385\n",
      "=== epoch:94, train acc:0.24, test acc:0.1968 ===\n",
      "train loss:2.271645677638689\n",
      "train loss:2.2787835095533215\n",
      "train loss:2.268201821114344\n",
      "=== epoch:95, train acc:0.23666666666666666, test acc:0.1947 ===\n",
      "train loss:2.2691735161921973\n",
      "train loss:2.2733150699023796\n",
      "train loss:2.27967461169873\n",
      "=== epoch:96, train acc:0.23666666666666666, test acc:0.1956 ===\n",
      "train loss:2.2739492736053277\n",
      "train loss:2.2743981710825905\n",
      "train loss:2.268661166686573\n",
      "=== epoch:97, train acc:0.24, test acc:0.1935 ===\n",
      "train loss:2.267975753234117\n",
      "train loss:2.2752428932050264\n",
      "train loss:2.2737510380142023\n",
      "=== epoch:98, train acc:0.23, test acc:0.1932 ===\n",
      "train loss:2.273700404517817\n",
      "train loss:2.273303792705391\n",
      "train loss:2.2813019135564545\n",
      "=== epoch:99, train acc:0.24, test acc:0.1958 ===\n",
      "train loss:2.2811416328394456\n",
      "train loss:2.2830582618636357\n",
      "train loss:2.291063189508313\n",
      "=== epoch:100, train acc:0.24333333333333335, test acc:0.1966 ===\n",
      "train loss:2.2802147253680896\n",
      "train loss:2.270592239445738\n",
      "train loss:2.271417819080781\n",
      "=== epoch:101, train acc:0.24666666666666667, test acc:0.1976 ===\n",
      "train loss:2.2622757885879192\n",
      "train loss:2.273919256929534\n",
      "train loss:2.2704249501000877\n",
      "=== epoch:102, train acc:0.24333333333333335, test acc:0.1981 ===\n",
      "train loss:2.2687238255859836\n",
      "train loss:2.2645350333313106\n",
      "train loss:2.265607446767143\n",
      "=== epoch:103, train acc:0.2633333333333333, test acc:0.203 ===\n",
      "train loss:2.274986650607884\n",
      "train loss:2.2715873510633084\n",
      "train loss:2.2731533201747025\n",
      "=== epoch:104, train acc:0.2633333333333333, test acc:0.2075 ===\n",
      "train loss:2.2618107373812277\n",
      "train loss:2.2762420902398315\n",
      "train loss:2.270501818861592\n",
      "=== epoch:105, train acc:0.26666666666666666, test acc:0.2089 ===\n",
      "train loss:2.2720437714844595\n",
      "train loss:2.265275253995554\n",
      "train loss:2.2677412888913593\n",
      "=== epoch:106, train acc:0.26666666666666666, test acc:0.2085 ===\n",
      "train loss:2.2805249428106538\n",
      "train loss:2.274177409160133\n",
      "train loss:2.280405768893109\n",
      "=== epoch:107, train acc:0.2733333333333333, test acc:0.2096 ===\n",
      "train loss:2.267173940486025\n",
      "train loss:2.273461241219677\n",
      "train loss:2.2729218045130413\n",
      "=== epoch:108, train acc:0.2733333333333333, test acc:0.212 ===\n",
      "train loss:2.2670778845529247\n",
      "train loss:2.2687793873473527\n",
      "train loss:2.262420260090647\n",
      "=== epoch:109, train acc:0.2733333333333333, test acc:0.2089 ===\n",
      "train loss:2.2690375813904513\n",
      "train loss:2.274913205093956\n",
      "train loss:2.2731208452413743\n",
      "=== epoch:110, train acc:0.28, test acc:0.2096 ===\n",
      "train loss:2.270065589831041\n",
      "train loss:2.27127902728254\n",
      "train loss:2.2704821098928605\n",
      "=== epoch:111, train acc:0.26666666666666666, test acc:0.2084 ===\n",
      "train loss:2.2784127904803357\n",
      "train loss:2.270652157970489\n",
      "train loss:2.2706498930075645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.26, test acc:0.2085 ===\n",
      "train loss:2.2664576342005547\n",
      "train loss:2.2732535096466795\n",
      "train loss:2.2805065228008146\n",
      "=== epoch:113, train acc:0.26, test acc:0.2072 ===\n",
      "train loss:2.270181451612015\n",
      "train loss:2.2797767795929746\n",
      "train loss:2.283542753412449\n",
      "=== epoch:114, train acc:0.26, test acc:0.2042 ===\n",
      "train loss:2.2579051333159064\n",
      "train loss:2.269565854533451\n",
      "train loss:2.2720326974688665\n",
      "=== epoch:115, train acc:0.27, test acc:0.2085 ===\n",
      "train loss:2.2741719094540156\n",
      "train loss:2.260937989114373\n",
      "train loss:2.2611318285312483\n",
      "=== epoch:116, train acc:0.27, test acc:0.2065 ===\n",
      "train loss:2.2634293011664868\n",
      "train loss:2.2683154190393706\n",
      "train loss:2.2811963287383064\n",
      "=== epoch:117, train acc:0.27, test acc:0.2079 ===\n",
      "train loss:2.2694424909744826\n",
      "train loss:2.270139572556597\n",
      "train loss:2.2733723956824\n",
      "=== epoch:118, train acc:0.27666666666666667, test acc:0.2103 ===\n",
      "train loss:2.2538283318262495\n",
      "train loss:2.282233938934948\n",
      "train loss:2.2674222565447724\n",
      "=== epoch:119, train acc:0.28, test acc:0.2109 ===\n",
      "train loss:2.2596537799096397\n",
      "train loss:2.268947045446694\n",
      "train loss:2.258383025619385\n",
      "=== epoch:120, train acc:0.25333333333333335, test acc:0.2064 ===\n",
      "train loss:2.2583943016166894\n",
      "train loss:2.2781968902128282\n",
      "train loss:2.2554478307263266\n",
      "=== epoch:121, train acc:0.25666666666666665, test acc:0.2058 ===\n",
      "train loss:2.2800179332010213\n",
      "train loss:2.2674757549627143\n",
      "train loss:2.285723200703885\n",
      "=== epoch:122, train acc:0.25333333333333335, test acc:0.2044 ===\n",
      "train loss:2.2634988628446377\n",
      "train loss:2.2642972930777288\n",
      "train loss:2.257191113669223\n",
      "=== epoch:123, train acc:0.25666666666666665, test acc:0.2086 ===\n",
      "train loss:2.264086074902193\n",
      "train loss:2.248512644471065\n",
      "train loss:2.278350156190095\n",
      "=== epoch:124, train acc:0.26, test acc:0.2104 ===\n",
      "train loss:2.251930433857599\n",
      "train loss:2.286114268059357\n",
      "train loss:2.2692728836154807\n",
      "=== epoch:125, train acc:0.27666666666666667, test acc:0.214 ===\n",
      "train loss:2.2651473165660563\n",
      "train loss:2.2722415938397087\n",
      "train loss:2.266932559354915\n",
      "=== epoch:126, train acc:0.27666666666666667, test acc:0.2146 ===\n",
      "train loss:2.270538322439569\n",
      "train loss:2.2773112742327926\n",
      "train loss:2.242008621064622\n",
      "=== epoch:127, train acc:0.27, test acc:0.2143 ===\n",
      "train loss:2.2571961847232176\n",
      "train loss:2.2659078373086867\n",
      "train loss:2.269902648446906\n",
      "=== epoch:128, train acc:0.2633333333333333, test acc:0.2126 ===\n",
      "train loss:2.279826498431785\n",
      "train loss:2.258667275511309\n",
      "train loss:2.2525175303523124\n",
      "=== epoch:129, train acc:0.26666666666666666, test acc:0.2111 ===\n",
      "train loss:2.2557546859280633\n",
      "train loss:2.2553027276176034\n",
      "train loss:2.2690531831766565\n",
      "=== epoch:130, train acc:0.26666666666666666, test acc:0.2127 ===\n",
      "train loss:2.2628488216346807\n",
      "train loss:2.2650076053654855\n",
      "train loss:2.2718376309561332\n",
      "=== epoch:131, train acc:0.26666666666666666, test acc:0.2128 ===\n",
      "train loss:2.262009795522964\n",
      "train loss:2.2438073707074326\n",
      "train loss:2.2531819008078413\n",
      "=== epoch:132, train acc:0.2633333333333333, test acc:0.2101 ===\n",
      "train loss:2.2555015301866788\n",
      "train loss:2.2728065568725233\n",
      "train loss:2.2857852256212023\n",
      "=== epoch:133, train acc:0.2633333333333333, test acc:0.2137 ===\n",
      "train loss:2.2564048557841763\n",
      "train loss:2.255930554597261\n",
      "train loss:2.25728582689226\n",
      "=== epoch:134, train acc:0.26666666666666666, test acc:0.2133 ===\n",
      "train loss:2.253214291264564\n",
      "train loss:2.2648101696895666\n",
      "train loss:2.2542624155145496\n",
      "=== epoch:135, train acc:0.27, test acc:0.2159 ===\n",
      "train loss:2.271485742090962\n",
      "train loss:2.253534111667182\n",
      "train loss:2.268619238707014\n",
      "=== epoch:136, train acc:0.2733333333333333, test acc:0.2185 ===\n",
      "train loss:2.2411471584676868\n",
      "train loss:2.2930370496159838\n",
      "train loss:2.256859920526131\n",
      "=== epoch:137, train acc:0.29, test acc:0.2234 ===\n",
      "train loss:2.2652362374766413\n",
      "train loss:2.265309429359937\n",
      "train loss:2.2641111754321\n",
      "=== epoch:138, train acc:0.29, test acc:0.2243 ===\n",
      "train loss:2.270716590242584\n",
      "train loss:2.270764202330193\n",
      "train loss:2.2779701248644524\n",
      "=== epoch:139, train acc:0.29, test acc:0.2259 ===\n",
      "train loss:2.2591378010803074\n",
      "train loss:2.2622446534371883\n",
      "train loss:2.2625911302712254\n",
      "=== epoch:140, train acc:0.2866666666666667, test acc:0.2272 ===\n",
      "train loss:2.2634712152930376\n",
      "train loss:2.2660757283714026\n",
      "train loss:2.2733654541433324\n",
      "=== epoch:141, train acc:0.29, test acc:0.229 ===\n",
      "train loss:2.2575974350942003\n",
      "train loss:2.263371951733467\n",
      "train loss:2.2598005096850184\n",
      "=== epoch:142, train acc:0.29333333333333333, test acc:0.2318 ===\n",
      "train loss:2.262113303454163\n",
      "train loss:2.26282235004899\n",
      "train loss:2.2527712004519787\n",
      "=== epoch:143, train acc:0.29333333333333333, test acc:0.2286 ===\n",
      "train loss:2.2587711298674016\n",
      "train loss:2.2563187264741176\n",
      "train loss:2.259212723945348\n",
      "=== epoch:144, train acc:0.29333333333333333, test acc:0.2279 ===\n",
      "train loss:2.2599392390222635\n",
      "train loss:2.2578913850003555\n",
      "train loss:2.271970943017363\n",
      "=== epoch:145, train acc:0.2833333333333333, test acc:0.2265 ===\n",
      "train loss:2.2569621483717164\n",
      "train loss:2.2661784942300547\n",
      "train loss:2.2579021959039647\n",
      "=== epoch:146, train acc:0.2866666666666667, test acc:0.2264 ===\n",
      "train loss:2.255421680352909\n",
      "train loss:2.2505091673432314\n",
      "train loss:2.2550415687469356\n",
      "=== epoch:147, train acc:0.2866666666666667, test acc:0.2274 ===\n",
      "train loss:2.256934177228486\n",
      "train loss:2.245474604823165\n",
      "train loss:2.277456349829207\n",
      "=== epoch:148, train acc:0.29, test acc:0.228 ===\n",
      "train loss:2.2461258838881295\n",
      "train loss:2.2667907945526657\n",
      "train loss:2.2511374107523507\n",
      "=== epoch:149, train acc:0.2866666666666667, test acc:0.2297 ===\n",
      "train loss:2.267462766473773\n",
      "train loss:2.252086029232406\n",
      "train loss:2.248895112894503\n",
      "=== epoch:150, train acc:0.2866666666666667, test acc:0.2272 ===\n",
      "train loss:2.260446651585122\n",
      "train loss:2.2474389629926077\n",
      "train loss:2.271720382354282\n",
      "=== epoch:151, train acc:0.2866666666666667, test acc:0.2261 ===\n",
      "train loss:2.2589203674461134\n",
      "train loss:2.263830301442636\n",
      "train loss:2.2545887207458306\n",
      "=== epoch:152, train acc:0.29, test acc:0.2288 ===\n",
      "train loss:2.2611155179930016\n",
      "train loss:2.2598327142582795\n",
      "train loss:2.250061965925254\n",
      "=== epoch:153, train acc:0.2866666666666667, test acc:0.2281 ===\n",
      "train loss:2.2488500851727573\n",
      "train loss:2.2620632486242416\n",
      "train loss:2.260665384472267\n",
      "=== epoch:154, train acc:0.29, test acc:0.229 ===\n",
      "train loss:2.244135068686872\n",
      "train loss:2.2530366228937897\n",
      "train loss:2.261291304076991\n",
      "=== epoch:155, train acc:0.29333333333333333, test acc:0.2302 ===\n",
      "train loss:2.257888654838194\n",
      "train loss:2.2668163367489327\n",
      "train loss:2.226710776054668\n",
      "=== epoch:156, train acc:0.29333333333333333, test acc:0.2309 ===\n",
      "train loss:2.2618646461787733\n",
      "train loss:2.255733742654003\n",
      "train loss:2.2401455376143393\n",
      "=== epoch:157, train acc:0.29333333333333333, test acc:0.2303 ===\n",
      "train loss:2.259160543190861\n",
      "train loss:2.24418881458307\n",
      "train loss:2.2584488146694137\n",
      "=== epoch:158, train acc:0.29333333333333333, test acc:0.2309 ===\n",
      "train loss:2.2364094671197163\n",
      "train loss:2.252470376868821\n",
      "train loss:2.2494609498327414\n",
      "=== epoch:159, train acc:0.2833333333333333, test acc:0.2307 ===\n",
      "train loss:2.263776319934301\n",
      "train loss:2.2701410880904738\n",
      "train loss:2.2619686474934833\n",
      "=== epoch:160, train acc:0.2833333333333333, test acc:0.2313 ===\n",
      "train loss:2.2646767908324335\n",
      "train loss:2.2656185321936335\n",
      "train loss:2.261621737488959\n",
      "=== epoch:161, train acc:0.29333333333333333, test acc:0.2342 ===\n",
      "train loss:2.2673119721615405\n",
      "train loss:2.2351993602551348\n",
      "train loss:2.250214910596556\n",
      "=== epoch:162, train acc:0.2866666666666667, test acc:0.2318 ===\n",
      "train loss:2.279844711111404\n",
      "train loss:2.232315875151133\n",
      "train loss:2.2568559769985224\n",
      "=== epoch:163, train acc:0.29333333333333333, test acc:0.2335 ===\n",
      "train loss:2.265833195787756\n",
      "train loss:2.2561799287510578\n",
      "train loss:2.2512517223135826\n",
      "=== epoch:164, train acc:0.29333333333333333, test acc:0.2356 ===\n",
      "train loss:2.264905053204574\n",
      "train loss:2.2497668664541766\n",
      "train loss:2.271182384082854\n",
      "=== epoch:165, train acc:0.29333333333333333, test acc:0.2365 ===\n",
      "train loss:2.269640666399918\n",
      "train loss:2.255748654222011\n",
      "train loss:2.2687977457696684\n",
      "=== epoch:166, train acc:0.29333333333333333, test acc:0.2366 ===\n",
      "train loss:2.2673263144098055\n",
      "train loss:2.2427911596013668\n",
      "train loss:2.257845113654505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.2966666666666667, test acc:0.2365 ===\n",
      "train loss:2.247177852916477\n",
      "train loss:2.250514509452909\n",
      "train loss:2.2662390343007046\n",
      "=== epoch:168, train acc:0.3, test acc:0.2375 ===\n",
      "train loss:2.2408003838936725\n",
      "train loss:2.2679051884480512\n",
      "train loss:2.25304461304386\n",
      "=== epoch:169, train acc:0.3, test acc:0.2381 ===\n",
      "train loss:2.250572605879545\n",
      "train loss:2.252017296224897\n",
      "train loss:2.2675784711634517\n",
      "=== epoch:170, train acc:0.3, test acc:0.2368 ===\n",
      "train loss:2.2517256296095343\n",
      "train loss:2.2583545552683124\n",
      "train loss:2.258025790661401\n",
      "=== epoch:171, train acc:0.3, test acc:0.2375 ===\n",
      "train loss:2.265808194790244\n",
      "train loss:2.235275606123382\n",
      "train loss:2.2436325115730553\n",
      "=== epoch:172, train acc:0.30333333333333334, test acc:0.2377 ===\n",
      "train loss:2.255630509275437\n",
      "train loss:2.245394822558703\n",
      "train loss:2.2393267758743467\n",
      "=== epoch:173, train acc:0.30333333333333334, test acc:0.2382 ===\n",
      "train loss:2.2454180182193286\n",
      "train loss:2.2683261322933923\n",
      "train loss:2.2507467595795574\n",
      "=== epoch:174, train acc:0.30333333333333334, test acc:0.2385 ===\n",
      "train loss:2.265464134710893\n",
      "train loss:2.2572954321833865\n",
      "train loss:2.2528661736955846\n",
      "=== epoch:175, train acc:0.30333333333333334, test acc:0.2403 ===\n",
      "train loss:2.2148411746648358\n",
      "train loss:2.249398128773251\n",
      "train loss:2.24108543729703\n",
      "=== epoch:176, train acc:0.30333333333333334, test acc:0.2441 ===\n",
      "train loss:2.243980404650342\n",
      "train loss:2.239319627025892\n",
      "train loss:2.24328734490516\n",
      "=== epoch:177, train acc:0.30666666666666664, test acc:0.2455 ===\n",
      "train loss:2.229772356927905\n",
      "train loss:2.230317750078816\n",
      "train loss:2.2481552010741206\n",
      "=== epoch:178, train acc:0.31, test acc:0.2465 ===\n",
      "train loss:2.250496343339123\n",
      "train loss:2.2436481844877694\n",
      "train loss:2.2350034814139734\n",
      "=== epoch:179, train acc:0.30333333333333334, test acc:0.2454 ===\n",
      "train loss:2.22719406795213\n",
      "train loss:2.2505916206586267\n",
      "train loss:2.2533908585564486\n",
      "=== epoch:180, train acc:0.30666666666666664, test acc:0.2467 ===\n",
      "train loss:2.243189833054527\n",
      "train loss:2.2447453261668335\n",
      "train loss:2.2360566386536087\n",
      "=== epoch:181, train acc:0.30666666666666664, test acc:0.2482 ===\n",
      "train loss:2.2372101368316324\n",
      "train loss:2.2326635697954145\n",
      "train loss:2.2474072390351623\n",
      "=== epoch:182, train acc:0.31, test acc:0.2506 ===\n",
      "train loss:2.237334309773278\n",
      "train loss:2.220251111019555\n",
      "train loss:2.2461406748250043\n",
      "=== epoch:183, train acc:0.31, test acc:0.2526 ===\n",
      "train loss:2.2504687213435504\n",
      "train loss:2.244576318454843\n",
      "train loss:2.2577908211027005\n",
      "=== epoch:184, train acc:0.32, test acc:0.2546 ===\n",
      "train loss:2.2598572377432773\n",
      "train loss:2.2285302417424133\n",
      "train loss:2.255115525769057\n",
      "=== epoch:185, train acc:0.31666666666666665, test acc:0.2548 ===\n",
      "train loss:2.2249315617426033\n",
      "train loss:2.240824156011098\n",
      "train loss:2.24570516920728\n",
      "=== epoch:186, train acc:0.31666666666666665, test acc:0.2553 ===\n",
      "train loss:2.256405713770158\n",
      "train loss:2.2439375449431553\n",
      "train loss:2.2464994790833486\n",
      "=== epoch:187, train acc:0.31666666666666665, test acc:0.2559 ===\n",
      "train loss:2.2133766880387977\n",
      "train loss:2.2329607962749263\n",
      "train loss:2.220264728613533\n",
      "=== epoch:188, train acc:0.31666666666666665, test acc:0.256 ===\n",
      "train loss:2.2424418133996857\n",
      "train loss:2.21380250375286\n",
      "train loss:2.2233513115007932\n",
      "=== epoch:189, train acc:0.31666666666666665, test acc:0.2556 ===\n",
      "train loss:2.2303228477343873\n",
      "train loss:2.2448285740067138\n",
      "train loss:2.25367540198069\n",
      "=== epoch:190, train acc:0.31333333333333335, test acc:0.2575 ===\n",
      "train loss:2.243621500903525\n",
      "train loss:2.229628414730325\n",
      "train loss:2.238243909590681\n",
      "=== epoch:191, train acc:0.31333333333333335, test acc:0.2567 ===\n",
      "train loss:2.223365228109324\n",
      "train loss:2.223077230854593\n",
      "train loss:2.2136059434750917\n",
      "=== epoch:192, train acc:0.30666666666666664, test acc:0.2549 ===\n",
      "train loss:2.2489382509045845\n",
      "train loss:2.233692187224838\n",
      "train loss:2.2320734860158855\n",
      "=== epoch:193, train acc:0.30666666666666664, test acc:0.2556 ===\n",
      "train loss:2.2497501367333625\n",
      "train loss:2.2501838922844404\n",
      "train loss:2.2154948063672206\n",
      "=== epoch:194, train acc:0.31333333333333335, test acc:0.2556 ===\n",
      "train loss:2.2279375436673203\n",
      "train loss:2.233505690333691\n",
      "train loss:2.2262047025776757\n",
      "=== epoch:195, train acc:0.31666666666666665, test acc:0.257 ===\n",
      "train loss:2.2235522735698137\n",
      "train loss:2.2355186039799686\n",
      "train loss:2.23595847954328\n",
      "=== epoch:196, train acc:0.31333333333333335, test acc:0.2554 ===\n",
      "train loss:2.2357722342429853\n",
      "train loss:2.2498172995508474\n",
      "train loss:2.2363667095810915\n",
      "=== epoch:197, train acc:0.31666666666666665, test acc:0.255 ===\n",
      "train loss:2.2323111449603705\n",
      "train loss:2.2423292878435315\n",
      "train loss:2.2408085480449254\n",
      "=== epoch:198, train acc:0.31333333333333335, test acc:0.2564 ===\n",
      "train loss:2.235281147874651\n",
      "train loss:2.2211292502016438\n",
      "train loss:2.219711210687753\n",
      "=== epoch:199, train acc:0.31333333333333335, test acc:0.2588 ===\n",
      "train loss:2.2486576115185226\n",
      "train loss:2.2216972984779346\n",
      "train loss:2.2267456237224827\n",
      "=== epoch:200, train acc:0.31, test acc:0.2566 ===\n",
      "train loss:2.2236355272993586\n",
      "train loss:2.21821411975366\n",
      "train loss:2.25684945711481\n",
      "=== epoch:201, train acc:0.31, test acc:0.257 ===\n",
      "train loss:2.2452991666318804\n",
      "train loss:2.243196542062628\n",
      "train loss:2.24451045067028\n",
      "=== epoch:202, train acc:0.31, test acc:0.2582 ===\n",
      "train loss:2.2514707013686244\n",
      "train loss:2.2472875783100297\n",
      "train loss:2.2115463248403318\n",
      "=== epoch:203, train acc:0.30666666666666664, test acc:0.2583 ===\n",
      "train loss:2.233436057200233\n",
      "train loss:2.222293891429102\n",
      "train loss:2.23301349223773\n",
      "=== epoch:204, train acc:0.31, test acc:0.2587 ===\n",
      "train loss:2.2261176160119014\n",
      "train loss:2.2334186341737565\n",
      "train loss:2.2276729229834173\n",
      "=== epoch:205, train acc:0.31, test acc:0.2607 ===\n",
      "train loss:2.2392432370480435\n",
      "train loss:2.2449000812304023\n",
      "train loss:2.2511917764568183\n",
      "=== epoch:206, train acc:0.31333333333333335, test acc:0.2632 ===\n",
      "train loss:2.2615810526009703\n",
      "train loss:2.2198853070826825\n",
      "train loss:2.240442943625245\n",
      "=== epoch:207, train acc:0.31666666666666665, test acc:0.2639 ===\n",
      "train loss:2.216961801405724\n",
      "train loss:2.252813058522632\n",
      "train loss:2.213459288261751\n",
      "=== epoch:208, train acc:0.32, test acc:0.2649 ===\n",
      "train loss:2.209426079261774\n",
      "train loss:2.223604387945964\n",
      "train loss:2.21302930198741\n",
      "=== epoch:209, train acc:0.3233333333333333, test acc:0.267 ===\n",
      "train loss:2.2105846074113153\n",
      "train loss:2.206170423698587\n",
      "train loss:2.2259553068390003\n",
      "=== epoch:210, train acc:0.32666666666666666, test acc:0.2683 ===\n",
      "train loss:2.246313965645275\n",
      "train loss:2.218700078531982\n",
      "train loss:2.2296882357911896\n",
      "=== epoch:211, train acc:0.32666666666666666, test acc:0.2689 ===\n",
      "train loss:2.2121839433761745\n",
      "train loss:2.2457394282023966\n",
      "train loss:2.241121172020825\n",
      "=== epoch:212, train acc:0.32666666666666666, test acc:0.2694 ===\n",
      "train loss:2.216447041403325\n",
      "train loss:2.237372322614843\n",
      "train loss:2.2364558048443617\n",
      "=== epoch:213, train acc:0.33, test acc:0.2694 ===\n",
      "train loss:2.215109131419455\n",
      "train loss:2.216574463670353\n",
      "train loss:2.2192185454020468\n",
      "=== epoch:214, train acc:0.3233333333333333, test acc:0.2678 ===\n",
      "train loss:2.2203694651027677\n",
      "train loss:2.250993597874001\n",
      "train loss:2.239980529192355\n",
      "=== epoch:215, train acc:0.3333333333333333, test acc:0.2693 ===\n",
      "train loss:2.2275904070074897\n",
      "train loss:2.210287148032535\n",
      "train loss:2.2000920609326027\n",
      "=== epoch:216, train acc:0.3333333333333333, test acc:0.2685 ===\n",
      "train loss:2.2336396410496078\n",
      "train loss:2.216424162337138\n",
      "train loss:2.2114353662937742\n",
      "=== epoch:217, train acc:0.33, test acc:0.2685 ===\n",
      "train loss:2.244469861085083\n",
      "train loss:2.2120314164489123\n",
      "train loss:2.214174886520019\n",
      "=== epoch:218, train acc:0.3333333333333333, test acc:0.2699 ===\n",
      "train loss:2.2139240103424895\n",
      "train loss:2.2500586028930107\n",
      "train loss:2.2350391127064726\n",
      "=== epoch:219, train acc:0.3333333333333333, test acc:0.2679 ===\n",
      "train loss:2.213905157198898\n",
      "train loss:2.225972127081034\n",
      "train loss:2.176677481584944\n",
      "=== epoch:220, train acc:0.32666666666666666, test acc:0.2695 ===\n",
      "train loss:2.224007724831361\n",
      "train loss:2.1942074822254183\n",
      "train loss:2.2378900581116694\n",
      "=== epoch:221, train acc:0.33666666666666667, test acc:0.2691 ===\n",
      "train loss:2.210029677670168\n",
      "train loss:2.218513812853782\n",
      "train loss:2.207873999335034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.33666666666666667, test acc:0.2709 ===\n",
      "train loss:2.2483869404802506\n",
      "train loss:2.228156527174842\n",
      "train loss:2.2431748745822304\n",
      "=== epoch:223, train acc:0.3333333333333333, test acc:0.2746 ===\n",
      "train loss:2.22933320883052\n",
      "train loss:2.201584754788151\n",
      "train loss:2.214209383602645\n",
      "=== epoch:224, train acc:0.34, test acc:0.2758 ===\n",
      "train loss:2.2396622384363076\n",
      "train loss:2.2562664632895957\n",
      "train loss:2.2280057594487364\n",
      "=== epoch:225, train acc:0.3433333333333333, test acc:0.2789 ===\n",
      "train loss:2.2106885384539496\n",
      "train loss:2.194895450502548\n",
      "train loss:2.2106966443737592\n",
      "=== epoch:226, train acc:0.3433333333333333, test acc:0.2781 ===\n",
      "train loss:2.1929700356995405\n",
      "train loss:2.2202934599203443\n",
      "train loss:2.2135324396230094\n",
      "=== epoch:227, train acc:0.34, test acc:0.2745 ===\n",
      "train loss:2.197237844727532\n",
      "train loss:2.211765962009437\n",
      "train loss:2.2001654417484797\n",
      "=== epoch:228, train acc:0.34, test acc:0.2749 ===\n",
      "train loss:2.215716762201063\n",
      "train loss:2.216259549712003\n",
      "train loss:2.2219351429839875\n",
      "=== epoch:229, train acc:0.34, test acc:0.2736 ===\n",
      "train loss:2.2102335045232406\n",
      "train loss:2.202462329268762\n",
      "train loss:2.2338769094954696\n",
      "=== epoch:230, train acc:0.3433333333333333, test acc:0.2754 ===\n",
      "train loss:2.2253212570908016\n",
      "train loss:2.208717144721896\n",
      "train loss:2.2035755911579384\n",
      "=== epoch:231, train acc:0.34, test acc:0.2732 ===\n",
      "train loss:2.2318876173361644\n",
      "train loss:2.1891401884580937\n",
      "train loss:2.2432724760145386\n",
      "=== epoch:232, train acc:0.3333333333333333, test acc:0.272 ===\n",
      "train loss:2.1731887792699034\n",
      "train loss:2.226076836834347\n",
      "train loss:2.220076446248052\n",
      "=== epoch:233, train acc:0.3333333333333333, test acc:0.2716 ===\n",
      "train loss:2.2168231018731435\n",
      "train loss:2.2283698436395136\n",
      "train loss:2.203853267645924\n",
      "=== epoch:234, train acc:0.33666666666666667, test acc:0.2718 ===\n",
      "train loss:2.225845577261851\n",
      "train loss:2.204744413374478\n",
      "train loss:2.20645503532353\n",
      "=== epoch:235, train acc:0.33666666666666667, test acc:0.2728 ===\n",
      "train loss:2.2143891427910734\n",
      "train loss:2.222798516893344\n",
      "train loss:2.227798999055621\n",
      "=== epoch:236, train acc:0.34, test acc:0.2725 ===\n",
      "train loss:2.24660010694742\n",
      "train loss:2.231631556787537\n",
      "train loss:2.224390781546762\n",
      "=== epoch:237, train acc:0.34, test acc:0.2754 ===\n",
      "train loss:2.2044442795989796\n",
      "train loss:2.217993915566931\n",
      "train loss:2.1949885163882037\n",
      "=== epoch:238, train acc:0.3433333333333333, test acc:0.2771 ===\n",
      "train loss:2.2015395842510332\n",
      "train loss:2.158376236262208\n",
      "train loss:2.1800384512438216\n",
      "=== epoch:239, train acc:0.3433333333333333, test acc:0.2734 ===\n",
      "train loss:2.220213262950259\n",
      "train loss:2.2032530989296366\n",
      "train loss:2.1860272104011558\n",
      "=== epoch:240, train acc:0.3433333333333333, test acc:0.2738 ===\n",
      "train loss:2.1998542557307035\n",
      "train loss:2.208328936877883\n",
      "train loss:2.205107690718457\n",
      "=== epoch:241, train acc:0.34, test acc:0.2732 ===\n",
      "train loss:2.176524012245792\n",
      "train loss:2.194802454574573\n",
      "train loss:2.2106811429286837\n",
      "=== epoch:242, train acc:0.33666666666666667, test acc:0.2732 ===\n",
      "train loss:2.2006189080830687\n",
      "train loss:2.189951006252607\n",
      "train loss:2.2039327389186036\n",
      "=== epoch:243, train acc:0.3333333333333333, test acc:0.2723 ===\n",
      "train loss:2.1951568515848745\n",
      "train loss:2.2082360529173797\n",
      "train loss:2.2207857820233783\n",
      "=== epoch:244, train acc:0.3333333333333333, test acc:0.2718 ===\n",
      "train loss:2.202064809221112\n",
      "train loss:2.2274244794089886\n",
      "train loss:2.232215892141966\n",
      "=== epoch:245, train acc:0.33, test acc:0.2721 ===\n",
      "train loss:2.2132071325593063\n",
      "train loss:2.1995666924911035\n",
      "train loss:2.185878040627543\n",
      "=== epoch:246, train acc:0.34, test acc:0.2745 ===\n",
      "train loss:2.188613377942066\n",
      "train loss:2.2007679021399498\n",
      "train loss:2.2009443886112745\n",
      "=== epoch:247, train acc:0.33666666666666667, test acc:0.2745 ===\n",
      "train loss:2.1871771522664964\n",
      "train loss:2.1914318055715616\n",
      "train loss:2.16759773533775\n",
      "=== epoch:248, train acc:0.3333333333333333, test acc:0.2736 ===\n",
      "train loss:2.194216414685579\n",
      "train loss:2.1929163249580887\n",
      "train loss:2.238929749886548\n",
      "=== epoch:249, train acc:0.33, test acc:0.2715 ===\n",
      "train loss:2.2157691134884057\n",
      "train loss:2.1957325986579375\n",
      "train loss:2.2200154922099613\n",
      "=== epoch:250, train acc:0.33, test acc:0.2728 ===\n",
      "train loss:2.2202578402200808\n",
      "train loss:2.1738410384602744\n",
      "train loss:2.1801591017571873\n",
      "=== epoch:251, train acc:0.33, test acc:0.2728 ===\n",
      "train loss:2.1566567056279635\n",
      "train loss:2.181478898004597\n",
      "train loss:2.203355169327855\n",
      "=== epoch:252, train acc:0.33, test acc:0.2693 ===\n",
      "train loss:2.1969283345927595\n",
      "train loss:2.1913769380190007\n",
      "train loss:2.1902313206245028\n",
      "=== epoch:253, train acc:0.3233333333333333, test acc:0.2685 ===\n",
      "train loss:2.192534312948805\n",
      "train loss:2.221599947813191\n",
      "train loss:2.1701164719967845\n",
      "=== epoch:254, train acc:0.32, test acc:0.2663 ===\n",
      "train loss:2.183723780359826\n",
      "train loss:2.1862097999453622\n",
      "train loss:2.2064400620732445\n",
      "=== epoch:255, train acc:0.3233333333333333, test acc:0.2661 ===\n",
      "train loss:2.1954080154186353\n",
      "train loss:2.1945124987703326\n",
      "train loss:2.169667258680522\n",
      "=== epoch:256, train acc:0.32666666666666666, test acc:0.2678 ===\n",
      "train loss:2.166867730128639\n",
      "train loss:2.153034990837615\n",
      "train loss:2.143417547118445\n",
      "=== epoch:257, train acc:0.3233333333333333, test acc:0.2649 ===\n",
      "train loss:2.1399720991423914\n",
      "train loss:2.1701457422221186\n",
      "train loss:2.2002221627628002\n",
      "=== epoch:258, train acc:0.32, test acc:0.2645 ===\n",
      "train loss:2.2066524174986326\n",
      "train loss:2.1968864426803085\n",
      "train loss:2.13799593554011\n",
      "=== epoch:259, train acc:0.32, test acc:0.2651 ===\n",
      "train loss:2.206507978851745\n",
      "train loss:2.1892898470958757\n",
      "train loss:2.194090987536805\n",
      "=== epoch:260, train acc:0.3233333333333333, test acc:0.2652 ===\n",
      "train loss:2.189616697329909\n",
      "train loss:2.1781152947279607\n",
      "train loss:2.1982507933068822\n",
      "=== epoch:261, train acc:0.32, test acc:0.2652 ===\n",
      "train loss:2.2003686025371194\n",
      "train loss:2.2031006100067008\n",
      "train loss:2.1694525377905185\n",
      "=== epoch:262, train acc:0.32, test acc:0.2657 ===\n",
      "train loss:2.194944488016889\n",
      "train loss:2.2032279215741712\n",
      "train loss:2.175316307310012\n",
      "=== epoch:263, train acc:0.32, test acc:0.2674 ===\n",
      "train loss:2.147019235066132\n",
      "train loss:2.19427931844431\n",
      "train loss:2.1891685187169543\n",
      "=== epoch:264, train acc:0.32, test acc:0.2684 ===\n",
      "train loss:2.178860809435841\n",
      "train loss:2.172800475778975\n",
      "train loss:2.1830952792640654\n",
      "=== epoch:265, train acc:0.32666666666666666, test acc:0.2706 ===\n",
      "train loss:2.1685580964747087\n",
      "train loss:2.147163420244133\n",
      "train loss:2.1948800765914105\n",
      "=== epoch:266, train acc:0.32666666666666666, test acc:0.2704 ===\n",
      "train loss:2.1796913388181185\n",
      "train loss:2.1708051186743633\n",
      "train loss:2.208254124138876\n",
      "=== epoch:267, train acc:0.32666666666666666, test acc:0.269 ===\n",
      "train loss:2.227903354326371\n",
      "train loss:2.1763645615969387\n",
      "train loss:2.1599676606539835\n",
      "=== epoch:268, train acc:0.32666666666666666, test acc:0.2701 ===\n",
      "train loss:2.180138015031706\n",
      "train loss:2.17582866075662\n",
      "train loss:2.1451749903635617\n",
      "=== epoch:269, train acc:0.31666666666666665, test acc:0.2679 ===\n",
      "train loss:2.189572746447876\n",
      "train loss:2.192565110721433\n",
      "train loss:2.1944883222054057\n",
      "=== epoch:270, train acc:0.31666666666666665, test acc:0.2705 ===\n",
      "train loss:2.144679417783002\n",
      "train loss:2.1809841023056165\n",
      "train loss:2.205703786818293\n",
      "=== epoch:271, train acc:0.3233333333333333, test acc:0.2714 ===\n",
      "train loss:2.1804258871245916\n",
      "train loss:2.17309104801028\n",
      "train loss:2.2065132446310742\n",
      "=== epoch:272, train acc:0.3233333333333333, test acc:0.269 ===\n",
      "train loss:2.1562229140172127\n",
      "train loss:2.1808300058969294\n",
      "train loss:2.165736076650381\n",
      "=== epoch:273, train acc:0.32, test acc:0.2682 ===\n",
      "train loss:2.1758962711360157\n",
      "train loss:2.209341550893003\n",
      "train loss:2.1909960317004615\n",
      "=== epoch:274, train acc:0.3233333333333333, test acc:0.2692 ===\n",
      "train loss:2.1642130338262846\n",
      "train loss:2.1709695720869018\n",
      "train loss:2.179019315846176\n",
      "=== epoch:275, train acc:0.3233333333333333, test acc:0.2714 ===\n",
      "train loss:2.1340556314553663\n",
      "train loss:2.179591315737257\n",
      "train loss:2.1406000057221295\n",
      "=== epoch:276, train acc:0.32666666666666666, test acc:0.2722 ===\n",
      "train loss:2.119483284531595\n",
      "train loss:2.1801607926758155\n",
      "train loss:2.161901273775886\n",
      "=== epoch:277, train acc:0.32666666666666666, test acc:0.2734 ===\n",
      "train loss:2.1630979630433624\n",
      "train loss:2.099803600553864\n",
      "train loss:2.1418718883212513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.32666666666666666, test acc:0.2726 ===\n",
      "train loss:2.159356188944273\n",
      "train loss:2.1316138162506095\n",
      "train loss:2.157319241245557\n",
      "=== epoch:279, train acc:0.32666666666666666, test acc:0.275 ===\n",
      "train loss:2.140784522611724\n",
      "train loss:2.1767079996710166\n",
      "train loss:2.20522659550812\n",
      "=== epoch:280, train acc:0.33, test acc:0.2757 ===\n",
      "train loss:2.160542601586604\n",
      "train loss:2.169129854755295\n",
      "train loss:2.18876984941552\n",
      "=== epoch:281, train acc:0.33, test acc:0.2757 ===\n",
      "train loss:2.1238397117560166\n",
      "train loss:2.1649290860298267\n",
      "train loss:2.223674451905357\n",
      "=== epoch:282, train acc:0.33666666666666667, test acc:0.277 ===\n",
      "train loss:2.1238143749490233\n",
      "train loss:2.1737856148207326\n",
      "train loss:2.083409322252959\n",
      "=== epoch:283, train acc:0.32666666666666666, test acc:0.2739 ===\n",
      "train loss:2.1836673670503775\n",
      "train loss:2.1259285755679014\n",
      "train loss:2.1590470822384393\n",
      "=== epoch:284, train acc:0.32666666666666666, test acc:0.2742 ===\n",
      "train loss:2.2101985438219445\n",
      "train loss:2.1797423490174728\n",
      "train loss:2.172314095283718\n",
      "=== epoch:285, train acc:0.32666666666666666, test acc:0.2749 ===\n",
      "train loss:2.183609610712206\n",
      "train loss:2.1071545177585773\n",
      "train loss:2.2105066629498737\n",
      "=== epoch:286, train acc:0.32666666666666666, test acc:0.2763 ===\n",
      "train loss:2.2227352472150246\n",
      "train loss:2.1286386324264153\n",
      "train loss:2.1584360971397647\n",
      "=== epoch:287, train acc:0.32666666666666666, test acc:0.2761 ===\n",
      "train loss:2.13747367961985\n",
      "train loss:2.182910318087277\n",
      "train loss:2.149716458595299\n",
      "=== epoch:288, train acc:0.32666666666666666, test acc:0.2753 ===\n",
      "train loss:2.1365624633371927\n",
      "train loss:2.1408816220130826\n",
      "train loss:2.160424747637977\n",
      "=== epoch:289, train acc:0.3233333333333333, test acc:0.2735 ===\n",
      "train loss:2.1412555451271356\n",
      "train loss:2.1854167129709143\n",
      "train loss:2.1245640436121405\n",
      "=== epoch:290, train acc:0.32666666666666666, test acc:0.2731 ===\n",
      "train loss:2.1614116755343913\n",
      "train loss:2.156229082786841\n",
      "train loss:2.172780124415609\n",
      "=== epoch:291, train acc:0.32666666666666666, test acc:0.2746 ===\n",
      "train loss:2.1298752221652544\n",
      "train loss:2.1148957393402377\n",
      "train loss:2.1436673197492326\n",
      "=== epoch:292, train acc:0.32666666666666666, test acc:0.2731 ===\n",
      "train loss:2.095794935352421\n",
      "train loss:2.103408853748583\n",
      "train loss:2.135482068045842\n",
      "=== epoch:293, train acc:0.32, test acc:0.2725 ===\n",
      "train loss:2.12709309577325\n",
      "train loss:2.1563256221710647\n",
      "train loss:2.1727289172020523\n",
      "=== epoch:294, train acc:0.32, test acc:0.2717 ===\n",
      "train loss:2.1053347772187654\n",
      "train loss:2.12272339766285\n",
      "train loss:2.132504494083762\n",
      "=== epoch:295, train acc:0.32, test acc:0.2698 ===\n",
      "train loss:2.1508861038224514\n",
      "train loss:2.1839558150128453\n",
      "train loss:2.1428165370241765\n",
      "=== epoch:296, train acc:0.32, test acc:0.2706 ===\n",
      "train loss:2.169070044653395\n",
      "train loss:2.1570665901950807\n",
      "train loss:2.120013597898927\n",
      "=== epoch:297, train acc:0.32, test acc:0.2703 ===\n",
      "train loss:2.1563345772498765\n",
      "train loss:2.0899957602143306\n",
      "train loss:2.1818177646668637\n",
      "=== epoch:298, train acc:0.32, test acc:0.2699 ===\n",
      "train loss:2.142702271619924\n",
      "train loss:2.1350125523596715\n",
      "train loss:2.133909124386896\n",
      "=== epoch:299, train acc:0.32, test acc:0.2703 ===\n",
      "train loss:2.1580273927554448\n",
      "train loss:2.1238235239488055\n",
      "train loss:2.15397091144213\n",
      "=== epoch:300, train acc:0.32, test acc:0.2693 ===\n",
      "train loss:2.150282425471232\n",
      "train loss:2.1498360715864475\n",
      "train loss:2.129002669812438\n",
      "=== epoch:301, train acc:0.32, test acc:0.2711 ===\n",
      "train loss:2.13024154581637\n",
      "train loss:2.1534867421225936\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.2713\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.3\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db32e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf1520",
   "metadata": {},
   "source": [
    "- 드롭아웃 0.1은 오버피팅의 여지가 있고\n",
    "- 드롭아웃 0.3은 학습이 저조한 경향이 있어 드롭아웃 0.2가 최적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a52f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
