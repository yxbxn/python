{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e48f49",
   "metadata": {},
   "source": [
    "## 바른 학습을 위해\n",
    "- 오버피팅 방지하는 다양한 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bb94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eaef7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6743c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad2a4b",
   "metadata": {},
   "source": [
    "epoch마다 train acc와 test acc 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5402ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.09, test acc:0.1032\n",
      "epoch:1, train acc:0.09666666666666666, test acc:0.1038\n",
      "epoch:2, train acc:0.09333333333333334, test acc:0.1052\n",
      "epoch:3, train acc:0.10333333333333333, test acc:0.11\n",
      "epoch:4, train acc:0.10333333333333333, test acc:0.1136\n",
      "epoch:5, train acc:0.11, test acc:0.1154\n",
      "epoch:6, train acc:0.11666666666666667, test acc:0.121\n",
      "epoch:7, train acc:0.15333333333333332, test acc:0.1285\n",
      "epoch:8, train acc:0.15666666666666668, test acc:0.1361\n",
      "epoch:9, train acc:0.17, test acc:0.1402\n",
      "epoch:10, train acc:0.2, test acc:0.1572\n",
      "epoch:11, train acc:0.21333333333333335, test acc:0.165\n",
      "epoch:12, train acc:0.25666666666666665, test acc:0.1903\n",
      "epoch:13, train acc:0.2866666666666667, test acc:0.2089\n",
      "epoch:14, train acc:0.31666666666666665, test acc:0.2305\n",
      "epoch:15, train acc:0.33666666666666667, test acc:0.2481\n",
      "epoch:16, train acc:0.3433333333333333, test acc:0.2492\n",
      "epoch:17, train acc:0.36666666666666664, test acc:0.2649\n",
      "epoch:18, train acc:0.3933333333333333, test acc:0.29\n",
      "epoch:19, train acc:0.41, test acc:0.2936\n",
      "epoch:20, train acc:0.42, test acc:0.3055\n",
      "epoch:21, train acc:0.45, test acc:0.3339\n",
      "epoch:22, train acc:0.47, test acc:0.3494\n",
      "epoch:23, train acc:0.49, test acc:0.3659\n",
      "epoch:24, train acc:0.5033333333333333, test acc:0.381\n",
      "epoch:25, train acc:0.5133333333333333, test acc:0.3917\n",
      "epoch:26, train acc:0.51, test acc:0.3864\n",
      "epoch:27, train acc:0.5333333333333333, test acc:0.4087\n",
      "epoch:28, train acc:0.54, test acc:0.4313\n",
      "epoch:29, train acc:0.58, test acc:0.4578\n",
      "epoch:30, train acc:0.6033333333333334, test acc:0.4705\n",
      "epoch:31, train acc:0.63, test acc:0.49\n",
      "epoch:32, train acc:0.61, test acc:0.4857\n",
      "epoch:33, train acc:0.6333333333333333, test acc:0.5029\n",
      "epoch:34, train acc:0.6533333333333333, test acc:0.5119\n",
      "epoch:35, train acc:0.6533333333333333, test acc:0.5203\n",
      "epoch:36, train acc:0.64, test acc:0.5153\n",
      "epoch:37, train acc:0.6466666666666666, test acc:0.5246\n",
      "epoch:38, train acc:0.6366666666666667, test acc:0.524\n",
      "epoch:39, train acc:0.64, test acc:0.5231\n",
      "epoch:40, train acc:0.6533333333333333, test acc:0.5281\n",
      "epoch:41, train acc:0.65, test acc:0.521\n",
      "epoch:42, train acc:0.66, test acc:0.5451\n",
      "epoch:43, train acc:0.6733333333333333, test acc:0.5437\n",
      "epoch:44, train acc:0.68, test acc:0.5538\n",
      "epoch:45, train acc:0.6966666666666667, test acc:0.5625\n",
      "epoch:46, train acc:0.7, test acc:0.5566\n",
      "epoch:47, train acc:0.67, test acc:0.5598\n",
      "epoch:48, train acc:0.6833333333333333, test acc:0.5588\n",
      "epoch:49, train acc:0.7133333333333334, test acc:0.5658\n",
      "epoch:50, train acc:0.7, test acc:0.5683\n",
      "epoch:51, train acc:0.7233333333333334, test acc:0.5884\n",
      "epoch:52, train acc:0.7366666666666667, test acc:0.5923\n",
      "epoch:53, train acc:0.7333333333333333, test acc:0.5907\n",
      "epoch:54, train acc:0.7333333333333333, test acc:0.5887\n",
      "epoch:55, train acc:0.7433333333333333, test acc:0.5882\n",
      "epoch:56, train acc:0.75, test acc:0.5955\n",
      "epoch:57, train acc:0.7533333333333333, test acc:0.5918\n",
      "epoch:58, train acc:0.7533333333333333, test acc:0.6037\n",
      "epoch:59, train acc:0.76, test acc:0.598\n",
      "epoch:60, train acc:0.7633333333333333, test acc:0.6029\n",
      "epoch:61, train acc:0.7633333333333333, test acc:0.6028\n",
      "epoch:62, train acc:0.7666666666666667, test acc:0.6113\n",
      "epoch:63, train acc:0.77, test acc:0.6057\n",
      "epoch:64, train acc:0.76, test acc:0.6063\n",
      "epoch:65, train acc:0.7633333333333333, test acc:0.6145\n",
      "epoch:66, train acc:0.7666666666666667, test acc:0.6256\n",
      "epoch:67, train acc:0.7566666666666667, test acc:0.6171\n",
      "epoch:68, train acc:0.7733333333333333, test acc:0.6226\n",
      "epoch:69, train acc:0.7633333333333333, test acc:0.6221\n",
      "epoch:70, train acc:0.7633333333333333, test acc:0.607\n",
      "epoch:71, train acc:0.7766666666666666, test acc:0.6215\n",
      "epoch:72, train acc:0.7366666666666667, test acc:0.5866\n",
      "epoch:73, train acc:0.7633333333333333, test acc:0.6115\n",
      "epoch:74, train acc:0.77, test acc:0.6154\n",
      "epoch:75, train acc:0.7766666666666666, test acc:0.6166\n",
      "epoch:76, train acc:0.76, test acc:0.6041\n",
      "epoch:77, train acc:0.7566666666666667, test acc:0.6088\n",
      "epoch:78, train acc:0.7533333333333333, test acc:0.6057\n",
      "epoch:79, train acc:0.7366666666666667, test acc:0.5942\n",
      "epoch:80, train acc:0.78, test acc:0.6295\n",
      "epoch:81, train acc:0.7566666666666667, test acc:0.6334\n",
      "epoch:82, train acc:0.7533333333333333, test acc:0.6104\n",
      "epoch:83, train acc:0.7633333333333333, test acc:0.6198\n",
      "epoch:84, train acc:0.78, test acc:0.6275\n",
      "epoch:85, train acc:0.77, test acc:0.6434\n",
      "epoch:86, train acc:0.7666666666666667, test acc:0.6388\n",
      "epoch:87, train acc:0.7766666666666666, test acc:0.63\n",
      "epoch:88, train acc:0.8, test acc:0.6362\n",
      "epoch:89, train acc:0.8033333333333333, test acc:0.6375\n",
      "epoch:90, train acc:0.8, test acc:0.6257\n",
      "epoch:91, train acc:0.8, test acc:0.65\n",
      "epoch:92, train acc:0.8233333333333334, test acc:0.6515\n",
      "epoch:93, train acc:0.7933333333333333, test acc:0.6451\n",
      "epoch:94, train acc:0.8033333333333333, test acc:0.6619\n",
      "epoch:95, train acc:0.82, test acc:0.6536\n",
      "epoch:96, train acc:0.8066666666666666, test acc:0.6611\n",
      "epoch:97, train acc:0.8266666666666667, test acc:0.659\n",
      "epoch:98, train acc:0.8333333333333334, test acc:0.6707\n",
      "epoch:99, train acc:0.8333333333333334, test acc:0.668\n",
      "epoch:100, train acc:0.8066666666666666, test acc:0.6427\n",
      "epoch:101, train acc:0.8266666666666667, test acc:0.6593\n",
      "epoch:102, train acc:0.8166666666666667, test acc:0.6568\n",
      "epoch:103, train acc:0.8233333333333334, test acc:0.6626\n",
      "epoch:104, train acc:0.83, test acc:0.6713\n",
      "epoch:105, train acc:0.82, test acc:0.6539\n",
      "epoch:106, train acc:0.84, test acc:0.6664\n",
      "epoch:107, train acc:0.84, test acc:0.6651\n",
      "epoch:108, train acc:0.83, test acc:0.6728\n",
      "epoch:109, train acc:0.8233333333333334, test acc:0.6611\n",
      "epoch:110, train acc:0.8166666666666667, test acc:0.6614\n",
      "epoch:111, train acc:0.8266666666666667, test acc:0.661\n",
      "epoch:112, train acc:0.8433333333333334, test acc:0.6724\n",
      "epoch:113, train acc:0.8233333333333334, test acc:0.6681\n",
      "epoch:114, train acc:0.8533333333333334, test acc:0.6844\n",
      "epoch:115, train acc:0.85, test acc:0.6868\n",
      "epoch:116, train acc:0.8533333333333334, test acc:0.6767\n",
      "epoch:117, train acc:0.8466666666666667, test acc:0.6758\n",
      "epoch:118, train acc:0.8366666666666667, test acc:0.6752\n",
      "epoch:119, train acc:0.8466666666666667, test acc:0.6855\n",
      "epoch:120, train acc:0.8333333333333334, test acc:0.6906\n",
      "epoch:121, train acc:0.85, test acc:0.6885\n",
      "epoch:122, train acc:0.8, test acc:0.6599\n",
      "epoch:123, train acc:0.84, test acc:0.6825\n",
      "epoch:124, train acc:0.8233333333333334, test acc:0.6722\n",
      "epoch:125, train acc:0.83, test acc:0.6813\n",
      "epoch:126, train acc:0.85, test acc:0.6846\n",
      "epoch:127, train acc:0.8533333333333334, test acc:0.6916\n",
      "epoch:128, train acc:0.8466666666666667, test acc:0.6972\n",
      "epoch:129, train acc:0.85, test acc:0.6963\n",
      "epoch:130, train acc:0.86, test acc:0.6975\n",
      "epoch:131, train acc:0.8466666666666667, test acc:0.6941\n",
      "epoch:132, train acc:0.86, test acc:0.6985\n",
      "epoch:133, train acc:0.85, test acc:0.6934\n",
      "epoch:134, train acc:0.8366666666666667, test acc:0.7008\n",
      "epoch:135, train acc:0.8433333333333334, test acc:0.6916\n",
      "epoch:136, train acc:0.8566666666666667, test acc:0.6962\n",
      "epoch:137, train acc:0.8633333333333333, test acc:0.7053\n",
      "epoch:138, train acc:0.8433333333333334, test acc:0.7001\n",
      "epoch:139, train acc:0.8533333333333334, test acc:0.701\n",
      "epoch:140, train acc:0.8566666666666667, test acc:0.7044\n",
      "epoch:141, train acc:0.8633333333333333, test acc:0.7082\n",
      "epoch:142, train acc:0.8566666666666667, test acc:0.7009\n",
      "epoch:143, train acc:0.84, test acc:0.6959\n",
      "epoch:144, train acc:0.8533333333333334, test acc:0.7009\n",
      "epoch:145, train acc:0.85, test acc:0.6982\n",
      "epoch:146, train acc:0.85, test acc:0.7013\n",
      "epoch:147, train acc:0.8533333333333334, test acc:0.6952\n",
      "epoch:148, train acc:0.8566666666666667, test acc:0.7073\n",
      "epoch:149, train acc:0.8433333333333334, test acc:0.7042\n",
      "epoch:150, train acc:0.8566666666666667, test acc:0.7012\n",
      "epoch:151, train acc:0.8633333333333333, test acc:0.7064\n",
      "epoch:152, train acc:0.8466666666666667, test acc:0.7064\n",
      "epoch:153, train acc:0.85, test acc:0.7067\n",
      "epoch:154, train acc:0.8433333333333334, test acc:0.7016\n",
      "epoch:155, train acc:0.85, test acc:0.7091\n",
      "epoch:156, train acc:0.8566666666666667, test acc:0.6986\n",
      "epoch:157, train acc:0.85, test acc:0.6979\n",
      "epoch:158, train acc:0.8633333333333333, test acc:0.7031\n",
      "epoch:159, train acc:0.8666666666666667, test acc:0.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:160, train acc:0.86, test acc:0.7175\n",
      "epoch:161, train acc:0.86, test acc:0.7119\n",
      "epoch:162, train acc:0.85, test acc:0.7002\n",
      "epoch:163, train acc:0.8433333333333334, test acc:0.6957\n",
      "epoch:164, train acc:0.8533333333333334, test acc:0.7033\n",
      "epoch:165, train acc:0.8566666666666667, test acc:0.7083\n",
      "epoch:166, train acc:0.8466666666666667, test acc:0.7058\n",
      "epoch:167, train acc:0.8566666666666667, test acc:0.7111\n",
      "epoch:168, train acc:0.8533333333333334, test acc:0.7062\n",
      "epoch:169, train acc:0.85, test acc:0.7024\n",
      "epoch:170, train acc:0.8533333333333334, test acc:0.7052\n",
      "epoch:171, train acc:0.84, test acc:0.7006\n",
      "epoch:172, train acc:0.85, test acc:0.708\n",
      "epoch:173, train acc:0.8466666666666667, test acc:0.7057\n",
      "epoch:174, train acc:0.8533333333333334, test acc:0.7133\n",
      "epoch:175, train acc:0.8433333333333334, test acc:0.702\n",
      "epoch:176, train acc:0.8533333333333334, test acc:0.7088\n",
      "epoch:177, train acc:0.8533333333333334, test acc:0.7119\n",
      "epoch:178, train acc:0.8566666666666667, test acc:0.7089\n",
      "epoch:179, train acc:0.86, test acc:0.7106\n",
      "epoch:180, train acc:0.8466666666666667, test acc:0.7037\n",
      "epoch:181, train acc:0.84, test acc:0.7016\n",
      "epoch:182, train acc:0.8633333333333333, test acc:0.708\n",
      "epoch:183, train acc:0.8566666666666667, test acc:0.7131\n",
      "epoch:184, train acc:0.8566666666666667, test acc:0.7079\n",
      "epoch:185, train acc:0.87, test acc:0.7209\n",
      "epoch:186, train acc:0.8566666666666667, test acc:0.7096\n",
      "epoch:187, train acc:0.85, test acc:0.7054\n",
      "epoch:188, train acc:0.8466666666666667, test acc:0.7036\n",
      "epoch:189, train acc:0.85, test acc:0.706\n",
      "epoch:190, train acc:0.8566666666666667, test acc:0.7041\n",
      "epoch:191, train acc:0.8466666666666667, test acc:0.7059\n",
      "epoch:192, train acc:0.8566666666666667, test acc:0.7055\n",
      "epoch:193, train acc:0.8533333333333334, test acc:0.7004\n",
      "epoch:194, train acc:0.85, test acc:0.704\n",
      "epoch:195, train acc:0.8666666666666667, test acc:0.7041\n",
      "epoch:196, train acc:0.8666666666666667, test acc:0.7093\n",
      "epoch:197, train acc:0.87, test acc:0.7136\n",
      "epoch:198, train acc:0.86, test acc:0.7095\n",
      "epoch:199, train acc:0.8433333333333334, test acc:0.6981\n",
      "epoch:200, train acc:0.8533333333333334, test acc:0.7117\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75feb17",
   "metadata": {},
   "source": [
    "그래프로 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103ae786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59009d1",
   "metadata": {},
   "source": [
    "### 가중치 감소로 오버피팅 억제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db74104c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5419a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c0401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bee16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c9a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7657ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.10333333333333333, test acc:0.0976\n",
      "epoch:1, train acc:0.11, test acc:0.1004\n",
      "epoch:2, train acc:0.12666666666666668, test acc:0.1083\n",
      "epoch:3, train acc:0.14333333333333334, test acc:0.1145\n",
      "epoch:4, train acc:0.14333333333333334, test acc:0.1208\n",
      "epoch:5, train acc:0.16333333333333333, test acc:0.1288\n",
      "epoch:6, train acc:0.18666666666666668, test acc:0.1333\n",
      "epoch:7, train acc:0.2, test acc:0.1484\n",
      "epoch:8, train acc:0.22, test acc:0.1549\n",
      "epoch:9, train acc:0.21666666666666667, test acc:0.1745\n",
      "epoch:10, train acc:0.23333333333333334, test acc:0.1817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-df2a3bf0be5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtest_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\multi_layer_net.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\multi_layer_net.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Untitled Folder\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5333c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1aaef8",
   "metadata": {},
   "source": [
    "- 가중치 감쇠 0.1을 주지 않으면 train acc가 0.99 넘게 오버피팅이 발생\n",
    "- 가중치 감쇠 0.2 적용 시, 학습이 제대로 되지 않는 모습 적절한 가중치 감쇠 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb4e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay（가중치 감쇠） 설정\n",
    "weight_decay_lambda = 0.2 # weight decay를 사용하지 않을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20623aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6f7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c56402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.11666666666666667, test acc:0.1014\n",
      "epoch:1, train acc:0.12333333333333334, test acc:0.1047\n",
      "epoch:2, train acc:0.13666666666666666, test acc:0.1114\n",
      "epoch:3, train acc:0.14666666666666667, test acc:0.1208\n",
      "epoch:4, train acc:0.17666666666666667, test acc:0.1482\n",
      "epoch:5, train acc:0.2, test acc:0.1734\n",
      "epoch:6, train acc:0.23, test acc:0.1849\n",
      "epoch:7, train acc:0.25, test acc:0.193\n",
      "epoch:8, train acc:0.27, test acc:0.2128\n",
      "epoch:9, train acc:0.31333333333333335, test acc:0.2432\n",
      "epoch:10, train acc:0.3566666666666667, test acc:0.2786\n",
      "epoch:11, train acc:0.36666666666666664, test acc:0.2865\n",
      "epoch:12, train acc:0.42333333333333334, test acc:0.3194\n",
      "epoch:13, train acc:0.4166666666666667, test acc:0.3182\n",
      "epoch:14, train acc:0.42333333333333334, test acc:0.3364\n",
      "epoch:15, train acc:0.46, test acc:0.3526\n",
      "epoch:16, train acc:0.4766666666666667, test acc:0.3656\n",
      "epoch:17, train acc:0.4866666666666667, test acc:0.3824\n",
      "epoch:18, train acc:0.49333333333333335, test acc:0.3924\n",
      "epoch:19, train acc:0.47333333333333333, test acc:0.3843\n",
      "epoch:20, train acc:0.48333333333333334, test acc:0.3907\n",
      "epoch:21, train acc:0.49, test acc:0.397\n",
      "epoch:22, train acc:0.48, test acc:0.3956\n",
      "epoch:23, train acc:0.5, test acc:0.3947\n",
      "epoch:24, train acc:0.5033333333333333, test acc:0.404\n",
      "epoch:25, train acc:0.51, test acc:0.4001\n",
      "epoch:26, train acc:0.5166666666666667, test acc:0.4135\n",
      "epoch:27, train acc:0.52, test acc:0.4218\n",
      "epoch:28, train acc:0.5233333333333333, test acc:0.4293\n",
      "epoch:29, train acc:0.5233333333333333, test acc:0.4183\n",
      "epoch:30, train acc:0.52, test acc:0.4335\n",
      "epoch:31, train acc:0.5266666666666666, test acc:0.4187\n",
      "epoch:32, train acc:0.5166666666666667, test acc:0.4138\n",
      "epoch:33, train acc:0.53, test acc:0.4326\n",
      "epoch:34, train acc:0.5233333333333333, test acc:0.4227\n",
      "epoch:35, train acc:0.54, test acc:0.4365\n",
      "epoch:36, train acc:0.53, test acc:0.4301\n",
      "epoch:37, train acc:0.5333333333333333, test acc:0.4318\n",
      "epoch:38, train acc:0.5466666666666666, test acc:0.4429\n",
      "epoch:39, train acc:0.55, test acc:0.4466\n",
      "epoch:40, train acc:0.54, test acc:0.4494\n",
      "epoch:41, train acc:0.5533333333333333, test acc:0.4517\n",
      "epoch:42, train acc:0.5366666666666666, test acc:0.4376\n",
      "epoch:43, train acc:0.5366666666666666, test acc:0.4398\n",
      "epoch:44, train acc:0.5333333333333333, test acc:0.4353\n",
      "epoch:45, train acc:0.5433333333333333, test acc:0.4387\n",
      "epoch:46, train acc:0.5366666666666666, test acc:0.4399\n",
      "epoch:47, train acc:0.5366666666666666, test acc:0.435\n",
      "epoch:48, train acc:0.5433333333333333, test acc:0.4298\n",
      "epoch:49, train acc:0.5366666666666666, test acc:0.4307\n",
      "epoch:50, train acc:0.54, test acc:0.4341\n",
      "epoch:51, train acc:0.5433333333333333, test acc:0.445\n",
      "epoch:52, train acc:0.55, test acc:0.4397\n",
      "epoch:53, train acc:0.5533333333333333, test acc:0.4564\n",
      "epoch:54, train acc:0.55, test acc:0.4507\n",
      "epoch:55, train acc:0.5466666666666666, test acc:0.4527\n",
      "epoch:56, train acc:0.5466666666666666, test acc:0.4525\n",
      "epoch:57, train acc:0.5466666666666666, test acc:0.4605\n",
      "epoch:58, train acc:0.55, test acc:0.4588\n",
      "epoch:59, train acc:0.5533333333333333, test acc:0.4637\n",
      "epoch:60, train acc:0.5533333333333333, test acc:0.4624\n",
      "epoch:61, train acc:0.56, test acc:0.456\n",
      "epoch:62, train acc:0.5466666666666666, test acc:0.4524\n",
      "epoch:63, train acc:0.54, test acc:0.442\n",
      "epoch:64, train acc:0.5333333333333333, test acc:0.4359\n",
      "epoch:65, train acc:0.5366666666666666, test acc:0.4421\n",
      "epoch:66, train acc:0.5333333333333333, test acc:0.44\n",
      "epoch:67, train acc:0.5266666666666666, test acc:0.4394\n",
      "epoch:68, train acc:0.5233333333333333, test acc:0.4329\n",
      "epoch:69, train acc:0.5366666666666666, test acc:0.4358\n",
      "epoch:70, train acc:0.5333333333333333, test acc:0.4379\n",
      "epoch:71, train acc:0.5266666666666666, test acc:0.431\n",
      "epoch:72, train acc:0.5266666666666666, test acc:0.4308\n",
      "epoch:73, train acc:0.5266666666666666, test acc:0.4261\n",
      "epoch:74, train acc:0.53, test acc:0.4302\n",
      "epoch:75, train acc:0.5266666666666666, test acc:0.4309\n",
      "epoch:76, train acc:0.5166666666666667, test acc:0.425\n",
      "epoch:77, train acc:0.52, test acc:0.4311\n",
      "epoch:78, train acc:0.52, test acc:0.4319\n",
      "epoch:79, train acc:0.52, test acc:0.4292\n",
      "epoch:80, train acc:0.52, test acc:0.4334\n",
      "epoch:81, train acc:0.52, test acc:0.4311\n",
      "epoch:82, train acc:0.5166666666666667, test acc:0.4292\n",
      "epoch:83, train acc:0.52, test acc:0.4266\n",
      "epoch:84, train acc:0.5133333333333333, test acc:0.4152\n",
      "epoch:85, train acc:0.5033333333333333, test acc:0.4192\n",
      "epoch:86, train acc:0.5166666666666667, test acc:0.421\n",
      "epoch:87, train acc:0.5133333333333333, test acc:0.4163\n",
      "epoch:88, train acc:0.5133333333333333, test acc:0.4214\n",
      "epoch:89, train acc:0.5133333333333333, test acc:0.4136\n",
      "epoch:90, train acc:0.5, test acc:0.4047\n",
      "epoch:91, train acc:0.5, test acc:0.4103\n",
      "epoch:92, train acc:0.49333333333333335, test acc:0.4065\n",
      "epoch:93, train acc:0.49333333333333335, test acc:0.4047\n",
      "epoch:94, train acc:0.4866666666666667, test acc:0.4014\n",
      "epoch:95, train acc:0.48, test acc:0.3975\n",
      "epoch:96, train acc:0.48333333333333334, test acc:0.3985\n",
      "epoch:97, train acc:0.4766666666666667, test acc:0.3931\n",
      "epoch:98, train acc:0.47333333333333333, test acc:0.3929\n",
      "epoch:99, train acc:0.4666666666666667, test acc:0.3896\n",
      "epoch:100, train acc:0.4633333333333333, test acc:0.3881\n",
      "epoch:101, train acc:0.4666666666666667, test acc:0.3886\n",
      "epoch:102, train acc:0.47333333333333333, test acc:0.3912\n",
      "epoch:103, train acc:0.4666666666666667, test acc:0.389\n",
      "epoch:104, train acc:0.4633333333333333, test acc:0.3893\n",
      "epoch:105, train acc:0.4633333333333333, test acc:0.3826\n",
      "epoch:106, train acc:0.4633333333333333, test acc:0.3895\n",
      "epoch:107, train acc:0.4533333333333333, test acc:0.3782\n",
      "epoch:108, train acc:0.44333333333333336, test acc:0.375\n",
      "epoch:109, train acc:0.45, test acc:0.3752\n",
      "epoch:110, train acc:0.43666666666666665, test acc:0.3736\n",
      "epoch:111, train acc:0.44, test acc:0.3781\n",
      "epoch:112, train acc:0.44, test acc:0.3784\n",
      "epoch:113, train acc:0.44, test acc:0.3753\n",
      "epoch:114, train acc:0.44, test acc:0.3736\n",
      "epoch:115, train acc:0.43666666666666665, test acc:0.3666\n",
      "epoch:116, train acc:0.43, test acc:0.368\n",
      "epoch:117, train acc:0.44333333333333336, test acc:0.3705\n",
      "epoch:118, train acc:0.4266666666666667, test acc:0.3661\n",
      "epoch:119, train acc:0.42333333333333334, test acc:0.3613\n",
      "epoch:120, train acc:0.42333333333333334, test acc:0.3626\n",
      "epoch:121, train acc:0.42, test acc:0.3615\n",
      "epoch:122, train acc:0.42333333333333334, test acc:0.3625\n",
      "epoch:123, train acc:0.43666666666666665, test acc:0.3673\n",
      "epoch:124, train acc:0.43666666666666665, test acc:0.3673\n",
      "epoch:125, train acc:0.44333333333333336, test acc:0.3696\n",
      "epoch:126, train acc:0.44666666666666666, test acc:0.3727\n",
      "epoch:127, train acc:0.44333333333333336, test acc:0.3779\n",
      "epoch:128, train acc:0.42, test acc:0.3641\n",
      "epoch:129, train acc:0.4066666666666667, test acc:0.3587\n",
      "epoch:130, train acc:0.4166666666666667, test acc:0.3618\n",
      "epoch:131, train acc:0.42333333333333334, test acc:0.3624\n",
      "epoch:132, train acc:0.4066666666666667, test acc:0.3514\n",
      "epoch:133, train acc:0.4033333333333333, test acc:0.3468\n",
      "epoch:134, train acc:0.4033333333333333, test acc:0.3464\n",
      "epoch:135, train acc:0.4066666666666667, test acc:0.351\n",
      "epoch:136, train acc:0.41, test acc:0.3502\n",
      "epoch:137, train acc:0.4066666666666667, test acc:0.3484\n",
      "epoch:138, train acc:0.41333333333333333, test acc:0.3509\n",
      "epoch:139, train acc:0.4, test acc:0.3484\n",
      "epoch:140, train acc:0.41333333333333333, test acc:0.3511\n",
      "epoch:141, train acc:0.42, test acc:0.3536\n",
      "epoch:142, train acc:0.4033333333333333, test acc:0.3452\n",
      "epoch:143, train acc:0.41, test acc:0.3468\n",
      "epoch:144, train acc:0.3933333333333333, test acc:0.3378\n",
      "epoch:145, train acc:0.39, test acc:0.3329\n",
      "epoch:146, train acc:0.38666666666666666, test acc:0.3349\n",
      "epoch:147, train acc:0.38666666666666666, test acc:0.3387\n",
      "epoch:148, train acc:0.38666666666666666, test acc:0.3411\n",
      "epoch:149, train acc:0.3933333333333333, test acc:0.339\n",
      "epoch:150, train acc:0.3933333333333333, test acc:0.3382\n",
      "epoch:151, train acc:0.38666666666666666, test acc:0.3376\n",
      "epoch:152, train acc:0.39666666666666667, test acc:0.3377\n",
      "epoch:153, train acc:0.4033333333333333, test acc:0.3374\n",
      "epoch:154, train acc:0.4, test acc:0.3362\n",
      "epoch:155, train acc:0.39, test acc:0.3328\n",
      "epoch:156, train acc:0.38666666666666666, test acc:0.3309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:157, train acc:0.3933333333333333, test acc:0.3313\n",
      "epoch:158, train acc:0.3933333333333333, test acc:0.3265\n",
      "epoch:159, train acc:0.38666666666666666, test acc:0.3222\n",
      "epoch:160, train acc:0.37666666666666665, test acc:0.3194\n",
      "epoch:161, train acc:0.37666666666666665, test acc:0.3168\n",
      "epoch:162, train acc:0.38, test acc:0.3179\n",
      "epoch:163, train acc:0.37666666666666665, test acc:0.3168\n",
      "epoch:164, train acc:0.37666666666666665, test acc:0.3149\n",
      "epoch:165, train acc:0.36666666666666664, test acc:0.3121\n",
      "epoch:166, train acc:0.36, test acc:0.3056\n",
      "epoch:167, train acc:0.35333333333333333, test acc:0.2993\n",
      "epoch:168, train acc:0.3433333333333333, test acc:0.2942\n",
      "epoch:169, train acc:0.33666666666666667, test acc:0.2907\n",
      "epoch:170, train acc:0.32666666666666666, test acc:0.2834\n",
      "epoch:171, train acc:0.33, test acc:0.2809\n",
      "epoch:172, train acc:0.32666666666666666, test acc:0.28\n",
      "epoch:173, train acc:0.32666666666666666, test acc:0.2793\n",
      "epoch:174, train acc:0.32666666666666666, test acc:0.2771\n",
      "epoch:175, train acc:0.32666666666666666, test acc:0.2743\n",
      "epoch:176, train acc:0.31666666666666665, test acc:0.2751\n",
      "epoch:177, train acc:0.31, test acc:0.2691\n",
      "epoch:178, train acc:0.31, test acc:0.2667\n",
      "epoch:179, train acc:0.30333333333333334, test acc:0.2647\n",
      "epoch:180, train acc:0.30333333333333334, test acc:0.2637\n",
      "epoch:181, train acc:0.3, test acc:0.2629\n",
      "epoch:182, train acc:0.3, test acc:0.2628\n",
      "epoch:183, train acc:0.3, test acc:0.2608\n",
      "epoch:184, train acc:0.3, test acc:0.2606\n",
      "epoch:185, train acc:0.3, test acc:0.26\n",
      "epoch:186, train acc:0.3, test acc:0.2628\n",
      "epoch:187, train acc:0.3, test acc:0.2618\n",
      "epoch:188, train acc:0.3, test acc:0.2599\n",
      "epoch:189, train acc:0.3, test acc:0.2584\n",
      "epoch:190, train acc:0.3, test acc:0.2584\n",
      "epoch:191, train acc:0.3, test acc:0.2581\n",
      "epoch:192, train acc:0.29, test acc:0.2532\n",
      "epoch:193, train acc:0.28, test acc:0.2487\n",
      "epoch:194, train acc:0.28, test acc:0.2475\n",
      "epoch:195, train acc:0.27666666666666667, test acc:0.2448\n",
      "epoch:196, train acc:0.2733333333333333, test acc:0.2428\n",
      "epoch:197, train acc:0.27, test acc:0.2415\n",
      "epoch:198, train acc:0.27, test acc:0.2402\n",
      "epoch:199, train acc:0.27, test acc:0.2408\n",
      "epoch:200, train acc:0.27, test acc:0.2415\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1e3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49d370",
   "metadata": {},
   "source": [
    "### 드롭아웃으로 오버피팅 억제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddff94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6586c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 줄이기\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d12697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76b0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306466211116495\n",
      "=== epoch:1, train acc:0.08333333333333333, test acc:0.0935 ===\n",
      "train loss:2.3233742007586047\n",
      "train loss:2.316996688370058\n",
      "train loss:2.3128754499657806\n",
      "=== epoch:2, train acc:0.1, test acc:0.101 ===\n",
      "train loss:2.305261699158031\n",
      "train loss:2.323233781629496\n",
      "train loss:2.322380959380987\n",
      "=== epoch:3, train acc:0.10666666666666667, test acc:0.1037 ===\n",
      "train loss:2.3299841311425937\n",
      "train loss:2.2944871777306837\n",
      "train loss:2.313756615900901\n",
      "=== epoch:4, train acc:0.10333333333333333, test acc:0.1087 ===\n",
      "train loss:2.3115197446062776\n",
      "train loss:2.3146947128784494\n",
      "train loss:2.2915479274684913\n",
      "=== epoch:5, train acc:0.12, test acc:0.1127 ===\n",
      "train loss:2.3060189595239247\n",
      "train loss:2.3060533806881014\n",
      "train loss:2.3087166566969675\n",
      "=== epoch:6, train acc:0.11, test acc:0.1165 ===\n",
      "train loss:2.300769070790303\n",
      "train loss:2.307209865920115\n",
      "train loss:2.290991967723541\n",
      "=== epoch:7, train acc:0.11666666666666667, test acc:0.1241 ===\n",
      "train loss:2.3042449149116804\n",
      "train loss:2.2975776964477137\n",
      "train loss:2.2885401175155495\n",
      "=== epoch:8, train acc:0.13, test acc:0.1316 ===\n",
      "train loss:2.290009505624647\n",
      "train loss:2.3114718620468526\n",
      "train loss:2.314299966573357\n",
      "=== epoch:9, train acc:0.12333333333333334, test acc:0.1372 ===\n",
      "train loss:2.300997411163083\n",
      "train loss:2.292089951101843\n",
      "train loss:2.2936505064186385\n",
      "=== epoch:10, train acc:0.13333333333333333, test acc:0.1406 ===\n",
      "train loss:2.3057349982420634\n",
      "train loss:2.294437959087648\n",
      "train loss:2.2972333948046346\n",
      "=== epoch:11, train acc:0.14, test acc:0.146 ===\n",
      "train loss:2.2966845916733263\n",
      "train loss:2.302595062438771\n",
      "train loss:2.2936272236303665\n",
      "=== epoch:12, train acc:0.15, test acc:0.1514 ===\n",
      "train loss:2.287574277852396\n",
      "train loss:2.2908719939764004\n",
      "train loss:2.2864183601058214\n",
      "=== epoch:13, train acc:0.16, test acc:0.1565 ===\n",
      "train loss:2.2900563264228655\n",
      "train loss:2.279374210136909\n",
      "train loss:2.2798698285287617\n",
      "=== epoch:14, train acc:0.15666666666666668, test acc:0.1615 ===\n",
      "train loss:2.2927065558190676\n",
      "train loss:2.2836143283649046\n",
      "train loss:2.2726964474231117\n",
      "=== epoch:15, train acc:0.17, test acc:0.17 ===\n",
      "train loss:2.2960013969785003\n",
      "train loss:2.2689028758724774\n",
      "train loss:2.3006900325557735\n",
      "=== epoch:16, train acc:0.17, test acc:0.1749 ===\n",
      "train loss:2.275545627425918\n",
      "train loss:2.300626775246902\n",
      "train loss:2.2834397872269903\n",
      "=== epoch:17, train acc:0.18333333333333332, test acc:0.1792 ===\n",
      "train loss:2.2854016707761486\n",
      "train loss:2.275003031339694\n",
      "train loss:2.264771086151391\n",
      "=== epoch:18, train acc:0.19333333333333333, test acc:0.188 ===\n",
      "train loss:2.2621317308720457\n",
      "train loss:2.269633891726682\n",
      "train loss:2.2817180115804274\n",
      "=== epoch:19, train acc:0.19333333333333333, test acc:0.1881 ===\n",
      "train loss:2.2827966059700158\n",
      "train loss:2.277116157370144\n",
      "train loss:2.2743481096972133\n",
      "=== epoch:20, train acc:0.19666666666666666, test acc:0.1909 ===\n",
      "train loss:2.271351861406633\n",
      "train loss:2.274971025352863\n",
      "train loss:2.282493765244773\n",
      "=== epoch:21, train acc:0.19666666666666666, test acc:0.1911 ===\n",
      "train loss:2.2847516496402545\n",
      "train loss:2.281258419481394\n",
      "train loss:2.2808844123978633\n",
      "=== epoch:22, train acc:0.20333333333333334, test acc:0.1955 ===\n",
      "train loss:2.291271840559267\n",
      "train loss:2.2796015335111512\n",
      "train loss:2.2634768516493193\n",
      "=== epoch:23, train acc:0.21333333333333335, test acc:0.2009 ===\n",
      "train loss:2.2665197128286847\n",
      "train loss:2.2768140916321418\n",
      "train loss:2.2785730658557726\n",
      "=== epoch:24, train acc:0.22333333333333333, test acc:0.2104 ===\n",
      "train loss:2.2663376392810712\n",
      "train loss:2.274713623840891\n",
      "train loss:2.2633045335824757\n",
      "=== epoch:25, train acc:0.23333333333333334, test acc:0.2194 ===\n",
      "train loss:2.258350522315239\n",
      "train loss:2.263467401701561\n",
      "train loss:2.263810696225091\n",
      "=== epoch:26, train acc:0.24333333333333335, test acc:0.2248 ===\n",
      "train loss:2.2719670864073733\n",
      "train loss:2.258505320526888\n",
      "train loss:2.2721435310239184\n",
      "=== epoch:27, train acc:0.25, test acc:0.2254 ===\n",
      "train loss:2.2667403604002296\n",
      "train loss:2.255703762010085\n",
      "train loss:2.2550799902834817\n",
      "=== epoch:28, train acc:0.25333333333333335, test acc:0.2319 ===\n",
      "train loss:2.2628350101872896\n",
      "train loss:2.2541026848274903\n",
      "train loss:2.2513549666058568\n",
      "=== epoch:29, train acc:0.26666666666666666, test acc:0.2345 ===\n",
      "train loss:2.27104335020848\n",
      "train loss:2.266373564037674\n",
      "train loss:2.2463008280965533\n",
      "=== epoch:30, train acc:0.2733333333333333, test acc:0.2378 ===\n",
      "train loss:2.234608790240035\n",
      "train loss:2.257359700564871\n",
      "train loss:2.2516675582899257\n",
      "=== epoch:31, train acc:0.27, test acc:0.2433 ===\n",
      "train loss:2.2460809665510375\n",
      "train loss:2.2692647383740514\n",
      "train loss:2.2580551559271207\n",
      "=== epoch:32, train acc:0.2833333333333333, test acc:0.2473 ===\n",
      "train loss:2.2514674039957336\n",
      "train loss:2.259232688923554\n",
      "train loss:2.254856259730866\n",
      "=== epoch:33, train acc:0.2833333333333333, test acc:0.2519 ===\n",
      "train loss:2.2359951467459744\n",
      "train loss:2.2411223544755603\n",
      "train loss:2.246183040606921\n",
      "=== epoch:34, train acc:0.2966666666666667, test acc:0.2551 ===\n",
      "train loss:2.2355342739651434\n",
      "train loss:2.2583691397117778\n",
      "train loss:2.2462930817236413\n",
      "=== epoch:35, train acc:0.3, test acc:0.2542 ===\n",
      "train loss:2.250624086878304\n",
      "train loss:2.2509002890509837\n",
      "train loss:2.238299275075596\n",
      "=== epoch:36, train acc:0.30666666666666664, test acc:0.2541 ===\n",
      "train loss:2.2469496470793904\n",
      "train loss:2.2270729149146855\n",
      "train loss:2.249170334568686\n",
      "=== epoch:37, train acc:0.31666666666666665, test acc:0.2617 ===\n",
      "train loss:2.2151523810020137\n",
      "train loss:2.2349687067928143\n",
      "train loss:2.222176044617786\n",
      "=== epoch:38, train acc:0.31666666666666665, test acc:0.2601 ===\n",
      "train loss:2.2353180610683046\n",
      "train loss:2.2392839645559044\n",
      "train loss:2.2554847349947846\n",
      "=== epoch:39, train acc:0.32666666666666666, test acc:0.2612 ===\n",
      "train loss:2.2522851644798494\n",
      "train loss:2.2691303186269622\n",
      "train loss:2.2120275118514803\n",
      "=== epoch:40, train acc:0.32666666666666666, test acc:0.2614 ===\n",
      "train loss:2.233681913580767\n",
      "train loss:2.2379408299392383\n",
      "train loss:2.237635800680383\n",
      "=== epoch:41, train acc:0.3233333333333333, test acc:0.2604 ===\n",
      "train loss:2.2311098030616296\n",
      "train loss:2.204024102053522\n",
      "train loss:2.229517581285359\n",
      "=== epoch:42, train acc:0.31666666666666665, test acc:0.2592 ===\n",
      "train loss:2.2480636673351673\n",
      "train loss:2.2324528887559136\n",
      "train loss:2.2086472348960715\n",
      "=== epoch:43, train acc:0.31666666666666665, test acc:0.2613 ===\n",
      "train loss:2.2221036284932363\n",
      "train loss:2.2226573237758105\n",
      "train loss:2.228207205098986\n",
      "=== epoch:44, train acc:0.31333333333333335, test acc:0.2637 ===\n",
      "train loss:2.222286467570423\n",
      "train loss:2.228216764631159\n",
      "train loss:2.2027883032968463\n",
      "=== epoch:45, train acc:0.31333333333333335, test acc:0.2593 ===\n",
      "train loss:2.175825519634632\n",
      "train loss:2.2138095574150434\n",
      "train loss:2.224458255014681\n",
      "=== epoch:46, train acc:0.31333333333333335, test acc:0.2562 ===\n",
      "train loss:2.20016953170136\n",
      "train loss:2.2305340690599884\n",
      "train loss:2.2050653626787793\n",
      "=== epoch:47, train acc:0.30666666666666664, test acc:0.2535 ===\n",
      "train loss:2.204181707905084\n",
      "train loss:2.200520706809454\n",
      "train loss:2.2086306634284476\n",
      "=== epoch:48, train acc:0.30666666666666664, test acc:0.2517 ===\n",
      "train loss:2.2189813096507076\n",
      "train loss:2.207840655563053\n",
      "train loss:2.1910574040759934\n",
      "=== epoch:49, train acc:0.30666666666666664, test acc:0.248 ===\n",
      "train loss:2.2289168518644864\n",
      "train loss:2.194211712716603\n",
      "train loss:2.2117735733704755\n",
      "=== epoch:50, train acc:0.29333333333333333, test acc:0.2455 ===\n",
      "train loss:2.2140305935525157\n",
      "train loss:2.231060525386386\n",
      "train loss:2.1683812080913203\n",
      "=== epoch:51, train acc:0.2866666666666667, test acc:0.2424 ===\n",
      "train loss:2.1715043750530034\n",
      "train loss:2.2179230949048576\n",
      "train loss:2.223687022218727\n",
      "=== epoch:52, train acc:0.2866666666666667, test acc:0.2369 ===\n",
      "train loss:2.1866634306045283\n",
      "train loss:2.1606003196243586\n",
      "train loss:2.216471425981635\n",
      "=== epoch:53, train acc:0.2866666666666667, test acc:0.2339 ===\n",
      "train loss:2.17530738761391\n",
      "train loss:2.163543264998564\n",
      "train loss:2.1955676651925735\n",
      "=== epoch:54, train acc:0.29, test acc:0.2362 ===\n",
      "train loss:2.2113123911587937\n",
      "train loss:2.1864020673574216\n",
      "train loss:2.1699184315435724\n",
      "=== epoch:55, train acc:0.29333333333333333, test acc:0.2363 ===\n",
      "train loss:2.1638485031866854\n",
      "train loss:2.1991380107183787\n",
      "train loss:2.1637002537235914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:56, train acc:0.29333333333333333, test acc:0.2372 ===\n",
      "train loss:2.2043457542969236\n",
      "train loss:2.190724808082194\n",
      "train loss:2.12176917575993\n",
      "=== epoch:57, train acc:0.29, test acc:0.235 ===\n",
      "train loss:2.201563117509279\n",
      "train loss:2.21408780497264\n",
      "train loss:2.156440709260063\n",
      "=== epoch:58, train acc:0.2966666666666667, test acc:0.236 ===\n",
      "train loss:2.185494282222665\n",
      "train loss:2.179938471891265\n",
      "train loss:2.1669958492015713\n",
      "=== epoch:59, train acc:0.30333333333333334, test acc:0.2393 ===\n",
      "train loss:2.155080301619888\n",
      "train loss:2.214972568723867\n",
      "train loss:2.1924286573298772\n",
      "=== epoch:60, train acc:0.30333333333333334, test acc:0.2384 ===\n",
      "train loss:2.1954490380953584\n",
      "train loss:2.1917199165158703\n",
      "train loss:2.135285917841273\n",
      "=== epoch:61, train acc:0.3, test acc:0.2344 ===\n",
      "train loss:2.1374873671940895\n",
      "train loss:2.1712117660836183\n",
      "train loss:2.157337407301946\n",
      "=== epoch:62, train acc:0.30333333333333334, test acc:0.2358 ===\n",
      "train loss:2.197214408911791\n",
      "train loss:2.1340088662450527\n",
      "train loss:2.1963347648446616\n",
      "=== epoch:63, train acc:0.31, test acc:0.239 ===\n",
      "train loss:2.0961473835332782\n",
      "train loss:2.1556404134038156\n",
      "train loss:2.145186422866775\n",
      "=== epoch:64, train acc:0.30333333333333334, test acc:0.2352 ===\n",
      "train loss:2.1047583066942135\n",
      "train loss:2.17416321067493\n",
      "train loss:2.136464455604462\n",
      "=== epoch:65, train acc:0.3, test acc:0.2329 ===\n",
      "train loss:2.095199340756926\n",
      "train loss:2.1280060004313297\n",
      "train loss:2.1508010494399574\n",
      "=== epoch:66, train acc:0.3, test acc:0.2286 ===\n",
      "train loss:2.1491211067169003\n",
      "train loss:2.188961098552264\n",
      "train loss:2.1609381744049823\n",
      "=== epoch:67, train acc:0.30333333333333334, test acc:0.2331 ===\n",
      "train loss:2.1405163138459025\n",
      "train loss:2.1293217140319594\n",
      "train loss:2.159784360535679\n",
      "=== epoch:68, train acc:0.30333333333333334, test acc:0.2295 ===\n",
      "train loss:2.1665215432176983\n",
      "train loss:2.1190703258067587\n",
      "train loss:2.1568460648894527\n",
      "=== epoch:69, train acc:0.3, test acc:0.2273 ===\n",
      "train loss:2.170126839609187\n",
      "train loss:2.1709183254058817\n",
      "train loss:2.1750460640745963\n",
      "=== epoch:70, train acc:0.30333333333333334, test acc:0.2313 ===\n",
      "train loss:2.1516825187335904\n",
      "train loss:2.175807002980665\n",
      "train loss:2.1746239355833823\n",
      "=== epoch:71, train acc:0.31, test acc:0.2362 ===\n",
      "train loss:2.077103026731032\n",
      "train loss:2.1143349405387286\n",
      "train loss:2.1176134605231867\n",
      "=== epoch:72, train acc:0.30333333333333334, test acc:0.2345 ===\n",
      "train loss:2.169600097198854\n",
      "train loss:2.0909819749760112\n",
      "train loss:2.1027689477333427\n",
      "=== epoch:73, train acc:0.30666666666666664, test acc:0.2368 ===\n",
      "train loss:2.1212013433462173\n",
      "train loss:2.109703888278283\n",
      "train loss:2.1571999518097926\n",
      "=== epoch:74, train acc:0.30666666666666664, test acc:0.2378 ===\n",
      "train loss:2.109575903841557\n",
      "train loss:2.091084240127404\n",
      "train loss:2.1726588023881166\n",
      "=== epoch:75, train acc:0.30666666666666664, test acc:0.2383 ===\n",
      "train loss:2.1165126104410485\n",
      "train loss:2.1022525852333307\n",
      "train loss:2.0140884658957856\n",
      "=== epoch:76, train acc:0.30333333333333334, test acc:0.2353 ===\n",
      "train loss:2.120465052456174\n",
      "train loss:2.1057812096584296\n",
      "train loss:2.073566312069658\n",
      "=== epoch:77, train acc:0.30666666666666664, test acc:0.2387 ===\n",
      "train loss:2.0063600918365263\n",
      "train loss:2.0455780146046676\n",
      "train loss:2.0772572522998973\n",
      "=== epoch:78, train acc:0.30333333333333334, test acc:0.2325 ===\n",
      "train loss:2.143493080130799\n",
      "train loss:2.068131661541089\n",
      "train loss:2.082874160344243\n",
      "=== epoch:79, train acc:0.30333333333333334, test acc:0.2348 ===\n",
      "train loss:2.044735241403992\n",
      "train loss:2.0835912799630263\n",
      "train loss:2.0685884237235816\n",
      "=== epoch:80, train acc:0.3, test acc:0.2346 ===\n",
      "train loss:2.140197848367549\n",
      "train loss:2.0900989694716725\n",
      "train loss:2.079859770449529\n",
      "=== epoch:81, train acc:0.30333333333333334, test acc:0.237 ===\n",
      "train loss:2.07826003382277\n",
      "train loss:2.1109006022836727\n",
      "train loss:2.085782782184177\n",
      "=== epoch:82, train acc:0.30333333333333334, test acc:0.2385 ===\n",
      "train loss:2.050714121979553\n",
      "train loss:2.017120493526961\n",
      "train loss:2.068569892746531\n",
      "=== epoch:83, train acc:0.30666666666666664, test acc:0.2391 ===\n",
      "train loss:2.154741115688295\n",
      "train loss:2.102921123146962\n",
      "train loss:2.127026817473995\n",
      "=== epoch:84, train acc:0.31333333333333335, test acc:0.2468 ===\n",
      "train loss:2.1174355933169964\n",
      "train loss:2.058312749914067\n",
      "train loss:2.0838782531105857\n",
      "=== epoch:85, train acc:0.31333333333333335, test acc:0.2508 ===\n",
      "train loss:1.9999144715841848\n",
      "train loss:2.094235247661783\n",
      "train loss:2.0710289838161295\n",
      "=== epoch:86, train acc:0.31666666666666665, test acc:0.2508 ===\n",
      "train loss:1.962951165302697\n",
      "train loss:2.1029572331143274\n",
      "train loss:2.100354090592684\n",
      "=== epoch:87, train acc:0.31333333333333335, test acc:0.25 ===\n",
      "train loss:2.0949216334010505\n",
      "train loss:2.001015997987105\n",
      "train loss:2.066105628699365\n",
      "=== epoch:88, train acc:0.31666666666666665, test acc:0.2509 ===\n",
      "train loss:2.0981840242166054\n",
      "train loss:2.053092753418431\n",
      "train loss:2.0666996677843845\n",
      "=== epoch:89, train acc:0.32, test acc:0.2545 ===\n",
      "train loss:2.123406989792541\n",
      "train loss:2.087528664847112\n",
      "train loss:2.108928030580263\n",
      "=== epoch:90, train acc:0.3333333333333333, test acc:0.2613 ===\n",
      "train loss:2.080345918222377\n",
      "train loss:2.0285288320436736\n",
      "train loss:1.9975793759875915\n",
      "=== epoch:91, train acc:0.3333333333333333, test acc:0.2633 ===\n",
      "train loss:2.0010705444051404\n",
      "train loss:1.9825810895798133\n",
      "train loss:2.0740051988891444\n",
      "=== epoch:92, train acc:0.33666666666666667, test acc:0.2616 ===\n",
      "train loss:2.0799947947013546\n",
      "train loss:1.9554917851969507\n",
      "train loss:1.9620829559855135\n",
      "=== epoch:93, train acc:0.3333333333333333, test acc:0.257 ===\n",
      "train loss:2.045447353978871\n",
      "train loss:2.0266142699301266\n",
      "train loss:2.013073431991896\n",
      "=== epoch:94, train acc:0.33666666666666667, test acc:0.2589 ===\n",
      "train loss:1.9700995200417593\n",
      "train loss:2.0597891491057245\n",
      "train loss:1.924463444088442\n",
      "=== epoch:95, train acc:0.33666666666666667, test acc:0.2597 ===\n",
      "train loss:2.0126721395479774\n",
      "train loss:1.9742468024440378\n",
      "train loss:2.0370932172848124\n",
      "=== epoch:96, train acc:0.3333333333333333, test acc:0.2637 ===\n",
      "train loss:2.047839689931791\n",
      "train loss:2.055010692565265\n",
      "train loss:2.031704936162639\n",
      "=== epoch:97, train acc:0.32666666666666666, test acc:0.2689 ===\n",
      "train loss:2.086728812914129\n",
      "train loss:2.025600659233238\n",
      "train loss:1.9228989213061882\n",
      "=== epoch:98, train acc:0.32666666666666666, test acc:0.272 ===\n",
      "train loss:1.980111611418458\n",
      "train loss:2.0427352839722053\n",
      "train loss:2.0122394285697367\n",
      "=== epoch:99, train acc:0.3466666666666667, test acc:0.2774 ===\n",
      "train loss:1.9710721370176754\n",
      "train loss:2.0520410722583526\n",
      "train loss:1.9252601091984871\n",
      "=== epoch:100, train acc:0.3466666666666667, test acc:0.2771 ===\n",
      "train loss:2.096549575065842\n",
      "train loss:2.058671294047095\n",
      "train loss:2.009575307889618\n",
      "=== epoch:101, train acc:0.35333333333333333, test acc:0.2788 ===\n",
      "train loss:2.0216313617497037\n",
      "train loss:2.069756165033512\n",
      "train loss:2.0958592559973273\n",
      "=== epoch:102, train acc:0.35333333333333333, test acc:0.2832 ===\n",
      "train loss:1.9759118986026982\n",
      "train loss:1.928232361361231\n",
      "train loss:1.9306104123263919\n",
      "=== epoch:103, train acc:0.3566666666666667, test acc:0.2815 ===\n",
      "train loss:1.9732486877805235\n",
      "train loss:2.026683973009633\n",
      "train loss:1.994379439942019\n",
      "=== epoch:104, train acc:0.3566666666666667, test acc:0.2834 ===\n",
      "train loss:1.9596710308120233\n",
      "train loss:2.0712337546525355\n",
      "train loss:2.048992533824373\n",
      "=== epoch:105, train acc:0.36, test acc:0.2878 ===\n",
      "train loss:2.0596536875919393\n",
      "train loss:1.9104685928129987\n",
      "train loss:2.066678671525226\n",
      "=== epoch:106, train acc:0.36, test acc:0.2918 ===\n",
      "train loss:1.9754689404095338\n",
      "train loss:1.9205523961415818\n",
      "train loss:1.9298552677457301\n",
      "=== epoch:107, train acc:0.36333333333333334, test acc:0.2899 ===\n",
      "train loss:1.9511642140415142\n",
      "train loss:1.8787648961853531\n",
      "train loss:1.9350564932672016\n",
      "=== epoch:108, train acc:0.36, test acc:0.2905 ===\n",
      "train loss:2.0457107021910885\n",
      "train loss:1.9253613193892403\n",
      "train loss:2.044716693778173\n",
      "=== epoch:109, train acc:0.36666666666666664, test acc:0.2977 ===\n",
      "train loss:2.001470676211955\n",
      "train loss:1.992995753346786\n",
      "train loss:2.0207822040190675\n",
      "=== epoch:110, train acc:0.37, test acc:0.2997 ===\n",
      "train loss:2.013799685900152\n",
      "train loss:1.9469390074474737\n",
      "train loss:1.8848027541557408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:111, train acc:0.37333333333333335, test acc:0.303 ===\n",
      "train loss:1.863452083852164\n",
      "train loss:1.937497807885072\n",
      "train loss:1.969950842592458\n",
      "=== epoch:112, train acc:0.37, test acc:0.3027 ===\n",
      "train loss:1.9049242890656959\n",
      "train loss:2.017450237110186\n",
      "train loss:1.835137259118409\n",
      "=== epoch:113, train acc:0.37333333333333335, test acc:0.306 ===\n",
      "train loss:1.9738056651486022\n",
      "train loss:1.9669826390060792\n",
      "train loss:1.9638809475322234\n",
      "=== epoch:114, train acc:0.37333333333333335, test acc:0.3074 ===\n",
      "train loss:1.947992724339209\n",
      "train loss:1.9648766440084484\n",
      "train loss:1.9905953751590344\n",
      "=== epoch:115, train acc:0.38666666666666666, test acc:0.3137 ===\n",
      "train loss:2.073229472587781\n",
      "train loss:2.001055140157013\n",
      "train loss:1.883459914052046\n",
      "=== epoch:116, train acc:0.4, test acc:0.3175 ===\n",
      "train loss:1.9715764168907208\n",
      "train loss:1.9532246361397336\n",
      "train loss:2.0137251617989946\n",
      "=== epoch:117, train acc:0.4033333333333333, test acc:0.3217 ===\n",
      "train loss:2.0072060775902303\n",
      "train loss:1.9603636658612413\n",
      "train loss:1.9550114175511046\n",
      "=== epoch:118, train acc:0.4066666666666667, test acc:0.3219 ===\n",
      "train loss:1.885017062953431\n",
      "train loss:1.8895416826579001\n",
      "train loss:1.8947465154142729\n",
      "=== epoch:119, train acc:0.4033333333333333, test acc:0.3201 ===\n",
      "train loss:1.8552356140450899\n",
      "train loss:1.9365921897213685\n",
      "train loss:1.8968552182221823\n",
      "=== epoch:120, train acc:0.4, test acc:0.3223 ===\n",
      "train loss:1.9660937032613084\n",
      "train loss:1.9179496054361116\n",
      "train loss:1.8679196626831647\n",
      "=== epoch:121, train acc:0.4033333333333333, test acc:0.3222 ===\n",
      "train loss:1.9181336613500322\n",
      "train loss:1.9588848723547656\n",
      "train loss:1.9112120510320258\n",
      "=== epoch:122, train acc:0.4066666666666667, test acc:0.3279 ===\n",
      "train loss:1.9428524536329028\n",
      "train loss:1.9360273467166624\n",
      "train loss:1.8310146635423836\n",
      "=== epoch:123, train acc:0.4066666666666667, test acc:0.3302 ===\n",
      "train loss:1.8845972365136319\n",
      "train loss:1.9136446423616598\n",
      "train loss:1.9513628266037728\n",
      "=== epoch:124, train acc:0.4066666666666667, test acc:0.3289 ===\n",
      "train loss:1.8999139258945368\n",
      "train loss:1.7436191233528284\n",
      "train loss:1.9633620079528364\n",
      "=== epoch:125, train acc:0.4, test acc:0.3241 ===\n",
      "train loss:1.8078206443892393\n",
      "train loss:1.9204161737519514\n",
      "train loss:1.9692494248375538\n",
      "=== epoch:126, train acc:0.41, test acc:0.3274 ===\n",
      "train loss:1.8495257007477273\n",
      "train loss:1.8848171274290086\n",
      "train loss:1.896807106938679\n",
      "=== epoch:127, train acc:0.4166666666666667, test acc:0.3328 ===\n",
      "train loss:1.9463139324575893\n",
      "train loss:1.9467062871570346\n",
      "train loss:1.9488738095153086\n",
      "=== epoch:128, train acc:0.42, test acc:0.3368 ===\n",
      "train loss:1.8875726606137162\n",
      "train loss:1.882605702478759\n",
      "train loss:1.9098870956035958\n",
      "=== epoch:129, train acc:0.42333333333333334, test acc:0.338 ===\n",
      "train loss:1.9276721882692482\n",
      "train loss:1.8804383636124242\n",
      "train loss:1.877575203711257\n",
      "=== epoch:130, train acc:0.42333333333333334, test acc:0.3379 ===\n",
      "train loss:1.874869470499332\n",
      "train loss:1.8347752151398244\n",
      "train loss:1.9046259965815606\n",
      "=== epoch:131, train acc:0.43, test acc:0.3438 ===\n",
      "train loss:1.9562914691765356\n",
      "train loss:1.7830735662921955\n",
      "train loss:1.8085722992757807\n",
      "=== epoch:132, train acc:0.43, test acc:0.3433 ===\n",
      "train loss:1.8431579923732528\n",
      "train loss:1.820205641233764\n",
      "train loss:1.8824685962172387\n",
      "=== epoch:133, train acc:0.44, test acc:0.3501 ===\n",
      "train loss:1.9471758210465586\n",
      "train loss:1.9387082708468153\n",
      "train loss:1.8295844230772655\n",
      "=== epoch:134, train acc:0.43666666666666665, test acc:0.352 ===\n",
      "train loss:1.8863842683703818\n",
      "train loss:1.8979563979446197\n",
      "train loss:1.8874920252782115\n",
      "=== epoch:135, train acc:0.43666666666666665, test acc:0.3539 ===\n",
      "train loss:1.766198065415861\n",
      "train loss:1.790087729946218\n",
      "train loss:1.8359418005865962\n",
      "=== epoch:136, train acc:0.43666666666666665, test acc:0.3508 ===\n",
      "train loss:1.8247117369754406\n",
      "train loss:1.8385637125601533\n",
      "train loss:1.9072738092688164\n",
      "=== epoch:137, train acc:0.44, test acc:0.3547 ===\n",
      "train loss:1.9173286519184223\n",
      "train loss:1.8598588161658984\n",
      "train loss:1.6957436621442066\n",
      "=== epoch:138, train acc:0.44, test acc:0.3578 ===\n",
      "train loss:1.8419253994828595\n",
      "train loss:1.8563795237200855\n",
      "train loss:1.8685046302136465\n",
      "=== epoch:139, train acc:0.43666666666666665, test acc:0.3596 ===\n",
      "train loss:1.9092898101685765\n",
      "train loss:1.8378940258212757\n",
      "train loss:1.7307121659583504\n",
      "=== epoch:140, train acc:0.45, test acc:0.362 ===\n",
      "train loss:1.921209739286273\n",
      "train loss:1.9430290350368924\n",
      "train loss:1.9762455307716535\n",
      "=== epoch:141, train acc:0.4633333333333333, test acc:0.3679 ===\n",
      "train loss:1.7770123139522243\n",
      "train loss:1.818934408089961\n",
      "train loss:1.8679358866506193\n",
      "=== epoch:142, train acc:0.45666666666666667, test acc:0.3659 ===\n",
      "train loss:1.8589132858490516\n",
      "train loss:1.8521994076542447\n",
      "train loss:1.8526514095098443\n",
      "=== epoch:143, train acc:0.4533333333333333, test acc:0.3654 ===\n",
      "train loss:1.809842574348896\n",
      "train loss:1.828976228192824\n",
      "train loss:1.8875240369376975\n",
      "=== epoch:144, train acc:0.45666666666666667, test acc:0.3689 ===\n",
      "train loss:1.8277376801492873\n",
      "train loss:1.7947614781598957\n",
      "train loss:1.8396395809696298\n",
      "=== epoch:145, train acc:0.4533333333333333, test acc:0.3716 ===\n",
      "train loss:1.7804115223304375\n",
      "train loss:1.7556824123598886\n",
      "train loss:1.7931502216594848\n",
      "=== epoch:146, train acc:0.4533333333333333, test acc:0.3727 ===\n",
      "train loss:1.6574393178431628\n",
      "train loss:1.7929381225118413\n",
      "train loss:1.7315551723651157\n",
      "=== epoch:147, train acc:0.4533333333333333, test acc:0.3705 ===\n",
      "train loss:1.8290707046627848\n",
      "train loss:1.8197239568243662\n",
      "train loss:1.8403142870625413\n",
      "=== epoch:148, train acc:0.47333333333333333, test acc:0.3795 ===\n",
      "train loss:1.9267036905764983\n",
      "train loss:1.8015016369917185\n",
      "train loss:1.7847627306552893\n",
      "=== epoch:149, train acc:0.49, test acc:0.3848 ===\n",
      "train loss:1.7521190638221582\n",
      "train loss:1.8567626135548054\n",
      "train loss:1.9187238467023007\n",
      "=== epoch:150, train acc:0.48, test acc:0.3838 ===\n",
      "train loss:1.7634433806132994\n",
      "train loss:1.7059626972168829\n",
      "train loss:2.0104631015999987\n",
      "=== epoch:151, train acc:0.4866666666666667, test acc:0.3825 ===\n",
      "train loss:1.7830327053299195\n",
      "train loss:1.8379623375874352\n",
      "train loss:1.8028815046991016\n",
      "=== epoch:152, train acc:0.48333333333333334, test acc:0.3855 ===\n",
      "train loss:1.8307731171941615\n",
      "train loss:1.7890442329647467\n",
      "train loss:1.7971813290147596\n",
      "=== epoch:153, train acc:0.49333333333333335, test acc:0.3912 ===\n",
      "train loss:1.9061444168470063\n",
      "train loss:1.854118395072082\n",
      "train loss:1.8826491128521545\n",
      "=== epoch:154, train acc:0.5033333333333333, test acc:0.3989 ===\n",
      "train loss:1.7834515315176218\n",
      "train loss:1.7522203012017477\n",
      "train loss:1.6638197349589163\n",
      "=== epoch:155, train acc:0.5066666666666667, test acc:0.3955 ===\n",
      "train loss:1.7276052791568994\n",
      "train loss:1.7313195758271933\n",
      "train loss:1.642404802938783\n",
      "=== epoch:156, train acc:0.4866666666666667, test acc:0.3898 ===\n",
      "train loss:1.7132303900702632\n",
      "train loss:1.7194705578706464\n",
      "train loss:1.7743966388969232\n",
      "=== epoch:157, train acc:0.49333333333333335, test acc:0.3885 ===\n",
      "train loss:1.774510671833936\n",
      "train loss:1.7432010411035117\n",
      "train loss:1.8711055844826194\n",
      "=== epoch:158, train acc:0.49666666666666665, test acc:0.3916 ===\n",
      "train loss:1.7005424321709806\n",
      "train loss:1.569724105866936\n",
      "train loss:1.7408298147956731\n",
      "=== epoch:159, train acc:0.49333333333333335, test acc:0.3893 ===\n",
      "train loss:1.660870391042236\n",
      "train loss:1.8229695404279482\n",
      "train loss:1.7283972647915748\n",
      "=== epoch:160, train acc:0.49333333333333335, test acc:0.3905 ===\n",
      "train loss:1.726467484046831\n",
      "train loss:1.8038296539317051\n",
      "train loss:1.687605147499169\n",
      "=== epoch:161, train acc:0.49333333333333335, test acc:0.3912 ===\n",
      "train loss:1.8259191479980126\n",
      "train loss:1.7226172390316723\n",
      "train loss:1.6660484387227086\n",
      "=== epoch:162, train acc:0.49333333333333335, test acc:0.3938 ===\n",
      "train loss:1.7125585680419582\n",
      "train loss:1.8129335080652489\n",
      "train loss:1.8801586078450847\n",
      "=== epoch:163, train acc:0.49666666666666665, test acc:0.396 ===\n",
      "train loss:1.7921393489950452\n",
      "train loss:1.7789450955522776\n",
      "train loss:1.7122385311070107\n",
      "=== epoch:164, train acc:0.49666666666666665, test acc:0.3963 ===\n",
      "train loss:1.8206450777678997\n",
      "train loss:1.811054917373853\n",
      "train loss:1.8078561997829983\n",
      "=== epoch:165, train acc:0.5033333333333333, test acc:0.4006 ===\n",
      "train loss:1.7246025573148578\n",
      "train loss:1.691201594482689\n",
      "train loss:1.7323912438207782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:166, train acc:0.49666666666666665, test acc:0.4022 ===\n",
      "train loss:1.601684135433723\n",
      "train loss:1.7534902978161329\n",
      "train loss:1.6689916438129972\n",
      "=== epoch:167, train acc:0.49666666666666665, test acc:0.4021 ===\n",
      "train loss:1.767247388982888\n",
      "train loss:1.7652262820605011\n",
      "train loss:1.7727762141998011\n",
      "=== epoch:168, train acc:0.51, test acc:0.4072 ===\n",
      "train loss:1.6497015887287703\n",
      "train loss:1.8147428397102647\n",
      "train loss:1.7310005145524383\n",
      "=== epoch:169, train acc:0.51, test acc:0.4066 ===\n",
      "train loss:1.7079196307432756\n",
      "train loss:1.7113215901791798\n",
      "train loss:1.7317891399700982\n",
      "=== epoch:170, train acc:0.5133333333333333, test acc:0.4084 ===\n",
      "train loss:1.790786745746264\n",
      "train loss:1.8151818308201715\n",
      "train loss:1.7186703009842716\n",
      "=== epoch:171, train acc:0.5233333333333333, test acc:0.4118 ===\n",
      "train loss:1.6778716516976533\n",
      "train loss:1.768381609365936\n",
      "train loss:1.6891317476807217\n",
      "=== epoch:172, train acc:0.53, test acc:0.4156 ===\n",
      "train loss:1.6391650514771285\n",
      "train loss:1.6751290743088896\n",
      "train loss:1.7667181471304871\n",
      "=== epoch:173, train acc:0.5366666666666666, test acc:0.4183 ===\n",
      "train loss:1.6907334101748888\n",
      "train loss:1.7454665775065237\n",
      "train loss:1.7378241656069577\n",
      "=== epoch:174, train acc:0.5366666666666666, test acc:0.4185 ===\n",
      "train loss:1.6731617141719428\n",
      "train loss:1.7197805229276355\n",
      "train loss:1.7751625898201149\n",
      "=== epoch:175, train acc:0.54, test acc:0.4192 ===\n",
      "train loss:1.741912228683132\n",
      "train loss:1.6844273424469034\n",
      "train loss:1.6351793154968979\n",
      "=== epoch:176, train acc:0.5366666666666666, test acc:0.4218 ===\n",
      "train loss:1.6902212963028616\n",
      "train loss:1.57468276361576\n",
      "train loss:1.707432025341697\n",
      "=== epoch:177, train acc:0.54, test acc:0.421 ===\n",
      "train loss:1.6783240534001047\n",
      "train loss:1.7392929856980783\n",
      "train loss:1.5724173987671386\n",
      "=== epoch:178, train acc:0.55, test acc:0.4223 ===\n",
      "train loss:1.7247349605023148\n",
      "train loss:1.6294788574945491\n",
      "train loss:1.5912112384159725\n",
      "=== epoch:179, train acc:0.55, test acc:0.4232 ===\n",
      "train loss:1.7310804614135156\n",
      "train loss:1.627958516241037\n",
      "train loss:1.7001622392434694\n",
      "=== epoch:180, train acc:0.54, test acc:0.4225 ===\n",
      "train loss:1.6144641944228466\n",
      "train loss:1.7285393986956397\n",
      "train loss:1.7787172363641854\n",
      "=== epoch:181, train acc:0.55, test acc:0.4255 ===\n",
      "train loss:1.6105142779313482\n",
      "train loss:1.6235072318149764\n",
      "train loss:1.7167728002644445\n",
      "=== epoch:182, train acc:0.5566666666666666, test acc:0.4295 ===\n",
      "train loss:1.5179784772185088\n",
      "train loss:1.5677420088595801\n",
      "train loss:1.7249818738592742\n",
      "=== epoch:183, train acc:0.5566666666666666, test acc:0.4296 ===\n",
      "train loss:1.7401551689627706\n",
      "train loss:1.6368073193421253\n",
      "train loss:1.5484931385768923\n",
      "=== epoch:184, train acc:0.5466666666666666, test acc:0.4311 ===\n",
      "train loss:1.5974244107887225\n",
      "train loss:1.695956168950966\n",
      "train loss:1.6288922333343112\n",
      "=== epoch:185, train acc:0.5533333333333333, test acc:0.4291 ===\n",
      "train loss:1.6604919701883278\n",
      "train loss:1.6273283864287966\n",
      "train loss:1.7403766261338998\n",
      "=== epoch:186, train acc:0.55, test acc:0.4328 ===\n",
      "train loss:1.54806900916699\n",
      "train loss:1.5925704868591493\n",
      "train loss:1.6814110990387412\n",
      "=== epoch:187, train acc:0.5466666666666666, test acc:0.4285 ===\n",
      "train loss:1.5863674724521801\n",
      "train loss:1.6666648080751756\n",
      "train loss:1.6084668661606312\n",
      "=== epoch:188, train acc:0.54, test acc:0.4298 ===\n",
      "train loss:1.7393499320306212\n",
      "train loss:1.5584883253748152\n",
      "train loss:1.5069289880666847\n",
      "=== epoch:189, train acc:0.55, test acc:0.4347 ===\n",
      "train loss:1.5427575357779955\n",
      "train loss:1.5336068870055564\n",
      "train loss:1.6504807799034722\n",
      "=== epoch:190, train acc:0.54, test acc:0.4346 ===\n",
      "train loss:1.540493425598388\n",
      "train loss:1.7589093052630633\n",
      "train loss:1.6502430734953624\n",
      "=== epoch:191, train acc:0.5433333333333333, test acc:0.4381 ===\n",
      "train loss:1.59962158800939\n",
      "train loss:1.601770200825981\n",
      "train loss:1.616589464461352\n",
      "=== epoch:192, train acc:0.56, test acc:0.441 ===\n",
      "train loss:1.5612813083260255\n",
      "train loss:1.51013095548976\n",
      "train loss:1.575434308080463\n",
      "=== epoch:193, train acc:0.56, test acc:0.4398 ===\n",
      "train loss:1.7201677629291967\n",
      "train loss:1.477165709742033\n",
      "train loss:1.6411379247501543\n",
      "=== epoch:194, train acc:0.5633333333333334, test acc:0.4414 ===\n",
      "train loss:1.618538203603042\n",
      "train loss:1.4916871314289881\n",
      "train loss:1.4881884191481232\n",
      "=== epoch:195, train acc:0.5566666666666666, test acc:0.4412 ===\n",
      "train loss:1.6018101084317837\n",
      "train loss:1.5023306979212088\n",
      "train loss:1.6234176269373717\n",
      "=== epoch:196, train acc:0.5666666666666667, test acc:0.4415 ===\n",
      "train loss:1.5761382275843647\n",
      "train loss:1.5327480943091152\n",
      "train loss:1.7252882538933654\n",
      "=== epoch:197, train acc:0.56, test acc:0.4439 ===\n",
      "train loss:1.5448585464902522\n",
      "train loss:1.5806991800504733\n",
      "train loss:1.4633821282826411\n",
      "=== epoch:198, train acc:0.57, test acc:0.4453 ===\n",
      "train loss:1.6032439963682368\n",
      "train loss:1.54545251606266\n",
      "train loss:1.5548369940912679\n",
      "=== epoch:199, train acc:0.5566666666666666, test acc:0.4434 ===\n",
      "train loss:1.6002185774943707\n",
      "train loss:1.5735079616510104\n",
      "train loss:1.607459703283637\n",
      "=== epoch:200, train acc:0.5533333333333333, test acc:0.4457 ===\n",
      "train loss:1.61106208651671\n",
      "train loss:1.5676701173058332\n",
      "train loss:1.495056265865563\n",
      "=== epoch:201, train acc:0.56, test acc:0.4477 ===\n",
      "train loss:1.4766437532505103\n",
      "train loss:1.5143720234868252\n",
      "train loss:1.4613072581129987\n",
      "=== epoch:202, train acc:0.56, test acc:0.4503 ===\n",
      "train loss:1.640829854478072\n",
      "train loss:1.60761518209723\n",
      "train loss:1.6524062699151592\n",
      "=== epoch:203, train acc:0.56, test acc:0.4527 ===\n",
      "train loss:1.6205358394578697\n",
      "train loss:1.4416077848682733\n",
      "train loss:1.6150049258460817\n",
      "=== epoch:204, train acc:0.5666666666666667, test acc:0.4526 ===\n",
      "train loss:1.501044427598424\n",
      "train loss:1.5741689822923988\n",
      "train loss:1.462139781751987\n",
      "=== epoch:205, train acc:0.5633333333333334, test acc:0.4533 ===\n",
      "train loss:1.5829506523174184\n",
      "train loss:1.5161850357891382\n",
      "train loss:1.4555729439479712\n",
      "=== epoch:206, train acc:0.5666666666666667, test acc:0.4552 ===\n",
      "train loss:1.6241504189463427\n",
      "train loss:1.4581638622630078\n",
      "train loss:1.4668230059280984\n",
      "=== epoch:207, train acc:0.5666666666666667, test acc:0.4572 ===\n",
      "train loss:1.5859037484907652\n",
      "train loss:1.6446975778374386\n",
      "train loss:1.6452889564113904\n",
      "=== epoch:208, train acc:0.58, test acc:0.4626 ===\n",
      "train loss:1.55723459005815\n",
      "train loss:1.5002854717867962\n",
      "train loss:1.5743642967363067\n",
      "=== epoch:209, train acc:0.5733333333333334, test acc:0.463 ===\n",
      "train loss:1.3581567274249275\n",
      "train loss:1.4593024313284875\n",
      "train loss:1.5232856839435454\n",
      "=== epoch:210, train acc:0.5733333333333334, test acc:0.4638 ===\n",
      "train loss:1.3810507484558212\n",
      "train loss:1.3882511512599387\n",
      "train loss:1.4423707856799155\n",
      "=== epoch:211, train acc:0.5733333333333334, test acc:0.4631 ===\n",
      "train loss:1.4943386591016838\n",
      "train loss:1.3958618833646446\n",
      "train loss:1.395149494937089\n",
      "=== epoch:212, train acc:0.5766666666666667, test acc:0.4642 ===\n",
      "train loss:1.4833030088495844\n",
      "train loss:1.6151506146045662\n",
      "train loss:1.6864757179365384\n",
      "=== epoch:213, train acc:0.58, test acc:0.4701 ===\n",
      "train loss:1.4668678557910595\n",
      "train loss:1.3823398601130026\n",
      "train loss:1.3480259270761374\n",
      "=== epoch:214, train acc:0.5866666666666667, test acc:0.4714 ===\n",
      "train loss:1.448968584271033\n",
      "train loss:1.46559123977792\n",
      "train loss:1.4623504609023597\n",
      "=== epoch:215, train acc:0.5966666666666667, test acc:0.4737 ===\n",
      "train loss:1.6216038829220363\n",
      "train loss:1.4391676413420635\n",
      "train loss:1.6171858656024087\n",
      "=== epoch:216, train acc:0.6033333333333334, test acc:0.4741 ===\n",
      "train loss:1.6002258135861576\n",
      "train loss:1.3762247272586554\n",
      "train loss:1.4383409130870746\n",
      "=== epoch:217, train acc:0.5966666666666667, test acc:0.4757 ===\n",
      "train loss:1.3947300279667334\n",
      "train loss:1.3757619387047257\n",
      "train loss:1.375314752198522\n",
      "=== epoch:218, train acc:0.6, test acc:0.4753 ===\n",
      "train loss:1.4759933774602099\n",
      "train loss:1.5022995666229428\n",
      "train loss:1.4622276942701817\n",
      "=== epoch:219, train acc:0.61, test acc:0.4794 ===\n",
      "train loss:1.3669341814983447\n",
      "train loss:1.3843379040169455\n",
      "train loss:1.4609514141464073\n",
      "=== epoch:220, train acc:0.61, test acc:0.4807 ===\n",
      "train loss:1.4469235535656446\n",
      "train loss:1.445915669811161\n",
      "train loss:1.5604840487444522\n",
      "=== epoch:221, train acc:0.6066666666666667, test acc:0.4837 ===\n",
      "train loss:1.5471660017280462\n",
      "train loss:1.467757238039431\n",
      "train loss:1.4438819131921738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.62, test acc:0.4851 ===\n",
      "train loss:1.4183465324123992\n",
      "train loss:1.4545543974882411\n",
      "train loss:1.4115579966448637\n",
      "=== epoch:223, train acc:0.6233333333333333, test acc:0.4878 ===\n",
      "train loss:1.4117753910437292\n",
      "train loss:1.3873947336072212\n",
      "train loss:1.3675283135712375\n",
      "=== epoch:224, train acc:0.6233333333333333, test acc:0.488 ===\n",
      "train loss:1.5479985881799125\n",
      "train loss:1.3541978959952385\n",
      "train loss:1.4034139705617479\n",
      "=== epoch:225, train acc:0.6233333333333333, test acc:0.4891 ===\n",
      "train loss:1.440403018738745\n",
      "train loss:1.4764444387355564\n",
      "train loss:1.4887438322602093\n",
      "=== epoch:226, train acc:0.62, test acc:0.4899 ===\n",
      "train loss:1.4090230323535555\n",
      "train loss:1.562307415010366\n",
      "train loss:1.4496912061393403\n",
      "=== epoch:227, train acc:0.6166666666666667, test acc:0.4912 ===\n",
      "train loss:1.4117036891269574\n",
      "train loss:1.4017382236684954\n",
      "train loss:1.4830368223933297\n",
      "=== epoch:228, train acc:0.62, test acc:0.4902 ===\n",
      "train loss:1.3148115400546394\n",
      "train loss:1.2912160687574583\n",
      "train loss:1.581487201004996\n",
      "=== epoch:229, train acc:0.6133333333333333, test acc:0.4869 ===\n",
      "train loss:1.296942387228282\n",
      "train loss:1.1682508715651234\n",
      "train loss:1.3931844566748233\n",
      "=== epoch:230, train acc:0.6066666666666667, test acc:0.4891 ===\n",
      "train loss:1.3935527557303922\n",
      "train loss:1.3715418668011181\n",
      "train loss:1.4232189725231696\n",
      "=== epoch:231, train acc:0.61, test acc:0.4901 ===\n",
      "train loss:1.2617164697526213\n",
      "train loss:1.3311602078963423\n",
      "train loss:1.275518506189129\n",
      "=== epoch:232, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.434589733930982\n",
      "train loss:1.4096389035882564\n",
      "train loss:1.3917692124003511\n",
      "=== epoch:233, train acc:0.61, test acc:0.4886 ===\n",
      "train loss:1.2818695302209142\n",
      "train loss:1.4026666009448507\n",
      "train loss:1.43017668191691\n",
      "=== epoch:234, train acc:0.62, test acc:0.4928 ===\n",
      "train loss:1.1710561071587469\n",
      "train loss:1.4822128479521124\n",
      "train loss:1.1993306322183\n",
      "=== epoch:235, train acc:0.61, test acc:0.4966 ===\n",
      "train loss:1.2656868294621497\n",
      "train loss:1.355177579547156\n",
      "train loss:1.1916068126361834\n",
      "=== epoch:236, train acc:0.6166666666666667, test acc:0.4939 ===\n",
      "train loss:1.3140271438601119\n",
      "train loss:1.3003115742701947\n",
      "train loss:1.32219638625986\n",
      "=== epoch:237, train acc:0.6166666666666667, test acc:0.4943 ===\n",
      "train loss:1.1644347289183075\n",
      "train loss:1.4207487909877816\n",
      "train loss:1.3074685913952524\n",
      "=== epoch:238, train acc:0.6166666666666667, test acc:0.4957 ===\n",
      "train loss:1.228960293123101\n",
      "train loss:1.4195609153070061\n",
      "train loss:1.40238838294953\n",
      "=== epoch:239, train acc:0.62, test acc:0.4973 ===\n",
      "train loss:1.3808227177664696\n",
      "train loss:1.3341466980320278\n",
      "train loss:1.4339088323483165\n",
      "=== epoch:240, train acc:0.6233333333333333, test acc:0.5005 ===\n",
      "train loss:1.3125003242997466\n",
      "train loss:1.3263370542515907\n",
      "train loss:1.4818384355857797\n",
      "=== epoch:241, train acc:0.6233333333333333, test acc:0.5043 ===\n",
      "train loss:1.3287529531646056\n",
      "train loss:1.3937851045421972\n",
      "train loss:1.2150911221815253\n",
      "=== epoch:242, train acc:0.6266666666666667, test acc:0.5054 ===\n",
      "train loss:1.3427367787132964\n",
      "train loss:1.3137481052621167\n",
      "train loss:1.4476330692196522\n",
      "=== epoch:243, train acc:0.6333333333333333, test acc:0.5105 ===\n",
      "train loss:1.308608411332156\n",
      "train loss:1.4778569546594766\n",
      "train loss:1.1400750300715272\n",
      "=== epoch:244, train acc:0.6266666666666667, test acc:0.5113 ===\n",
      "train loss:1.3619732985344646\n",
      "train loss:1.2083959315807078\n",
      "train loss:1.288040631833285\n",
      "=== epoch:245, train acc:0.63, test acc:0.5113 ===\n",
      "train loss:1.365655138211914\n",
      "train loss:1.3640579582060959\n",
      "train loss:1.2200629382725374\n",
      "=== epoch:246, train acc:0.6333333333333333, test acc:0.5107 ===\n",
      "train loss:1.3011220142843616\n",
      "train loss:1.310209685042117\n",
      "train loss:1.3672493602550668\n",
      "=== epoch:247, train acc:0.63, test acc:0.5128 ===\n",
      "train loss:1.351409787100981\n",
      "train loss:1.1595067385526727\n",
      "train loss:1.1410123166524082\n",
      "=== epoch:248, train acc:0.6333333333333333, test acc:0.5127 ===\n",
      "train loss:1.275452742568816\n",
      "train loss:1.396810384830021\n",
      "train loss:1.2612874106618572\n",
      "=== epoch:249, train acc:0.63, test acc:0.5145 ===\n",
      "train loss:1.21477018962594\n",
      "train loss:1.1997441465647338\n",
      "train loss:1.1639943701986015\n",
      "=== epoch:250, train acc:0.63, test acc:0.513 ===\n",
      "train loss:1.2743407539486264\n",
      "train loss:1.2497999396875503\n",
      "train loss:1.2040936949321057\n",
      "=== epoch:251, train acc:0.6333333333333333, test acc:0.5139 ===\n",
      "train loss:1.16388560137538\n",
      "train loss:1.219159498633611\n",
      "train loss:1.1364667686978864\n",
      "=== epoch:252, train acc:0.6366666666666667, test acc:0.5157 ===\n",
      "train loss:1.1921830934806392\n",
      "train loss:1.1298404912913282\n",
      "train loss:1.399244499397906\n",
      "=== epoch:253, train acc:0.6366666666666667, test acc:0.5178 ===\n",
      "train loss:1.1775865934841214\n",
      "train loss:1.2647793402124439\n",
      "train loss:1.2155369465695003\n",
      "=== epoch:254, train acc:0.64, test acc:0.518 ===\n",
      "train loss:1.2293331351631485\n",
      "train loss:1.1527264144535403\n",
      "train loss:1.3675531767986207\n",
      "=== epoch:255, train acc:0.64, test acc:0.5169 ===\n",
      "train loss:1.3133567579860903\n",
      "train loss:1.1467386893884572\n",
      "train loss:1.1356097313405282\n",
      "=== epoch:256, train acc:0.6433333333333333, test acc:0.5206 ===\n",
      "train loss:1.1908281649187138\n",
      "train loss:1.1513170784093583\n",
      "train loss:1.3369344993405123\n",
      "=== epoch:257, train acc:0.6466666666666666, test acc:0.5211 ===\n",
      "train loss:1.1115841186825335\n",
      "train loss:1.1910838726466628\n",
      "train loss:1.0521012179608429\n",
      "=== epoch:258, train acc:0.64, test acc:0.5175 ===\n",
      "train loss:1.17451663437281\n",
      "train loss:1.2180602075075082\n",
      "train loss:1.074061808073714\n",
      "=== epoch:259, train acc:0.6366666666666667, test acc:0.519 ===\n",
      "train loss:1.1520708482786135\n",
      "train loss:1.1476955321561184\n",
      "train loss:1.2157996911996296\n",
      "=== epoch:260, train acc:0.64, test acc:0.5217 ===\n",
      "train loss:1.1384795232510094\n",
      "train loss:1.0569084305750323\n",
      "train loss:1.1917819772760796\n",
      "=== epoch:261, train acc:0.65, test acc:0.5244 ===\n",
      "train loss:1.2550672082374927\n",
      "train loss:1.1750098295736886\n",
      "train loss:1.298699452657462\n",
      "=== epoch:262, train acc:0.6566666666666666, test acc:0.526 ===\n",
      "train loss:1.266089851659973\n",
      "train loss:1.1897316838105962\n",
      "train loss:1.241490250420644\n",
      "=== epoch:263, train acc:0.6566666666666666, test acc:0.527 ===\n",
      "train loss:1.1156105448835623\n",
      "train loss:1.1449100297684571\n",
      "train loss:1.2283657088276438\n",
      "=== epoch:264, train acc:0.66, test acc:0.5306 ===\n",
      "train loss:1.127053958045407\n",
      "train loss:1.1464714686856554\n",
      "train loss:1.2510267756488334\n",
      "=== epoch:265, train acc:0.6533333333333333, test acc:0.5313 ===\n",
      "train loss:1.1189348908774295\n",
      "train loss:1.0428306145948951\n",
      "train loss:1.100043088613373\n",
      "=== epoch:266, train acc:0.6566666666666666, test acc:0.5316 ===\n",
      "train loss:1.2035948815665156\n",
      "train loss:1.1714703350430284\n",
      "train loss:1.0719876856855255\n",
      "=== epoch:267, train acc:0.6566666666666666, test acc:0.5308 ===\n",
      "train loss:1.1297963247809804\n",
      "train loss:1.2411578860272308\n",
      "train loss:1.1340313077458084\n",
      "=== epoch:268, train acc:0.66, test acc:0.5345 ===\n",
      "train loss:1.1044595151182683\n",
      "train loss:1.1736621975423216\n",
      "train loss:1.1192580038802544\n",
      "=== epoch:269, train acc:0.6566666666666666, test acc:0.5347 ===\n",
      "train loss:1.134031774340811\n",
      "train loss:1.3073268938631466\n",
      "train loss:1.2590638206633664\n",
      "=== epoch:270, train acc:0.6566666666666666, test acc:0.5388 ===\n",
      "train loss:1.1740987646918501\n",
      "train loss:1.1560711788312081\n",
      "train loss:1.1788198372567735\n",
      "=== epoch:271, train acc:0.66, test acc:0.5398 ===\n",
      "train loss:1.2131893288602051\n",
      "train loss:1.160347351323801\n",
      "train loss:1.1173030820922358\n",
      "=== epoch:272, train acc:0.6633333333333333, test acc:0.542 ===\n",
      "train loss:1.2665271704856738\n",
      "train loss:1.0690772826265005\n",
      "train loss:1.1037228366337366\n",
      "=== epoch:273, train acc:0.6566666666666666, test acc:0.5425 ===\n",
      "train loss:1.0908080048774154\n",
      "train loss:1.05256551064852\n",
      "train loss:1.0974251514359603\n",
      "=== epoch:274, train acc:0.66, test acc:0.5465 ===\n",
      "train loss:1.0753521369933337\n",
      "train loss:1.0542607106786632\n",
      "train loss:1.0924397058510498\n",
      "=== epoch:275, train acc:0.6666666666666666, test acc:0.5498 ===\n",
      "train loss:1.1127191810877772\n",
      "train loss:1.1134829227528247\n",
      "train loss:1.1965425636670135\n",
      "=== epoch:276, train acc:0.66, test acc:0.5462 ===\n",
      "train loss:1.0092791055355754\n",
      "train loss:1.1412261092739895\n",
      "train loss:1.2213100351916082\n",
      "=== epoch:277, train acc:0.67, test acc:0.5462 ===\n",
      "train loss:1.136137435754212\n",
      "train loss:1.0566513610714923\n",
      "train loss:1.018401930575842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.6666666666666666, test acc:0.5486 ===\n",
      "train loss:1.0874193582387206\n",
      "train loss:1.0418094114709664\n",
      "train loss:1.0120216389474441\n",
      "=== epoch:279, train acc:0.6733333333333333, test acc:0.5512 ===\n",
      "train loss:1.0568060210001897\n",
      "train loss:1.1224952015772531\n",
      "train loss:1.1263569077099056\n",
      "=== epoch:280, train acc:0.67, test acc:0.5542 ===\n",
      "train loss:0.98461136874101\n",
      "train loss:1.1920972859060037\n",
      "train loss:1.1785947116287852\n",
      "=== epoch:281, train acc:0.6733333333333333, test acc:0.5558 ===\n",
      "train loss:1.084859618396277\n",
      "train loss:0.9360729176722776\n",
      "train loss:1.181873792830452\n",
      "=== epoch:282, train acc:0.6733333333333333, test acc:0.5555 ===\n",
      "train loss:0.9529161696389034\n",
      "train loss:1.076630310748851\n",
      "train loss:1.08050008463593\n",
      "=== epoch:283, train acc:0.67, test acc:0.5559 ===\n",
      "train loss:1.0963873432347955\n",
      "train loss:1.2305915423633498\n",
      "train loss:1.0654327781361803\n",
      "=== epoch:284, train acc:0.6733333333333333, test acc:0.5549 ===\n",
      "train loss:1.17976571224707\n",
      "train loss:1.0651040003310612\n",
      "train loss:1.07647080193693\n",
      "=== epoch:285, train acc:0.6766666666666666, test acc:0.5545 ===\n",
      "train loss:1.0434379198301322\n",
      "train loss:1.1946393542185163\n",
      "train loss:0.9248372179225274\n",
      "=== epoch:286, train acc:0.6766666666666666, test acc:0.5561 ===\n",
      "train loss:0.9760895067864449\n",
      "train loss:0.9953704347960022\n",
      "train loss:0.9419919315740007\n",
      "=== epoch:287, train acc:0.68, test acc:0.5572 ===\n",
      "train loss:0.9866982249887059\n",
      "train loss:0.9395837930402189\n",
      "train loss:0.9135119066975034\n",
      "=== epoch:288, train acc:0.68, test acc:0.5563 ===\n",
      "train loss:1.0480032276619893\n",
      "train loss:1.1123146916797213\n",
      "train loss:1.019298976649288\n",
      "=== epoch:289, train acc:0.6833333333333333, test acc:0.5581 ===\n",
      "train loss:1.0533770968204135\n",
      "train loss:1.0138834481951786\n",
      "train loss:1.034457727162962\n",
      "=== epoch:290, train acc:0.68, test acc:0.5572 ===\n",
      "train loss:1.1042829130158793\n",
      "train loss:1.0274356821157085\n",
      "train loss:1.0282452686175676\n",
      "=== epoch:291, train acc:0.6833333333333333, test acc:0.5619 ===\n",
      "train loss:1.0538385452053824\n",
      "train loss:0.9440829396652138\n",
      "train loss:1.028482267387899\n",
      "=== epoch:292, train acc:0.6866666666666666, test acc:0.5659 ===\n",
      "train loss:0.9323650884273765\n",
      "train loss:0.9043018828264329\n",
      "train loss:1.1737123619599352\n",
      "=== epoch:293, train acc:0.6966666666666667, test acc:0.5667 ===\n",
      "train loss:1.0046762831328353\n",
      "train loss:1.0105227361607048\n",
      "train loss:0.859192961577333\n",
      "=== epoch:294, train acc:0.6866666666666666, test acc:0.5664 ===\n",
      "train loss:0.9301344261153623\n",
      "train loss:1.0291152875650837\n",
      "train loss:1.0379859787976857\n",
      "=== epoch:295, train acc:0.6966666666666667, test acc:0.5685 ===\n",
      "train loss:0.8921797040558829\n",
      "train loss:0.856398456927686\n",
      "train loss:1.0352314036163812\n",
      "=== epoch:296, train acc:0.6966666666666667, test acc:0.5693 ===\n",
      "train loss:1.0942260367010597\n",
      "train loss:0.9750134344716517\n",
      "train loss:1.0634005020293436\n",
      "=== epoch:297, train acc:0.69, test acc:0.5719 ===\n",
      "train loss:0.9115175532700575\n",
      "train loss:0.9311663700583519\n",
      "train loss:0.9693783969338614\n",
      "=== epoch:298, train acc:0.69, test acc:0.5743 ===\n",
      "train loss:1.0597827325185911\n",
      "train loss:1.0504322188185946\n",
      "train loss:1.0189951686192409\n",
      "=== epoch:299, train acc:0.6966666666666667, test acc:0.5762 ===\n",
      "train loss:0.9447887138246603\n",
      "train loss:1.1564443552982862\n",
      "train loss:0.9787197033465947\n",
      "=== epoch:300, train acc:0.7033333333333334, test acc:0.5766 ===\n",
      "train loss:0.9791444733549995\n",
      "train loss:0.9787897074256374\n",
      "train loss:0.9087327081812776\n",
      "=== epoch:301, train acc:0.71, test acc:0.5751 ===\n",
      "train loss:0.9203411457996056\n",
      "train loss:1.0053928187220311\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5789\n"
     ]
    }
   ],
   "source": [
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ad3f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dccfd",
   "metadata": {},
   "source": [
    "드롭아웃 비율 조정해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a7a3b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3314751181089512\n",
      "=== epoch:1, train acc:0.08666666666666667, test acc:0.1034 ===\n",
      "train loss:2.3393209350684536\n",
      "train loss:2.316973392017008\n",
      "train loss:2.320330647975749\n",
      "=== epoch:2, train acc:0.09, test acc:0.1054 ===\n",
      "train loss:2.3096688153665212\n",
      "train loss:2.315644790181693\n",
      "train loss:2.3051668297623507\n",
      "=== epoch:3, train acc:0.07666666666666666, test acc:0.1104 ===\n",
      "train loss:2.3121373355348935\n",
      "train loss:2.3000943990132647\n",
      "train loss:2.2679346128534514\n",
      "=== epoch:4, train acc:0.08666666666666667, test acc:0.1112 ===\n",
      "train loss:2.2790382893603276\n",
      "train loss:2.2800470053793895\n",
      "train loss:2.3197652621345344\n",
      "=== epoch:5, train acc:0.09666666666666666, test acc:0.1157 ===\n",
      "train loss:2.2760483068444675\n",
      "train loss:2.3182318850256776\n",
      "train loss:2.3084666431057097\n",
      "=== epoch:6, train acc:0.10333333333333333, test acc:0.118 ===\n",
      "train loss:2.2794577794697\n",
      "train loss:2.294928824587181\n",
      "train loss:2.2701767258192604\n",
      "=== epoch:7, train acc:0.10333333333333333, test acc:0.1219 ===\n",
      "train loss:2.2787233305391514\n",
      "train loss:2.275167847259375\n",
      "train loss:2.2492261400537923\n",
      "=== epoch:8, train acc:0.11, test acc:0.1227 ===\n",
      "train loss:2.277677463851423\n",
      "train loss:2.2833027555985894\n",
      "train loss:2.2887755450892313\n",
      "=== epoch:9, train acc:0.12, test acc:0.1281 ===\n",
      "train loss:2.2603139608853904\n",
      "train loss:2.261413481444062\n",
      "train loss:2.2672874530879663\n",
      "=== epoch:10, train acc:0.12666666666666668, test acc:0.1301 ===\n",
      "train loss:2.280975767426994\n",
      "train loss:2.2533497105312517\n",
      "train loss:2.2388683325196403\n",
      "=== epoch:11, train acc:0.13, test acc:0.1341 ===\n",
      "train loss:2.268293509020627\n",
      "train loss:2.270409009696171\n",
      "train loss:2.246787803878088\n",
      "=== epoch:12, train acc:0.13666666666666666, test acc:0.1369 ===\n",
      "train loss:2.2363150216505976\n",
      "train loss:2.261381894991155\n",
      "train loss:2.2690194038559937\n",
      "=== epoch:13, train acc:0.14, test acc:0.1394 ===\n",
      "train loss:2.252182668023545\n",
      "train loss:2.2553891149993\n",
      "train loss:2.247501984269832\n",
      "=== epoch:14, train acc:0.14666666666666667, test acc:0.1431 ===\n",
      "train loss:2.2438956839722706\n",
      "train loss:2.2653975249232023\n",
      "train loss:2.229117508431663\n",
      "=== epoch:15, train acc:0.16333333333333333, test acc:0.1492 ===\n",
      "train loss:2.2395315616997906\n",
      "train loss:2.2371708792981146\n",
      "train loss:2.2347003811846777\n",
      "=== epoch:16, train acc:0.16666666666666666, test acc:0.1571 ===\n",
      "train loss:2.2562851910474335\n",
      "train loss:2.2396253390588696\n",
      "train loss:2.2458841572448267\n",
      "=== epoch:17, train acc:0.17333333333333334, test acc:0.1661 ===\n",
      "train loss:2.246802576636897\n",
      "train loss:2.2317929128132645\n",
      "train loss:2.206902036471575\n",
      "=== epoch:18, train acc:0.18333333333333332, test acc:0.1754 ===\n",
      "train loss:2.221058581037594\n",
      "train loss:2.223989798713319\n",
      "train loss:2.2455424940453867\n",
      "=== epoch:19, train acc:0.20333333333333334, test acc:0.1835 ===\n",
      "train loss:2.2085512827387452\n",
      "train loss:2.2256773192976893\n",
      "train loss:2.219811375773403\n",
      "=== epoch:20, train acc:0.21666666666666667, test acc:0.1886 ===\n",
      "train loss:2.195888435767331\n",
      "train loss:2.199453457946164\n",
      "train loss:2.218299751710297\n",
      "=== epoch:21, train acc:0.23333333333333334, test acc:0.1962 ===\n",
      "train loss:2.228064617897319\n",
      "train loss:2.199407575057315\n",
      "train loss:2.1884990424537647\n",
      "=== epoch:22, train acc:0.23666666666666666, test acc:0.1974 ===\n",
      "train loss:2.211859114420577\n",
      "train loss:2.2065132471685893\n",
      "train loss:2.2085999038001045\n",
      "=== epoch:23, train acc:0.24, test acc:0.2002 ===\n",
      "train loss:2.184738845797837\n",
      "train loss:2.1876451618721613\n",
      "train loss:2.2068063486140925\n",
      "=== epoch:24, train acc:0.26666666666666666, test acc:0.2087 ===\n",
      "train loss:2.221595304021757\n",
      "train loss:2.2100650715804315\n",
      "train loss:2.1752261951599663\n",
      "=== epoch:25, train acc:0.2733333333333333, test acc:0.2149 ===\n",
      "train loss:2.1734652086590063\n",
      "train loss:2.1464117947157875\n",
      "train loss:2.1708208232401667\n",
      "=== epoch:26, train acc:0.28, test acc:0.2187 ===\n",
      "train loss:2.193653062638936\n",
      "train loss:2.168038391438246\n",
      "train loss:2.1663721089979706\n",
      "=== epoch:27, train acc:0.2866666666666667, test acc:0.2221 ===\n",
      "train loss:2.2250697998705022\n",
      "train loss:2.1712327288357742\n",
      "train loss:2.146426847541231\n",
      "=== epoch:28, train acc:0.30333333333333334, test acc:0.2333 ===\n",
      "train loss:2.1945349573367143\n",
      "train loss:2.122321132553878\n",
      "train loss:2.174403691401599\n",
      "=== epoch:29, train acc:0.30666666666666664, test acc:0.2353 ===\n",
      "train loss:2.135877083362867\n",
      "train loss:2.178073662055428\n",
      "train loss:2.218221544813119\n",
      "=== epoch:30, train acc:0.31333333333333335, test acc:0.2481 ===\n",
      "train loss:2.150380079151246\n",
      "train loss:2.1649648168103854\n",
      "train loss:2.1769385106620134\n",
      "=== epoch:31, train acc:0.32, test acc:0.251 ===\n",
      "train loss:2.1495172025238296\n",
      "train loss:2.1722576954473922\n",
      "train loss:2.174520756220784\n",
      "=== epoch:32, train acc:0.3233333333333333, test acc:0.2599 ===\n",
      "train loss:2.1391107017658073\n",
      "train loss:2.160747122613695\n",
      "train loss:2.123087070196292\n",
      "=== epoch:33, train acc:0.3333333333333333, test acc:0.2697 ===\n",
      "train loss:2.060234799256805\n",
      "train loss:2.121065542866403\n",
      "train loss:2.1618339473568837\n",
      "=== epoch:34, train acc:0.33, test acc:0.2723 ===\n",
      "train loss:2.1555695860448956\n",
      "train loss:2.1095991432557732\n",
      "train loss:2.21852315413534\n",
      "=== epoch:35, train acc:0.3433333333333333, test acc:0.2785 ===\n",
      "train loss:2.1248873808312574\n",
      "train loss:2.151492618777807\n",
      "train loss:2.153594834473054\n",
      "=== epoch:36, train acc:0.36, test acc:0.292 ===\n",
      "train loss:2.1137145937162356\n",
      "train loss:2.1496351354735648\n",
      "train loss:2.1219488128280624\n",
      "=== epoch:37, train acc:0.36333333333333334, test acc:0.2972 ===\n",
      "train loss:2.1700131198432375\n",
      "train loss:2.0696233938403408\n",
      "train loss:2.106695903597171\n",
      "=== epoch:38, train acc:0.36333333333333334, test acc:0.2998 ===\n",
      "train loss:2.1397263894684966\n",
      "train loss:2.1065762631417\n",
      "train loss:2.105102455002205\n",
      "=== epoch:39, train acc:0.36666666666666664, test acc:0.3092 ===\n",
      "train loss:2.081202220631357\n",
      "train loss:2.1416842601049604\n",
      "train loss:2.0649587674815\n",
      "=== epoch:40, train acc:0.39, test acc:0.3156 ===\n",
      "train loss:2.102533139653063\n",
      "train loss:2.0798136325846106\n",
      "train loss:2.137988683819812\n",
      "=== epoch:41, train acc:0.39666666666666667, test acc:0.3209 ===\n",
      "train loss:2.120071543900722\n",
      "train loss:2.1069852521884234\n",
      "train loss:2.1204466201187\n",
      "=== epoch:42, train acc:0.41, test acc:0.3281 ===\n",
      "train loss:2.0856271410891734\n",
      "train loss:2.0700122187639582\n",
      "train loss:2.1136004404013047\n",
      "=== epoch:43, train acc:0.41333333333333333, test acc:0.3415 ===\n",
      "train loss:2.0533532237708885\n",
      "train loss:2.086742259944952\n",
      "train loss:2.1344584588534414\n",
      "=== epoch:44, train acc:0.4533333333333333, test acc:0.3478 ===\n",
      "train loss:2.0512321151466897\n",
      "train loss:2.037815799621777\n",
      "train loss:2.049596410459815\n",
      "=== epoch:45, train acc:0.44, test acc:0.3447 ===\n",
      "train loss:2.105469909625949\n",
      "train loss:2.0524171526964223\n",
      "train loss:2.0601417597630385\n",
      "=== epoch:46, train acc:0.43666666666666665, test acc:0.3439 ===\n",
      "train loss:2.0981503857460364\n",
      "train loss:2.06237092889476\n",
      "train loss:2.0598684064174\n",
      "=== epoch:47, train acc:0.4633333333333333, test acc:0.351 ===\n",
      "train loss:2.027399715363854\n",
      "train loss:2.04160080389209\n",
      "train loss:2.01416795277841\n",
      "=== epoch:48, train acc:0.4533333333333333, test acc:0.3458 ===\n",
      "train loss:2.0283666549146924\n",
      "train loss:2.0467030577807934\n",
      "train loss:1.9973881002958476\n",
      "=== epoch:49, train acc:0.46, test acc:0.3523 ===\n",
      "train loss:2.0039743403424044\n",
      "train loss:2.066295705278144\n",
      "train loss:1.991652387384154\n",
      "=== epoch:50, train acc:0.4633333333333333, test acc:0.3658 ===\n",
      "train loss:2.0283537709628963\n",
      "train loss:2.0586031740830815\n",
      "train loss:1.9911143401824245\n",
      "=== epoch:51, train acc:0.4866666666666667, test acc:0.3789 ===\n",
      "train loss:1.9767693710032939\n",
      "train loss:2.0026433829797456\n",
      "train loss:2.0341105874606606\n",
      "=== epoch:52, train acc:0.5, test acc:0.3842 ===\n",
      "train loss:1.9778097228859353\n",
      "train loss:2.0434224045743967\n",
      "train loss:1.9628631575653828\n",
      "=== epoch:53, train acc:0.51, test acc:0.3911 ===\n",
      "train loss:1.9223105633308373\n",
      "train loss:2.017177047819587\n",
      "train loss:2.018043646922051\n",
      "=== epoch:54, train acc:0.5133333333333333, test acc:0.3993 ===\n",
      "train loss:1.9500777434306327\n",
      "train loss:2.006454204994922\n",
      "train loss:1.994693738127192\n",
      "=== epoch:55, train acc:0.53, test acc:0.4055 ===\n",
      "train loss:1.9607747816432832\n",
      "train loss:1.9704989768692176\n",
      "train loss:2.0194607794402977\n",
      "=== epoch:56, train acc:0.5533333333333333, test acc:0.4144 ===\n",
      "train loss:1.9168497112643124\n",
      "train loss:1.9566571207526704\n",
      "train loss:2.023822897081169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.5566666666666666, test acc:0.4157 ===\n",
      "train loss:1.9721562038951685\n",
      "train loss:1.9831237895919558\n",
      "train loss:1.9249488649550446\n",
      "=== epoch:58, train acc:0.5666666666666667, test acc:0.4169 ===\n",
      "train loss:1.9987866740320683\n",
      "train loss:1.929361675205635\n",
      "train loss:2.000839817672491\n",
      "=== epoch:59, train acc:0.5833333333333334, test acc:0.4346 ===\n",
      "train loss:1.973932005282387\n",
      "train loss:1.9374767791678238\n",
      "train loss:1.8406987348124373\n",
      "=== epoch:60, train acc:0.58, test acc:0.439 ===\n",
      "train loss:2.000701086821687\n",
      "train loss:1.8417411838556945\n",
      "train loss:1.8541279760966933\n",
      "=== epoch:61, train acc:0.5833333333333334, test acc:0.4464 ===\n",
      "train loss:1.9125640679186355\n",
      "train loss:1.950086794273136\n",
      "train loss:1.9126243624838575\n",
      "=== epoch:62, train acc:0.5933333333333334, test acc:0.4487 ===\n",
      "train loss:1.9855500172449785\n",
      "train loss:1.8776971627603078\n",
      "train loss:2.0099015276676626\n",
      "=== epoch:63, train acc:0.5966666666666667, test acc:0.4552 ===\n",
      "train loss:1.8876429114634632\n",
      "train loss:1.9001630210457259\n",
      "train loss:1.825306074543248\n",
      "=== epoch:64, train acc:0.6, test acc:0.4582 ===\n",
      "train loss:1.8609838352319994\n",
      "train loss:1.8965170921872896\n",
      "train loss:1.8332325460134584\n",
      "=== epoch:65, train acc:0.6133333333333333, test acc:0.4636 ===\n",
      "train loss:1.7812601088404358\n",
      "train loss:1.8974534385724366\n",
      "train loss:1.8250341488140238\n",
      "=== epoch:66, train acc:0.6133333333333333, test acc:0.4694 ===\n",
      "train loss:1.802202690342329\n",
      "train loss:1.808501577507559\n",
      "train loss:1.878816483128099\n",
      "=== epoch:67, train acc:0.6166666666666667, test acc:0.474 ===\n",
      "train loss:1.821700787325427\n",
      "train loss:1.8692206553303057\n",
      "train loss:1.8018492087461437\n",
      "=== epoch:68, train acc:0.6266666666666667, test acc:0.4753 ===\n",
      "train loss:1.8099422673718566\n",
      "train loss:1.8287160064209425\n",
      "train loss:1.8534570907437875\n",
      "=== epoch:69, train acc:0.6333333333333333, test acc:0.481 ===\n",
      "train loss:1.9452779008658876\n",
      "train loss:1.846584673256376\n",
      "train loss:1.7781442631176925\n",
      "=== epoch:70, train acc:0.6366666666666667, test acc:0.4911 ===\n",
      "train loss:1.749266573857681\n",
      "train loss:1.7657503888548953\n",
      "train loss:1.8162667885491928\n",
      "=== epoch:71, train acc:0.6466666666666666, test acc:0.4915 ===\n",
      "train loss:1.8771804540176507\n",
      "train loss:1.879317527286778\n",
      "train loss:1.8091056653249669\n",
      "=== epoch:72, train acc:0.6433333333333333, test acc:0.4964 ===\n",
      "train loss:1.8065523010572355\n",
      "train loss:1.7321570476260575\n",
      "train loss:1.7861011387392494\n",
      "=== epoch:73, train acc:0.6333333333333333, test acc:0.5047 ===\n",
      "train loss:1.7239072431208913\n",
      "train loss:1.7696910892494098\n",
      "train loss:1.7254181727443705\n",
      "=== epoch:74, train acc:0.6433333333333333, test acc:0.5038 ===\n",
      "train loss:1.6777487734147636\n",
      "train loss:1.7440732669206278\n",
      "train loss:1.753136771168056\n",
      "=== epoch:75, train acc:0.6533333333333333, test acc:0.5086 ===\n",
      "train loss:1.7251700424586658\n",
      "train loss:1.7155686987129248\n",
      "train loss:1.7498576292376302\n",
      "=== epoch:76, train acc:0.6566666666666666, test acc:0.5094 ===\n",
      "train loss:1.6150389995385888\n",
      "train loss:1.604624815695182\n",
      "train loss:1.7372650692117326\n",
      "=== epoch:77, train acc:0.6633333333333333, test acc:0.5102 ===\n",
      "train loss:1.7205789924865034\n",
      "train loss:1.6387920989117066\n",
      "train loss:1.6844148802388539\n",
      "=== epoch:78, train acc:0.6566666666666666, test acc:0.5067 ===\n",
      "train loss:1.7151046456104957\n",
      "train loss:1.5927156125740212\n",
      "train loss:1.6504976501065576\n",
      "=== epoch:79, train acc:0.66, test acc:0.5017 ===\n",
      "train loss:1.5541408636552114\n",
      "train loss:1.693129377879037\n",
      "train loss:1.6471899989793817\n",
      "=== epoch:80, train acc:0.66, test acc:0.5046 ===\n",
      "train loss:1.6782397422711173\n",
      "train loss:1.5655953744846618\n",
      "train loss:1.6412818251659607\n",
      "=== epoch:81, train acc:0.67, test acc:0.5144 ===\n",
      "train loss:1.583055976589565\n",
      "train loss:1.64280547010722\n",
      "train loss:1.582238768312077\n",
      "=== epoch:82, train acc:0.69, test acc:0.5221 ===\n",
      "train loss:1.6145857918527633\n",
      "train loss:1.4797368681641156\n",
      "train loss:1.598329006650105\n",
      "=== epoch:83, train acc:0.6966666666666667, test acc:0.5224 ===\n",
      "train loss:1.531476424333644\n",
      "train loss:1.5890966968520714\n",
      "train loss:1.5083759332574502\n",
      "=== epoch:84, train acc:0.7033333333333334, test acc:0.5188 ===\n",
      "train loss:1.646554812269387\n",
      "train loss:1.573197344738831\n",
      "train loss:1.4946957631631221\n",
      "=== epoch:85, train acc:0.7, test acc:0.5269 ===\n",
      "train loss:1.5390340248408036\n",
      "train loss:1.5975901788257278\n",
      "train loss:1.577103010274842\n",
      "=== epoch:86, train acc:0.6933333333333334, test acc:0.5342 ===\n",
      "train loss:1.5876816173756771\n",
      "train loss:1.4823276214057461\n",
      "train loss:1.5725928988773952\n",
      "=== epoch:87, train acc:0.6966666666666667, test acc:0.5433 ===\n",
      "train loss:1.583194925371978\n",
      "train loss:1.5453211314276305\n",
      "train loss:1.5407893214676998\n",
      "=== epoch:88, train acc:0.69, test acc:0.5462 ===\n",
      "train loss:1.5034946228593393\n",
      "train loss:1.4332620021675857\n",
      "train loss:1.4513092916073738\n",
      "=== epoch:89, train acc:0.68, test acc:0.544 ===\n",
      "train loss:1.5671411136595352\n",
      "train loss:1.4484973370031438\n",
      "train loss:1.531887920799126\n",
      "=== epoch:90, train acc:0.7033333333333334, test acc:0.5516 ===\n",
      "train loss:1.4598795104106914\n",
      "train loss:1.4698309582108129\n",
      "train loss:1.4752815401698731\n",
      "=== epoch:91, train acc:0.7066666666666667, test acc:0.5565 ===\n",
      "train loss:1.5236771420230675\n",
      "train loss:1.3904486915764378\n",
      "train loss:1.387796259429332\n",
      "=== epoch:92, train acc:0.71, test acc:0.5571 ===\n",
      "train loss:1.49151893352701\n",
      "train loss:1.48398431809724\n",
      "train loss:1.396134445444086\n",
      "=== epoch:93, train acc:0.7233333333333334, test acc:0.5674 ===\n",
      "train loss:1.4479403405714448\n",
      "train loss:1.421130218512689\n",
      "train loss:1.2977363382824163\n",
      "=== epoch:94, train acc:0.7166666666666667, test acc:0.5619 ===\n",
      "train loss:1.3541961071159379\n",
      "train loss:1.3084477059209112\n",
      "train loss:1.347213247832475\n",
      "=== epoch:95, train acc:0.7133333333333334, test acc:0.566 ===\n",
      "train loss:1.3757414574287565\n",
      "train loss:1.3353537141323886\n",
      "train loss:1.206984232514094\n",
      "=== epoch:96, train acc:0.73, test acc:0.5717 ===\n",
      "train loss:1.3934018217166064\n",
      "train loss:1.3141243791063182\n",
      "train loss:1.3269712153445077\n",
      "=== epoch:97, train acc:0.7333333333333333, test acc:0.5741 ===\n",
      "train loss:1.2986069588839633\n",
      "train loss:1.3039707982939424\n",
      "train loss:1.2857458088642089\n",
      "=== epoch:98, train acc:0.7366666666666667, test acc:0.5791 ===\n",
      "train loss:1.302838899210085\n",
      "train loss:1.2726860944843055\n",
      "train loss:1.2943622514922333\n",
      "=== epoch:99, train acc:0.74, test acc:0.5816 ===\n",
      "train loss:1.2538384468722823\n",
      "train loss:1.2884114175105827\n",
      "train loss:1.1058264463759209\n",
      "=== epoch:100, train acc:0.74, test acc:0.5808 ===\n",
      "train loss:1.3356074708317138\n",
      "train loss:1.319102653776705\n",
      "train loss:1.2123044335483557\n",
      "=== epoch:101, train acc:0.7466666666666667, test acc:0.5873 ===\n",
      "train loss:1.2101517596473959\n",
      "train loss:1.1305438836691706\n",
      "train loss:1.2671950535205283\n",
      "=== epoch:102, train acc:0.7533333333333333, test acc:0.5877 ===\n",
      "train loss:1.198045376898906\n",
      "train loss:1.1266987941813036\n",
      "train loss:1.1631002218963364\n",
      "=== epoch:103, train acc:0.7533333333333333, test acc:0.5977 ===\n",
      "train loss:1.2670228899092737\n",
      "train loss:1.1490931822932366\n",
      "train loss:1.1578501862072605\n",
      "=== epoch:104, train acc:0.76, test acc:0.6057 ===\n",
      "train loss:1.1396031173192664\n",
      "train loss:1.1901952140212353\n",
      "train loss:1.180812465046181\n",
      "=== epoch:105, train acc:0.7533333333333333, test acc:0.6073 ===\n",
      "train loss:1.038353966018775\n",
      "train loss:1.1298728882292783\n",
      "train loss:1.2219888326418122\n",
      "=== epoch:106, train acc:0.7533333333333333, test acc:0.6075 ===\n",
      "train loss:1.1552436330160267\n",
      "train loss:1.1991368834335177\n",
      "train loss:1.0667593034527518\n",
      "=== epoch:107, train acc:0.7666666666666667, test acc:0.6127 ===\n",
      "train loss:1.1886763077418436\n",
      "train loss:1.1872951322136354\n",
      "train loss:1.123853673580222\n",
      "=== epoch:108, train acc:0.7766666666666666, test acc:0.6172 ===\n",
      "train loss:0.9606848924379296\n",
      "train loss:1.1338854753675953\n",
      "train loss:1.0860508163563356\n",
      "=== epoch:109, train acc:0.7766666666666666, test acc:0.6171 ===\n",
      "train loss:0.9855647560741662\n",
      "train loss:1.0083980332611022\n",
      "train loss:1.1431922653109021\n",
      "=== epoch:110, train acc:0.7766666666666666, test acc:0.6161 ===\n",
      "train loss:1.1642094676987111\n",
      "train loss:1.0589737822781573\n",
      "train loss:0.9355160218592412\n",
      "=== epoch:111, train acc:0.7766666666666666, test acc:0.6184 ===\n",
      "train loss:0.9345701572644071\n",
      "train loss:1.029973054700705\n",
      "train loss:0.9501567832203287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.7766666666666666, test acc:0.6251 ===\n",
      "train loss:1.0890030922829073\n",
      "train loss:1.0035228391521682\n",
      "train loss:0.9110133998514475\n",
      "=== epoch:113, train acc:0.7833333333333333, test acc:0.6251 ===\n",
      "train loss:1.0500320898645155\n",
      "train loss:0.9052904373140686\n",
      "train loss:0.9918238062995709\n",
      "=== epoch:114, train acc:0.7866666666666666, test acc:0.6286 ===\n",
      "train loss:1.0058597085337555\n",
      "train loss:0.9719382211748507\n",
      "train loss:0.8695512975452963\n",
      "=== epoch:115, train acc:0.7933333333333333, test acc:0.6323 ===\n",
      "train loss:0.937137262392272\n",
      "train loss:0.962628665529667\n",
      "train loss:0.9420410394762888\n",
      "=== epoch:116, train acc:0.7933333333333333, test acc:0.6335 ===\n",
      "train loss:0.9139496799634164\n",
      "train loss:0.9841397565505181\n",
      "train loss:0.893294135343492\n",
      "=== epoch:117, train acc:0.8, test acc:0.6352 ===\n",
      "train loss:0.8902889073956183\n",
      "train loss:0.8583793642647641\n",
      "train loss:1.0255823885991873\n",
      "=== epoch:118, train acc:0.7966666666666666, test acc:0.6363 ===\n",
      "train loss:0.8937293306467883\n",
      "train loss:0.8589326076024073\n",
      "train loss:0.9251781822417804\n",
      "=== epoch:119, train acc:0.8, test acc:0.6403 ===\n",
      "train loss:0.9868012514650517\n",
      "train loss:0.9089774311551853\n",
      "train loss:0.8546447973401047\n",
      "=== epoch:120, train acc:0.8, test acc:0.6446 ===\n",
      "train loss:0.7677750521869819\n",
      "train loss:0.9516587560243375\n",
      "train loss:0.9515917566933865\n",
      "=== epoch:121, train acc:0.8, test acc:0.6438 ===\n",
      "train loss:0.8385862322444037\n",
      "train loss:0.8540074776984733\n",
      "train loss:0.8137802976853908\n",
      "=== epoch:122, train acc:0.8033333333333333, test acc:0.6436 ===\n",
      "train loss:0.664438476755531\n",
      "train loss:0.8406219383120593\n",
      "train loss:0.9233091317988462\n",
      "=== epoch:123, train acc:0.8133333333333334, test acc:0.6502 ===\n",
      "train loss:0.8653301131958413\n",
      "train loss:0.7398981747088684\n",
      "train loss:0.7939363333237166\n",
      "=== epoch:124, train acc:0.8133333333333334, test acc:0.6591 ===\n",
      "train loss:0.8329577236590306\n",
      "train loss:0.7460256538505083\n",
      "train loss:0.7750664346852829\n",
      "=== epoch:125, train acc:0.8233333333333334, test acc:0.6555 ===\n",
      "train loss:0.9066516390334103\n",
      "train loss:0.9103161976241505\n",
      "train loss:0.7242966934798888\n",
      "=== epoch:126, train acc:0.8166666666666667, test acc:0.6533 ===\n",
      "train loss:0.655183298740838\n",
      "train loss:0.8244573848848965\n",
      "train loss:0.7658673419762623\n",
      "=== epoch:127, train acc:0.8233333333333334, test acc:0.653 ===\n",
      "train loss:0.7253315240310415\n",
      "train loss:0.7643891388564934\n",
      "train loss:0.7186021468235502\n",
      "=== epoch:128, train acc:0.8233333333333334, test acc:0.651 ===\n",
      "train loss:0.9353225703701961\n",
      "train loss:0.7643161858716813\n",
      "train loss:0.7647478214836607\n",
      "=== epoch:129, train acc:0.8233333333333334, test acc:0.6534 ===\n",
      "train loss:0.8593101863481446\n",
      "train loss:0.8220071228285398\n",
      "train loss:0.7612616348479111\n",
      "=== epoch:130, train acc:0.8233333333333334, test acc:0.6528 ===\n",
      "train loss:0.6731284618116179\n",
      "train loss:0.75747621066513\n",
      "train loss:0.7865882748190804\n",
      "=== epoch:131, train acc:0.8333333333333334, test acc:0.6605 ===\n",
      "train loss:0.8804815892565513\n",
      "train loss:0.6582376854619554\n",
      "train loss:0.6146876938119017\n",
      "=== epoch:132, train acc:0.8266666666666667, test acc:0.6593 ===\n",
      "train loss:0.6428759458350143\n",
      "train loss:0.5389811181601682\n",
      "train loss:0.6710441111262675\n",
      "=== epoch:133, train acc:0.82, test acc:0.6616 ===\n",
      "train loss:0.7753589775185248\n",
      "train loss:0.5540974585985282\n",
      "train loss:0.6100905606442167\n",
      "=== epoch:134, train acc:0.83, test acc:0.6624 ===\n",
      "train loss:0.6568837215973028\n",
      "train loss:0.5749503545177888\n",
      "train loss:0.7711802999364592\n",
      "=== epoch:135, train acc:0.83, test acc:0.6627 ===\n",
      "train loss:0.631701328544024\n",
      "train loss:0.5920103660687862\n",
      "train loss:0.6210008446815514\n",
      "=== epoch:136, train acc:0.8366666666666667, test acc:0.6678 ===\n",
      "train loss:0.6096156673984635\n",
      "train loss:0.5553082004129845\n",
      "train loss:0.6954423651747015\n",
      "=== epoch:137, train acc:0.8333333333333334, test acc:0.6701 ===\n",
      "train loss:0.5866523937480126\n",
      "train loss:0.5545559129211205\n",
      "train loss:0.6213208065731999\n",
      "=== epoch:138, train acc:0.8366666666666667, test acc:0.6689 ===\n",
      "train loss:0.597370173637325\n",
      "train loss:0.6190745369584103\n",
      "train loss:0.6465800224720817\n",
      "=== epoch:139, train acc:0.8333333333333334, test acc:0.6683 ===\n",
      "train loss:0.7405140609090358\n",
      "train loss:0.7002878241326697\n",
      "train loss:0.6139495347868494\n",
      "=== epoch:140, train acc:0.8366666666666667, test acc:0.667 ===\n",
      "train loss:0.583687183445412\n",
      "train loss:0.696203098758028\n",
      "train loss:0.6533735658807979\n",
      "=== epoch:141, train acc:0.8433333333333334, test acc:0.6747 ===\n",
      "train loss:0.5318753995265396\n",
      "train loss:0.5952990020928511\n",
      "train loss:0.6003486752333967\n",
      "=== epoch:142, train acc:0.84, test acc:0.6685 ===\n",
      "train loss:0.7453471611696995\n",
      "train loss:0.5792504629688068\n",
      "train loss:0.4815265194754246\n",
      "=== epoch:143, train acc:0.8433333333333334, test acc:0.669 ===\n",
      "train loss:0.4960983991837597\n",
      "train loss:0.6450299008726295\n",
      "train loss:0.6120536830960532\n",
      "=== epoch:144, train acc:0.85, test acc:0.671 ===\n",
      "train loss:0.635511188230613\n",
      "train loss:0.612550868692479\n",
      "train loss:0.654827725577217\n",
      "=== epoch:145, train acc:0.86, test acc:0.6799 ===\n",
      "train loss:0.5734227550321803\n",
      "train loss:0.5566536525775291\n",
      "train loss:0.6195058031748345\n",
      "=== epoch:146, train acc:0.8533333333333334, test acc:0.6856 ===\n",
      "train loss:0.5795636417577549\n",
      "train loss:0.5491847474410405\n",
      "train loss:0.5006060272592049\n",
      "=== epoch:147, train acc:0.85, test acc:0.6822 ===\n",
      "train loss:0.4670273381804847\n",
      "train loss:0.6964649196917789\n",
      "train loss:0.49819429707268037\n",
      "=== epoch:148, train acc:0.8566666666666667, test acc:0.6857 ===\n",
      "train loss:0.5736392583160608\n",
      "train loss:0.5980144968702749\n",
      "train loss:0.5089115768588964\n",
      "=== epoch:149, train acc:0.86, test acc:0.687 ===\n",
      "train loss:0.5380381073006709\n",
      "train loss:0.5468153296164381\n",
      "train loss:0.5775739038443566\n",
      "=== epoch:150, train acc:0.86, test acc:0.6845 ===\n",
      "train loss:0.3820256749942546\n",
      "train loss:0.5488408458906374\n",
      "train loss:0.7430034375276868\n",
      "=== epoch:151, train acc:0.8633333333333333, test acc:0.6884 ===\n",
      "train loss:0.5253371841253066\n",
      "train loss:0.5025632579771115\n",
      "train loss:0.5921659592700902\n",
      "=== epoch:152, train acc:0.8666666666666667, test acc:0.6885 ===\n",
      "train loss:0.4448185315367615\n",
      "train loss:0.5383812319134497\n",
      "train loss:0.5271138227082361\n",
      "=== epoch:153, train acc:0.8666666666666667, test acc:0.6895 ===\n",
      "train loss:0.5412203450578992\n",
      "train loss:0.5043897266764403\n",
      "train loss:0.5591647827647142\n",
      "=== epoch:154, train acc:0.87, test acc:0.6919 ===\n",
      "train loss:0.5904391798997429\n",
      "train loss:0.482437019664887\n",
      "train loss:0.6700995120264486\n",
      "=== epoch:155, train acc:0.8666666666666667, test acc:0.6944 ===\n",
      "train loss:0.5622785055213826\n",
      "train loss:0.49969750614510017\n",
      "train loss:0.3518296434964632\n",
      "=== epoch:156, train acc:0.8733333333333333, test acc:0.6921 ===\n",
      "train loss:0.37189347743569895\n",
      "train loss:0.5281726486165771\n",
      "train loss:0.4096759554695582\n",
      "=== epoch:157, train acc:0.8766666666666667, test acc:0.6953 ===\n",
      "train loss:0.45112682209069194\n",
      "train loss:0.5003373416232721\n",
      "train loss:0.6269931735539052\n",
      "=== epoch:158, train acc:0.88, test acc:0.7018 ===\n",
      "train loss:0.5093421234381308\n",
      "train loss:0.548252797165669\n",
      "train loss:0.4341344538601853\n",
      "=== epoch:159, train acc:0.8766666666666667, test acc:0.6987 ===\n",
      "train loss:0.5454577473751214\n",
      "train loss:0.47379563701633737\n",
      "train loss:0.38165244526468206\n",
      "=== epoch:160, train acc:0.8766666666666667, test acc:0.6979 ===\n",
      "train loss:0.5106936284105201\n",
      "train loss:0.43318584995531567\n",
      "train loss:0.5089290547455244\n",
      "=== epoch:161, train acc:0.88, test acc:0.6965 ===\n",
      "train loss:0.5852294045164999\n",
      "train loss:0.4019460569380597\n",
      "train loss:0.38220339619535326\n",
      "=== epoch:162, train acc:0.8766666666666667, test acc:0.6969 ===\n",
      "train loss:0.5345931830975238\n",
      "train loss:0.5512533475452516\n",
      "train loss:0.6280666833984223\n",
      "=== epoch:163, train acc:0.8766666666666667, test acc:0.6975 ===\n",
      "train loss:0.6115921319402531\n",
      "train loss:0.5336315865818667\n",
      "train loss:0.41677699563521986\n",
      "=== epoch:164, train acc:0.8766666666666667, test acc:0.7003 ===\n",
      "train loss:0.4615033160516171\n",
      "train loss:0.3995064152900841\n",
      "train loss:0.477573832060774\n",
      "=== epoch:165, train acc:0.8866666666666667, test acc:0.7059 ===\n",
      "train loss:0.4747476521331556\n",
      "train loss:0.398344483766911\n",
      "train loss:0.44723107207283946\n",
      "=== epoch:166, train acc:0.8866666666666667, test acc:0.7075 ===\n",
      "train loss:0.600023633833381\n",
      "train loss:0.3878932894337983\n",
      "train loss:0.3782841041207945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.8766666666666667, test acc:0.7051 ===\n",
      "train loss:0.3562913190380809\n",
      "train loss:0.44151522590917547\n",
      "train loss:0.524248096218429\n",
      "=== epoch:168, train acc:0.8766666666666667, test acc:0.7032 ===\n",
      "train loss:0.4566745931973897\n",
      "train loss:0.5046605454591169\n",
      "train loss:0.327084520129558\n",
      "=== epoch:169, train acc:0.8833333333333333, test acc:0.7055 ===\n",
      "train loss:0.4514508852399375\n",
      "train loss:0.42001646791621206\n",
      "train loss:0.3251089408047702\n",
      "=== epoch:170, train acc:0.88, test acc:0.7028 ===\n",
      "train loss:0.4210808354792249\n",
      "train loss:0.30821991259147974\n",
      "train loss:0.37709539835732925\n",
      "=== epoch:171, train acc:0.8833333333333333, test acc:0.707 ===\n",
      "train loss:0.32386604128962626\n",
      "train loss:0.4318487394349312\n",
      "train loss:0.3518430207159878\n",
      "=== epoch:172, train acc:0.8933333333333333, test acc:0.707 ===\n",
      "train loss:0.448123243288776\n",
      "train loss:0.4003471315834391\n",
      "train loss:0.45217807954375333\n",
      "=== epoch:173, train acc:0.89, test acc:0.7118 ===\n",
      "train loss:0.43224531604201916\n",
      "train loss:0.4203703924971432\n",
      "train loss:0.3860015932392718\n",
      "=== epoch:174, train acc:0.8966666666666666, test acc:0.7153 ===\n",
      "train loss:0.5459943975873413\n",
      "train loss:0.3543154889251571\n",
      "train loss:0.3831171722121461\n",
      "=== epoch:175, train acc:0.89, test acc:0.7156 ===\n",
      "train loss:0.3126486192968705\n",
      "train loss:0.4603282534406752\n",
      "train loss:0.4064853629391348\n",
      "=== epoch:176, train acc:0.8933333333333333, test acc:0.7134 ===\n",
      "train loss:0.40383576518974423\n",
      "train loss:0.4380860464293766\n",
      "train loss:0.37008923644619707\n",
      "=== epoch:177, train acc:0.8966666666666666, test acc:0.7116 ===\n",
      "train loss:0.3878436467348211\n",
      "train loss:0.4664181026233777\n",
      "train loss:0.37726925245063214\n",
      "=== epoch:178, train acc:0.8966666666666666, test acc:0.7096 ===\n",
      "train loss:0.42968407112419854\n",
      "train loss:0.4576789197918136\n",
      "train loss:0.4227078404043052\n",
      "=== epoch:179, train acc:0.89, test acc:0.7109 ===\n",
      "train loss:0.45345822883900977\n",
      "train loss:0.32685542755570596\n",
      "train loss:0.3565319861742166\n",
      "=== epoch:180, train acc:0.8933333333333333, test acc:0.7116 ===\n",
      "train loss:0.3022540159932102\n",
      "train loss:0.41610581616194686\n",
      "train loss:0.40162879181312283\n",
      "=== epoch:181, train acc:0.8866666666666667, test acc:0.7105 ===\n",
      "train loss:0.39894187101326006\n",
      "train loss:0.33697901500890765\n",
      "train loss:0.4030901068489303\n",
      "=== epoch:182, train acc:0.8966666666666666, test acc:0.707 ===\n",
      "train loss:0.35564394814814887\n",
      "train loss:0.3778917596830248\n",
      "train loss:0.30273475090910507\n",
      "=== epoch:183, train acc:0.8933333333333333, test acc:0.7091 ===\n",
      "train loss:0.3548134478595079\n",
      "train loss:0.4113196886649989\n",
      "train loss:0.28332287525691413\n",
      "=== epoch:184, train acc:0.8966666666666666, test acc:0.7176 ===\n",
      "train loss:0.28450973847032957\n",
      "train loss:0.3724887118876268\n",
      "train loss:0.31955864149904006\n",
      "=== epoch:185, train acc:0.8966666666666666, test acc:0.7193 ===\n",
      "train loss:0.41313058614600157\n",
      "train loss:0.2716309309366766\n",
      "train loss:0.3040519905116495\n",
      "=== epoch:186, train acc:0.9066666666666666, test acc:0.7191 ===\n",
      "train loss:0.3121712405732873\n",
      "train loss:0.36398450042886277\n",
      "train loss:0.2888472534179611\n",
      "=== epoch:187, train acc:0.91, test acc:0.7217 ===\n",
      "train loss:0.33943488234734503\n",
      "train loss:0.3744811626080731\n",
      "train loss:0.3367118626215916\n",
      "=== epoch:188, train acc:0.9033333333333333, test acc:0.7172 ===\n",
      "train loss:0.4697912029471013\n",
      "train loss:0.3193703832913469\n",
      "train loss:0.27139530727604844\n",
      "=== epoch:189, train acc:0.9, test acc:0.7217 ===\n",
      "train loss:0.3366488714660835\n",
      "train loss:0.29327754812999607\n",
      "train loss:0.3595112172885447\n",
      "=== epoch:190, train acc:0.9033333333333333, test acc:0.7185 ===\n",
      "train loss:0.2670021693569596\n",
      "train loss:0.31170942372893684\n",
      "train loss:0.39045899921288557\n",
      "=== epoch:191, train acc:0.91, test acc:0.7127 ===\n",
      "train loss:0.2852228240771037\n",
      "train loss:0.35623830718200866\n",
      "train loss:0.36830007965936284\n",
      "=== epoch:192, train acc:0.9133333333333333, test acc:0.7187 ===\n",
      "train loss:0.2692127867165659\n",
      "train loss:0.3574315980656331\n",
      "train loss:0.35017905201351823\n",
      "=== epoch:193, train acc:0.92, test acc:0.7221 ===\n",
      "train loss:0.44907040146009486\n",
      "train loss:0.2961521847204532\n",
      "train loss:0.35125074319213373\n",
      "=== epoch:194, train acc:0.9166666666666666, test acc:0.721 ===\n",
      "train loss:0.26832654241651804\n",
      "train loss:0.3378583089535842\n",
      "train loss:0.30384089927529007\n",
      "=== epoch:195, train acc:0.9166666666666666, test acc:0.7266 ===\n",
      "train loss:0.4334022014571122\n",
      "train loss:0.24117012008338087\n",
      "train loss:0.2904049329039599\n",
      "=== epoch:196, train acc:0.9233333333333333, test acc:0.7321 ===\n",
      "train loss:0.3403738667982195\n",
      "train loss:0.3169310497740884\n",
      "train loss:0.28725919298168173\n",
      "=== epoch:197, train acc:0.9266666666666666, test acc:0.7287 ===\n",
      "train loss:0.26060709053186815\n",
      "train loss:0.22489165512223125\n",
      "train loss:0.21934678396559343\n",
      "=== epoch:198, train acc:0.9133333333333333, test acc:0.7289 ===\n",
      "train loss:0.321155716381516\n",
      "train loss:0.3090857202472624\n",
      "train loss:0.3743670958713718\n",
      "=== epoch:199, train acc:0.92, test acc:0.7293 ===\n",
      "train loss:0.26305019432654314\n",
      "train loss:0.28634888516836626\n",
      "train loss:0.21336471897740933\n",
      "=== epoch:200, train acc:0.92, test acc:0.7266 ===\n",
      "train loss:0.3414233600342316\n",
      "train loss:0.2684553279726481\n",
      "train loss:0.2335385329030648\n",
      "=== epoch:201, train acc:0.9166666666666666, test acc:0.7283 ===\n",
      "train loss:0.23732587505868175\n",
      "train loss:0.31384190946903756\n",
      "train loss:0.331068479748481\n",
      "=== epoch:202, train acc:0.9266666666666666, test acc:0.7281 ===\n",
      "train loss:0.30570878183014033\n",
      "train loss:0.2998062077032113\n",
      "train loss:0.2920046286928063\n",
      "=== epoch:203, train acc:0.9266666666666666, test acc:0.731 ===\n",
      "train loss:0.186265941968274\n",
      "train loss:0.38078322999520803\n",
      "train loss:0.30439914370602916\n",
      "=== epoch:204, train acc:0.9266666666666666, test acc:0.7295 ===\n",
      "train loss:0.26487269570264216\n",
      "train loss:0.21588075236037582\n",
      "train loss:0.3298007696503245\n",
      "=== epoch:205, train acc:0.93, test acc:0.7309 ===\n",
      "train loss:0.341585672000251\n",
      "train loss:0.3460265683327349\n",
      "train loss:0.3245520174746007\n",
      "=== epoch:206, train acc:0.9233333333333333, test acc:0.7291 ===\n",
      "train loss:0.2698935624328511\n",
      "train loss:0.18956865379966475\n",
      "train loss:0.2888209216143328\n",
      "=== epoch:207, train acc:0.9333333333333333, test acc:0.7324 ===\n",
      "train loss:0.2667065591479961\n",
      "train loss:0.32870745878462837\n",
      "train loss:0.28548986665206033\n",
      "=== epoch:208, train acc:0.93, test acc:0.7339 ===\n",
      "train loss:0.27115194681761184\n",
      "train loss:0.2809987303990017\n",
      "train loss:0.25055336639135645\n",
      "=== epoch:209, train acc:0.9333333333333333, test acc:0.7332 ===\n",
      "train loss:0.2934456991058031\n",
      "train loss:0.2521022984076552\n",
      "train loss:0.21353937691636266\n",
      "=== epoch:210, train acc:0.9366666666666666, test acc:0.7336 ===\n",
      "train loss:0.3094416072540507\n",
      "train loss:0.2849192862566676\n",
      "train loss:0.2837667380224702\n",
      "=== epoch:211, train acc:0.94, test acc:0.7347 ===\n",
      "train loss:0.215857022780644\n",
      "train loss:0.16717098096069902\n",
      "train loss:0.2122250497529906\n",
      "=== epoch:212, train acc:0.9366666666666666, test acc:0.7386 ===\n",
      "train loss:0.2119072459095473\n",
      "train loss:0.24393206350913196\n",
      "train loss:0.2649277242861019\n",
      "=== epoch:213, train acc:0.9466666666666667, test acc:0.7395 ===\n",
      "train loss:0.22720114247761602\n",
      "train loss:0.16999372421897926\n",
      "train loss:0.24670954576016935\n",
      "=== epoch:214, train acc:0.9433333333333334, test acc:0.7385 ===\n",
      "train loss:0.3270696266799664\n",
      "train loss:0.19782575450544435\n",
      "train loss:0.2828579825069614\n",
      "=== epoch:215, train acc:0.9433333333333334, test acc:0.7379 ===\n",
      "train loss:0.22077795145132856\n",
      "train loss:0.1630071123983612\n",
      "train loss:0.19849498053099934\n",
      "=== epoch:216, train acc:0.95, test acc:0.7411 ===\n",
      "train loss:0.2866566580119868\n",
      "train loss:0.17916903599450273\n",
      "train loss:0.2086510414270055\n",
      "=== epoch:217, train acc:0.9466666666666667, test acc:0.7391 ===\n",
      "train loss:0.18544509717659952\n",
      "train loss:0.2585688978919371\n",
      "train loss:0.23504890037760426\n",
      "=== epoch:218, train acc:0.9466666666666667, test acc:0.7412 ===\n",
      "train loss:0.24861829868286375\n",
      "train loss:0.29921749962679384\n",
      "train loss:0.19142413552905047\n",
      "=== epoch:219, train acc:0.9433333333333334, test acc:0.7425 ===\n",
      "train loss:0.21463677891002572\n",
      "train loss:0.24167063393019472\n",
      "train loss:0.22609892142928206\n",
      "=== epoch:220, train acc:0.9433333333333334, test acc:0.7433 ===\n",
      "train loss:0.207197912146232\n",
      "train loss:0.2605493077328009\n",
      "train loss:0.27990574356914766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:221, train acc:0.95, test acc:0.7436 ===\n",
      "train loss:0.2271276964277159\n",
      "train loss:0.18655909850646868\n",
      "train loss:0.23835464321562724\n",
      "=== epoch:222, train acc:0.9533333333333334, test acc:0.7459 ===\n",
      "train loss:0.17816765786780828\n",
      "train loss:0.1947044701511259\n",
      "train loss:0.19624099282561439\n",
      "=== epoch:223, train acc:0.96, test acc:0.7466 ===\n",
      "train loss:0.21858868642913012\n",
      "train loss:0.15983837995678646\n",
      "train loss:0.17408571338961681\n",
      "=== epoch:224, train acc:0.96, test acc:0.7467 ===\n",
      "train loss:0.13903908868145978\n",
      "train loss:0.24917512807371453\n",
      "train loss:0.14552743883821867\n",
      "=== epoch:225, train acc:0.96, test acc:0.7426 ===\n",
      "train loss:0.20348761741265448\n",
      "train loss:0.24980619701078915\n",
      "train loss:0.15982454720589184\n",
      "=== epoch:226, train acc:0.95, test acc:0.7421 ===\n",
      "train loss:0.2426850875297005\n",
      "train loss:0.20134773976706338\n",
      "train loss:0.21605980760554658\n",
      "=== epoch:227, train acc:0.95, test acc:0.7449 ===\n",
      "train loss:0.14406161120697403\n",
      "train loss:0.2260123021769342\n",
      "train loss:0.16040793887406374\n",
      "=== epoch:228, train acc:0.9566666666666667, test acc:0.7419 ===\n",
      "train loss:0.20404795969117945\n",
      "train loss:0.2211347277432554\n",
      "train loss:0.2283407767755891\n",
      "=== epoch:229, train acc:0.9533333333333334, test acc:0.7426 ===\n",
      "train loss:0.12842225592803824\n",
      "train loss:0.1384336740668977\n",
      "train loss:0.17997003901065742\n",
      "=== epoch:230, train acc:0.9533333333333334, test acc:0.7454 ===\n",
      "train loss:0.17280480554280683\n",
      "train loss:0.1797225850385656\n",
      "train loss:0.1881366694849984\n",
      "=== epoch:231, train acc:0.9566666666666667, test acc:0.7438 ===\n",
      "train loss:0.17596390026038866\n",
      "train loss:0.1572932564718338\n",
      "train loss:0.13129735060416478\n",
      "=== epoch:232, train acc:0.9566666666666667, test acc:0.7461 ===\n",
      "train loss:0.1649365353816248\n",
      "train loss:0.2519495536288616\n",
      "train loss:0.20561249379468244\n",
      "=== epoch:233, train acc:0.9566666666666667, test acc:0.7433 ===\n",
      "train loss:0.1348551660861858\n",
      "train loss:0.12078777109577464\n",
      "train loss:0.18740363444422797\n",
      "=== epoch:234, train acc:0.9566666666666667, test acc:0.7441 ===\n",
      "train loss:0.16245391011395685\n",
      "train loss:0.17337329368736001\n",
      "train loss:0.1492043987438627\n",
      "=== epoch:235, train acc:0.9566666666666667, test acc:0.7431 ===\n",
      "train loss:0.1598406594548851\n",
      "train loss:0.1949773084017518\n",
      "train loss:0.2074233029899124\n",
      "=== epoch:236, train acc:0.9566666666666667, test acc:0.7452 ===\n",
      "train loss:0.16478627852518385\n",
      "train loss:0.22270913432404138\n",
      "train loss:0.1326949823286314\n",
      "=== epoch:237, train acc:0.9566666666666667, test acc:0.7438 ===\n",
      "train loss:0.11879926820649736\n",
      "train loss:0.2266651256485227\n",
      "train loss:0.18478820218386902\n",
      "=== epoch:238, train acc:0.9666666666666667, test acc:0.7503 ===\n",
      "train loss:0.14092263572917454\n",
      "train loss:0.14055909865203467\n",
      "train loss:0.12868168345487357\n",
      "=== epoch:239, train acc:0.9633333333333334, test acc:0.7493 ===\n",
      "train loss:0.14543491662033345\n",
      "train loss:0.1245118252256099\n",
      "train loss:0.13099742504679512\n",
      "=== epoch:240, train acc:0.9666666666666667, test acc:0.7485 ===\n",
      "train loss:0.22507858713665765\n",
      "train loss:0.11262372498444345\n",
      "train loss:0.19686982393374589\n",
      "=== epoch:241, train acc:0.9666666666666667, test acc:0.7497 ===\n",
      "train loss:0.15912651614139717\n",
      "train loss:0.1931271551304609\n",
      "train loss:0.11830671150812928\n",
      "=== epoch:242, train acc:0.97, test acc:0.7502 ===\n",
      "train loss:0.12460813925039017\n",
      "train loss:0.14293650384844483\n",
      "train loss:0.20496144758756066\n",
      "=== epoch:243, train acc:0.97, test acc:0.7537 ===\n",
      "train loss:0.12480236643254877\n",
      "train loss:0.15920219496364077\n",
      "train loss:0.14402096743764092\n",
      "=== epoch:244, train acc:0.97, test acc:0.7543 ===\n",
      "train loss:0.12690602305992627\n",
      "train loss:0.11127734101412465\n",
      "train loss:0.19674215710914317\n",
      "=== epoch:245, train acc:0.97, test acc:0.7565 ===\n",
      "train loss:0.14945542872186926\n",
      "train loss:0.10953191191498649\n",
      "train loss:0.15705586222046386\n",
      "=== epoch:246, train acc:0.9733333333333334, test acc:0.7535 ===\n",
      "train loss:0.17291065525631816\n",
      "train loss:0.17242549726247142\n",
      "train loss:0.15083641047624063\n",
      "=== epoch:247, train acc:0.9733333333333334, test acc:0.755 ===\n",
      "train loss:0.13778516144522748\n",
      "train loss:0.17427405256833522\n",
      "train loss:0.12345732847027742\n",
      "=== epoch:248, train acc:0.9733333333333334, test acc:0.7552 ===\n",
      "train loss:0.13635269046185683\n",
      "train loss:0.11510984628203862\n",
      "train loss:0.11010371205449537\n",
      "=== epoch:249, train acc:0.97, test acc:0.7551 ===\n",
      "train loss:0.15791512006331263\n",
      "train loss:0.10292316909595246\n",
      "train loss:0.1700790560723622\n",
      "=== epoch:250, train acc:0.97, test acc:0.7532 ===\n",
      "train loss:0.1782573651135888\n",
      "train loss:0.12462168927270399\n",
      "train loss:0.06892099215425056\n",
      "=== epoch:251, train acc:0.97, test acc:0.7534 ===\n",
      "train loss:0.15432864128443544\n",
      "train loss:0.1346954267990259\n",
      "train loss:0.16210396305293778\n",
      "=== epoch:252, train acc:0.9766666666666667, test acc:0.7579 ===\n",
      "train loss:0.11862870585477925\n",
      "train loss:0.1468038854488039\n",
      "train loss:0.12380475263700479\n",
      "=== epoch:253, train acc:0.9766666666666667, test acc:0.7568 ===\n",
      "train loss:0.08002957021298356\n",
      "train loss:0.13825520811445183\n",
      "train loss:0.08213348294838044\n",
      "=== epoch:254, train acc:0.9766666666666667, test acc:0.7579 ===\n",
      "train loss:0.10793703147071698\n",
      "train loss:0.09988498730389353\n",
      "train loss:0.1645720634845032\n",
      "=== epoch:255, train acc:0.9766666666666667, test acc:0.7557 ===\n",
      "train loss:0.09206592108435047\n",
      "train loss:0.11640372286416766\n",
      "train loss:0.08656317687498101\n",
      "=== epoch:256, train acc:0.9733333333333334, test acc:0.7534 ===\n",
      "train loss:0.16658340548616213\n",
      "train loss:0.08684851521658697\n",
      "train loss:0.08923912404838248\n",
      "=== epoch:257, train acc:0.9733333333333334, test acc:0.7536 ===\n",
      "train loss:0.131273893564967\n",
      "train loss:0.12943599311273254\n",
      "train loss:0.08554953130817677\n",
      "=== epoch:258, train acc:0.9766666666666667, test acc:0.7567 ===\n",
      "train loss:0.10879640831780674\n",
      "train loss:0.17626735517307537\n",
      "train loss:0.09640760936562312\n",
      "=== epoch:259, train acc:0.9766666666666667, test acc:0.7576 ===\n",
      "train loss:0.10342089796707159\n",
      "train loss:0.15176265671102765\n",
      "train loss:0.06902548129614045\n",
      "=== epoch:260, train acc:0.9766666666666667, test acc:0.7595 ===\n",
      "train loss:0.11874499020033062\n",
      "train loss:0.1235664252403224\n",
      "train loss:0.08168558061636658\n",
      "=== epoch:261, train acc:0.9766666666666667, test acc:0.7603 ===\n",
      "train loss:0.09124745857012354\n",
      "train loss:0.09966552730040629\n",
      "train loss:0.13329013191781874\n",
      "=== epoch:262, train acc:0.9766666666666667, test acc:0.7585 ===\n",
      "train loss:0.1455517400446086\n",
      "train loss:0.1207856503805544\n",
      "train loss:0.15135454997446832\n",
      "=== epoch:263, train acc:0.9766666666666667, test acc:0.7598 ===\n",
      "train loss:0.1520612810677011\n",
      "train loss:0.10975815964429779\n",
      "train loss:0.1944851611590102\n",
      "=== epoch:264, train acc:0.9766666666666667, test acc:0.7644 ===\n",
      "train loss:0.10016488663849575\n",
      "train loss:0.07586616803597088\n",
      "train loss:0.12738932523647228\n",
      "=== epoch:265, train acc:0.9766666666666667, test acc:0.7649 ===\n",
      "train loss:0.10901311463810304\n",
      "train loss:0.15522507332994395\n",
      "train loss:0.11855235822156113\n",
      "=== epoch:266, train acc:0.9766666666666667, test acc:0.7627 ===\n",
      "train loss:0.13509554050636505\n",
      "train loss:0.07628274437364618\n",
      "train loss:0.12041324627927195\n",
      "=== epoch:267, train acc:0.9766666666666667, test acc:0.7617 ===\n",
      "train loss:0.0685259095360872\n",
      "train loss:0.11263301699009393\n",
      "train loss:0.10333573237607804\n",
      "=== epoch:268, train acc:0.9766666666666667, test acc:0.7609 ===\n",
      "train loss:0.13822977036676407\n",
      "train loss:0.07093680633171029\n",
      "train loss:0.1032184930180701\n",
      "=== epoch:269, train acc:0.9766666666666667, test acc:0.7637 ===\n",
      "train loss:0.08359807002462331\n",
      "train loss:0.07910561430955201\n",
      "train loss:0.12487605410987589\n",
      "=== epoch:270, train acc:0.9766666666666667, test acc:0.7613 ===\n",
      "train loss:0.057277458758252235\n",
      "train loss:0.09741414025397457\n",
      "train loss:0.10448245673023913\n",
      "=== epoch:271, train acc:0.98, test acc:0.7642 ===\n",
      "train loss:0.059630311042018776\n",
      "train loss:0.0980496264157331\n",
      "train loss:0.08981113257648021\n",
      "=== epoch:272, train acc:0.9766666666666667, test acc:0.762 ===\n",
      "train loss:0.09422291623157733\n",
      "train loss:0.10239041534737849\n",
      "train loss:0.09527250939271738\n",
      "=== epoch:273, train acc:0.98, test acc:0.7632 ===\n",
      "train loss:0.10263603634933129\n",
      "train loss:0.05622545055781213\n",
      "train loss:0.09169707841857759\n",
      "=== epoch:274, train acc:0.9766666666666667, test acc:0.7614 ===\n",
      "train loss:0.09144234831367486\n",
      "train loss:0.06575422366214177\n",
      "train loss:0.09836021828767731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:275, train acc:0.9766666666666667, test acc:0.7672 ===\n",
      "train loss:0.06369305613702442\n",
      "train loss:0.05001647875081544\n",
      "train loss:0.11458420227072402\n",
      "=== epoch:276, train acc:0.9766666666666667, test acc:0.7669 ===\n",
      "train loss:0.07236331187954331\n",
      "train loss:0.13737500289632398\n",
      "train loss:0.11724276418711643\n",
      "=== epoch:277, train acc:0.9833333333333333, test acc:0.769 ===\n",
      "train loss:0.09473087589056853\n",
      "train loss:0.09456787329578048\n",
      "train loss:0.09085356604693154\n",
      "=== epoch:278, train acc:0.98, test acc:0.7664 ===\n",
      "train loss:0.08779360451016645\n",
      "train loss:0.058643266613138255\n",
      "train loss:0.13507327073429543\n",
      "=== epoch:279, train acc:0.9833333333333333, test acc:0.7676 ===\n",
      "train loss:0.10608131389104056\n",
      "train loss:0.11740214211224907\n",
      "train loss:0.07551924787853667\n",
      "=== epoch:280, train acc:0.9833333333333333, test acc:0.7666 ===\n",
      "train loss:0.06539642731794604\n",
      "train loss:0.07028589425900271\n",
      "train loss:0.06739836360786086\n",
      "=== epoch:281, train acc:0.9833333333333333, test acc:0.7691 ===\n",
      "train loss:0.1479921432537477\n",
      "train loss:0.10183846721057735\n",
      "train loss:0.0689563993921679\n",
      "=== epoch:282, train acc:0.9833333333333333, test acc:0.7703 ===\n",
      "train loss:0.1139740116313339\n",
      "train loss:0.07102635274196317\n",
      "train loss:0.08850732253045085\n",
      "=== epoch:283, train acc:0.98, test acc:0.768 ===\n",
      "train loss:0.061526133593022996\n",
      "train loss:0.07619890549392742\n",
      "train loss:0.06539386649157233\n",
      "=== epoch:284, train acc:0.9766666666666667, test acc:0.7648 ===\n",
      "train loss:0.06851425170584965\n",
      "train loss:0.07248978989147514\n",
      "train loss:0.08597386254608345\n",
      "=== epoch:285, train acc:0.9833333333333333, test acc:0.7681 ===\n",
      "train loss:0.08164988726462384\n",
      "train loss:0.04439732628230582\n",
      "train loss:0.06687785926796262\n",
      "=== epoch:286, train acc:0.98, test acc:0.7656 ===\n",
      "train loss:0.08412839804033849\n",
      "train loss:0.06532930956179776\n",
      "train loss:0.048708241267070226\n",
      "=== epoch:287, train acc:0.9833333333333333, test acc:0.7664 ===\n",
      "train loss:0.07079366803990389\n",
      "train loss:0.07529241455438564\n",
      "train loss:0.09028574643672442\n",
      "=== epoch:288, train acc:0.9833333333333333, test acc:0.7674 ===\n",
      "train loss:0.07328159022682405\n",
      "train loss:0.06391708914408842\n",
      "train loss:0.10160064482236013\n",
      "=== epoch:289, train acc:0.9833333333333333, test acc:0.767 ===\n",
      "train loss:0.05035522526795684\n",
      "train loss:0.09210709036814192\n",
      "train loss:0.08434478121071884\n",
      "=== epoch:290, train acc:0.99, test acc:0.7676 ===\n",
      "train loss:0.06868255158283809\n",
      "train loss:0.0749864629778736\n",
      "train loss:0.08200683146058958\n",
      "=== epoch:291, train acc:0.99, test acc:0.769 ===\n",
      "train loss:0.05256483315431068\n",
      "train loss:0.09990863306550202\n",
      "train loss:0.08098705603316964\n",
      "=== epoch:292, train acc:0.99, test acc:0.7716 ===\n",
      "train loss:0.10136035205525191\n",
      "train loss:0.047712717802691554\n",
      "train loss:0.0765037857004578\n",
      "=== epoch:293, train acc:0.99, test acc:0.7717 ===\n",
      "train loss:0.10577902894146617\n",
      "train loss:0.06639087016348331\n",
      "train loss:0.1079091991529123\n",
      "=== epoch:294, train acc:0.9833333333333333, test acc:0.7682 ===\n",
      "train loss:0.07196889871490737\n",
      "train loss:0.06809919105575729\n",
      "train loss:0.08185918895577533\n",
      "=== epoch:295, train acc:0.9833333333333333, test acc:0.7699 ===\n",
      "train loss:0.08977636423334893\n",
      "train loss:0.0865834292523031\n",
      "train loss:0.09901197667913182\n",
      "=== epoch:296, train acc:0.99, test acc:0.7722 ===\n",
      "train loss:0.05866753093611084\n",
      "train loss:0.08191150583720364\n",
      "train loss:0.06340839358215056\n",
      "=== epoch:297, train acc:0.99, test acc:0.7709 ===\n",
      "train loss:0.05320435024241235\n",
      "train loss:0.05825204871594305\n",
      "train loss:0.05616506582436518\n",
      "=== epoch:298, train acc:0.99, test acc:0.7735 ===\n",
      "train loss:0.07145344548443719\n",
      "train loss:0.038649999427917754\n",
      "train loss:0.0666227300161927\n",
      "=== epoch:299, train acc:0.99, test acc:0.7751 ===\n",
      "train loss:0.05978505846713872\n",
      "train loss:0.06825186792925869\n",
      "train loss:0.045503955874571254\n",
      "=== epoch:300, train acc:0.99, test acc:0.7744 ===\n",
      "train loss:0.06072517265334643\n",
      "train loss:0.061660115182687186\n",
      "train loss:0.04851051362601647\n",
      "=== epoch:301, train acc:0.99, test acc:0.7726 ===\n",
      "train loss:0.06938849631705087\n",
      "train loss:0.08191049921151389\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7726\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.1\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f325f4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0810cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2959210647840487\n",
      "=== epoch:1, train acc:0.06, test acc:0.0634 ===\n",
      "train loss:2.310403273288428\n",
      "train loss:2.309717111159578\n",
      "train loss:2.301306025939381\n",
      "=== epoch:2, train acc:0.06333333333333334, test acc:0.0656 ===\n",
      "train loss:2.3171689045428097\n",
      "train loss:2.3135769199630234\n",
      "train loss:2.2903067130252794\n",
      "=== epoch:3, train acc:0.06333333333333334, test acc:0.0672 ===\n",
      "train loss:2.301512804903795\n",
      "train loss:2.2937438779694914\n",
      "train loss:2.301702891448014\n",
      "=== epoch:4, train acc:0.06333333333333334, test acc:0.0664 ===\n",
      "train loss:2.303317523288316\n",
      "train loss:2.3028910463682273\n",
      "train loss:2.303676819906938\n",
      "=== epoch:5, train acc:0.06333333333333334, test acc:0.0681 ===\n",
      "train loss:2.302550624868801\n",
      "train loss:2.295723532825746\n",
      "train loss:2.2982014673736275\n",
      "=== epoch:6, train acc:0.06666666666666667, test acc:0.0702 ===\n",
      "train loss:2.313798894624564\n",
      "train loss:2.294495453289609\n",
      "train loss:2.3117153786873157\n",
      "=== epoch:7, train acc:0.06666666666666667, test acc:0.0705 ===\n",
      "train loss:2.303076107896964\n",
      "train loss:2.302335652390609\n",
      "train loss:2.307486675497276\n",
      "=== epoch:8, train acc:0.07333333333333333, test acc:0.0728 ===\n",
      "train loss:2.30207814269406\n",
      "train loss:2.3064332658669406\n",
      "train loss:2.3101012085344426\n",
      "=== epoch:9, train acc:0.07, test acc:0.0718 ===\n",
      "train loss:2.30435211762618\n",
      "train loss:2.2990072976850344\n",
      "train loss:2.3052742729046694\n",
      "=== epoch:10, train acc:0.06333333333333334, test acc:0.0711 ===\n",
      "train loss:2.2964635537156983\n",
      "train loss:2.309586249461594\n",
      "train loss:2.29427422609366\n",
      "=== epoch:11, train acc:0.06333333333333334, test acc:0.07 ===\n",
      "train loss:2.3030987108661143\n",
      "train loss:2.3036821661675106\n",
      "train loss:2.3015939926851767\n",
      "=== epoch:12, train acc:0.06333333333333334, test acc:0.0701 ===\n",
      "train loss:2.3004389880362415\n",
      "train loss:2.3005788082790493\n",
      "train loss:2.2964507657961795\n",
      "=== epoch:13, train acc:0.07, test acc:0.0739 ===\n",
      "train loss:2.305490104836701\n",
      "train loss:2.292263905348486\n",
      "train loss:2.2894203875093293\n",
      "=== epoch:14, train acc:0.07333333333333333, test acc:0.0762 ===\n",
      "train loss:2.3036854860613123\n",
      "train loss:2.3003223066392073\n",
      "train loss:2.302878949206849\n",
      "=== epoch:15, train acc:0.08, test acc:0.0815 ===\n",
      "train loss:2.2952250945305783\n",
      "train loss:2.304812085738685\n",
      "train loss:2.2999992672842655\n",
      "=== epoch:16, train acc:0.07666666666666666, test acc:0.0821 ===\n",
      "train loss:2.296599754782113\n",
      "train loss:2.303393721090042\n",
      "train loss:2.288804220478623\n",
      "=== epoch:17, train acc:0.08666666666666667, test acc:0.0864 ===\n",
      "train loss:2.2943853906477383\n",
      "train loss:2.300221984296144\n",
      "train loss:2.2996628053657315\n",
      "=== epoch:18, train acc:0.08666666666666667, test acc:0.089 ===\n",
      "train loss:2.2967302043868667\n",
      "train loss:2.298903666724759\n",
      "train loss:2.28970980751237\n",
      "=== epoch:19, train acc:0.09, test acc:0.0906 ===\n",
      "train loss:2.30186974712694\n",
      "train loss:2.3012543499990286\n",
      "train loss:2.3047863188571927\n",
      "=== epoch:20, train acc:0.09, test acc:0.0933 ===\n",
      "train loss:2.289581295673129\n",
      "train loss:2.2950694968363075\n",
      "train loss:2.299175786135928\n",
      "=== epoch:21, train acc:0.09, test acc:0.0969 ===\n",
      "train loss:2.3003282821871833\n",
      "train loss:2.293662871844098\n",
      "train loss:2.295323470103624\n",
      "=== epoch:22, train acc:0.09, test acc:0.0977 ===\n",
      "train loss:2.289435466471637\n",
      "train loss:2.2902272178896848\n",
      "train loss:2.2930002767520024\n",
      "=== epoch:23, train acc:0.09, test acc:0.0983 ===\n",
      "train loss:2.294576653668409\n",
      "train loss:2.305874791788363\n",
      "train loss:2.2930476402563063\n",
      "=== epoch:24, train acc:0.09333333333333334, test acc:0.1039 ===\n",
      "train loss:2.2910514740393957\n",
      "train loss:2.2955426330122144\n",
      "train loss:2.300260905676572\n",
      "=== epoch:25, train acc:0.09, test acc:0.1072 ===\n",
      "train loss:2.2850883116624625\n",
      "train loss:2.2924451101494343\n",
      "train loss:2.3039651666017638\n",
      "=== epoch:26, train acc:0.09333333333333334, test acc:0.1075 ===\n",
      "train loss:2.2911860843703558\n",
      "train loss:2.2954109676641146\n",
      "train loss:2.2958004998944683\n",
      "=== epoch:27, train acc:0.09666666666666666, test acc:0.1143 ===\n",
      "train loss:2.291271531641643\n",
      "train loss:2.291802434656523\n",
      "train loss:2.289716194193251\n",
      "=== epoch:28, train acc:0.1, test acc:0.1177 ===\n",
      "train loss:2.3000653084113507\n",
      "train loss:2.290674214837148\n",
      "train loss:2.2975958919526924\n",
      "=== epoch:29, train acc:0.10333333333333333, test acc:0.1185 ===\n",
      "train loss:2.2953266412774678\n",
      "train loss:2.285095269550165\n",
      "train loss:2.3005682648884513\n",
      "=== epoch:30, train acc:0.10333333333333333, test acc:0.1222 ===\n",
      "train loss:2.293549378108912\n",
      "train loss:2.2859070962367887\n",
      "train loss:2.291977607652868\n",
      "=== epoch:31, train acc:0.11333333333333333, test acc:0.1268 ===\n",
      "train loss:2.2943865412156446\n",
      "train loss:2.292379960242773\n",
      "train loss:2.2875323981785325\n",
      "=== epoch:32, train acc:0.10666666666666667, test acc:0.1259 ===\n",
      "train loss:2.2910976758109682\n",
      "train loss:2.2913779789612194\n",
      "train loss:2.2935688722094003\n",
      "=== epoch:33, train acc:0.11666666666666667, test acc:0.1291 ===\n",
      "train loss:2.296315285032957\n",
      "train loss:2.2936384029252315\n",
      "train loss:2.2921460426250264\n",
      "=== epoch:34, train acc:0.13333333333333333, test acc:0.1338 ===\n",
      "train loss:2.284263849794433\n",
      "train loss:2.294642470886613\n",
      "train loss:2.287628961740775\n",
      "=== epoch:35, train acc:0.13333333333333333, test acc:0.1325 ===\n",
      "train loss:2.2936198595116215\n",
      "train loss:2.2897712423991727\n",
      "train loss:2.297125649486114\n",
      "=== epoch:36, train acc:0.14, test acc:0.133 ===\n",
      "train loss:2.293272936046459\n",
      "train loss:2.290121794809491\n",
      "train loss:2.291806638507535\n",
      "=== epoch:37, train acc:0.15333333333333332, test acc:0.1358 ===\n",
      "train loss:2.2916644155232366\n",
      "train loss:2.296997498108446\n",
      "train loss:2.294745314052499\n",
      "=== epoch:38, train acc:0.15666666666666668, test acc:0.1359 ===\n",
      "train loss:2.2900483282606645\n",
      "train loss:2.3013172366155517\n",
      "train loss:2.288466452173415\n",
      "=== epoch:39, train acc:0.15, test acc:0.1381 ===\n",
      "train loss:2.2907890789054868\n",
      "train loss:2.2863209004410523\n",
      "train loss:2.2869806179918664\n",
      "=== epoch:40, train acc:0.14666666666666667, test acc:0.1429 ===\n",
      "train loss:2.2885475653979315\n",
      "train loss:2.2788186724773274\n",
      "train loss:2.286369647788927\n",
      "=== epoch:41, train acc:0.14666666666666667, test acc:0.1464 ===\n",
      "train loss:2.2933854959743654\n",
      "train loss:2.294345612350153\n",
      "train loss:2.284172490038612\n",
      "=== epoch:42, train acc:0.13666666666666666, test acc:0.1443 ===\n",
      "train loss:2.289280287019315\n",
      "train loss:2.28333027964375\n",
      "train loss:2.2939267546056814\n",
      "=== epoch:43, train acc:0.14, test acc:0.1469 ===\n",
      "train loss:2.2937134465982116\n",
      "train loss:2.288760799866408\n",
      "train loss:2.295066850922924\n",
      "=== epoch:44, train acc:0.13666666666666666, test acc:0.147 ===\n",
      "train loss:2.282589897150939\n",
      "train loss:2.2841705024071057\n",
      "train loss:2.2839770478153882\n",
      "=== epoch:45, train acc:0.15, test acc:0.1491 ===\n",
      "train loss:2.2883878952824235\n",
      "train loss:2.286926274089181\n",
      "train loss:2.2863958868436973\n",
      "=== epoch:46, train acc:0.15, test acc:0.1493 ===\n",
      "train loss:2.2793517562067387\n",
      "train loss:2.280008799059533\n",
      "train loss:2.2867080220159695\n",
      "=== epoch:47, train acc:0.15333333333333332, test acc:0.1497 ===\n",
      "train loss:2.2820302573606286\n",
      "train loss:2.2909256911245155\n",
      "train loss:2.284955366260783\n",
      "=== epoch:48, train acc:0.16333333333333333, test acc:0.1504 ===\n",
      "train loss:2.281379042715747\n",
      "train loss:2.2825831507172936\n",
      "train loss:2.290050530499571\n",
      "=== epoch:49, train acc:0.17, test acc:0.1498 ===\n",
      "train loss:2.284204601972079\n",
      "train loss:2.291063715919493\n",
      "train loss:2.2859055695135613\n",
      "=== epoch:50, train acc:0.17, test acc:0.1503 ===\n",
      "train loss:2.2926761101026893\n",
      "train loss:2.2950507815715806\n",
      "train loss:2.2835029076577795\n",
      "=== epoch:51, train acc:0.17, test acc:0.1503 ===\n",
      "train loss:2.2876616951006152\n",
      "train loss:2.2878235275050502\n",
      "train loss:2.279039037359133\n",
      "=== epoch:52, train acc:0.17, test acc:0.1491 ===\n",
      "train loss:2.2940804914554285\n",
      "train loss:2.2828100277925314\n",
      "train loss:2.2887951265254913\n",
      "=== epoch:53, train acc:0.16666666666666666, test acc:0.1481 ===\n",
      "train loss:2.2883364589751904\n",
      "train loss:2.2884825601516634\n",
      "train loss:2.283688880590029\n",
      "=== epoch:54, train acc:0.16666666666666666, test acc:0.1484 ===\n",
      "train loss:2.2814552859954644\n",
      "train loss:2.285823056375803\n",
      "train loss:2.283466369717962\n",
      "=== epoch:55, train acc:0.16666666666666666, test acc:0.1491 ===\n",
      "train loss:2.282524908125053\n",
      "train loss:2.2842539794623398\n",
      "train loss:2.272885175433056\n",
      "=== epoch:56, train acc:0.17, test acc:0.1498 ===\n",
      "train loss:2.2859842567211666\n",
      "train loss:2.2872547932428726\n",
      "train loss:2.283275615392374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:57, train acc:0.16666666666666666, test acc:0.1487 ===\n",
      "train loss:2.275802165070892\n",
      "train loss:2.2908857977089156\n",
      "train loss:2.282374887941925\n",
      "=== epoch:58, train acc:0.16666666666666666, test acc:0.1504 ===\n",
      "train loss:2.28095827052158\n",
      "train loss:2.2838101412891327\n",
      "train loss:2.2872185115216284\n",
      "=== epoch:59, train acc:0.17, test acc:0.1509 ===\n",
      "train loss:2.2835653945951195\n",
      "train loss:2.2882847893987885\n",
      "train loss:2.2880837724992116\n",
      "=== epoch:60, train acc:0.17, test acc:0.153 ===\n",
      "train loss:2.290100123577364\n",
      "train loss:2.298242782627925\n",
      "train loss:2.282750116603335\n",
      "=== epoch:61, train acc:0.17666666666666667, test acc:0.154 ===\n",
      "train loss:2.273254988776885\n",
      "train loss:2.289447676031024\n",
      "train loss:2.272374007568524\n",
      "=== epoch:62, train acc:0.18333333333333332, test acc:0.155 ===\n",
      "train loss:2.2777473972999065\n",
      "train loss:2.2811134262066592\n",
      "train loss:2.2894144816969946\n",
      "=== epoch:63, train acc:0.18333333333333332, test acc:0.1569 ===\n",
      "train loss:2.27637160495262\n",
      "train loss:2.27866582567482\n",
      "train loss:2.27678398773252\n",
      "=== epoch:64, train acc:0.19666666666666666, test acc:0.1584 ===\n",
      "train loss:2.2774993994406456\n",
      "train loss:2.280471637705402\n",
      "train loss:2.283636597677321\n",
      "=== epoch:65, train acc:0.2, test acc:0.159 ===\n",
      "train loss:2.289207247307384\n",
      "train loss:2.285255839724723\n",
      "train loss:2.282236387623759\n",
      "=== epoch:66, train acc:0.19666666666666666, test acc:0.1617 ===\n",
      "train loss:2.2720872363358513\n",
      "train loss:2.2746569113736697\n",
      "train loss:2.281975228236105\n",
      "=== epoch:67, train acc:0.20333333333333334, test acc:0.1656 ===\n",
      "train loss:2.2717240689195295\n",
      "train loss:2.278765891595342\n",
      "train loss:2.2794019739711078\n",
      "=== epoch:68, train acc:0.20333333333333334, test acc:0.1633 ===\n",
      "train loss:2.293477553833294\n",
      "train loss:2.283057362747065\n",
      "train loss:2.2824473902645686\n",
      "=== epoch:69, train acc:0.21, test acc:0.1672 ===\n",
      "train loss:2.2837795638106018\n",
      "train loss:2.2748566855157613\n",
      "train loss:2.2795773935558303\n",
      "=== epoch:70, train acc:0.21333333333333335, test acc:0.1694 ===\n",
      "train loss:2.2812869492940155\n",
      "train loss:2.2811221239141095\n",
      "train loss:2.27874572763875\n",
      "=== epoch:71, train acc:0.21333333333333335, test acc:0.1715 ===\n",
      "train loss:2.278362954154591\n",
      "train loss:2.2830520008708315\n",
      "train loss:2.29233432011581\n",
      "=== epoch:72, train acc:0.21666666666666667, test acc:0.1728 ===\n",
      "train loss:2.2809680594263755\n",
      "train loss:2.2761231170515583\n",
      "train loss:2.275467196153504\n",
      "=== epoch:73, train acc:0.21, test acc:0.1712 ===\n",
      "train loss:2.2866224676754277\n",
      "train loss:2.2694602090681757\n",
      "train loss:2.2827283786058574\n",
      "=== epoch:74, train acc:0.21666666666666667, test acc:0.1729 ===\n",
      "train loss:2.27273906430341\n",
      "train loss:2.2887703888553674\n",
      "train loss:2.2705999020869316\n",
      "=== epoch:75, train acc:0.22, test acc:0.1736 ===\n",
      "train loss:2.2956883780345065\n",
      "train loss:2.2821136016928865\n",
      "train loss:2.2792109992079075\n",
      "=== epoch:76, train acc:0.21666666666666667, test acc:0.1738 ===\n",
      "train loss:2.2765876481290817\n",
      "train loss:2.2848867853094994\n",
      "train loss:2.2755930292518913\n",
      "=== epoch:77, train acc:0.21666666666666667, test acc:0.174 ===\n",
      "train loss:2.282528839293836\n",
      "train loss:2.27439226845504\n",
      "train loss:2.272922941542467\n",
      "=== epoch:78, train acc:0.22333333333333333, test acc:0.1756 ===\n",
      "train loss:2.2706210445227715\n",
      "train loss:2.2731699309224225\n",
      "train loss:2.274868874624125\n",
      "=== epoch:79, train acc:0.22666666666666666, test acc:0.179 ===\n",
      "train loss:2.280303992826934\n",
      "train loss:2.280280152818405\n",
      "train loss:2.270277813648095\n",
      "=== epoch:80, train acc:0.22333333333333333, test acc:0.1848 ===\n",
      "train loss:2.2749382861851544\n",
      "train loss:2.269614547116435\n",
      "train loss:2.285445537019714\n",
      "=== epoch:81, train acc:0.22333333333333333, test acc:0.1883 ===\n",
      "train loss:2.2891191467532668\n",
      "train loss:2.2786017037323574\n",
      "train loss:2.278592657454243\n",
      "=== epoch:82, train acc:0.23, test acc:0.1883 ===\n",
      "train loss:2.282958681150463\n",
      "train loss:2.2760535352147\n",
      "train loss:2.271023801923548\n",
      "=== epoch:83, train acc:0.22333333333333333, test acc:0.189 ===\n",
      "train loss:2.280495642604137\n",
      "train loss:2.2717797322151223\n",
      "train loss:2.2672037107738343\n",
      "=== epoch:84, train acc:0.22666666666666666, test acc:0.1881 ===\n",
      "train loss:2.2787263180458597\n",
      "train loss:2.274402511819966\n",
      "train loss:2.301009695427286\n",
      "=== epoch:85, train acc:0.23333333333333334, test acc:0.1872 ===\n",
      "train loss:2.277843201181242\n",
      "train loss:2.2823151424739816\n",
      "train loss:2.275751767074566\n",
      "=== epoch:86, train acc:0.23333333333333334, test acc:0.1905 ===\n",
      "train loss:2.2765182742183216\n",
      "train loss:2.287203672057506\n",
      "train loss:2.2771164447220142\n",
      "=== epoch:87, train acc:0.23333333333333334, test acc:0.1943 ===\n",
      "train loss:2.2752833644092005\n",
      "train loss:2.2830499557252955\n",
      "train loss:2.2781236175130495\n",
      "=== epoch:88, train acc:0.23666666666666666, test acc:0.1969 ===\n",
      "train loss:2.2797769062443565\n",
      "train loss:2.2730059673380696\n",
      "train loss:2.278258254249683\n",
      "=== epoch:89, train acc:0.23333333333333334, test acc:0.1929 ===\n",
      "train loss:2.2677608150430952\n",
      "train loss:2.278231121911288\n",
      "train loss:2.2757742047232186\n",
      "=== epoch:90, train acc:0.23333333333333334, test acc:0.1942 ===\n",
      "train loss:2.2570478948557064\n",
      "train loss:2.282109957130385\n",
      "train loss:2.2773050595349345\n",
      "=== epoch:91, train acc:0.22666666666666666, test acc:0.194 ===\n",
      "train loss:2.280825401989618\n",
      "train loss:2.2668567546812053\n",
      "train loss:2.277460740286303\n",
      "=== epoch:92, train acc:0.22666666666666666, test acc:0.193 ===\n",
      "train loss:2.268639544761697\n",
      "train loss:2.2661070873975357\n",
      "train loss:2.2765611373336845\n",
      "=== epoch:93, train acc:0.23, test acc:0.1939 ===\n",
      "train loss:2.2835165592374813\n",
      "train loss:2.2782484874918962\n",
      "train loss:2.2721778995107385\n",
      "=== epoch:94, train acc:0.24, test acc:0.1968 ===\n",
      "train loss:2.271645677638689\n",
      "train loss:2.2787835095533215\n",
      "train loss:2.268201821114344\n",
      "=== epoch:95, train acc:0.23666666666666666, test acc:0.1947 ===\n",
      "train loss:2.2691735161921973\n",
      "train loss:2.2733150699023796\n",
      "train loss:2.27967461169873\n",
      "=== epoch:96, train acc:0.23666666666666666, test acc:0.1956 ===\n",
      "train loss:2.2739492736053277\n",
      "train loss:2.2743981710825905\n",
      "train loss:2.268661166686573\n",
      "=== epoch:97, train acc:0.24, test acc:0.1935 ===\n",
      "train loss:2.267975753234117\n",
      "train loss:2.2752428932050264\n",
      "train loss:2.2737510380142023\n",
      "=== epoch:98, train acc:0.23, test acc:0.1932 ===\n",
      "train loss:2.273700404517817\n",
      "train loss:2.273303792705391\n",
      "train loss:2.2813019135564545\n",
      "=== epoch:99, train acc:0.24, test acc:0.1958 ===\n",
      "train loss:2.2811416328394456\n",
      "train loss:2.2830582618636357\n",
      "train loss:2.291063189508313\n",
      "=== epoch:100, train acc:0.24333333333333335, test acc:0.1966 ===\n",
      "train loss:2.2802147253680896\n",
      "train loss:2.270592239445738\n",
      "train loss:2.271417819080781\n",
      "=== epoch:101, train acc:0.24666666666666667, test acc:0.1976 ===\n",
      "train loss:2.2622757885879192\n",
      "train loss:2.273919256929534\n",
      "train loss:2.2704249501000877\n",
      "=== epoch:102, train acc:0.24333333333333335, test acc:0.1981 ===\n",
      "train loss:2.2687238255859836\n",
      "train loss:2.2645350333313106\n",
      "train loss:2.265607446767143\n",
      "=== epoch:103, train acc:0.2633333333333333, test acc:0.203 ===\n",
      "train loss:2.274986650607884\n",
      "train loss:2.2715873510633084\n",
      "train loss:2.2731533201747025\n",
      "=== epoch:104, train acc:0.2633333333333333, test acc:0.2075 ===\n",
      "train loss:2.2618107373812277\n",
      "train loss:2.2762420902398315\n",
      "train loss:2.270501818861592\n",
      "=== epoch:105, train acc:0.26666666666666666, test acc:0.2089 ===\n",
      "train loss:2.2720437714844595\n",
      "train loss:2.265275253995554\n",
      "train loss:2.2677412888913593\n",
      "=== epoch:106, train acc:0.26666666666666666, test acc:0.2085 ===\n",
      "train loss:2.2805249428106538\n",
      "train loss:2.274177409160133\n",
      "train loss:2.280405768893109\n",
      "=== epoch:107, train acc:0.2733333333333333, test acc:0.2096 ===\n",
      "train loss:2.267173940486025\n",
      "train loss:2.273461241219677\n",
      "train loss:2.2729218045130413\n",
      "=== epoch:108, train acc:0.2733333333333333, test acc:0.212 ===\n",
      "train loss:2.2670778845529247\n",
      "train loss:2.2687793873473527\n",
      "train loss:2.262420260090647\n",
      "=== epoch:109, train acc:0.2733333333333333, test acc:0.2089 ===\n",
      "train loss:2.2690375813904513\n",
      "train loss:2.274913205093956\n",
      "train loss:2.2731208452413743\n",
      "=== epoch:110, train acc:0.28, test acc:0.2096 ===\n",
      "train loss:2.270065589831041\n",
      "train loss:2.27127902728254\n",
      "train loss:2.2704821098928605\n",
      "=== epoch:111, train acc:0.26666666666666666, test acc:0.2084 ===\n",
      "train loss:2.2784127904803357\n",
      "train loss:2.270652157970489\n",
      "train loss:2.2706498930075645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.26, test acc:0.2085 ===\n",
      "train loss:2.2664576342005547\n",
      "train loss:2.2732535096466795\n",
      "train loss:2.2805065228008146\n",
      "=== epoch:113, train acc:0.26, test acc:0.2072 ===\n",
      "train loss:2.270181451612015\n",
      "train loss:2.2797767795929746\n",
      "train loss:2.283542753412449\n",
      "=== epoch:114, train acc:0.26, test acc:0.2042 ===\n",
      "train loss:2.2579051333159064\n",
      "train loss:2.269565854533451\n",
      "train loss:2.2720326974688665\n",
      "=== epoch:115, train acc:0.27, test acc:0.2085 ===\n",
      "train loss:2.2741719094540156\n",
      "train loss:2.260937989114373\n",
      "train loss:2.2611318285312483\n",
      "=== epoch:116, train acc:0.27, test acc:0.2065 ===\n",
      "train loss:2.2634293011664868\n",
      "train loss:2.2683154190393706\n",
      "train loss:2.2811963287383064\n",
      "=== epoch:117, train acc:0.27, test acc:0.2079 ===\n",
      "train loss:2.2694424909744826\n",
      "train loss:2.270139572556597\n",
      "train loss:2.2733723956824\n",
      "=== epoch:118, train acc:0.27666666666666667, test acc:0.2103 ===\n",
      "train loss:2.2538283318262495\n",
      "train loss:2.282233938934948\n",
      "train loss:2.2674222565447724\n",
      "=== epoch:119, train acc:0.28, test acc:0.2109 ===\n",
      "train loss:2.2596537799096397\n",
      "train loss:2.268947045446694\n",
      "train loss:2.258383025619385\n",
      "=== epoch:120, train acc:0.25333333333333335, test acc:0.2064 ===\n",
      "train loss:2.2583943016166894\n",
      "train loss:2.2781968902128282\n",
      "train loss:2.2554478307263266\n",
      "=== epoch:121, train acc:0.25666666666666665, test acc:0.2058 ===\n",
      "train loss:2.2800179332010213\n",
      "train loss:2.2674757549627143\n",
      "train loss:2.285723200703885\n",
      "=== epoch:122, train acc:0.25333333333333335, test acc:0.2044 ===\n",
      "train loss:2.2634988628446377\n",
      "train loss:2.2642972930777288\n",
      "train loss:2.257191113669223\n",
      "=== epoch:123, train acc:0.25666666666666665, test acc:0.2086 ===\n",
      "train loss:2.264086074902193\n",
      "train loss:2.248512644471065\n",
      "train loss:2.278350156190095\n",
      "=== epoch:124, train acc:0.26, test acc:0.2104 ===\n",
      "train loss:2.251930433857599\n",
      "train loss:2.286114268059357\n",
      "train loss:2.2692728836154807\n",
      "=== epoch:125, train acc:0.27666666666666667, test acc:0.214 ===\n",
      "train loss:2.2651473165660563\n",
      "train loss:2.2722415938397087\n",
      "train loss:2.266932559354915\n",
      "=== epoch:126, train acc:0.27666666666666667, test acc:0.2146 ===\n",
      "train loss:2.270538322439569\n",
      "train loss:2.2773112742327926\n",
      "train loss:2.242008621064622\n",
      "=== epoch:127, train acc:0.27, test acc:0.2143 ===\n",
      "train loss:2.2571961847232176\n",
      "train loss:2.2659078373086867\n",
      "train loss:2.269902648446906\n",
      "=== epoch:128, train acc:0.2633333333333333, test acc:0.2126 ===\n",
      "train loss:2.279826498431785\n",
      "train loss:2.258667275511309\n",
      "train loss:2.2525175303523124\n",
      "=== epoch:129, train acc:0.26666666666666666, test acc:0.2111 ===\n",
      "train loss:2.2557546859280633\n",
      "train loss:2.2553027276176034\n",
      "train loss:2.2690531831766565\n",
      "=== epoch:130, train acc:0.26666666666666666, test acc:0.2127 ===\n",
      "train loss:2.2628488216346807\n",
      "train loss:2.2650076053654855\n",
      "train loss:2.2718376309561332\n",
      "=== epoch:131, train acc:0.26666666666666666, test acc:0.2128 ===\n",
      "train loss:2.262009795522964\n",
      "train loss:2.2438073707074326\n",
      "train loss:2.2531819008078413\n",
      "=== epoch:132, train acc:0.2633333333333333, test acc:0.2101 ===\n",
      "train loss:2.2555015301866788\n",
      "train loss:2.2728065568725233\n",
      "train loss:2.2857852256212023\n",
      "=== epoch:133, train acc:0.2633333333333333, test acc:0.2137 ===\n",
      "train loss:2.2564048557841763\n",
      "train loss:2.255930554597261\n",
      "train loss:2.25728582689226\n",
      "=== epoch:134, train acc:0.26666666666666666, test acc:0.2133 ===\n",
      "train loss:2.253214291264564\n",
      "train loss:2.2648101696895666\n",
      "train loss:2.2542624155145496\n",
      "=== epoch:135, train acc:0.27, test acc:0.2159 ===\n",
      "train loss:2.271485742090962\n",
      "train loss:2.253534111667182\n",
      "train loss:2.268619238707014\n",
      "=== epoch:136, train acc:0.2733333333333333, test acc:0.2185 ===\n",
      "train loss:2.2411471584676868\n",
      "train loss:2.2930370496159838\n",
      "train loss:2.256859920526131\n",
      "=== epoch:137, train acc:0.29, test acc:0.2234 ===\n",
      "train loss:2.2652362374766413\n",
      "train loss:2.265309429359937\n",
      "train loss:2.2641111754321\n",
      "=== epoch:138, train acc:0.29, test acc:0.2243 ===\n",
      "train loss:2.270716590242584\n",
      "train loss:2.270764202330193\n",
      "train loss:2.2779701248644524\n",
      "=== epoch:139, train acc:0.29, test acc:0.2259 ===\n",
      "train loss:2.2591378010803074\n",
      "train loss:2.2622446534371883\n",
      "train loss:2.2625911302712254\n",
      "=== epoch:140, train acc:0.2866666666666667, test acc:0.2272 ===\n",
      "train loss:2.2634712152930376\n",
      "train loss:2.2660757283714026\n",
      "train loss:2.2733654541433324\n",
      "=== epoch:141, train acc:0.29, test acc:0.229 ===\n",
      "train loss:2.2575974350942003\n",
      "train loss:2.263371951733467\n",
      "train loss:2.2598005096850184\n",
      "=== epoch:142, train acc:0.29333333333333333, test acc:0.2318 ===\n",
      "train loss:2.262113303454163\n",
      "train loss:2.26282235004899\n",
      "train loss:2.2527712004519787\n",
      "=== epoch:143, train acc:0.29333333333333333, test acc:0.2286 ===\n",
      "train loss:2.2587711298674016\n",
      "train loss:2.2563187264741176\n",
      "train loss:2.259212723945348\n",
      "=== epoch:144, train acc:0.29333333333333333, test acc:0.2279 ===\n",
      "train loss:2.2599392390222635\n",
      "train loss:2.2578913850003555\n",
      "train loss:2.271970943017363\n",
      "=== epoch:145, train acc:0.2833333333333333, test acc:0.2265 ===\n",
      "train loss:2.2569621483717164\n",
      "train loss:2.2661784942300547\n",
      "train loss:2.2579021959039647\n",
      "=== epoch:146, train acc:0.2866666666666667, test acc:0.2264 ===\n",
      "train loss:2.255421680352909\n",
      "train loss:2.2505091673432314\n",
      "train loss:2.2550415687469356\n",
      "=== epoch:147, train acc:0.2866666666666667, test acc:0.2274 ===\n",
      "train loss:2.256934177228486\n",
      "train loss:2.245474604823165\n",
      "train loss:2.277456349829207\n",
      "=== epoch:148, train acc:0.29, test acc:0.228 ===\n",
      "train loss:2.2461258838881295\n",
      "train loss:2.2667907945526657\n",
      "train loss:2.2511374107523507\n",
      "=== epoch:149, train acc:0.2866666666666667, test acc:0.2297 ===\n",
      "train loss:2.267462766473773\n",
      "train loss:2.252086029232406\n",
      "train loss:2.248895112894503\n",
      "=== epoch:150, train acc:0.2866666666666667, test acc:0.2272 ===\n",
      "train loss:2.260446651585122\n",
      "train loss:2.2474389629926077\n",
      "train loss:2.271720382354282\n",
      "=== epoch:151, train acc:0.2866666666666667, test acc:0.2261 ===\n",
      "train loss:2.2589203674461134\n",
      "train loss:2.263830301442636\n",
      "train loss:2.2545887207458306\n",
      "=== epoch:152, train acc:0.29, test acc:0.2288 ===\n",
      "train loss:2.2611155179930016\n",
      "train loss:2.2598327142582795\n",
      "train loss:2.250061965925254\n",
      "=== epoch:153, train acc:0.2866666666666667, test acc:0.2281 ===\n",
      "train loss:2.2488500851727573\n",
      "train loss:2.2620632486242416\n",
      "train loss:2.260665384472267\n",
      "=== epoch:154, train acc:0.29, test acc:0.229 ===\n",
      "train loss:2.244135068686872\n",
      "train loss:2.2530366228937897\n",
      "train loss:2.261291304076991\n",
      "=== epoch:155, train acc:0.29333333333333333, test acc:0.2302 ===\n",
      "train loss:2.257888654838194\n",
      "train loss:2.2668163367489327\n",
      "train loss:2.226710776054668\n",
      "=== epoch:156, train acc:0.29333333333333333, test acc:0.2309 ===\n",
      "train loss:2.2618646461787733\n",
      "train loss:2.255733742654003\n",
      "train loss:2.2401455376143393\n",
      "=== epoch:157, train acc:0.29333333333333333, test acc:0.2303 ===\n",
      "train loss:2.259160543190861\n",
      "train loss:2.24418881458307\n",
      "train loss:2.2584488146694137\n",
      "=== epoch:158, train acc:0.29333333333333333, test acc:0.2309 ===\n",
      "train loss:2.2364094671197163\n",
      "train loss:2.252470376868821\n",
      "train loss:2.2494609498327414\n",
      "=== epoch:159, train acc:0.2833333333333333, test acc:0.2307 ===\n",
      "train loss:2.263776319934301\n",
      "train loss:2.2701410880904738\n",
      "train loss:2.2619686474934833\n",
      "=== epoch:160, train acc:0.2833333333333333, test acc:0.2313 ===\n",
      "train loss:2.2646767908324335\n",
      "train loss:2.2656185321936335\n",
      "train loss:2.261621737488959\n",
      "=== epoch:161, train acc:0.29333333333333333, test acc:0.2342 ===\n",
      "train loss:2.2673119721615405\n",
      "train loss:2.2351993602551348\n",
      "train loss:2.250214910596556\n",
      "=== epoch:162, train acc:0.2866666666666667, test acc:0.2318 ===\n",
      "train loss:2.279844711111404\n",
      "train loss:2.232315875151133\n",
      "train loss:2.2568559769985224\n",
      "=== epoch:163, train acc:0.29333333333333333, test acc:0.2335 ===\n",
      "train loss:2.265833195787756\n",
      "train loss:2.2561799287510578\n",
      "train loss:2.2512517223135826\n",
      "=== epoch:164, train acc:0.29333333333333333, test acc:0.2356 ===\n",
      "train loss:2.264905053204574\n",
      "train loss:2.2497668664541766\n",
      "train loss:2.271182384082854\n",
      "=== epoch:165, train acc:0.29333333333333333, test acc:0.2365 ===\n",
      "train loss:2.269640666399918\n",
      "train loss:2.255748654222011\n",
      "train loss:2.2687977457696684\n",
      "=== epoch:166, train acc:0.29333333333333333, test acc:0.2366 ===\n",
      "train loss:2.2673263144098055\n",
      "train loss:2.2427911596013668\n",
      "train loss:2.257845113654505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.2966666666666667, test acc:0.2365 ===\n",
      "train loss:2.247177852916477\n",
      "train loss:2.250514509452909\n",
      "train loss:2.2662390343007046\n",
      "=== epoch:168, train acc:0.3, test acc:0.2375 ===\n",
      "train loss:2.2408003838936725\n",
      "train loss:2.2679051884480512\n",
      "train loss:2.25304461304386\n",
      "=== epoch:169, train acc:0.3, test acc:0.2381 ===\n",
      "train loss:2.250572605879545\n",
      "train loss:2.252017296224897\n",
      "train loss:2.2675784711634517\n",
      "=== epoch:170, train acc:0.3, test acc:0.2368 ===\n",
      "train loss:2.2517256296095343\n",
      "train loss:2.2583545552683124\n",
      "train loss:2.258025790661401\n",
      "=== epoch:171, train acc:0.3, test acc:0.2375 ===\n",
      "train loss:2.265808194790244\n",
      "train loss:2.235275606123382\n",
      "train loss:2.2436325115730553\n",
      "=== epoch:172, train acc:0.30333333333333334, test acc:0.2377 ===\n",
      "train loss:2.255630509275437\n",
      "train loss:2.245394822558703\n",
      "train loss:2.2393267758743467\n",
      "=== epoch:173, train acc:0.30333333333333334, test acc:0.2382 ===\n",
      "train loss:2.2454180182193286\n",
      "train loss:2.2683261322933923\n",
      "train loss:2.2507467595795574\n",
      "=== epoch:174, train acc:0.30333333333333334, test acc:0.2385 ===\n",
      "train loss:2.265464134710893\n",
      "train loss:2.2572954321833865\n",
      "train loss:2.2528661736955846\n",
      "=== epoch:175, train acc:0.30333333333333334, test acc:0.2403 ===\n",
      "train loss:2.2148411746648358\n",
      "train loss:2.249398128773251\n",
      "train loss:2.24108543729703\n",
      "=== epoch:176, train acc:0.30333333333333334, test acc:0.2441 ===\n",
      "train loss:2.243980404650342\n",
      "train loss:2.239319627025892\n",
      "train loss:2.24328734490516\n",
      "=== epoch:177, train acc:0.30666666666666664, test acc:0.2455 ===\n",
      "train loss:2.229772356927905\n",
      "train loss:2.230317750078816\n",
      "train loss:2.2481552010741206\n",
      "=== epoch:178, train acc:0.31, test acc:0.2465 ===\n",
      "train loss:2.250496343339123\n",
      "train loss:2.2436481844877694\n",
      "train loss:2.2350034814139734\n",
      "=== epoch:179, train acc:0.30333333333333334, test acc:0.2454 ===\n",
      "train loss:2.22719406795213\n",
      "train loss:2.2505916206586267\n",
      "train loss:2.2533908585564486\n",
      "=== epoch:180, train acc:0.30666666666666664, test acc:0.2467 ===\n",
      "train loss:2.243189833054527\n",
      "train loss:2.2447453261668335\n",
      "train loss:2.2360566386536087\n",
      "=== epoch:181, train acc:0.30666666666666664, test acc:0.2482 ===\n",
      "train loss:2.2372101368316324\n",
      "train loss:2.2326635697954145\n",
      "train loss:2.2474072390351623\n",
      "=== epoch:182, train acc:0.31, test acc:0.2506 ===\n",
      "train loss:2.237334309773278\n",
      "train loss:2.220251111019555\n",
      "train loss:2.2461406748250043\n",
      "=== epoch:183, train acc:0.31, test acc:0.2526 ===\n",
      "train loss:2.2504687213435504\n",
      "train loss:2.244576318454843\n",
      "train loss:2.2577908211027005\n",
      "=== epoch:184, train acc:0.32, test acc:0.2546 ===\n",
      "train loss:2.2598572377432773\n",
      "train loss:2.2285302417424133\n",
      "train loss:2.255115525769057\n",
      "=== epoch:185, train acc:0.31666666666666665, test acc:0.2548 ===\n",
      "train loss:2.2249315617426033\n",
      "train loss:2.240824156011098\n",
      "train loss:2.24570516920728\n",
      "=== epoch:186, train acc:0.31666666666666665, test acc:0.2553 ===\n",
      "train loss:2.256405713770158\n",
      "train loss:2.2439375449431553\n",
      "train loss:2.2464994790833486\n",
      "=== epoch:187, train acc:0.31666666666666665, test acc:0.2559 ===\n",
      "train loss:2.2133766880387977\n",
      "train loss:2.2329607962749263\n",
      "train loss:2.220264728613533\n",
      "=== epoch:188, train acc:0.31666666666666665, test acc:0.256 ===\n",
      "train loss:2.2424418133996857\n",
      "train loss:2.21380250375286\n",
      "train loss:2.2233513115007932\n",
      "=== epoch:189, train acc:0.31666666666666665, test acc:0.2556 ===\n",
      "train loss:2.2303228477343873\n",
      "train loss:2.2448285740067138\n",
      "train loss:2.25367540198069\n",
      "=== epoch:190, train acc:0.31333333333333335, test acc:0.2575 ===\n",
      "train loss:2.243621500903525\n",
      "train loss:2.229628414730325\n",
      "train loss:2.238243909590681\n",
      "=== epoch:191, train acc:0.31333333333333335, test acc:0.2567 ===\n",
      "train loss:2.223365228109324\n",
      "train loss:2.223077230854593\n",
      "train loss:2.2136059434750917\n",
      "=== epoch:192, train acc:0.30666666666666664, test acc:0.2549 ===\n",
      "train loss:2.2489382509045845\n",
      "train loss:2.233692187224838\n",
      "train loss:2.2320734860158855\n",
      "=== epoch:193, train acc:0.30666666666666664, test acc:0.2556 ===\n",
      "train loss:2.2497501367333625\n",
      "train loss:2.2501838922844404\n",
      "train loss:2.2154948063672206\n",
      "=== epoch:194, train acc:0.31333333333333335, test acc:0.2556 ===\n",
      "train loss:2.2279375436673203\n",
      "train loss:2.233505690333691\n",
      "train loss:2.2262047025776757\n",
      "=== epoch:195, train acc:0.31666666666666665, test acc:0.257 ===\n",
      "train loss:2.2235522735698137\n",
      "train loss:2.2355186039799686\n",
      "train loss:2.23595847954328\n",
      "=== epoch:196, train acc:0.31333333333333335, test acc:0.2554 ===\n",
      "train loss:2.2357722342429853\n",
      "train loss:2.2498172995508474\n",
      "train loss:2.2363667095810915\n",
      "=== epoch:197, train acc:0.31666666666666665, test acc:0.255 ===\n",
      "train loss:2.2323111449603705\n",
      "train loss:2.2423292878435315\n",
      "train loss:2.2408085480449254\n",
      "=== epoch:198, train acc:0.31333333333333335, test acc:0.2564 ===\n",
      "train loss:2.235281147874651\n",
      "train loss:2.2211292502016438\n",
      "train loss:2.219711210687753\n",
      "=== epoch:199, train acc:0.31333333333333335, test acc:0.2588 ===\n",
      "train loss:2.2486576115185226\n",
      "train loss:2.2216972984779346\n",
      "train loss:2.2267456237224827\n",
      "=== epoch:200, train acc:0.31, test acc:0.2566 ===\n",
      "train loss:2.2236355272993586\n",
      "train loss:2.21821411975366\n",
      "train loss:2.25684945711481\n",
      "=== epoch:201, train acc:0.31, test acc:0.257 ===\n",
      "train loss:2.2452991666318804\n",
      "train loss:2.243196542062628\n",
      "train loss:2.24451045067028\n",
      "=== epoch:202, train acc:0.31, test acc:0.2582 ===\n",
      "train loss:2.2514707013686244\n",
      "train loss:2.2472875783100297\n",
      "train loss:2.2115463248403318\n",
      "=== epoch:203, train acc:0.30666666666666664, test acc:0.2583 ===\n",
      "train loss:2.233436057200233\n",
      "train loss:2.222293891429102\n",
      "train loss:2.23301349223773\n",
      "=== epoch:204, train acc:0.31, test acc:0.2587 ===\n",
      "train loss:2.2261176160119014\n",
      "train loss:2.2334186341737565\n",
      "train loss:2.2276729229834173\n",
      "=== epoch:205, train acc:0.31, test acc:0.2607 ===\n",
      "train loss:2.2392432370480435\n",
      "train loss:2.2449000812304023\n",
      "train loss:2.2511917764568183\n",
      "=== epoch:206, train acc:0.31333333333333335, test acc:0.2632 ===\n",
      "train loss:2.2615810526009703\n",
      "train loss:2.2198853070826825\n",
      "train loss:2.240442943625245\n",
      "=== epoch:207, train acc:0.31666666666666665, test acc:0.2639 ===\n",
      "train loss:2.216961801405724\n",
      "train loss:2.252813058522632\n",
      "train loss:2.213459288261751\n",
      "=== epoch:208, train acc:0.32, test acc:0.2649 ===\n",
      "train loss:2.209426079261774\n",
      "train loss:2.223604387945964\n",
      "train loss:2.21302930198741\n",
      "=== epoch:209, train acc:0.3233333333333333, test acc:0.267 ===\n",
      "train loss:2.2105846074113153\n",
      "train loss:2.206170423698587\n",
      "train loss:2.2259553068390003\n",
      "=== epoch:210, train acc:0.32666666666666666, test acc:0.2683 ===\n",
      "train loss:2.246313965645275\n",
      "train loss:2.218700078531982\n",
      "train loss:2.2296882357911896\n",
      "=== epoch:211, train acc:0.32666666666666666, test acc:0.2689 ===\n",
      "train loss:2.2121839433761745\n",
      "train loss:2.2457394282023966\n",
      "train loss:2.241121172020825\n",
      "=== epoch:212, train acc:0.32666666666666666, test acc:0.2694 ===\n",
      "train loss:2.216447041403325\n",
      "train loss:2.237372322614843\n",
      "train loss:2.2364558048443617\n",
      "=== epoch:213, train acc:0.33, test acc:0.2694 ===\n",
      "train loss:2.215109131419455\n",
      "train loss:2.216574463670353\n",
      "train loss:2.2192185454020468\n",
      "=== epoch:214, train acc:0.3233333333333333, test acc:0.2678 ===\n",
      "train loss:2.2203694651027677\n",
      "train loss:2.250993597874001\n",
      "train loss:2.239980529192355\n",
      "=== epoch:215, train acc:0.3333333333333333, test acc:0.2693 ===\n",
      "train loss:2.2275904070074897\n",
      "train loss:2.210287148032535\n",
      "train loss:2.2000920609326027\n",
      "=== epoch:216, train acc:0.3333333333333333, test acc:0.2685 ===\n",
      "train loss:2.2336396410496078\n",
      "train loss:2.216424162337138\n",
      "train loss:2.2114353662937742\n",
      "=== epoch:217, train acc:0.33, test acc:0.2685 ===\n",
      "train loss:2.244469861085083\n",
      "train loss:2.2120314164489123\n",
      "train loss:2.214174886520019\n",
      "=== epoch:218, train acc:0.3333333333333333, test acc:0.2699 ===\n",
      "train loss:2.2139240103424895\n",
      "train loss:2.2500586028930107\n",
      "train loss:2.2350391127064726\n",
      "=== epoch:219, train acc:0.3333333333333333, test acc:0.2679 ===\n",
      "train loss:2.213905157198898\n",
      "train loss:2.225972127081034\n",
      "train loss:2.176677481584944\n",
      "=== epoch:220, train acc:0.32666666666666666, test acc:0.2695 ===\n",
      "train loss:2.224007724831361\n",
      "train loss:2.1942074822254183\n",
      "train loss:2.2378900581116694\n",
      "=== epoch:221, train acc:0.33666666666666667, test acc:0.2691 ===\n",
      "train loss:2.210029677670168\n",
      "train loss:2.218513812853782\n",
      "train loss:2.207873999335034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.33666666666666667, test acc:0.2709 ===\n",
      "train loss:2.2483869404802506\n",
      "train loss:2.228156527174842\n",
      "train loss:2.2431748745822304\n",
      "=== epoch:223, train acc:0.3333333333333333, test acc:0.2746 ===\n",
      "train loss:2.22933320883052\n",
      "train loss:2.201584754788151\n",
      "train loss:2.214209383602645\n",
      "=== epoch:224, train acc:0.34, test acc:0.2758 ===\n",
      "train loss:2.2396622384363076\n",
      "train loss:2.2562664632895957\n",
      "train loss:2.2280057594487364\n",
      "=== epoch:225, train acc:0.3433333333333333, test acc:0.2789 ===\n",
      "train loss:2.2106885384539496\n",
      "train loss:2.194895450502548\n",
      "train loss:2.2106966443737592\n",
      "=== epoch:226, train acc:0.3433333333333333, test acc:0.2781 ===\n",
      "train loss:2.1929700356995405\n",
      "train loss:2.2202934599203443\n",
      "train loss:2.2135324396230094\n",
      "=== epoch:227, train acc:0.34, test acc:0.2745 ===\n",
      "train loss:2.197237844727532\n",
      "train loss:2.211765962009437\n",
      "train loss:2.2001654417484797\n",
      "=== epoch:228, train acc:0.34, test acc:0.2749 ===\n",
      "train loss:2.215716762201063\n",
      "train loss:2.216259549712003\n",
      "train loss:2.2219351429839875\n",
      "=== epoch:229, train acc:0.34, test acc:0.2736 ===\n",
      "train loss:2.2102335045232406\n",
      "train loss:2.202462329268762\n",
      "train loss:2.2338769094954696\n",
      "=== epoch:230, train acc:0.3433333333333333, test acc:0.2754 ===\n",
      "train loss:2.2253212570908016\n",
      "train loss:2.208717144721896\n",
      "train loss:2.2035755911579384\n",
      "=== epoch:231, train acc:0.34, test acc:0.2732 ===\n",
      "train loss:2.2318876173361644\n",
      "train loss:2.1891401884580937\n",
      "train loss:2.2432724760145386\n",
      "=== epoch:232, train acc:0.3333333333333333, test acc:0.272 ===\n",
      "train loss:2.1731887792699034\n",
      "train loss:2.226076836834347\n",
      "train loss:2.220076446248052\n",
      "=== epoch:233, train acc:0.3333333333333333, test acc:0.2716 ===\n",
      "train loss:2.2168231018731435\n",
      "train loss:2.2283698436395136\n",
      "train loss:2.203853267645924\n",
      "=== epoch:234, train acc:0.33666666666666667, test acc:0.2718 ===\n",
      "train loss:2.225845577261851\n",
      "train loss:2.204744413374478\n",
      "train loss:2.20645503532353\n",
      "=== epoch:235, train acc:0.33666666666666667, test acc:0.2728 ===\n",
      "train loss:2.2143891427910734\n",
      "train loss:2.222798516893344\n",
      "train loss:2.227798999055621\n",
      "=== epoch:236, train acc:0.34, test acc:0.2725 ===\n",
      "train loss:2.24660010694742\n",
      "train loss:2.231631556787537\n",
      "train loss:2.224390781546762\n",
      "=== epoch:237, train acc:0.34, test acc:0.2754 ===\n",
      "train loss:2.2044442795989796\n",
      "train loss:2.217993915566931\n",
      "train loss:2.1949885163882037\n",
      "=== epoch:238, train acc:0.3433333333333333, test acc:0.2771 ===\n",
      "train loss:2.2015395842510332\n",
      "train loss:2.158376236262208\n",
      "train loss:2.1800384512438216\n",
      "=== epoch:239, train acc:0.3433333333333333, test acc:0.2734 ===\n",
      "train loss:2.220213262950259\n",
      "train loss:2.2032530989296366\n",
      "train loss:2.1860272104011558\n",
      "=== epoch:240, train acc:0.3433333333333333, test acc:0.2738 ===\n",
      "train loss:2.1998542557307035\n",
      "train loss:2.208328936877883\n",
      "train loss:2.205107690718457\n",
      "=== epoch:241, train acc:0.34, test acc:0.2732 ===\n",
      "train loss:2.176524012245792\n",
      "train loss:2.194802454574573\n",
      "train loss:2.2106811429286837\n",
      "=== epoch:242, train acc:0.33666666666666667, test acc:0.2732 ===\n",
      "train loss:2.2006189080830687\n",
      "train loss:2.189951006252607\n",
      "train loss:2.2039327389186036\n",
      "=== epoch:243, train acc:0.3333333333333333, test acc:0.2723 ===\n",
      "train loss:2.1951568515848745\n",
      "train loss:2.2082360529173797\n",
      "train loss:2.2207857820233783\n",
      "=== epoch:244, train acc:0.3333333333333333, test acc:0.2718 ===\n",
      "train loss:2.202064809221112\n",
      "train loss:2.2274244794089886\n",
      "train loss:2.232215892141966\n",
      "=== epoch:245, train acc:0.33, test acc:0.2721 ===\n",
      "train loss:2.2132071325593063\n",
      "train loss:2.1995666924911035\n",
      "train loss:2.185878040627543\n",
      "=== epoch:246, train acc:0.34, test acc:0.2745 ===\n",
      "train loss:2.188613377942066\n",
      "train loss:2.2007679021399498\n",
      "train loss:2.2009443886112745\n",
      "=== epoch:247, train acc:0.33666666666666667, test acc:0.2745 ===\n",
      "train loss:2.1871771522664964\n",
      "train loss:2.1914318055715616\n",
      "train loss:2.16759773533775\n",
      "=== epoch:248, train acc:0.3333333333333333, test acc:0.2736 ===\n",
      "train loss:2.194216414685579\n",
      "train loss:2.1929163249580887\n",
      "train loss:2.238929749886548\n",
      "=== epoch:249, train acc:0.33, test acc:0.2715 ===\n",
      "train loss:2.2157691134884057\n",
      "train loss:2.1957325986579375\n",
      "train loss:2.2200154922099613\n",
      "=== epoch:250, train acc:0.33, test acc:0.2728 ===\n",
      "train loss:2.2202578402200808\n",
      "train loss:2.1738410384602744\n",
      "train loss:2.1801591017571873\n",
      "=== epoch:251, train acc:0.33, test acc:0.2728 ===\n",
      "train loss:2.1566567056279635\n",
      "train loss:2.181478898004597\n",
      "train loss:2.203355169327855\n",
      "=== epoch:252, train acc:0.33, test acc:0.2693 ===\n",
      "train loss:2.1969283345927595\n",
      "train loss:2.1913769380190007\n",
      "train loss:2.1902313206245028\n",
      "=== epoch:253, train acc:0.3233333333333333, test acc:0.2685 ===\n",
      "train loss:2.192534312948805\n",
      "train loss:2.221599947813191\n",
      "train loss:2.1701164719967845\n",
      "=== epoch:254, train acc:0.32, test acc:0.2663 ===\n",
      "train loss:2.183723780359826\n",
      "train loss:2.1862097999453622\n",
      "train loss:2.2064400620732445\n",
      "=== epoch:255, train acc:0.3233333333333333, test acc:0.2661 ===\n",
      "train loss:2.1954080154186353\n",
      "train loss:2.1945124987703326\n",
      "train loss:2.169667258680522\n",
      "=== epoch:256, train acc:0.32666666666666666, test acc:0.2678 ===\n",
      "train loss:2.166867730128639\n",
      "train loss:2.153034990837615\n",
      "train loss:2.143417547118445\n",
      "=== epoch:257, train acc:0.3233333333333333, test acc:0.2649 ===\n",
      "train loss:2.1399720991423914\n",
      "train loss:2.1701457422221186\n",
      "train loss:2.2002221627628002\n",
      "=== epoch:258, train acc:0.32, test acc:0.2645 ===\n",
      "train loss:2.2066524174986326\n",
      "train loss:2.1968864426803085\n",
      "train loss:2.13799593554011\n",
      "=== epoch:259, train acc:0.32, test acc:0.2651 ===\n",
      "train loss:2.206507978851745\n",
      "train loss:2.1892898470958757\n",
      "train loss:2.194090987536805\n",
      "=== epoch:260, train acc:0.3233333333333333, test acc:0.2652 ===\n",
      "train loss:2.189616697329909\n",
      "train loss:2.1781152947279607\n",
      "train loss:2.1982507933068822\n",
      "=== epoch:261, train acc:0.32, test acc:0.2652 ===\n",
      "train loss:2.2003686025371194\n",
      "train loss:2.2031006100067008\n",
      "train loss:2.1694525377905185\n",
      "=== epoch:262, train acc:0.32, test acc:0.2657 ===\n",
      "train loss:2.194944488016889\n",
      "train loss:2.2032279215741712\n",
      "train loss:2.175316307310012\n",
      "=== epoch:263, train acc:0.32, test acc:0.2674 ===\n",
      "train loss:2.147019235066132\n",
      "train loss:2.19427931844431\n",
      "train loss:2.1891685187169543\n",
      "=== epoch:264, train acc:0.32, test acc:0.2684 ===\n",
      "train loss:2.178860809435841\n",
      "train loss:2.172800475778975\n",
      "train loss:2.1830952792640654\n",
      "=== epoch:265, train acc:0.32666666666666666, test acc:0.2706 ===\n",
      "train loss:2.1685580964747087\n",
      "train loss:2.147163420244133\n",
      "train loss:2.1948800765914105\n",
      "=== epoch:266, train acc:0.32666666666666666, test acc:0.2704 ===\n",
      "train loss:2.1796913388181185\n",
      "train loss:2.1708051186743633\n",
      "train loss:2.208254124138876\n",
      "=== epoch:267, train acc:0.32666666666666666, test acc:0.269 ===\n",
      "train loss:2.227903354326371\n",
      "train loss:2.1763645615969387\n",
      "train loss:2.1599676606539835\n",
      "=== epoch:268, train acc:0.32666666666666666, test acc:0.2701 ===\n",
      "train loss:2.180138015031706\n",
      "train loss:2.17582866075662\n",
      "train loss:2.1451749903635617\n",
      "=== epoch:269, train acc:0.31666666666666665, test acc:0.2679 ===\n",
      "train loss:2.189572746447876\n",
      "train loss:2.192565110721433\n",
      "train loss:2.1944883222054057\n",
      "=== epoch:270, train acc:0.31666666666666665, test acc:0.2705 ===\n",
      "train loss:2.144679417783002\n",
      "train loss:2.1809841023056165\n",
      "train loss:2.205703786818293\n",
      "=== epoch:271, train acc:0.3233333333333333, test acc:0.2714 ===\n",
      "train loss:2.1804258871245916\n",
      "train loss:2.17309104801028\n",
      "train loss:2.2065132446310742\n",
      "=== epoch:272, train acc:0.3233333333333333, test acc:0.269 ===\n",
      "train loss:2.1562229140172127\n",
      "train loss:2.1808300058969294\n",
      "train loss:2.165736076650381\n",
      "=== epoch:273, train acc:0.32, test acc:0.2682 ===\n",
      "train loss:2.1758962711360157\n",
      "train loss:2.209341550893003\n",
      "train loss:2.1909960317004615\n",
      "=== epoch:274, train acc:0.3233333333333333, test acc:0.2692 ===\n",
      "train loss:2.1642130338262846\n",
      "train loss:2.1709695720869018\n",
      "train loss:2.179019315846176\n",
      "=== epoch:275, train acc:0.3233333333333333, test acc:0.2714 ===\n",
      "train loss:2.1340556314553663\n",
      "train loss:2.179591315737257\n",
      "train loss:2.1406000057221295\n",
      "=== epoch:276, train acc:0.32666666666666666, test acc:0.2722 ===\n",
      "train loss:2.119483284531595\n",
      "train loss:2.1801607926758155\n",
      "train loss:2.161901273775886\n",
      "=== epoch:277, train acc:0.32666666666666666, test acc:0.2734 ===\n",
      "train loss:2.1630979630433624\n",
      "train loss:2.099803600553864\n",
      "train loss:2.1418718883212513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.32666666666666666, test acc:0.2726 ===\n",
      "train loss:2.159356188944273\n",
      "train loss:2.1316138162506095\n",
      "train loss:2.157319241245557\n",
      "=== epoch:279, train acc:0.32666666666666666, test acc:0.275 ===\n",
      "train loss:2.140784522611724\n",
      "train loss:2.1767079996710166\n",
      "train loss:2.20522659550812\n",
      "=== epoch:280, train acc:0.33, test acc:0.2757 ===\n",
      "train loss:2.160542601586604\n",
      "train loss:2.169129854755295\n",
      "train loss:2.18876984941552\n",
      "=== epoch:281, train acc:0.33, test acc:0.2757 ===\n",
      "train loss:2.1238397117560166\n",
      "train loss:2.1649290860298267\n",
      "train loss:2.223674451905357\n",
      "=== epoch:282, train acc:0.33666666666666667, test acc:0.277 ===\n",
      "train loss:2.1238143749490233\n",
      "train loss:2.1737856148207326\n",
      "train loss:2.083409322252959\n",
      "=== epoch:283, train acc:0.32666666666666666, test acc:0.2739 ===\n",
      "train loss:2.1836673670503775\n",
      "train loss:2.1259285755679014\n",
      "train loss:2.1590470822384393\n",
      "=== epoch:284, train acc:0.32666666666666666, test acc:0.2742 ===\n",
      "train loss:2.2101985438219445\n",
      "train loss:2.1797423490174728\n",
      "train loss:2.172314095283718\n",
      "=== epoch:285, train acc:0.32666666666666666, test acc:0.2749 ===\n",
      "train loss:2.183609610712206\n",
      "train loss:2.1071545177585773\n",
      "train loss:2.2105066629498737\n",
      "=== epoch:286, train acc:0.32666666666666666, test acc:0.2763 ===\n",
      "train loss:2.2227352472150246\n",
      "train loss:2.1286386324264153\n",
      "train loss:2.1584360971397647\n",
      "=== epoch:287, train acc:0.32666666666666666, test acc:0.2761 ===\n",
      "train loss:2.13747367961985\n",
      "train loss:2.182910318087277\n",
      "train loss:2.149716458595299\n",
      "=== epoch:288, train acc:0.32666666666666666, test acc:0.2753 ===\n",
      "train loss:2.1365624633371927\n",
      "train loss:2.1408816220130826\n",
      "train loss:2.160424747637977\n",
      "=== epoch:289, train acc:0.3233333333333333, test acc:0.2735 ===\n",
      "train loss:2.1412555451271356\n",
      "train loss:2.1854167129709143\n",
      "train loss:2.1245640436121405\n",
      "=== epoch:290, train acc:0.32666666666666666, test acc:0.2731 ===\n",
      "train loss:2.1614116755343913\n",
      "train loss:2.156229082786841\n",
      "train loss:2.172780124415609\n",
      "=== epoch:291, train acc:0.32666666666666666, test acc:0.2746 ===\n",
      "train loss:2.1298752221652544\n",
      "train loss:2.1148957393402377\n",
      "train loss:2.1436673197492326\n",
      "=== epoch:292, train acc:0.32666666666666666, test acc:0.2731 ===\n",
      "train loss:2.095794935352421\n",
      "train loss:2.103408853748583\n",
      "train loss:2.135482068045842\n",
      "=== epoch:293, train acc:0.32, test acc:0.2725 ===\n",
      "train loss:2.12709309577325\n",
      "train loss:2.1563256221710647\n",
      "train loss:2.1727289172020523\n",
      "=== epoch:294, train acc:0.32, test acc:0.2717 ===\n",
      "train loss:2.1053347772187654\n",
      "train loss:2.12272339766285\n",
      "train loss:2.132504494083762\n",
      "=== epoch:295, train acc:0.32, test acc:0.2698 ===\n",
      "train loss:2.1508861038224514\n",
      "train loss:2.1839558150128453\n",
      "train loss:2.1428165370241765\n",
      "=== epoch:296, train acc:0.32, test acc:0.2706 ===\n",
      "train loss:2.169070044653395\n",
      "train loss:2.1570665901950807\n",
      "train loss:2.120013597898927\n",
      "=== epoch:297, train acc:0.32, test acc:0.2703 ===\n",
      "train loss:2.1563345772498765\n",
      "train loss:2.0899957602143306\n",
      "train loss:2.1818177646668637\n",
      "=== epoch:298, train acc:0.32, test acc:0.2699 ===\n",
      "train loss:2.142702271619924\n",
      "train loss:2.1350125523596715\n",
      "train loss:2.133909124386896\n",
      "=== epoch:299, train acc:0.32, test acc:0.2703 ===\n",
      "train loss:2.1580273927554448\n",
      "train loss:2.1238235239488055\n",
      "train loss:2.15397091144213\n",
      "=== epoch:300, train acc:0.32, test acc:0.2693 ===\n",
      "train loss:2.150282425471232\n",
      "train loss:2.1498360715864475\n",
      "train loss:2.129002669812438\n",
      "=== epoch:301, train acc:0.32, test acc:0.2711 ===\n",
      "train loss:2.13024154581637\n",
      "train loss:2.1534867421225936\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.2713\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.3\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db32e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf1520",
   "metadata": {},
   "source": [
    "- 드롭아웃 0.1은 오버피팅의 여지가 있고\n",
    "- 드롭아웃 0.3은 학습이 저조한 경향이 있어 드롭아웃 0.2가 최적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd52746",
   "metadata": {},
   "source": [
    "### 적절한 하이퍼 파라미터 값 찾기\n",
    "- 검증 데이터 셋 분리하기\n",
    "- 적절한 하이퍼 파라미터 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99b78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    if x.ndim == 2:\n",
    "        x = x[permutation,:]\n",
    "    else:\n",
    "        x = x[permutation,:,:,:]\n",
    "\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f728dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드하기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f5649",
   "metadata": {},
   "source": [
    "데이터 뒤섞기 이전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e97624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736cd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 뒤섞기\n",
    "\n",
    "x_train,t_train = shuffle_dataset(x_train,t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df49d3",
   "metadata": {},
   "source": [
    "데이터 뒤섞기 이후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53180b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 7, ..., 0, 3, 9], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459fea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# 검증셋 분리하기\n",
    "valid_rate = 0.2\n",
    "\n",
    "valid_num = int(x_train.shape[0] * valid_rate)\n",
    "\n",
    "print(valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2657e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:valid_num]\n",
    "\n",
    "x_train = x_train[valid_num:]\n",
    "\n",
    "t_val = t_train[:valid_num]\n",
    "\n",
    "t_train = t_train[valid_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ac2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋 데이터 개수  48000\n",
      "검증셋 데이터 개수  48000\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련셋 데이터 개수 \",len(x_train))\n",
    "print(\"검증셋 데이터 개수 \",len(t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28fab361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "from common.trainer import Trainer\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4b887",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 무작위 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bfc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.9688333333333333 | lr:0.006736392077547212, weight decay:4.914114150355648e-08\n",
      "val acc:0.852 | lr:0.00013950531508548087, weight decay:1.4081597054781761e-06\n",
      "val acc:0.13966666666666666 | lr:1.4196575901750695e-06, weight decay:3.4400846846160196e-07\n",
      "val acc:0.8611666666666666 | lr:0.0001339576278800374, weight decay:1.1040389848682265e-06\n",
      "val acc:0.9645833333333333 | lr:0.004980197628233759, weight decay:1.1521876315210054e-06\n",
      "val acc:0.9334166666666667 | lr:0.0005982720382525896, weight decay:1.1230745135054847e-08\n",
      "val acc:0.22983333333333333 | lr:1.2186535925433861e-05, weight decay:4.219517995456539e-08\n",
      "val acc:0.9639166666666666 | lr:0.0032902134730075075, weight decay:8.973025390127156e-05\n"
     ]
    }
   ],
   "source": [
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 하이퍼파라미터의 범위 지정\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4) # 가중치 감소 계수의 범위 10^-8 ~ 10^-4\n",
    "    lr = 10 ** np.random.uniform(-6, -2) # 학습률의 범위 10^-6 ~ 10^-2\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0661af",
   "metadata": {},
   "source": [
    "그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd841a7",
   "metadata": {},
   "source": [
    "### 오버피팅 방지방법 결합해보기\n",
    "- 학습률 0.01\n",
    "- 드롭아웃 사용\n",
    "- 가중치 감쇠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef87eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717bc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    if x.ndim == 2:\n",
    "        x = x[permutation,:]\n",
    "    else:\n",
    "        x = x[permutation,:,:,:]\n",
    "\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478aba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 훈련 데이터 뒤섞기\n",
    "\n",
    "x_train,t_train = shuffle_dataset(x_train,t_train)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 줄이기\n",
    "x_train = x_train[:20000]\n",
    "t_train = t_train[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10016d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:62.749699836960914\n",
      "=== epoch:1, train acc:0.10075, test acc:0.0959 ===\n",
      "train loss:62.619500133625365\n",
      "train loss:62.503274760189534\n",
      "train loss:62.37773491254179\n",
      "train loss:62.22631639938371\n",
      "train loss:62.08647963980972\n",
      "train loss:61.97214208303095\n",
      "train loss:61.87386410938551\n",
      "train loss:61.7508551322893\n",
      "train loss:61.65773407900303\n",
      "train loss:61.51865316515463\n",
      "train loss:61.40421333665094\n",
      "train loss:61.289777188712286\n",
      "train loss:61.16796130047642\n",
      "train loss:61.04114295184373\n",
      "train loss:60.89780715142061\n",
      "train loss:60.80962308597452\n",
      "train loss:60.68572206981453\n",
      "train loss:60.57038256756276\n",
      "train loss:60.45005383514057\n",
      "train loss:60.33935739939549\n",
      "train loss:60.22445714121642\n",
      "train loss:60.11089610069653\n",
      "train loss:59.98531577369717\n",
      "train loss:59.873912429087014\n",
      "train loss:59.7561475143881\n",
      "train loss:59.63824245393796\n",
      "train loss:59.526596040389805\n",
      "train loss:59.4051963415655\n",
      "train loss:59.295675780933\n",
      "train loss:59.18569490284222\n",
      "train loss:59.057190171328045\n",
      "train loss:58.95821334625607\n",
      "train loss:58.838284545213185\n",
      "train loss:58.728527093705765\n",
      "train loss:58.60472987471272\n",
      "train loss:58.498818645275485\n",
      "train loss:58.38927830703038\n",
      "train loss:58.2598659748669\n",
      "train loss:58.155883273794906\n",
      "train loss:58.03894784841195\n",
      "train loss:57.93748177065587\n",
      "train loss:57.8280706288873\n",
      "train loss:57.69723896397672\n",
      "train loss:57.6081360341082\n",
      "train loss:57.49322803599779\n",
      "train loss:57.37749553569763\n",
      "train loss:57.278975323938205\n",
      "train loss:57.16097452498659\n",
      "train loss:57.04015086356869\n",
      "train loss:56.936712278913035\n",
      "train loss:56.82966879398561\n",
      "train loss:56.722575289883245\n",
      "train loss:56.61106952263642\n",
      "train loss:56.50441501823161\n",
      "train loss:56.392629904577184\n",
      "train loss:56.28241865848421\n",
      "train loss:56.17563301287495\n",
      "train loss:56.06570835786702\n",
      "train loss:55.9600653133935\n",
      "train loss:55.844995782080915\n",
      "train loss:55.737837453505975\n",
      "train loss:55.63315958153759\n",
      "train loss:55.53479008996473\n",
      "train loss:55.42622134265737\n",
      "train loss:55.31774979870229\n",
      "train loss:55.205384337370184\n",
      "train loss:55.105701393018094\n",
      "train loss:54.99575764554637\n",
      "train loss:54.893846485975466\n",
      "train loss:54.79577848005988\n",
      "train loss:54.682913403485806\n",
      "train loss:54.57667901393366\n",
      "train loss:54.46700932653002\n",
      "train loss:54.37207377565228\n",
      "train loss:54.27092370379404\n",
      "train loss:54.16193999562977\n",
      "train loss:54.054593853429125\n",
      "train loss:53.95318635569336\n",
      "train loss:53.852589237844754\n",
      "train loss:53.75585743869468\n",
      "train loss:53.640929593167854\n",
      "train loss:53.54217735468818\n",
      "train loss:53.446871888105576\n",
      "train loss:53.342520225489515\n",
      "train loss:53.236846989399865\n",
      "train loss:53.13256288755075\n",
      "train loss:53.039103555604584\n",
      "train loss:52.92741384735439\n",
      "train loss:52.83187693475681\n",
      "train loss:52.73672119062161\n",
      "train loss:52.630153848661074\n",
      "train loss:52.52742288753981\n",
      "train loss:52.424926754012155\n",
      "train loss:52.32859008820571\n",
      "train loss:52.22779163779333\n",
      "train loss:52.12817803031911\n",
      "train loss:52.027202037465536\n",
      "train loss:51.923780677569056\n",
      "train loss:51.83093897712235\n",
      "train loss:51.73322803843856\n",
      "train loss:51.6303935267075\n",
      "train loss:51.53378987928844\n",
      "train loss:51.43089368197506\n",
      "train loss:51.33967747288278\n",
      "train loss:51.23632389083644\n",
      "train loss:51.13374021606848\n",
      "train loss:51.04356444364976\n",
      "train loss:50.9518117338124\n",
      "train loss:50.848461864814844\n",
      "train loss:50.74595378771468\n",
      "train loss:50.65284868776865\n",
      "train loss:50.559652424472944\n",
      "train loss:50.448523917773336\n",
      "train loss:50.365660151549925\n",
      "train loss:50.26870275596448\n",
      "train loss:50.17734674011563\n",
      "train loss:50.079223347804394\n",
      "train loss:49.98339323077257\n",
      "train loss:49.88964852659454\n",
      "train loss:49.79315330549226\n",
      "train loss:49.693298000751014\n",
      "train loss:49.601626036335354\n",
      "train loss:49.50519934819368\n",
      "train loss:49.40815531095\n",
      "train loss:49.31908691085238\n",
      "train loss:49.2266153366098\n",
      "train loss:49.13116611819467\n",
      "train loss:49.03455504384559\n",
      "train loss:48.94103774911826\n",
      "train loss:48.85261524472047\n",
      "train loss:48.75705911727312\n",
      "train loss:48.668417244040505\n",
      "train loss:48.570710043291754\n",
      "train loss:48.476847477316696\n",
      "train loss:48.38655380129823\n",
      "train loss:48.29288443499004\n",
      "train loss:48.20138722862501\n",
      "train loss:48.107847496849125\n",
      "train loss:48.02035934292012\n",
      "train loss:47.9274771491495\n",
      "train loss:47.842172106911434\n",
      "train loss:47.748763951480896\n",
      "train loss:47.651780362092644\n",
      "train loss:47.56473100180207\n",
      "train loss:47.47606626919772\n",
      "train loss:47.38239076579939\n",
      "train loss:47.29443576964957\n",
      "train loss:47.2048636708486\n",
      "train loss:47.113201002289614\n",
      "train loss:47.02606765488897\n",
      "train loss:46.936007884225376\n",
      "train loss:46.84357213661281\n",
      "train loss:46.755608215930415\n",
      "train loss:46.66738559627384\n",
      "train loss:46.57883879569486\n",
      "train loss:46.492590265031566\n",
      "train loss:46.40221373660165\n",
      "train loss:46.31377651972386\n",
      "train loss:46.22754864011736\n",
      "train loss:46.14040057026212\n",
      "train loss:46.050664208916324\n",
      "train loss:45.96480071186683\n",
      "train loss:45.880331841206086\n",
      "train loss:45.788742759435124\n",
      "train loss:45.703136075489546\n",
      "train loss:45.612255336290104\n",
      "train loss:45.52876326319913\n",
      "train loss:45.44504991501109\n",
      "train loss:45.353320775915876\n",
      "train loss:45.273671148211655\n",
      "train loss:45.18401424800284\n",
      "train loss:45.097597053221016\n",
      "train loss:45.013188863161616\n",
      "train loss:44.928083736581875\n",
      "train loss:44.84414748691011\n",
      "train loss:44.75726709858431\n",
      "train loss:44.67274278648636\n",
      "train loss:44.59099115526892\n",
      "train loss:44.50628785310573\n",
      "train loss:44.420095703406375\n",
      "train loss:44.33667576816364\n",
      "train loss:44.25457847490203\n",
      "train loss:44.167821483152\n",
      "train loss:44.0864439555608\n",
      "train loss:44.002005523471084\n",
      "train loss:43.91585477032152\n",
      "train loss:43.83437977502949\n",
      "train loss:43.753577545881214\n",
      "train loss:43.66791248143713\n",
      "train loss:43.58593207786827\n",
      "train loss:43.50412913037514\n",
      "train loss:43.42206641519467\n",
      "train loss:43.33973911446637\n",
      "train loss:43.25539091163824\n",
      "train loss:43.17602310674812\n",
      "train loss:43.094612808957095\n",
      "train loss:43.01113100116513\n",
      "train loss:42.92931190806535\n",
      "train loss:42.848195552143984\n",
      "train loss:42.769645117980296\n",
      "=== epoch:2, train acc:0.24095, test acc:0.2442 ===\n",
      "train loss:42.68650872083458\n",
      "train loss:42.60640863251841\n",
      "train loss:42.53046548713706\n",
      "train loss:42.446080158396434\n",
      "train loss:42.36610474595447\n",
      "train loss:42.284142261540495\n",
      "train loss:42.204051142032355\n",
      "train loss:42.12610890863712\n",
      "train loss:42.04796058467748\n",
      "train loss:41.96844503097919\n",
      "train loss:41.88686356097534\n",
      "train loss:41.810659188507536\n",
      "train loss:41.727748923282476\n",
      "train loss:41.65211979869015\n",
      "train loss:41.571423780076195\n",
      "train loss:41.49335976668303\n",
      "train loss:41.421243474942806\n",
      "train loss:41.33908022816008\n",
      "train loss:41.26201340029324\n",
      "train loss:41.18125328213649\n",
      "train loss:41.10668184608801\n",
      "train loss:41.02638742154862\n",
      "train loss:40.95005703579352\n",
      "train loss:40.87269508440187\n",
      "train loss:40.79506858832142\n",
      "train loss:40.71619390307005\n",
      "train loss:40.641572428887514\n",
      "train loss:40.56657894877926\n",
      "train loss:40.49214666087042\n",
      "train loss:40.41038419660057\n",
      "train loss:40.33727722663358\n",
      "train loss:40.259305183812714\n",
      "train loss:40.18334275642487\n",
      "train loss:40.109834746934176\n",
      "train loss:40.03378453820585\n",
      "train loss:39.95557159819281\n",
      "train loss:39.88325096870181\n",
      "train loss:39.806561664469314\n",
      "train loss:39.733519226145404\n",
      "train loss:39.662283460980476\n",
      "train loss:39.58365734814129\n",
      "train loss:39.5084914591586\n",
      "train loss:39.433540448856476\n",
      "train loss:39.36252375347251\n",
      "train loss:39.28743195356537\n",
      "train loss:39.21407733993116\n",
      "train loss:39.140660646582795\n",
      "train loss:39.06459070238393\n",
      "train loss:38.99115294569761\n",
      "train loss:38.921464307448325\n",
      "train loss:38.84697806151276\n",
      "train loss:38.7722021621903\n",
      "train loss:38.69879182484304\n",
      "train loss:38.625337004208134\n",
      "train loss:38.555694569999915\n",
      "train loss:38.48164064617192\n",
      "train loss:38.41304329968383\n",
      "train loss:38.334469485833196\n",
      "train loss:38.26272087992186\n",
      "train loss:38.19483316382074\n",
      "train loss:38.12004583303901\n",
      "train loss:38.05126907825233\n",
      "train loss:37.97983484726622\n",
      "train loss:37.90776171310051\n",
      "train loss:37.83924734947161\n",
      "train loss:37.76361365408342\n",
      "train loss:37.695212072790845\n",
      "train loss:37.624267530310256\n",
      "train loss:37.55379471887258\n",
      "train loss:37.48517545990659\n",
      "train loss:37.41562442695989\n",
      "train loss:37.3449088507533\n",
      "train loss:37.27142906509392\n",
      "train loss:37.20636627034017\n",
      "train loss:37.13333480395782\n",
      "train loss:37.05992470330045\n",
      "train loss:36.9934925692459\n",
      "train loss:36.92376442232477\n",
      "train loss:36.857173856398084\n",
      "train loss:36.78553250180784\n",
      "train loss:36.71518557537234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:36.651083544229735\n",
      "train loss:36.57653460932748\n",
      "train loss:36.51174094032987\n",
      "train loss:36.4474310466676\n",
      "train loss:36.37616984744049\n",
      "train loss:36.30841424263046\n",
      "train loss:36.239798959029166\n",
      "train loss:36.17216181020773\n",
      "train loss:36.1007997244408\n",
      "train loss:36.03582723152475\n",
      "train loss:35.96676299990218\n",
      "train loss:35.90534258757076\n",
      "train loss:35.83675538746763\n",
      "train loss:35.768261886185314\n",
      "train loss:35.70219728739488\n",
      "train loss:35.63243474914295\n",
      "train loss:35.566290596313436\n",
      "train loss:35.50246642828683\n",
      "train loss:35.43458701256148\n",
      "train loss:35.37182593387608\n",
      "train loss:35.299876078348554\n",
      "train loss:35.23181874523647\n",
      "train loss:35.169449719289965\n",
      "train loss:35.105077683712096\n",
      "train loss:35.04397235985681\n",
      "train loss:34.97473693219937\n",
      "train loss:34.907940786056905\n",
      "train loss:34.845891799923805\n",
      "train loss:34.7806672971653\n",
      "train loss:34.71487317931262\n",
      "train loss:34.65074874392734\n",
      "train loss:34.587591643470056\n",
      "train loss:34.519631778380315\n",
      "train loss:34.45908077874473\n",
      "train loss:34.39079945281744\n",
      "train loss:34.329435353756594\n",
      "train loss:34.264230374225164\n",
      "train loss:34.20046103014532\n",
      "train loss:34.13851958939572\n",
      "train loss:34.07402296325838\n",
      "train loss:34.0077524523931\n",
      "train loss:33.947484990134015\n",
      "train loss:33.88418982791912\n",
      "train loss:33.82089901737974\n",
      "train loss:33.758081403201956\n",
      "train loss:33.69251180472801\n",
      "train loss:33.63150885758712\n",
      "train loss:33.56625493042587\n",
      "train loss:33.50568527696783\n",
      "train loss:33.446888711197076\n",
      "train loss:33.38024540942817\n",
      "train loss:33.32112894356725\n",
      "train loss:33.261686071449205\n",
      "train loss:33.19493169027693\n",
      "train loss:33.13573912502455\n",
      "train loss:33.071152341351144\n",
      "train loss:33.01139317792473\n",
      "train loss:32.95157299772105\n",
      "train loss:32.88902621510612\n",
      "train loss:32.8300468623133\n",
      "train loss:32.76934097170947\n",
      "train loss:32.70907958028768\n",
      "train loss:32.64896606191695\n",
      "train loss:32.58203101903766\n",
      "train loss:32.52322727084112\n",
      "train loss:32.46541686504433\n",
      "train loss:32.404758623849915\n",
      "train loss:32.344617847239135\n",
      "train loss:32.2825008113877\n",
      "train loss:32.225826568358414\n",
      "train loss:32.159699403711954\n",
      "train loss:32.10213325750914\n",
      "train loss:32.04276022721238\n",
      "train loss:31.984380422931096\n",
      "train loss:31.92695203833892\n",
      "train loss:31.86656131751763\n",
      "train loss:31.808150216987155\n",
      "train loss:31.746223585819443\n",
      "train loss:31.68956062054004\n",
      "train loss:31.635177000733204\n",
      "train loss:31.575253439095874\n",
      "train loss:31.51572466858609\n",
      "train loss:31.451398892721713\n",
      "train loss:31.39716769157931\n",
      "train loss:31.34156507614938\n",
      "train loss:31.279448148271886\n",
      "train loss:31.22294872394794\n",
      "train loss:31.16872965409953\n",
      "train loss:31.108387771348834\n",
      "train loss:31.055391620455406\n",
      "train loss:30.992415740796872\n",
      "train loss:30.935976030781966\n",
      "train loss:30.878745507493868\n",
      "train loss:30.82357081371268\n",
      "train loss:30.761815440584623\n",
      "train loss:30.705984283079687\n",
      "train loss:30.649297672570114\n",
      "train loss:30.591809366455653\n",
      "train loss:30.537150343469172\n",
      "train loss:30.480552724779297\n",
      "train loss:30.42297650053248\n",
      "train loss:30.371314169340533\n",
      "train loss:30.311766789609536\n",
      "train loss:30.259097003424795\n",
      "train loss:30.19719860864637\n",
      "train loss:30.146131672015024\n",
      "train loss:30.080700154135833\n",
      "train loss:30.035699493197903\n",
      "train loss:29.974218740685586\n",
      "train loss:29.92306074707302\n",
      "train loss:29.870078333214394\n",
      "train loss:29.815532209539928\n",
      "train loss:29.762485998050977\n",
      "train loss:29.704075796351354\n",
      "train loss:29.650770174651097\n",
      "train loss:29.59551067049082\n",
      "train loss:29.5432954926722\n",
      "train loss:29.491028697166342\n",
      "train loss:29.434865131144953\n",
      "=== epoch:3, train acc:0.11685, test acc:0.1213 ===\n",
      "train loss:29.376628727235207\n",
      "train loss:29.32445021785006\n",
      "train loss:29.27347111692314\n",
      "train loss:29.214382835916282\n",
      "train loss:29.165184106838357\n",
      "train loss:29.104403435455403\n",
      "train loss:29.053091123463897\n",
      "train loss:28.99901085456071\n",
      "train loss:28.941374859439104\n",
      "train loss:28.894585162281654\n",
      "train loss:28.839974781009197\n",
      "train loss:28.7865933690675\n",
      "train loss:28.736615232330454\n",
      "train loss:28.685573220255417\n",
      "train loss:28.627726753626725\n",
      "train loss:28.578116098177674\n",
      "train loss:28.522240398254382\n",
      "train loss:28.473586762372243\n",
      "train loss:28.414967294281208\n",
      "train loss:28.36762613230599\n",
      "train loss:28.31880723317544\n",
      "train loss:28.26503490793459\n",
      "train loss:28.209781553011847\n",
      "train loss:28.16220231744162\n",
      "train loss:28.10874084727761\n",
      "train loss:28.052871753141886\n",
      "train loss:28.006465592688652\n",
      "train loss:27.958124841266624\n",
      "train loss:27.9018208255522\n",
      "train loss:27.849402250951854\n",
      "train loss:27.800122181985284\n",
      "train loss:27.75350092138493\n",
      "train loss:27.69907092238813\n",
      "train loss:27.64848956307784\n",
      "train loss:27.593863564743966\n",
      "train loss:27.54620804469028\n",
      "train loss:27.497258310966053\n",
      "train loss:27.44963815296843\n",
      "train loss:27.391785977202638\n",
      "train loss:27.346177162669157\n",
      "train loss:27.294549074537475\n",
      "train loss:27.247949583534727\n",
      "train loss:27.19986591323311\n",
      "train loss:27.148994781911448\n",
      "train loss:27.09771876889412\n",
      "train loss:27.05036997407931\n",
      "train loss:26.999816476077974\n",
      "train loss:26.94715302812537\n",
      "train loss:26.902997196367593\n",
      "train loss:26.84898299868702\n",
      "train loss:26.796836323428288\n",
      "train loss:26.75134467036766\n",
      "train loss:26.700522307143977\n",
      "train loss:26.65644878572665\n",
      "train loss:26.60582933418157\n",
      "train loss:26.558699216936372\n",
      "train loss:26.50940973840293\n",
      "train loss:26.459016072863445\n",
      "train loss:26.414280694850127\n",
      "train loss:26.364445542731964\n",
      "train loss:26.318068368546676\n",
      "train loss:26.2664868101793\n",
      "train loss:26.221899002716523\n",
      "train loss:26.17589397368932\n",
      "train loss:26.12332729614988\n",
      "train loss:26.073396443933454\n",
      "train loss:26.029762117363624\n",
      "train loss:25.983606915206067\n",
      "train loss:25.93350433065165\n",
      "train loss:25.88372297530341\n",
      "train loss:25.83957599424707\n",
      "train loss:25.790263464998603\n",
      "train loss:25.74262836944658\n",
      "train loss:25.70117988329953\n",
      "train loss:25.65651433502711\n",
      "train loss:25.604548094587482\n",
      "train loss:25.56068563679803\n",
      "train loss:25.510988975811383\n",
      "train loss:25.468518469324778\n",
      "train loss:25.42131576483706\n",
      "train loss:25.375468711827445\n",
      "train loss:25.331746539884186\n",
      "train loss:25.288146643977363\n",
      "train loss:25.235135454452053\n",
      "train loss:25.189789095879263\n",
      "train loss:25.14208712227602\n",
      "train loss:25.103117941867215\n",
      "train loss:25.05645937472252\n",
      "train loss:25.005076851910317\n",
      "train loss:24.966339623875125\n",
      "train loss:24.919379646710635\n",
      "train loss:24.873761558628743\n",
      "train loss:24.827551074782907\n",
      "train loss:24.779966698772334\n",
      "train loss:24.735585610899737\n",
      "train loss:24.69339896159517\n",
      "train loss:24.6481331363713\n",
      "train loss:24.60798083968523\n",
      "train loss:24.559118613698036\n",
      "train loss:24.515724491001055\n",
      "train loss:24.47139354457968\n",
      "train loss:24.43127671014789\n",
      "train loss:24.382481498868593\n",
      "train loss:24.339792043459298\n",
      "train loss:24.296730798303035\n",
      "train loss:24.252999793286605\n",
      "train loss:24.205742880616135\n",
      "train loss:24.15967345695956\n",
      "train loss:24.119045572353485\n",
      "train loss:24.07762958514766\n",
      "train loss:24.031239810682152\n",
      "train loss:23.98940978765208\n",
      "train loss:23.942840217164374\n",
      "train loss:23.89664308420418\n",
      "train loss:23.857508223739984\n",
      "train loss:23.813990212186514\n",
      "train loss:23.770316287741096\n",
      "train loss:23.730100279980036\n",
      "train loss:23.69064268341881\n",
      "train loss:23.644840140248537\n",
      "train loss:23.60040846391203\n",
      "train loss:23.562484581730423\n",
      "train loss:23.516031508175924\n",
      "train loss:23.47416454118626\n",
      "train loss:23.429428990726773\n",
      "train loss:23.390770429326437\n",
      "train loss:23.34538212313175\n",
      "train loss:23.30381144419566\n",
      "train loss:23.265760250733845\n",
      "train loss:23.21935830488793\n",
      "train loss:23.184784474490087\n",
      "train loss:23.142648298504994\n",
      "train loss:23.093233659365737\n",
      "train loss:23.06075850821488\n",
      "train loss:23.014066430885535\n",
      "train loss:22.972429887190213\n",
      "train loss:22.93126261843722\n",
      "train loss:22.89061335211615\n",
      "train loss:22.850092716657805\n",
      "train loss:22.81071863779383\n",
      "train loss:22.77137943053013\n",
      "train loss:22.722929582856263\n",
      "train loss:22.681447000442315\n",
      "train loss:22.646177244604065\n",
      "train loss:22.60483299074434\n",
      "train loss:22.56370706239225\n",
      "train loss:22.51805229183122\n",
      "train loss:22.482214497110633\n",
      "train loss:22.43747909330566\n",
      "train loss:22.39783758983483\n",
      "train loss:22.362638700617275\n",
      "train loss:22.31276617711238\n",
      "train loss:22.281608403261323\n",
      "train loss:22.241820978904762\n",
      "train loss:22.20024797308266\n",
      "train loss:22.164231679631925\n",
      "train loss:22.11497131601762\n",
      "train loss:22.08538434404327\n",
      "train loss:22.04464957015882\n",
      "train loss:22.000703946386345\n",
      "train loss:21.96778367261368\n",
      "train loss:21.925515391636857\n",
      "train loss:21.882117051815314\n",
      "train loss:21.848018741954846\n",
      "train loss:21.809116574646655\n",
      "train loss:21.773698302827672\n",
      "train loss:21.729550743141647\n",
      "train loss:21.694946505452968\n",
      "train loss:21.650419482625406\n",
      "train loss:21.61768627410693\n",
      "train loss:21.578431440864772\n",
      "train loss:21.536921091518348\n",
      "train loss:21.49933910239487\n",
      "train loss:21.45742565277782\n",
      "train loss:21.421961956116835\n",
      "train loss:21.381567729469722\n",
      "train loss:21.348572144219947\n",
      "train loss:21.302880283471364\n",
      "train loss:21.26810906779285\n",
      "train loss:21.229527053618625\n",
      "train loss:21.19745091368381\n",
      "train loss:21.155460624799062\n",
      "train loss:21.11353905431296\n",
      "train loss:21.085636138951106\n",
      "train loss:21.045063978852717\n",
      "train loss:21.009773095928935\n",
      "train loss:20.97151298649191\n",
      "train loss:20.929323584483488\n",
      "train loss:20.89536253006775\n",
      "train loss:20.85865822975021\n",
      "train loss:20.820783933122566\n",
      "train loss:20.78690770974358\n",
      "train loss:20.74295658548885\n",
      "train loss:20.70973795693862\n",
      "train loss:20.671544924934782\n",
      "train loss:20.634937798893244\n",
      "train loss:20.59500676027845\n",
      "train loss:20.562112587255037\n",
      "train loss:20.52577238078069\n",
      "train loss:20.492306378058064\n",
      "=== epoch:4, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:20.447847915465932\n",
      "train loss:20.418264526299215\n",
      "train loss:20.379300445597753\n",
      "train loss:20.349225467335447\n",
      "train loss:20.309878552613853\n",
      "train loss:20.27467752021871\n",
      "train loss:20.23655587247291\n",
      "train loss:20.199445222722407\n",
      "train loss:20.160484014966823\n",
      "train loss:20.129079959352218\n",
      "train loss:20.096570884553653\n",
      "train loss:20.051485382589973\n",
      "train loss:20.015082825176865\n",
      "train loss:19.986372490007096\n",
      "train loss:19.953566161586348\n",
      "train loss:19.916787006436852\n",
      "train loss:19.88106605044554\n",
      "train loss:19.847724555798344\n",
      "train loss:19.812069486729783\n",
      "train loss:19.776758870055616\n",
      "train loss:19.744220470267386\n",
      "train loss:19.708575128765307\n",
      "train loss:19.675634811533897\n",
      "train loss:19.631297721378594\n",
      "train loss:19.60850962129433\n",
      "train loss:19.568125749129326\n",
      "train loss:19.533899343725643\n",
      "train loss:19.505534497243826\n",
      "train loss:19.467413831309543\n",
      "train loss:19.42873054916939\n",
      "train loss:19.393299878923273\n",
      "train loss:19.352262946391626\n",
      "train loss:19.328348314725822\n",
      "train loss:19.29336928960506\n",
      "train loss:19.261172481267828\n",
      "train loss:19.223545759028678\n",
      "train loss:19.188135846556296\n",
      "train loss:19.15976073378244\n",
      "train loss:19.12356662706841\n",
      "train loss:19.096137649906748\n",
      "train loss:19.05847966667536\n",
      "train loss:19.021042753413198\n",
      "train loss:18.9927854937697\n",
      "train loss:18.95872576109949\n",
      "train loss:18.92780200509404\n",
      "train loss:18.894068136269247\n",
      "train loss:18.85515886920859\n",
      "train loss:18.82793229297681\n",
      "train loss:18.78683090080223\n",
      "train loss:18.760388749018546\n",
      "train loss:18.724597659833375\n",
      "train loss:18.69698394996536\n",
      "train loss:18.65603836014082\n",
      "train loss:18.6331443717034\n",
      "train loss:18.59862708541999\n",
      "train loss:18.558936317405742\n",
      "train loss:18.532631635945904\n",
      "train loss:18.4894376418629\n",
      "train loss:18.4652768705722\n",
      "train loss:18.42512583120186\n",
      "train loss:18.405115257563416\n",
      "train loss:18.366752028228007\n",
      "train loss:18.333734403305236\n",
      "train loss:18.302598201440063\n",
      "train loss:18.27692037933526\n",
      "train loss:18.243385354916718\n",
      "train loss:18.205904518034608\n",
      "train loss:18.173163759450006\n",
      "train loss:18.145698493616102\n",
      "train loss:18.112151297942667\n",
      "train loss:18.084115048444932\n",
      "train loss:18.05056776000567\n",
      "train loss:18.019578899103415\n",
      "train loss:17.98559135007413\n",
      "train loss:17.95831157549798\n",
      "train loss:17.92573209715534\n",
      "train loss:17.89407280826065\n",
      "train loss:17.86602471157288\n",
      "train loss:17.829609134105787\n",
      "train loss:17.79753344012795\n",
      "train loss:17.766703831160996\n",
      "train loss:17.741018413691467\n",
      "train loss:17.70983968091794\n",
      "train loss:17.673947496630348\n",
      "train loss:17.642706104112676\n",
      "train loss:17.61162091399878\n",
      "train loss:17.582545847275732\n",
      "train loss:17.55436688293633\n",
      "train loss:17.518716463423125\n",
      "train loss:17.489654806946863\n",
      "train loss:17.462005034293902\n",
      "train loss:17.43069443730895\n",
      "train loss:17.405262385645553\n",
      "train loss:17.37152829787544\n",
      "train loss:17.34716355112333\n",
      "train loss:17.31571569807561\n",
      "train loss:17.28011147459388\n",
      "train loss:17.252836564279345\n",
      "train loss:17.22396529827383\n",
      "train loss:17.195058206572618\n",
      "train loss:17.163776571679897\n",
      "train loss:17.133195160067004\n",
      "train loss:17.10393174537649\n",
      "train loss:17.070035709940722\n",
      "train loss:17.046193066663825\n",
      "train loss:17.016095120880532\n",
      "train loss:16.984980832217623\n",
      "train loss:16.958937617501203\n",
      "train loss:16.92620176069159\n",
      "train loss:16.898243585528114\n",
      "train loss:16.86854960237124\n",
      "train loss:16.840824069587587\n",
      "train loss:16.815310262361827\n",
      "train loss:16.778087634950868\n",
      "train loss:16.750524783097948\n",
      "train loss:16.72160710462914\n",
      "train loss:16.690953991838803\n",
      "train loss:16.66733345103416\n",
      "train loss:16.638836045443007\n",
      "train loss:16.605767580096103\n",
      "train loss:16.585875044350544\n",
      "train loss:16.549891075772013\n",
      "train loss:16.51567060973544\n",
      "train loss:16.49664465827573\n",
      "train loss:16.465475132168226\n",
      "train loss:16.435744210241324\n",
      "train loss:16.4129968969672\n",
      "train loss:16.380294132211482\n",
      "train loss:16.350837756666465\n",
      "train loss:16.326478182903145\n",
      "train loss:16.300233235249156\n",
      "train loss:16.272333843406432\n",
      "train loss:16.246735994504178\n",
      "train loss:16.211191376809047\n",
      "train loss:16.18578586797397\n",
      "train loss:16.15710309158259\n",
      "train loss:16.132467666061792\n",
      "train loss:16.10734526673263\n",
      "train loss:16.078231881786806\n",
      "train loss:16.041394860812282\n",
      "train loss:16.018875167794068\n",
      "train loss:15.992835305271607\n",
      "train loss:15.967145791944874\n",
      "train loss:15.931944214903744\n",
      "train loss:15.909693221628768\n",
      "train loss:15.880730002130061\n",
      "train loss:15.859241889906079\n",
      "train loss:15.830799697144604\n",
      "train loss:15.798342244906081\n",
      "train loss:15.771481419432527\n",
      "train loss:15.75037597836683\n",
      "train loss:15.719492546022652\n",
      "train loss:15.693036027651775\n",
      "train loss:15.665871716473761\n",
      "train loss:15.645796166206754\n",
      "train loss:15.620140921629213\n",
      "train loss:15.583921375963795\n",
      "train loss:15.563213202342215\n",
      "train loss:15.531928465789843\n",
      "train loss:15.506821078692433\n",
      "train loss:15.478172815069252\n",
      "train loss:15.460925134997002\n",
      "train loss:15.424901498037089\n",
      "train loss:15.408019462658547\n",
      "train loss:15.38381600347494\n",
      "train loss:15.35282799592025\n",
      "train loss:15.331161881868582\n",
      "train loss:15.295843426289096\n",
      "train loss:15.2752396832855\n",
      "train loss:15.245066849363358\n",
      "train loss:15.223455585952287\n",
      "train loss:15.196564643271621\n",
      "train loss:15.163931472723771\n",
      "train loss:15.140117867789392\n",
      "train loss:15.121546397827561\n",
      "train loss:15.090273798063258\n",
      "train loss:15.063623751850525\n",
      "train loss:15.048613890810994\n",
      "train loss:15.022088151318961\n",
      "train loss:14.992192470671242\n",
      "train loss:14.963932132249154\n",
      "train loss:14.936929392662432\n",
      "train loss:14.915542429305297\n",
      "train loss:14.88438621741939\n",
      "train loss:14.8601499542883\n",
      "train loss:14.8343286326499\n",
      "train loss:14.814883334709236\n",
      "train loss:14.780717681887698\n",
      "train loss:14.764518485598863\n",
      "train loss:14.742121096233515\n",
      "train loss:14.715172965495583\n",
      "train loss:14.688246748604968\n",
      "train loss:14.665325069443067\n",
      "train loss:14.635525103981966\n",
      "train loss:14.617987839993528\n",
      "train loss:14.595797211876082\n",
      "train loss:14.563547024538213\n",
      "train loss:14.547953410930319\n",
      "train loss:14.51778609510435\n",
      "train loss:14.489573363555738\n",
      "=== epoch:5, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:14.469077319097513\n",
      "train loss:14.448621621008177\n",
      "train loss:14.422035028298199\n",
      "train loss:14.39155500071457\n",
      "train loss:14.365509892372279\n",
      "train loss:14.351528582291076\n",
      "train loss:14.323473466697106\n",
      "train loss:14.29861627818504\n",
      "train loss:14.277224353256441\n",
      "train loss:14.249180700367997\n",
      "train loss:14.232230564440396\n",
      "train loss:14.206979494507912\n",
      "train loss:14.181793791691241\n",
      "train loss:14.152710539211467\n",
      "train loss:14.13734804127974\n",
      "train loss:14.112397265881114\n",
      "train loss:14.0850431300758\n",
      "train loss:14.060411321736407\n",
      "train loss:14.041249257414059\n",
      "train loss:14.01693332059022\n",
      "train loss:13.99155083720894\n",
      "train loss:13.967723967457744\n",
      "train loss:13.945116053726517\n",
      "train loss:13.918691017556153\n",
      "train loss:13.895274993375281\n",
      "train loss:13.879833556017617\n",
      "train loss:13.855240343059494\n",
      "train loss:13.829358790268854\n",
      "train loss:13.810223920005518\n",
      "train loss:13.776779696626395\n",
      "train loss:13.75703334945166\n",
      "train loss:13.733105122654647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:13.718077820578031\n",
      "train loss:13.683872343284282\n",
      "train loss:13.673110806308733\n",
      "train loss:13.649868121359024\n",
      "train loss:13.622821550425222\n",
      "train loss:13.602440851495428\n",
      "train loss:13.578462622130093\n",
      "train loss:13.552949419445738\n",
      "train loss:13.531592896442923\n",
      "train loss:13.510144768227553\n",
      "train loss:13.488180617188466\n",
      "train loss:13.467272677486243\n",
      "train loss:13.4434374868524\n",
      "train loss:13.427501443002406\n",
      "train loss:13.399946702194892\n",
      "train loss:13.376072766731365\n",
      "train loss:13.357876601155327\n",
      "train loss:13.334849323096378\n",
      "train loss:13.313376512221481\n",
      "train loss:13.289528477734184\n",
      "train loss:13.269200451932967\n",
      "train loss:13.246464491095384\n",
      "train loss:13.226204412723208\n",
      "train loss:13.198420086050017\n",
      "train loss:13.180882411731632\n",
      "train loss:13.160463599803254\n",
      "train loss:13.131206976336228\n",
      "train loss:13.11881278182138\n",
      "train loss:13.09182847041273\n",
      "train loss:13.066959559716002\n",
      "train loss:13.05003159308906\n",
      "train loss:13.025034862932863\n",
      "train loss:13.004713027981444\n",
      "train loss:12.990093840681613\n",
      "train loss:12.971023317637158\n",
      "train loss:12.940818757097707\n",
      "train loss:12.917140866139382\n",
      "train loss:12.900608539838046\n",
      "train loss:12.878030147216258\n",
      "train loss:12.855348399850357\n",
      "train loss:12.840158995809626\n",
      "train loss:12.820839937343822\n",
      "train loss:12.796025938316241\n",
      "train loss:12.773429618751921\n",
      "train loss:12.75162001805021\n",
      "train loss:12.732234850291544\n",
      "train loss:12.706304547543343\n",
      "train loss:12.696382593718386\n",
      "train loss:12.671443477381096\n",
      "train loss:12.653262948982556\n",
      "train loss:12.635371905738452\n",
      "train loss:12.604747411536646\n",
      "train loss:12.58395244573286\n",
      "train loss:12.56564785775015\n",
      "train loss:12.54640470072852\n",
      "train loss:12.524958444334272\n",
      "train loss:12.505193215736215\n",
      "train loss:12.490860950452367\n",
      "train loss:12.465651510689515\n",
      "train loss:12.446463150491594\n",
      "train loss:12.418773099963682\n",
      "train loss:12.402759744078027\n",
      "train loss:12.383910532135687\n",
      "train loss:12.361613355247199\n",
      "train loss:12.33979390188866\n",
      "train loss:12.320245264726136\n",
      "train loss:12.303166677949552\n",
      "train loss:12.275749171278362\n",
      "train loss:12.259635967879646\n",
      "train loss:12.2472871587653\n",
      "train loss:12.220045583032718\n",
      "train loss:12.202273512229944\n",
      "train loss:12.182974851878868\n",
      "train loss:12.164461132008078\n",
      "train loss:12.144879182291877\n",
      "train loss:12.118118057810022\n",
      "train loss:12.099539907929994\n",
      "train loss:12.084251421892251\n",
      "train loss:12.065096156605192\n",
      "train loss:12.052888953425832\n",
      "train loss:12.02758697405873\n",
      "train loss:12.009295159411261\n",
      "train loss:11.983412122216953\n",
      "train loss:11.970270944743623\n",
      "train loss:11.952824257149675\n",
      "train loss:11.926765413543897\n",
      "train loss:11.908142493374832\n",
      "train loss:11.89653280510763\n",
      "train loss:11.87793438744908\n",
      "train loss:11.849488509755432\n",
      "train loss:11.836365809118519\n",
      "train loss:11.819302160654436\n",
      "train loss:11.792451466265586\n",
      "train loss:11.776373529219327\n",
      "train loss:11.75896847073592\n",
      "train loss:11.735444888541839\n",
      "train loss:11.722568816188463\n",
      "train loss:11.69985181119906\n",
      "train loss:11.679230119778989\n",
      "train loss:11.66451352720144\n",
      "train loss:11.639730925015249\n",
      "train loss:11.622885072328337\n",
      "train loss:11.603218555002375\n",
      "train loss:11.595170700927863\n",
      "train loss:11.57446213550571\n",
      "train loss:11.553642255607244\n",
      "train loss:11.534800736784334\n",
      "train loss:11.515598955938955\n",
      "train loss:11.49689233586028\n",
      "train loss:11.479150794416306\n",
      "train loss:11.460542365552403\n",
      "train loss:11.441119854863894\n",
      "train loss:11.415490406027503\n",
      "train loss:11.405462799930113\n",
      "train loss:11.384805037279211\n",
      "train loss:11.373551186039352\n",
      "train loss:11.349945699618257\n",
      "train loss:11.332235454737805\n",
      "train loss:11.317429480280241\n",
      "train loss:11.295829467718441\n",
      "train loss:11.277770806721335\n",
      "train loss:11.260524084141762\n",
      "train loss:11.240960166342667\n",
      "train loss:11.220502128028112\n",
      "train loss:11.208283727500131\n",
      "train loss:11.185730865847608\n",
      "train loss:11.162286259973751\n",
      "train loss:11.159723195726158\n",
      "train loss:11.136788121945672\n",
      "train loss:11.117068809671865\n",
      "train loss:11.10127857027632\n",
      "train loss:11.0819064127834\n",
      "train loss:11.066399870913486\n",
      "train loss:11.036600882577495\n",
      "train loss:11.036059272886225\n",
      "train loss:11.013275259083851\n",
      "train loss:10.993378739617857\n",
      "train loss:10.985453370126258\n",
      "train loss:10.961194499590977\n",
      "train loss:10.946840790334576\n",
      "train loss:10.930165592531125\n",
      "train loss:10.912066831309893\n",
      "train loss:10.88834779393146\n",
      "train loss:10.875510702673008\n",
      "train loss:10.85771741658748\n",
      "train loss:10.832306689049402\n",
      "train loss:10.82378730364973\n",
      "train loss:10.81137772355684\n",
      "train loss:10.781934706689885\n",
      "train loss:10.766889499428872\n",
      "train loss:10.751035462396677\n",
      "train loss:10.737590330708086\n",
      "train loss:10.723155050494878\n",
      "train loss:10.698904350033139\n",
      "train loss:10.693673632585863\n",
      "train loss:10.674188876190897\n",
      "train loss:10.656930184677613\n",
      "train loss:10.631327271731081\n",
      "train loss:10.622861134772714\n",
      "train loss:10.600842940586425\n",
      "train loss:10.588370706641943\n",
      "train loss:10.566956192889826\n",
      "train loss:10.557611951386033\n",
      "train loss:10.53473297022615\n",
      "train loss:10.519554167266758\n",
      "train loss:10.507986715192175\n",
      "train loss:10.493211842359516\n",
      "train loss:10.47372384763781\n",
      "=== epoch:6, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:10.458360546220387\n",
      "train loss:10.439526302598903\n",
      "train loss:10.423177190437588\n",
      "train loss:10.405901000919172\n",
      "train loss:10.387744361654555\n",
      "train loss:10.369755700298326\n",
      "train loss:10.36342135392007\n",
      "train loss:10.338102839033834\n",
      "train loss:10.330160851676991\n",
      "train loss:10.308101864826025\n",
      "train loss:10.295312334744425\n",
      "train loss:10.286306091493996\n",
      "train loss:10.260031036395866\n",
      "train loss:10.242730458755918\n",
      "train loss:10.234632345955934\n",
      "train loss:10.215435663487192\n",
      "train loss:10.202803794231505\n",
      "train loss:10.17929101180005\n",
      "train loss:10.16861169715378\n",
      "train loss:10.154511721362727\n",
      "train loss:10.14132327234896\n",
      "train loss:10.115480552999824\n",
      "train loss:10.10357394414673\n",
      "train loss:10.086791165834578\n",
      "train loss:10.069500400107742\n",
      "train loss:10.06140815154696\n",
      "train loss:10.052231328389402\n",
      "train loss:10.025678368808466\n",
      "train loss:10.009891045208905\n",
      "train loss:9.997061793758897\n",
      "train loss:9.981798403865664\n",
      "train loss:9.968694172870938\n",
      "train loss:9.954886528321161\n",
      "train loss:9.925419933653622\n",
      "train loss:9.92276656057686\n",
      "train loss:9.909954733212528\n",
      "train loss:9.892284024234147\n",
      "train loss:9.871966954916852\n",
      "train loss:9.859682168705545\n",
      "train loss:9.839813813572967\n",
      "train loss:9.833195813480756\n",
      "train loss:9.81779055591519\n",
      "train loss:9.798128447477788\n",
      "train loss:9.784646424611726\n",
      "train loss:9.766336227653744\n",
      "train loss:9.757814122869455\n",
      "train loss:9.739111582499326\n",
      "train loss:9.726328949554267\n",
      "train loss:9.713061838991429\n",
      "train loss:9.69759787812189\n",
      "train loss:9.681178649214186\n",
      "train loss:9.67266608100115\n",
      "train loss:9.654116575822588\n",
      "train loss:9.634065762974464\n",
      "train loss:9.620802887663125\n",
      "train loss:9.613190831965083\n",
      "train loss:9.588411844879786\n",
      "train loss:9.57730654292522\n",
      "train loss:9.56866693636832\n",
      "train loss:9.545155524587285\n",
      "train loss:9.534402729650402\n",
      "train loss:9.521062927066078\n",
      "train loss:9.502723081343614\n",
      "train loss:9.49030614189306\n",
      "train loss:9.475089801485007\n",
      "train loss:9.457711265901926\n",
      "train loss:9.450700162643628\n",
      "train loss:9.433624102281193\n",
      "train loss:9.414873343039593\n",
      "train loss:9.400412110575491\n",
      "train loss:9.391799440382322\n",
      "train loss:9.372564627064426\n",
      "train loss:9.358431271602699\n",
      "train loss:9.349531155671144\n",
      "train loss:9.335636960768294\n",
      "train loss:9.319473894559813\n",
      "train loss:9.311593128095975\n",
      "train loss:9.289175849850713\n",
      "train loss:9.280236951760639\n",
      "train loss:9.266171838479632\n",
      "train loss:9.245786048425757\n",
      "train loss:9.240140531234342\n",
      "train loss:9.224139231841198\n",
      "train loss:9.206352431810997\n",
      "train loss:9.19662506493479\n",
      "train loss:9.176543524652732\n",
      "train loss:9.166927568547115\n",
      "train loss:9.15430605490133\n",
      "train loss:9.141131964200843\n",
      "train loss:9.127347114003255\n",
      "train loss:9.114434752344964\n",
      "train loss:9.104838200121549\n",
      "train loss:9.08490437994436\n",
      "train loss:9.066995203772482\n",
      "train loss:9.061651741658359\n",
      "train loss:9.044319810202781\n",
      "train loss:9.030524722359598\n",
      "train loss:9.019503963456177\n",
      "train loss:9.010762720672206\n",
      "train loss:8.989515684548982\n",
      "train loss:8.978064922652083\n",
      "train loss:8.969739452540368\n",
      "train loss:8.95477461485102\n",
      "train loss:8.941883217128261\n",
      "train loss:8.923846048646107\n",
      "train loss:8.909671635730271\n",
      "train loss:8.896431778887411\n",
      "train loss:8.8885233801365\n",
      "train loss:8.872385568353527\n",
      "train loss:8.854515295787907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:8.852589060357253\n",
      "train loss:8.831706487584531\n",
      "train loss:8.823731235326527\n",
      "train loss:8.805667059366721\n",
      "train loss:8.79656154061816\n",
      "train loss:8.78019260581134\n",
      "train loss:8.766428879224614\n",
      "train loss:8.754963367228491\n",
      "train loss:8.736914022494403\n",
      "train loss:8.728534913109913\n",
      "train loss:8.71417286229424\n",
      "train loss:8.70270035422999\n",
      "train loss:8.6842455950503\n",
      "train loss:8.679101392428356\n",
      "train loss:8.665504707726049\n",
      "train loss:8.650231085804421\n",
      "train loss:8.642767053368031\n",
      "train loss:8.623322640671036\n",
      "train loss:8.609628081397215\n",
      "train loss:8.602097484484457\n",
      "train loss:8.597426892566244\n",
      "train loss:8.580906825909599\n",
      "train loss:8.567289751963918\n",
      "train loss:8.555515344776037\n",
      "train loss:8.543247590192758\n",
      "train loss:8.532546531350242\n",
      "train loss:8.506855933798235\n",
      "train loss:8.502209345743111\n",
      "train loss:8.492266181409576\n",
      "train loss:8.476290576372389\n",
      "train loss:8.469707998849302\n",
      "train loss:8.450427156905\n",
      "train loss:8.438218401765587\n",
      "train loss:8.434131375412033\n",
      "train loss:8.412266621003605\n",
      "train loss:8.40365416319702\n",
      "train loss:8.39387090379218\n",
      "train loss:8.37504971617397\n",
      "train loss:8.358844573292833\n",
      "train loss:8.358981830006405\n",
      "train loss:8.339661203022786\n",
      "train loss:8.333288038257962\n",
      "train loss:8.313164365150696\n",
      "train loss:8.305769773198978\n",
      "train loss:8.297805748243999\n",
      "train loss:8.28171995559714\n",
      "train loss:8.278320158218621\n",
      "train loss:8.264623470582293\n",
      "train loss:8.247755619949205\n",
      "train loss:8.232478395534372\n",
      "train loss:8.224898865673016\n",
      "train loss:8.209953198012569\n",
      "train loss:8.198313482923515\n",
      "train loss:8.18102082368404\n",
      "train loss:8.176489380588938\n",
      "train loss:8.159975491291412\n",
      "train loss:8.151135111957862\n",
      "train loss:8.139371779148595\n",
      "train loss:8.131836298627253\n",
      "train loss:8.113272238240587\n",
      "train loss:8.097265181597852\n",
      "train loss:8.098531229312718\n",
      "train loss:8.08843840384045\n",
      "train loss:8.070993843407539\n",
      "train loss:8.064215164722889\n",
      "train loss:8.050480666596345\n",
      "train loss:8.0309779365205\n",
      "train loss:8.021168227012993\n",
      "train loss:8.016267476082074\n",
      "train loss:8.00102699506902\n",
      "train loss:7.9851880574938825\n",
      "train loss:7.973811439723724\n",
      "train loss:7.97039090362222\n",
      "train loss:7.964347578422487\n",
      "train loss:7.94681218682873\n",
      "train loss:7.9285371364787665\n",
      "train loss:7.9260583084535705\n",
      "train loss:7.912082949039347\n",
      "train loss:7.9010922229979865\n",
      "train loss:7.888945592084607\n",
      "train loss:7.873218319049022\n",
      "train loss:7.856270263187877\n",
      "train loss:7.854880835630084\n",
      "train loss:7.846661537039651\n",
      "train loss:7.839412123881656\n",
      "train loss:7.832140996923409\n",
      "train loss:7.805955622918429\n",
      "train loss:7.797421821934619\n",
      "train loss:7.784692879850844\n",
      "train loss:7.777496905989288\n",
      "=== epoch:7, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:7.770700616406131\n",
      "train loss:7.7552982534825645\n",
      "train loss:7.746231596412594\n",
      "train loss:7.734366331256411\n",
      "train loss:7.731818904840237\n",
      "train loss:7.7158912698985755\n",
      "train loss:7.701571998068146\n",
      "train loss:7.685942460870046\n",
      "train loss:7.683229190798043\n",
      "train loss:7.671941914304271\n",
      "train loss:7.659283332001338\n",
      "train loss:7.646377367668325\n",
      "train loss:7.644999408532348\n",
      "train loss:7.630409391331561\n",
      "train loss:7.618933982273766\n",
      "train loss:7.606145529337493\n",
      "train loss:7.593360695272104\n",
      "train loss:7.57902347930881\n",
      "train loss:7.573021419970139\n",
      "train loss:7.558077084318247\n",
      "train loss:7.548671995379117\n",
      "train loss:7.545155718123025\n",
      "train loss:7.536939562405051\n",
      "train loss:7.529098193709358\n",
      "train loss:7.506120859407243\n",
      "train loss:7.500993793300801\n",
      "train loss:7.490546387619624\n",
      "train loss:7.4872805571040395\n",
      "train loss:7.476189826445074\n",
      "train loss:7.45852518215486\n",
      "train loss:7.4524909514343705\n",
      "train loss:7.442378863733369\n",
      "train loss:7.4264250523477715\n",
      "train loss:7.421111643369093\n",
      "train loss:7.412080254870092\n",
      "train loss:7.393699739578307\n",
      "train loss:7.390712459228785\n",
      "train loss:7.373632574777627\n",
      "train loss:7.36903459285984\n",
      "train loss:7.356630259010267\n",
      "train loss:7.351717203192759\n",
      "train loss:7.32550306956917\n",
      "train loss:7.330498020286802\n",
      "train loss:7.317177917962134\n",
      "train loss:7.30941030037494\n",
      "train loss:7.297287515928289\n",
      "train loss:7.285172008145672\n",
      "train loss:7.284558142652996\n",
      "train loss:7.265722052982587\n",
      "train loss:7.2618139893431835\n",
      "train loss:7.255610508207006\n",
      "train loss:7.236617367276946\n",
      "train loss:7.230219971725552\n",
      "train loss:7.218676608613038\n",
      "train loss:7.203293602422553\n",
      "train loss:7.194038390154402\n",
      "train loss:7.187696066504772\n",
      "train loss:7.174079554845685\n",
      "train loss:7.170642515163985\n",
      "train loss:7.1604758267270405\n",
      "train loss:7.1529729056827165\n",
      "train loss:7.135691196220775\n",
      "train loss:7.129569728147912\n",
      "train loss:7.116879099827889\n",
      "train loss:7.110253923947352\n",
      "train loss:7.098288018777866\n",
      "train loss:7.086179617834095\n",
      "train loss:7.082409537209921\n",
      "train loss:7.070874650731719\n",
      "train loss:7.055003552993444\n",
      "train loss:7.055079272865749\n",
      "train loss:7.0339567284409785\n",
      "train loss:7.032107313566071\n",
      "train loss:7.023286225435744\n",
      "train loss:7.018356670454452\n",
      "train loss:7.001238992474677\n",
      "train loss:7.000614527733194\n",
      "train loss:6.987511299077248\n",
      "train loss:6.980624994440223\n",
      "train loss:6.962346887685706\n",
      "train loss:6.9631089456411885\n",
      "train loss:6.951213385690255\n",
      "train loss:6.943854357929771\n",
      "train loss:6.935954213010239\n",
      "train loss:6.92174039420563\n",
      "train loss:6.912066664040082\n",
      "train loss:6.902036043938843\n",
      "train loss:6.900682423945762\n",
      "train loss:6.891260159080953\n",
      "train loss:6.8795443673107055\n",
      "train loss:6.866437882928539\n",
      "train loss:6.85421487511422\n",
      "train loss:6.846747040008019\n",
      "train loss:6.841497797781114\n",
      "train loss:6.835249729468492\n",
      "train loss:6.823854777098687\n",
      "train loss:6.811253693089176\n",
      "train loss:6.805205222987535\n",
      "train loss:6.7959841543710855\n",
      "train loss:6.787925721699882\n",
      "train loss:6.7777543623692456\n",
      "train loss:6.766632384867154\n",
      "train loss:6.756891216067857\n",
      "train loss:6.7413313522706275\n",
      "train loss:6.750162567071462\n",
      "train loss:6.731668629271943\n",
      "train loss:6.72052477682099\n",
      "train loss:6.717252736206823\n",
      "train loss:6.700531203255633\n",
      "train loss:6.6937359695666245\n",
      "train loss:6.6812907971305595\n",
      "train loss:6.6797035487832055\n",
      "train loss:6.666871771647779\n",
      "train loss:6.660982473425046\n",
      "train loss:6.652473184841724\n",
      "train loss:6.6421110136240475\n",
      "train loss:6.640193551578479\n",
      "train loss:6.628858119835431\n",
      "train loss:6.6147265929243915\n",
      "train loss:6.605256149267919\n",
      "train loss:6.598769545168724\n",
      "train loss:6.588298498617908\n",
      "train loss:6.579765091349051\n",
      "train loss:6.574645466154151\n",
      "train loss:6.5682984235245545\n",
      "train loss:6.55715675068279\n",
      "train loss:6.5423161200016136\n",
      "train loss:6.5461334848706105\n",
      "train loss:6.531650885588257\n",
      "train loss:6.519462766866915\n",
      "train loss:6.514601161559716\n",
      "train loss:6.507457279710925\n",
      "train loss:6.4946149707461185\n",
      "train loss:6.485465505419448\n",
      "train loss:6.484133623848924\n",
      "train loss:6.468889805804704\n",
      "train loss:6.4714447400348\n",
      "train loss:6.462705934663785\n",
      "train loss:6.449159532717189\n",
      "train loss:6.443997787645424\n",
      "train loss:6.4394053135945875\n",
      "train loss:6.431209455122623\n",
      "train loss:6.417962565041776\n",
      "train loss:6.405083741603985\n",
      "train loss:6.396196075878942\n",
      "train loss:6.392880065557749\n",
      "train loss:6.383207529563716\n",
      "train loss:6.379774569566657\n",
      "train loss:6.369159490389713\n",
      "train loss:6.357298842817732\n",
      "train loss:6.350361876373045\n",
      "train loss:6.34116520272397\n",
      "train loss:6.336855944739513\n",
      "train loss:6.322928184992044\n",
      "train loss:6.319774949102946\n",
      "train loss:6.30939002277082\n",
      "train loss:6.30029846344769\n",
      "train loss:6.29233779999632\n",
      "train loss:6.284893920495309\n",
      "train loss:6.272654245098803\n",
      "train loss:6.27466890718879\n",
      "train loss:6.2611597410517845\n",
      "train loss:6.246135019567312\n",
      "train loss:6.243744132165921\n",
      "train loss:6.244530870280189\n",
      "train loss:6.226391017511921\n",
      "train loss:6.22079287299063\n",
      "train loss:6.214328096530929\n",
      "train loss:6.214685273253178\n",
      "train loss:6.192540658304925\n",
      "train loss:6.192587293279786\n",
      "train loss:6.175114118882169\n",
      "train loss:6.175089332010989\n",
      "train loss:6.164731469475656\n",
      "train loss:6.15885687513404\n",
      "train loss:6.160058660111304\n",
      "train loss:6.13925446472148\n",
      "train loss:6.141305285079999\n",
      "train loss:6.128194262707822\n",
      "train loss:6.116589696336497\n",
      "train loss:6.112209606916382\n",
      "train loss:6.115549639819972\n",
      "train loss:6.093521977184323\n",
      "train loss:6.089312977115826\n",
      "train loss:6.079932270010248\n",
      "train loss:6.0745484182365015\n",
      "train loss:6.071527846604624\n",
      "train loss:6.059956740825395\n",
      "train loss:6.050879547924982\n",
      "train loss:6.049978483402621\n",
      "train loss:6.041730991518648\n",
      "train loss:6.027581556963465\n",
      "train loss:6.019862890222976\n",
      "train loss:6.013103694733779\n",
      "train loss:6.004909221713458\n",
      "train loss:6.009127379506394\n",
      "train loss:5.997140226915706\n",
      "train loss:5.987441378495852\n",
      "train loss:5.978919184247748\n",
      "train loss:5.973441563285521\n",
      "=== epoch:8, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:5.971094325612192\n",
      "train loss:5.953195541276447\n",
      "train loss:5.944563396710171\n",
      "train loss:5.937195964172078\n",
      "train loss:5.942233514611315\n",
      "train loss:5.928635347719544\n",
      "train loss:5.918327007419592\n",
      "train loss:5.9203214773076835\n",
      "train loss:5.903370334113078\n",
      "train loss:5.89301479273543\n",
      "train loss:5.897642384866228\n",
      "train loss:5.886906781527463\n",
      "train loss:5.88057639858885\n",
      "train loss:5.874699883409685\n",
      "train loss:5.861447437321277\n",
      "train loss:5.855899903986732\n",
      "train loss:5.851136523747554\n",
      "train loss:5.848403515310965\n",
      "train loss:5.834283556707221\n",
      "train loss:5.826343854364003\n",
      "train loss:5.816682495747789\n",
      "train loss:5.816640594234545\n",
      "train loss:5.799221580393279\n",
      "train loss:5.798596632161289\n",
      "train loss:5.793009883662435\n",
      "train loss:5.787363415986292\n",
      "train loss:5.775418751504153\n",
      "train loss:5.769122353372778\n",
      "train loss:5.7705297151751616\n",
      "train loss:5.770741234669539\n",
      "train loss:5.753216945558883\n",
      "train loss:5.7426984582185145\n",
      "train loss:5.7309773991555755\n",
      "train loss:5.73586908464367\n",
      "train loss:5.721725732744382\n",
      "train loss:5.71554842659655\n",
      "train loss:5.713819268411155\n",
      "train loss:5.701695847931574\n",
      "train loss:5.687992498479\n",
      "train loss:5.682692668379133\n",
      "train loss:5.679823138944867\n",
      "train loss:5.683683524317561\n",
      "train loss:5.664836140826914\n",
      "train loss:5.664885733798926\n",
      "train loss:5.661892615958601\n",
      "train loss:5.649638440059196\n",
      "train loss:5.645322756334015\n",
      "train loss:5.640972098402038\n",
      "train loss:5.627148756691591\n",
      "train loss:5.617980387368065\n",
      "train loss:5.616250823882849\n",
      "train loss:5.606182993274091\n",
      "train loss:5.600773902385077\n",
      "train loss:5.59506714359361\n",
      "train loss:5.5896836570688375\n",
      "train loss:5.582555338327845\n",
      "train loss:5.57288199006157\n",
      "train loss:5.572731683351643\n",
      "train loss:5.5652065590829505\n",
      "train loss:5.560513051889162\n",
      "train loss:5.55518156934212\n",
      "train loss:5.54614741138665\n",
      "train loss:5.536956347618704\n",
      "train loss:5.526845085125808\n",
      "train loss:5.522126969225603\n",
      "train loss:5.516437527030732\n",
      "train loss:5.511133461419205\n",
      "train loss:5.497659981538444\n",
      "train loss:5.498057909400355\n",
      "train loss:5.491462099816163\n",
      "train loss:5.48862367944475\n",
      "train loss:5.476621984742887\n",
      "train loss:5.472268987777884\n",
      "train loss:5.469977155240514\n",
      "train loss:5.45235065287569\n",
      "train loss:5.451513149336347\n",
      "train loss:5.451982475574368\n",
      "train loss:5.443078554840726\n",
      "train loss:5.439002333697125\n",
      "train loss:5.430532686459761\n",
      "train loss:5.425699787411071\n",
      "train loss:5.414012156836273\n",
      "train loss:5.408907526399609\n",
      "train loss:5.403491413589623\n",
      "train loss:5.398259896240178\n",
      "train loss:5.3800314763123565\n",
      "train loss:5.3889887010285165\n",
      "train loss:5.378891049090655\n",
      "train loss:5.3704594013627185\n",
      "train loss:5.362789745730797\n",
      "train loss:5.36221877858151\n",
      "train loss:5.348596540346708\n",
      "train loss:5.348646949631855\n",
      "train loss:5.336177521812589\n",
      "train loss:5.343302814524707\n",
      "train loss:5.329619271894655\n",
      "train loss:5.3256332024930355\n",
      "train loss:5.316839722726935\n",
      "train loss:5.3139802222625185\n",
      "train loss:5.306491759328826\n",
      "train loss:5.3025672669613435\n",
      "train loss:5.2889093844253985\n",
      "train loss:5.286743493735183\n",
      "train loss:5.281870016111034\n",
      "train loss:5.28159125902482\n",
      "train loss:5.2768155120771425\n",
      "train loss:5.270484430439066\n",
      "train loss:5.259503143918366\n",
      "train loss:5.254609297552423\n",
      "train loss:5.243395599452432\n",
      "train loss:5.243205817711189\n",
      "train loss:5.237066563924611\n",
      "train loss:5.228749383971765\n",
      "train loss:5.225575649476102\n",
      "train loss:5.2193949265224475\n",
      "train loss:5.213136691171836\n",
      "train loss:5.20363397074877\n",
      "train loss:5.1950402678726455\n",
      "train loss:5.2008745588914085\n",
      "train loss:5.193972153015562\n",
      "train loss:5.179550192229635\n",
      "train loss:5.178804196540641\n",
      "train loss:5.165158262958146\n",
      "train loss:5.165881686665447\n",
      "train loss:5.1567404572709545\n",
      "train loss:5.158557160247412\n",
      "train loss:5.152224490217225\n",
      "train loss:5.1410151039973995\n",
      "train loss:5.137325296447643\n",
      "train loss:5.1319524747794985\n",
      "train loss:5.126556210959602\n",
      "train loss:5.121431294138766\n",
      "train loss:5.114553453198253\n",
      "train loss:5.1062047206732935\n",
      "train loss:5.099263810620451\n",
      "train loss:5.094445073469645\n",
      "train loss:5.0844950614404425\n",
      "train loss:5.079957158279528\n",
      "train loss:5.079045197552576\n",
      "train loss:5.0717447792000865\n",
      "train loss:5.070228290158518\n",
      "train loss:5.069513384079576\n",
      "train loss:5.062554310666037\n",
      "train loss:5.058665394914529\n",
      "train loss:5.046937398376768\n",
      "train loss:5.043073578896982\n",
      "train loss:5.037302608124878\n",
      "train loss:5.022376130868995\n",
      "train loss:5.025568477919766\n",
      "train loss:5.026853151551554\n",
      "train loss:5.011916439582608\n",
      "train loss:5.009445336232575\n",
      "train loss:5.008560871383112\n",
      "train loss:4.994547331111956\n",
      "train loss:4.995692647233318\n",
      "train loss:4.980940069711577\n",
      "train loss:4.978361473819848\n",
      "train loss:4.9795440514139635\n",
      "train loss:4.969873158314938\n",
      "train loss:4.959555916052681\n",
      "train loss:4.960934897159365\n",
      "train loss:4.948803758726937\n",
      "train loss:4.950317432054787\n",
      "train loss:4.950906051587744\n",
      "train loss:4.942356970348585\n",
      "train loss:4.93189019291382\n",
      "train loss:4.934325329002797\n",
      "train loss:4.929776242872816\n",
      "train loss:4.921175113681869\n",
      "train loss:4.914376713241817\n",
      "train loss:4.901614787764346\n",
      "train loss:4.905395032406487\n",
      "train loss:4.904695983239323\n",
      "train loss:4.89101960572301\n",
      "train loss:4.889187442875768\n",
      "train loss:4.882859767681842\n",
      "train loss:4.8808509477198605\n",
      "train loss:4.875020936092923\n",
      "train loss:4.867952349738083\n",
      "train loss:4.868853757017179\n",
      "train loss:4.8573225349884765\n",
      "train loss:4.852951153314471\n",
      "train loss:4.844271735223329\n",
      "train loss:4.8389631314367785\n",
      "train loss:4.838825975806692\n",
      "train loss:4.833018681816386\n",
      "train loss:4.823481890681148\n",
      "train loss:4.81845391898465\n",
      "train loss:4.815018230712348\n",
      "train loss:4.8110199519921455\n",
      "train loss:4.802425046850416\n",
      "train loss:4.799796968819612\n",
      "train loss:4.7923926340849485\n",
      "train loss:4.788763048280877\n",
      "train loss:4.784348764845366\n",
      "train loss:4.77606817650396\n",
      "train loss:4.776894320753529\n",
      "train loss:4.775208806182445\n",
      "train loss:4.773408884333762\n",
      "train loss:4.766357240013377\n",
      "=== epoch:9, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:4.7516215389943435\n",
      "train loss:4.752731668675324\n",
      "train loss:4.750896161276935\n",
      "train loss:4.744964552372393\n",
      "train loss:4.7384027976396474\n",
      "train loss:4.733949004735441\n",
      "train loss:4.735013499704319\n",
      "train loss:4.727853260063576\n",
      "train loss:4.714868170166822\n",
      "train loss:4.7090252356967675\n",
      "train loss:4.710417597675352\n",
      "train loss:4.7014531148023195\n",
      "train loss:4.7011037754249\n",
      "train loss:4.692072044445984\n",
      "train loss:4.689396019974893\n",
      "train loss:4.688715899001207\n",
      "train loss:4.683582204996002\n",
      "train loss:4.673972021632439\n",
      "train loss:4.676105394089511\n",
      "train loss:4.662087557014013\n",
      "train loss:4.663699514373063\n",
      "train loss:4.66007450827596\n",
      "train loss:4.6488443723769635\n",
      "train loss:4.644362702977253\n",
      "train loss:4.640896848471497\n",
      "train loss:4.6337539082079715\n",
      "train loss:4.629148084448294\n",
      "train loss:4.6265872620339845\n",
      "train loss:4.632074740576719\n",
      "train loss:4.609856422925228\n",
      "train loss:4.621917478497256\n",
      "train loss:4.613576009121688\n",
      "train loss:4.598653705850504\n",
      "train loss:4.596476767733634\n",
      "train loss:4.589417144688178\n",
      "train loss:4.592014950568549\n",
      "train loss:4.585134317377817\n",
      "train loss:4.580620853248611\n",
      "train loss:4.582040995772083\n",
      "train loss:4.571866693492687\n",
      "train loss:4.569343719285255\n",
      "train loss:4.560229398289974\n",
      "train loss:4.55499644752314\n",
      "train loss:4.556845765007753\n",
      "train loss:4.5473515868438685\n",
      "train loss:4.547008309050598\n",
      "train loss:4.545698244264358\n",
      "train loss:4.532346814227584\n",
      "train loss:4.535002878406059\n",
      "train loss:4.524652673919757\n",
      "train loss:4.522702122009823\n",
      "train loss:4.5181212704076135\n",
      "train loss:4.5101466411109605\n",
      "train loss:4.509744208455764\n",
      "train loss:4.506631122910315\n",
      "train loss:4.5043073939825655\n",
      "train loss:4.494918683623551\n",
      "train loss:4.490860164675511\n",
      "train loss:4.496615369194849\n",
      "train loss:4.484425354933757\n",
      "train loss:4.483445117272947\n",
      "train loss:4.478758718345572\n",
      "train loss:4.461293520107996\n",
      "train loss:4.465613304323894\n",
      "train loss:4.461812505804899\n",
      "train loss:4.460644935970416\n",
      "train loss:4.458741799363466\n",
      "train loss:4.451675748239452\n",
      "train loss:4.451722422910801\n",
      "train loss:4.446516337935885\n",
      "train loss:4.434288844284913\n",
      "train loss:4.4289572946075015\n",
      "train loss:4.425977368855374\n",
      "train loss:4.421916452129477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.411989481216278\n",
      "train loss:4.409308950017884\n",
      "train loss:4.419323624200198\n",
      "train loss:4.403561594880346\n",
      "train loss:4.399213028685331\n",
      "train loss:4.399508154555748\n",
      "train loss:4.392493689025842\n",
      "train loss:4.387891453205306\n",
      "train loss:4.3874928999928\n",
      "train loss:4.380586007791448\n",
      "train loss:4.378014393143074\n",
      "train loss:4.374263732617671\n",
      "train loss:4.368679721892127\n",
      "train loss:4.368664725612019\n",
      "train loss:4.351497454724864\n",
      "train loss:4.356219020434672\n",
      "train loss:4.352640014190239\n",
      "train loss:4.347666133309712\n",
      "train loss:4.333365890097764\n",
      "train loss:4.342832540179881\n",
      "train loss:4.334511407601141\n",
      "train loss:4.324117091966537\n",
      "train loss:4.329781723423162\n",
      "train loss:4.320598246393048\n",
      "train loss:4.317756539501805\n",
      "train loss:4.319460575286518\n",
      "train loss:4.3169920852904085\n",
      "train loss:4.3052476707963745\n",
      "train loss:4.3026260990673775\n",
      "train loss:4.292492407292315\n",
      "train loss:4.292795312910624\n",
      "train loss:4.293159064275029\n",
      "train loss:4.288030662053297\n",
      "train loss:4.281640530750185\n",
      "train loss:4.280434798189731\n",
      "train loss:4.2739654629103185\n",
      "train loss:4.274837119868829\n",
      "train loss:4.269899815237357\n",
      "train loss:4.264813549893406\n",
      "train loss:4.256152236843468\n",
      "train loss:4.25279743764793\n",
      "train loss:4.252807680041496\n",
      "train loss:4.246260212456684\n",
      "train loss:4.239724718057428\n",
      "train loss:4.240698129911835\n",
      "train loss:4.242298019429981\n",
      "train loss:4.227852284487046\n",
      "train loss:4.231156449576515\n",
      "train loss:4.224119035940912\n",
      "train loss:4.218909020960959\n",
      "train loss:4.216100305643987\n",
      "train loss:4.2118801613584065\n",
      "train loss:4.218811053118916\n",
      "train loss:4.203275174150074\n",
      "train loss:4.202125946232844\n",
      "train loss:4.202723603674329\n",
      "train loss:4.19063362892091\n",
      "train loss:4.1894541960244585\n",
      "train loss:4.182116057036374\n",
      "train loss:4.182471293806504\n",
      "train loss:4.181370018292687\n",
      "train loss:4.168310153287144\n",
      "train loss:4.165149596098653\n",
      "train loss:4.174919096440474\n",
      "train loss:4.163640741631591\n",
      "train loss:4.159804736280261\n",
      "train loss:4.155974293744573\n",
      "train loss:4.153348856149748\n",
      "train loss:4.1466785792845045\n",
      "train loss:4.147577816487809\n",
      "train loss:4.1420839680763075\n",
      "train loss:4.137639536781038\n",
      "train loss:4.133500701648099\n",
      "train loss:4.131603464210319\n",
      "train loss:4.125191625387506\n",
      "train loss:4.1296332218087874\n",
      "train loss:4.118705320526348\n",
      "train loss:4.112243613723497\n",
      "train loss:4.110186723092582\n",
      "train loss:4.113922882061068\n",
      "train loss:4.101863377378139\n",
      "train loss:4.102715607424104\n",
      "train loss:4.097700134290901\n",
      "train loss:4.096340832310427\n",
      "train loss:4.090115516144742\n",
      "train loss:4.09180068995523\n",
      "train loss:4.084150687631068\n",
      "train loss:4.091575622328947\n",
      "train loss:4.080890743116033\n",
      "train loss:4.070187511938494\n",
      "train loss:4.071921754118906\n",
      "train loss:4.06258019159424\n",
      "train loss:4.061741732426232\n",
      "train loss:4.053996225419445\n",
      "train loss:4.05470311646347\n",
      "train loss:4.047947592766368\n",
      "train loss:4.049272730894094\n",
      "train loss:4.048850485179206\n",
      "train loss:4.042261607591765\n",
      "train loss:4.0418078553866845\n",
      "train loss:4.032721832852787\n",
      "train loss:4.032304084515525\n",
      "train loss:4.033567491610999\n",
      "train loss:4.016621250522519\n",
      "train loss:4.022471057729311\n",
      "train loss:4.018915831304724\n",
      "train loss:4.015000591109672\n",
      "train loss:4.012357170204686\n",
      "train loss:4.01032713225762\n",
      "train loss:4.000910063814395\n",
      "train loss:4.003714681326626\n",
      "train loss:3.990233043690039\n",
      "train loss:3.994807020185224\n",
      "train loss:3.992959487113684\n",
      "train loss:3.9876790300307228\n",
      "train loss:3.987178417731511\n",
      "train loss:3.981472886451233\n",
      "train loss:3.9680753935869397\n",
      "train loss:3.975986138546845\n",
      "train loss:3.968796953354946\n",
      "train loss:3.963476980496998\n",
      "train loss:3.971424556137103\n",
      "train loss:3.9644572294367455\n",
      "train loss:3.956704823729882\n",
      "train loss:3.953218771012393\n",
      "train loss:3.9418162244428157\n",
      "=== epoch:10, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:3.9484911745160316\n",
      "train loss:3.9446282981063336\n",
      "train loss:3.9343589304522686\n",
      "train loss:3.9426851132296097\n",
      "train loss:3.936860141217961\n",
      "train loss:3.9267156166351245\n",
      "train loss:3.9225758049298083\n",
      "train loss:3.926794224846149\n",
      "train loss:3.905206720433648\n",
      "train loss:3.918808202690361\n",
      "train loss:3.9127155132353204\n",
      "train loss:3.9133661994610165\n",
      "train loss:3.904538483927366\n",
      "train loss:3.912465250554365\n",
      "train loss:3.90721856049852\n",
      "train loss:3.9049636944220274\n",
      "train loss:3.893121536441247\n",
      "train loss:3.8916323760447913\n",
      "train loss:3.8896318399553995\n",
      "train loss:3.8866258105272635\n",
      "train loss:3.882163560723003\n",
      "train loss:3.8876010222109967\n",
      "train loss:3.8742490298084418\n",
      "train loss:3.8780517474470715\n",
      "train loss:3.8685528036797496\n",
      "train loss:3.8596389336175925\n",
      "train loss:3.859137418251591\n",
      "train loss:3.855566933731766\n",
      "train loss:3.865333441191901\n",
      "train loss:3.8538596432162273\n",
      "train loss:3.847743385065261\n",
      "train loss:3.8496270849660297\n",
      "train loss:3.8507196065696014\n",
      "train loss:3.8373214043146424\n",
      "train loss:3.83697757696733\n",
      "train loss:3.820077735949517\n",
      "train loss:3.8322311070821398\n",
      "train loss:3.826189365009923\n",
      "train loss:3.8196494800114404\n",
      "train loss:3.8287710778736175\n",
      "train loss:3.820485144759961\n",
      "train loss:3.821592948884986\n",
      "train loss:3.8109663363665383\n",
      "train loss:3.8152303976068738\n",
      "train loss:3.8056088334480944\n",
      "train loss:3.8118973785333594\n",
      "train loss:3.8056402703513466\n",
      "train loss:3.7998625317903416\n",
      "train loss:3.806028548827099\n",
      "train loss:3.7994585797432947\n",
      "train loss:3.787463953742858\n",
      "train loss:3.78627656748838\n",
      "train loss:3.7761578893367025\n",
      "train loss:3.7826116341321856\n",
      "train loss:3.772691757015859\n",
      "train loss:3.7692132926467785\n",
      "train loss:3.769117555304242\n",
      "train loss:3.769698059118645\n",
      "train loss:3.765536316191946\n",
      "train loss:3.7733166658801\n",
      "train loss:3.7603770503329015\n",
      "train loss:3.753484503454131\n",
      "train loss:3.7454060443489183\n",
      "train loss:3.7550999491285135\n",
      "train loss:3.7494128847561625\n",
      "train loss:3.7447335257447323\n",
      "train loss:3.7386345696207712\n",
      "train loss:3.745824387972003\n",
      "train loss:3.735304148565672\n",
      "train loss:3.7300132000095014\n",
      "train loss:3.734511388733048\n",
      "train loss:3.7318191845174935\n",
      "train loss:3.715639562514313\n",
      "train loss:3.7177803409007555\n",
      "train loss:3.7209053134090384\n",
      "train loss:3.7176077086524453\n",
      "train loss:3.7134302397494814\n",
      "train loss:3.7095498289341395\n",
      "train loss:3.7047152957419227\n",
      "train loss:3.705978458919303\n",
      "train loss:3.7049356569911236\n",
      "train loss:3.7022506220309244\n",
      "train loss:3.6980812366162077\n",
      "train loss:3.69339601337723\n",
      "train loss:3.693527806180528\n",
      "train loss:3.689192224188436\n",
      "train loss:3.686071215895897\n",
      "train loss:3.689232016948866\n",
      "train loss:3.6835349004425035\n",
      "train loss:3.6824699864094557\n",
      "train loss:3.6752573260122663\n",
      "train loss:3.669674190539486\n",
      "train loss:3.674295492029695\n",
      "train loss:3.666154678559953\n",
      "train loss:3.665955391125876\n",
      "train loss:3.66610136568261\n",
      "train loss:3.6572786505835504\n",
      "train loss:3.6558524399488714\n",
      "train loss:3.65893048704066\n",
      "train loss:3.6465501676060583\n",
      "train loss:3.649514772332663\n",
      "train loss:3.644655175265237\n",
      "train loss:3.6399955763104703\n",
      "train loss:3.639773297381884\n",
      "train loss:3.6333266724484377\n",
      "train loss:3.634739896789389\n",
      "train loss:3.6325493814114704\n",
      "train loss:3.6221313949941276\n",
      "train loss:3.622947026509899\n",
      "train loss:3.627501596612019\n",
      "train loss:3.618574252544269\n",
      "train loss:3.6134704280624437\n",
      "train loss:3.614704712625917\n",
      "train loss:3.6164110701632968\n",
      "train loss:3.6041868269920565\n",
      "train loss:3.6104079382044247\n",
      "train loss:3.6049227470193443\n",
      "train loss:3.601629411172198\n",
      "train loss:3.5982007190199115\n",
      "train loss:3.600260227449202\n",
      "train loss:3.594987777990503\n",
      "train loss:3.5880298797870873\n",
      "train loss:3.5811896589599272\n",
      "train loss:3.5924664112553844\n",
      "train loss:3.5833810896367044\n",
      "train loss:3.580352574386219\n",
      "train loss:3.575528637510106\n",
      "train loss:3.580976477583435\n",
      "train loss:3.5816143527633724\n",
      "train loss:3.5739346407604184\n",
      "train loss:3.5725961919425027\n",
      "train loss:3.5660768644791876\n",
      "train loss:3.567439445142214\n",
      "train loss:3.5650526828609244\n",
      "train loss:3.5627512745773906\n",
      "train loss:3.5543673891072656\n",
      "train loss:3.5566225179722406\n",
      "train loss:3.548112921020357\n",
      "train loss:3.553563061167054\n",
      "train loss:3.5534198120167675\n",
      "train loss:3.5404487667666893\n",
      "train loss:3.5471813949574496\n",
      "train loss:3.543230141705157\n",
      "train loss:3.5433284963858203\n",
      "train loss:3.537942708759796\n",
      "train loss:3.53537317377024\n",
      "train loss:3.5358294883289116\n",
      "train loss:3.5379232636943163\n",
      "train loss:3.5185337178088085\n",
      "train loss:3.521273872644007\n",
      "train loss:3.5176102076824916\n",
      "train loss:3.516006509098432\n",
      "train loss:3.518335500641868\n",
      "train loss:3.5085328461361525\n",
      "train loss:3.5095278440844835\n",
      "train loss:3.5050118677729163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:3.506993961092923\n",
      "train loss:3.5067376287678353\n",
      "train loss:3.5043746287224122\n",
      "train loss:3.4912795530009815\n",
      "train loss:3.4914663359918965\n",
      "train loss:3.4957455140054714\n",
      "train loss:3.486279520572719\n",
      "train loss:3.4823851538959234\n",
      "train loss:3.4908323083949897\n",
      "train loss:3.4838046740908473\n",
      "train loss:3.4855271181327536\n",
      "train loss:3.478600144108886\n",
      "train loss:3.474794826095016\n",
      "train loss:3.4688162529134683\n",
      "train loss:3.478448588477434\n",
      "train loss:3.4696775530554516\n",
      "train loss:3.4753892852699177\n",
      "train loss:3.467823497821649\n",
      "train loss:3.4625357770515786\n",
      "train loss:3.4672137772502745\n",
      "train loss:3.4582244865072833\n",
      "train loss:3.458247532016065\n",
      "train loss:3.444483244649856\n",
      "train loss:3.4499925453760545\n",
      "train loss:3.4498370347877194\n",
      "train loss:3.446343585704489\n",
      "train loss:3.4379437368595975\n",
      "train loss:3.4453090098280947\n",
      "train loss:3.4440273100404575\n",
      "train loss:3.43904543029457\n",
      "train loss:3.427156069812175\n",
      "train loss:3.440512430908198\n",
      "train loss:3.434051835669951\n",
      "train loss:3.4333233791095727\n",
      "train loss:3.4311173712130483\n",
      "train loss:3.419803334776193\n",
      "train loss:3.425400078184732\n",
      "train loss:3.410452922929609\n",
      "train loss:3.410549173916557\n",
      "train loss:3.424481206065701\n",
      "train loss:3.406176374049579\n",
      "train loss:3.407724696143285\n",
      "train loss:3.4037909890591704\n",
      "train loss:3.405127528857868\n",
      "=== epoch:11, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:3.407840389964486\n",
      "train loss:3.4021638526620626\n",
      "train loss:3.402619283218425\n",
      "train loss:3.3946174078722473\n",
      "train loss:3.390839693274956\n",
      "train loss:3.391475414372885\n",
      "train loss:3.3867761216473857\n",
      "train loss:3.391526746321029\n",
      "train loss:3.384268068556939\n",
      "train loss:3.377988344394428\n",
      "train loss:3.379202076445368\n",
      "train loss:3.382985074397453\n",
      "train loss:3.3755114266471535\n",
      "train loss:3.3816349271934234\n",
      "train loss:3.369225623198189\n",
      "train loss:3.3695208117982465\n",
      "train loss:3.3736519378302345\n",
      "train loss:3.3748933381350446\n",
      "train loss:3.360205926144384\n",
      "train loss:3.3513935479901615\n",
      "train loss:3.358093681941418\n",
      "train loss:3.367416229624715\n",
      "train loss:3.359523437574264\n",
      "train loss:3.3548446202011055\n",
      "train loss:3.3541295576259853\n",
      "train loss:3.342455120569953\n",
      "train loss:3.351029818371395\n",
      "train loss:3.3438581186811103\n",
      "train loss:3.345628930034459\n",
      "train loss:3.3406602505583054\n",
      "train loss:3.340254445508785\n",
      "train loss:3.341374224315074\n",
      "train loss:3.335229119541717\n",
      "train loss:3.3287087755497913\n",
      "train loss:3.3292471231667675\n",
      "train loss:3.32913189617088\n",
      "train loss:3.3345370787382063\n",
      "train loss:3.32425010521123\n",
      "train loss:3.323491498268866\n",
      "train loss:3.3235386149546122\n",
      "train loss:3.3204067308039056\n",
      "train loss:3.319832517622544\n",
      "train loss:3.3203369285874578\n",
      "train loss:3.3169557592947294\n",
      "train loss:3.3014240216557047\n",
      "train loss:3.3175100993968503\n",
      "train loss:3.3046110014044467\n",
      "train loss:3.3072166896866717\n",
      "train loss:3.3016821304444552\n",
      "train loss:3.305612214603312\n",
      "train loss:3.292538699098113\n",
      "train loss:3.303781041991567\n",
      "train loss:3.296168835985661\n",
      "train loss:3.2935787495609423\n",
      "train loss:3.286228086726318\n",
      "train loss:3.2827672400902825\n",
      "train loss:3.292928284477728\n",
      "train loss:3.286029745849385\n",
      "train loss:3.2899621383075885\n",
      "train loss:3.2862780882060934\n",
      "train loss:3.285111651168743\n",
      "train loss:3.2807760838048\n",
      "train loss:3.2782808006339272\n",
      "train loss:3.2800362737159547\n",
      "train loss:3.2741379250209217\n",
      "train loss:3.263800098825396\n",
      "train loss:3.268194023992409\n",
      "train loss:3.266609900554223\n",
      "train loss:3.255424089580954\n",
      "train loss:3.2665546245396078\n",
      "train loss:3.2698065325355703\n",
      "train loss:3.2554885481539637\n",
      "train loss:3.257367205617768\n",
      "train loss:3.2512863439346886\n",
      "train loss:3.2482979485348666\n",
      "train loss:3.24895288396208\n",
      "train loss:3.2576793989298816\n",
      "train loss:3.243109040144018\n",
      "train loss:3.2435545239299386\n",
      "train loss:3.242513041431692\n",
      "train loss:3.2454375229584698\n",
      "train loss:3.241321564683821\n",
      "train loss:3.236490070621982\n",
      "train loss:3.234885459754664\n",
      "train loss:3.2359933097264326\n",
      "train loss:3.230788034650327\n",
      "train loss:3.229642151717098\n",
      "train loss:3.2268013520988217\n",
      "train loss:3.2281318805789443\n",
      "train loss:3.227287678583085\n",
      "train loss:3.225099592103122\n",
      "train loss:3.2220723167638847\n",
      "train loss:3.2243160168070633\n",
      "train loss:3.2235986777632792\n",
      "train loss:3.216830741929786\n",
      "train loss:3.214641776456868\n",
      "train loss:3.21172293288683\n",
      "train loss:3.2081128197892363\n",
      "train loss:3.2111433592178793\n",
      "train loss:3.2036047866387936\n",
      "train loss:3.2061494010940685\n",
      "train loss:3.20376506498169\n",
      "train loss:3.198779026676342\n",
      "train loss:3.18971136137691\n",
      "train loss:3.193968448977239\n",
      "train loss:3.1944727881784916\n",
      "train loss:3.191022895992272\n",
      "train loss:3.183697688145237\n",
      "train loss:3.188083002462139\n",
      "train loss:3.1854529585904814\n",
      "train loss:3.180450596484386\n",
      "train loss:3.1853001096666747\n",
      "train loss:3.1833033026835613\n",
      "train loss:3.174228578816636\n",
      "train loss:3.177200456777481\n",
      "train loss:3.187531561862194\n",
      "train loss:3.1736295940130157\n",
      "train loss:3.1655221046618687\n",
      "train loss:3.1692186746799407\n",
      "train loss:3.1802339441926315\n",
      "train loss:3.164691054028297\n",
      "train loss:3.157780620258824\n",
      "train loss:3.165004030675843\n",
      "train loss:3.1606133138063317\n",
      "train loss:3.163049046776921\n",
      "train loss:3.1558187810957574\n",
      "train loss:3.161932006199752\n",
      "train loss:3.1570443122105996\n",
      "train loss:3.156496930432028\n",
      "train loss:3.1527980333326058\n",
      "train loss:3.1515924013594026\n",
      "train loss:3.1511384450850746\n",
      "train loss:3.1464178395747613\n",
      "train loss:3.1504271277096523\n",
      "train loss:3.147453806977778\n",
      "train loss:3.1387519079348536\n",
      "train loss:3.143123820583308\n",
      "train loss:3.1317081396999145\n",
      "train loss:3.1453370635126676\n",
      "train loss:3.145093178505933\n",
      "train loss:3.132621746642907\n",
      "train loss:3.1351177323285397\n",
      "train loss:3.1302569563907556\n",
      "train loss:3.136273616731442\n",
      "train loss:3.1264231191275287\n",
      "train loss:3.1298969668849907\n",
      "train loss:3.1300259408386797\n",
      "train loss:3.127527491014294\n",
      "train loss:3.1229581148994736\n",
      "train loss:3.1163225868886473\n",
      "train loss:3.122178387135422\n",
      "train loss:3.1186494038826553\n",
      "train loss:3.1123235743160924\n",
      "train loss:3.111734039181913\n",
      "train loss:3.1163162947374556\n",
      "train loss:3.11215439744742\n",
      "train loss:3.1055719783256808\n",
      "train loss:3.108157183794681\n",
      "train loss:3.107641365978937\n",
      "train loss:3.1031450156829847\n",
      "train loss:3.0986373576430295\n",
      "train loss:3.097415053968584\n",
      "train loss:3.107703844258074\n",
      "train loss:3.0960066093529504\n",
      "train loss:3.0883027291022307\n",
      "train loss:3.097401177202433\n",
      "train loss:3.094761153706321\n",
      "train loss:3.0825964426969277\n",
      "train loss:3.0899494880542213\n",
      "train loss:3.088453917998675\n",
      "train loss:3.084961697490349\n",
      "train loss:3.0856243983616856\n",
      "train loss:3.0876711428936083\n",
      "train loss:3.0835528979412175\n",
      "train loss:3.0826570447012887\n",
      "train loss:3.0754723823680017\n",
      "train loss:3.0752678231056194\n",
      "train loss:3.0735977366010063\n",
      "train loss:3.0651864380783223\n",
      "train loss:3.0730217570753178\n",
      "train loss:3.0763847299605103\n",
      "train loss:3.0705013697316677\n",
      "train loss:3.064065570395608\n",
      "train loss:3.065039700087664\n",
      "train loss:3.0581056348523017\n",
      "train loss:3.070900083650385\n",
      "train loss:3.0576165803586712\n",
      "train loss:3.06285887779808\n",
      "train loss:3.058260517981912\n",
      "train loss:3.0604514575388393\n",
      "train loss:3.052362429094967\n",
      "train loss:3.0478905495631485\n",
      "train loss:3.0562175768873536\n",
      "train loss:3.0453906367403003\n",
      "train loss:3.0509518769851596\n",
      "train loss:3.049506320955404\n",
      "train loss:3.0422856291245863\n",
      "train loss:3.0433415372626396\n",
      "train loss:3.0457116743557826\n",
      "train loss:3.0483912921712504\n",
      "=== epoch:12, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:3.0405992773415105\n",
      "train loss:3.0326637624854604\n",
      "train loss:3.03703844984509\n",
      "train loss:3.037305780766717\n",
      "train loss:3.029907119956003\n",
      "train loss:3.0351710263738765\n",
      "train loss:3.0250371381806063\n",
      "train loss:3.0268065043034778\n",
      "train loss:3.027235585778015\n",
      "train loss:3.02879089823745\n",
      "train loss:3.0205109036298206\n",
      "train loss:3.0258250244214127\n",
      "train loss:3.01789384991947\n",
      "train loss:3.029060103033839\n",
      "train loss:3.0218775550985875\n",
      "train loss:3.017382407139433\n",
      "train loss:3.0175008998028705\n",
      "train loss:3.0210931811389914\n",
      "train loss:3.019387910489276\n",
      "train loss:3.0082511485011882\n",
      "train loss:3.014787053738385\n",
      "train loss:3.0047591596816994\n",
      "train loss:3.008547540688013\n",
      "train loss:3.006506562332822\n",
      "train loss:3.003999230261888\n",
      "train loss:3.0002645655031683\n",
      "train loss:3.0035330209539284\n",
      "train loss:2.993650618027525\n",
      "train loss:2.996706350936516\n",
      "train loss:2.99867014617145\n",
      "train loss:3.0007678700031395\n",
      "train loss:2.999882894626386\n",
      "train loss:2.991562876455207\n",
      "train loss:3.0041471037805865\n",
      "train loss:2.989174391010712\n",
      "train loss:2.9912577619259433\n",
      "train loss:2.985480257346336\n",
      "train loss:2.980805617721234\n",
      "train loss:2.980605790930899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.976639981448925\n",
      "train loss:2.983446407161016\n",
      "train loss:2.9787331173207114\n",
      "train loss:2.9753600171763424\n",
      "train loss:2.9782146170327275\n",
      "train loss:2.9773225913761996\n",
      "train loss:2.9739535523697773\n",
      "train loss:2.975755909196101\n",
      "train loss:2.975680424663511\n",
      "train loss:2.96997395061027\n",
      "train loss:2.975574895415025\n",
      "train loss:2.9652328751852135\n",
      "train loss:2.9684976781027106\n",
      "train loss:2.9634574838479386\n",
      "train loss:2.9654761959813856\n",
      "train loss:2.9665906500338584\n",
      "train loss:2.9691579112575712\n",
      "train loss:2.958160637493554\n",
      "train loss:2.956147072806567\n",
      "train loss:2.9562259465254845\n",
      "train loss:2.963211682992214\n",
      "train loss:2.9495881500472834\n",
      "train loss:2.95748154408107\n",
      "train loss:2.9505761328195494\n",
      "train loss:2.9536876989708922\n",
      "train loss:2.94756003810084\n",
      "train loss:2.9506016226264946\n",
      "train loss:2.9550792828295775\n",
      "train loss:2.946436090807375\n",
      "train loss:2.9448532335260036\n",
      "train loss:2.948284668450045\n",
      "train loss:2.945561714052424\n",
      "train loss:2.9401319786979165\n",
      "train loss:2.9456215219326185\n",
      "train loss:2.937005798948679\n",
      "train loss:2.9418802700702162\n",
      "train loss:2.94219505893112\n",
      "train loss:2.9416338937197066\n",
      "train loss:2.9281189876108495\n",
      "train loss:2.936208019006809\n",
      "train loss:2.939212848520805\n",
      "train loss:2.9329256179525935\n",
      "train loss:2.926326396146039\n",
      "train loss:2.9278356916378563\n",
      "train loss:2.923124316832173\n",
      "train loss:2.9270663684039864\n",
      "train loss:2.919395817506489\n",
      "train loss:2.923194468153147\n",
      "train loss:2.9261192491587478\n",
      "train loss:2.9150117464857814\n",
      "train loss:2.9138302396669733\n",
      "train loss:2.9222712403273783\n",
      "train loss:2.9160730608201346\n",
      "train loss:2.9210664750816253\n",
      "train loss:2.9142989585395345\n",
      "train loss:2.9137521008730234\n",
      "train loss:2.91174731236313\n",
      "train loss:2.914163882744561\n",
      "train loss:2.909461735830392\n",
      "train loss:2.912652561840608\n",
      "train loss:2.904385553369396\n",
      "train loss:2.9020115066261027\n",
      "train loss:2.8959110680939255\n",
      "train loss:2.904402007749495\n",
      "train loss:2.902375335070225\n",
      "train loss:2.9002163976867834\n",
      "train loss:2.902556046755854\n",
      "train loss:2.9013116373033694\n",
      "train loss:2.898099052595386\n",
      "train loss:2.8972244628115105\n",
      "train loss:2.905729015655705\n",
      "train loss:2.8978601785436586\n",
      "train loss:2.892644064730039\n",
      "train loss:2.8933445627599887\n",
      "train loss:2.8858412451367994\n",
      "train loss:2.8892861271715384\n",
      "train loss:2.885646245025538\n",
      "train loss:2.88843066037836\n",
      "train loss:2.8864960792487775\n",
      "train loss:2.882047930586655\n",
      "train loss:2.8781842181717834\n",
      "train loss:2.8887279267291617\n",
      "train loss:2.8833542579662255\n",
      "train loss:2.8783183254765135\n",
      "train loss:2.867136842251303\n",
      "train loss:2.8825467245270033\n",
      "train loss:2.8764603736993344\n",
      "train loss:2.8779815767251717\n",
      "train loss:2.8786012735440103\n",
      "train loss:2.8755897119198988\n",
      "train loss:2.876550635247841\n",
      "train loss:2.873447628770942\n",
      "train loss:2.872980667747113\n",
      "train loss:2.8719646576589737\n",
      "train loss:2.863275627289208\n",
      "train loss:2.873695246899968\n",
      "train loss:2.8649992800904305\n",
      "train loss:2.8654696068899894\n",
      "train loss:2.8543214373354684\n",
      "train loss:2.8548379873294154\n",
      "train loss:2.8527516488833022\n",
      "train loss:2.858213378440667\n",
      "train loss:2.851709499605364\n",
      "train loss:2.858362985504203\n",
      "train loss:2.852876002278609\n",
      "train loss:2.8545177304766423\n",
      "train loss:2.8527569435912348\n",
      "train loss:2.859763964604599\n",
      "train loss:2.8471107271444795\n",
      "train loss:2.849387218704422\n",
      "train loss:2.8529289138005653\n",
      "train loss:2.8465137870464523\n",
      "train loss:2.8392874307270395\n",
      "train loss:2.839623157311613\n",
      "train loss:2.841218194392794\n",
      "train loss:2.847840046802297\n",
      "train loss:2.848914128696456\n",
      "train loss:2.8475727214110824\n",
      "train loss:2.83704603228684\n",
      "train loss:2.838676151285191\n",
      "train loss:2.842686316146879\n",
      "train loss:2.8309681353133174\n",
      "train loss:2.837639867811463\n",
      "train loss:2.838064530428259\n",
      "train loss:2.8392035109547766\n",
      "train loss:2.836936924827897\n",
      "train loss:2.833653308363204\n",
      "train loss:2.831937497560234\n",
      "train loss:2.8319432100025432\n",
      "train loss:2.8267281735234295\n",
      "train loss:2.82877442387841\n",
      "train loss:2.833222707238777\n",
      "train loss:2.8225743568118267\n",
      "train loss:2.8256119030168545\n",
      "train loss:2.825765099580064\n",
      "train loss:2.8298337141155896\n",
      "train loss:2.82060562754197\n",
      "train loss:2.8205421329922657\n",
      "train loss:2.819222096590467\n",
      "train loss:2.8200560998062745\n",
      "train loss:2.815178631521145\n",
      "train loss:2.817676120494438\n",
      "train loss:2.8130258244207615\n",
      "train loss:2.819430752559846\n",
      "train loss:2.8191452708118265\n",
      "train loss:2.8088674450103928\n",
      "train loss:2.8131654338910317\n",
      "train loss:2.8067172956477098\n",
      "train loss:2.81055654362261\n",
      "train loss:2.8064362802447826\n",
      "train loss:2.8082763130975605\n",
      "train loss:2.8056717180811486\n",
      "train loss:2.799050648896918\n",
      "train loss:2.798794734714888\n",
      "train loss:2.799794304260649\n",
      "train loss:2.7984800088587285\n",
      "train loss:2.797153570769011\n",
      "train loss:2.810538446791172\n",
      "train loss:2.796807341713773\n",
      "train loss:2.794894948354015\n",
      "train loss:2.7985717825781364\n",
      "=== epoch:13, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.8021614948826654\n",
      "train loss:2.7997648492753604\n",
      "train loss:2.804499482016336\n",
      "train loss:2.7942260190945594\n",
      "train loss:2.7918077902357488\n",
      "train loss:2.796996309797411\n",
      "train loss:2.7920796360604667\n",
      "train loss:2.796683815569064\n",
      "train loss:2.7833407953882228\n",
      "train loss:2.7877779072225115\n",
      "train loss:2.7772312939140247\n",
      "train loss:2.789776689658133\n",
      "train loss:2.784430195031867\n",
      "train loss:2.7849566660647884\n",
      "train loss:2.7874683180370865\n",
      "train loss:2.7850127249057692\n",
      "train loss:2.777840747835096\n",
      "train loss:2.7737084642390277\n",
      "train loss:2.7848353417092566\n",
      "train loss:2.7818220779104617\n",
      "train loss:2.7795406296827045\n",
      "train loss:2.774864202710689\n",
      "train loss:2.778423636540565\n",
      "train loss:2.7762553602986153\n",
      "train loss:2.776408471693621\n",
      "train loss:2.7740956957117784\n",
      "train loss:2.7685473939837486\n",
      "train loss:2.7732112242702516\n",
      "train loss:2.7683974287712982\n",
      "train loss:2.7710357575803823\n",
      "train loss:2.76832489886756\n",
      "train loss:2.7646664695087404\n",
      "train loss:2.7717274278493877\n",
      "train loss:2.76195831432124\n",
      "train loss:2.7646208859003396\n",
      "train loss:2.7692601165592263\n",
      "train loss:2.7615426834305263\n",
      "train loss:2.7603193354110123\n",
      "train loss:2.770988739994688\n",
      "train loss:2.7595701119493508\n",
      "train loss:2.760140254619115\n",
      "train loss:2.753551921523931\n",
      "train loss:2.7567585481267836\n",
      "train loss:2.7518698718595016\n",
      "train loss:2.7478276001276036\n",
      "train loss:2.7639637863066224\n",
      "train loss:2.752154956492649\n",
      "train loss:2.7588634534404233\n",
      "train loss:2.7515856512436705\n",
      "train loss:2.754886229570826\n",
      "train loss:2.7574710984322883\n",
      "train loss:2.75329680666161\n",
      "train loss:2.7445244598183365\n",
      "train loss:2.741573874372298\n",
      "train loss:2.745763959201582\n",
      "train loss:2.7425214941930807\n",
      "train loss:2.7400336239646514\n",
      "train loss:2.74244950896485\n",
      "train loss:2.7462502494252585\n",
      "train loss:2.743021863695615\n",
      "train loss:2.7428985375735655\n",
      "train loss:2.7383722995378754\n",
      "train loss:2.734830125831335\n",
      "train loss:2.735981844127299\n",
      "train loss:2.7294491041310027\n",
      "train loss:2.7357942490118576\n",
      "train loss:2.731142738780718\n",
      "train loss:2.7329191572613296\n",
      "train loss:2.731273564031994\n",
      "train loss:2.7323683472873213\n",
      "train loss:2.728034953701405\n",
      "train loss:2.734922930949143\n",
      "train loss:2.7326874528636864\n",
      "train loss:2.7250016655818694\n",
      "train loss:2.7294165841221076\n",
      "train loss:2.730822505285083\n",
      "train loss:2.7242811789698234\n",
      "train loss:2.7357208366584365\n",
      "train loss:2.725686466658993\n",
      "train loss:2.720206380521673\n",
      "train loss:2.7240809786111546\n",
      "train loss:2.716790178411841\n",
      "train loss:2.716627408551415\n",
      "train loss:2.726176364780877\n",
      "train loss:2.718819132583469\n",
      "train loss:2.72179918818783\n",
      "train loss:2.7059104308716004\n",
      "train loss:2.7221825120589807\n",
      "train loss:2.709382566974808\n",
      "train loss:2.7206760122309\n",
      "train loss:2.721488135774592\n",
      "train loss:2.7137877526136527\n",
      "train loss:2.70682190590922\n",
      "train loss:2.7102849097986197\n",
      "train loss:2.715252589437042\n",
      "train loss:2.708318234273959\n",
      "train loss:2.704777749980269\n",
      "train loss:2.7046215227278547\n",
      "train loss:2.7069268887075726\n",
      "train loss:2.7060774697303556\n",
      "train loss:2.708818504543541\n",
      "train loss:2.6990847206860775\n",
      "train loss:2.7022413908323357\n",
      "train loss:2.704283930253189\n",
      "train loss:2.703648759038223\n",
      "train loss:2.7029313145666247\n",
      "train loss:2.7082640877915303\n",
      "train loss:2.6985811858633553\n",
      "train loss:2.691113047023851\n",
      "train loss:2.7000440991438843\n",
      "train loss:2.6996216539101616\n",
      "train loss:2.705294618323312\n",
      "train loss:2.7000405286593083\n",
      "train loss:2.691994407611824\n",
      "train loss:2.7059773189709033\n",
      "train loss:2.6964014933048857\n",
      "train loss:2.6934641748019987\n",
      "train loss:2.6914290756875285\n",
      "train loss:2.6890268562369113\n",
      "train loss:2.6947861989309034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.6946847454869634\n",
      "train loss:2.6821559442835117\n",
      "train loss:2.6932739862991815\n",
      "train loss:2.6989070022697077\n",
      "train loss:2.682096234424223\n",
      "train loss:2.6868760827625184\n",
      "train loss:2.6809222713025553\n",
      "train loss:2.688438027127056\n",
      "train loss:2.6760477488257255\n",
      "train loss:2.694492857237318\n",
      "train loss:2.690078102455413\n",
      "train loss:2.676921896473519\n",
      "train loss:2.679823725636977\n",
      "train loss:2.677296473358645\n",
      "train loss:2.684278694359397\n",
      "train loss:2.6819180938022416\n",
      "train loss:2.677534433501274\n",
      "train loss:2.675188718395214\n",
      "train loss:2.678957173075719\n",
      "train loss:2.673231858409459\n",
      "train loss:2.674047772294455\n",
      "train loss:2.6808019248522106\n",
      "train loss:2.6728664257998975\n",
      "train loss:2.6761822595080242\n",
      "train loss:2.674159085860472\n",
      "train loss:2.666230310469118\n",
      "train loss:2.6713081751894268\n",
      "train loss:2.663728910980119\n",
      "train loss:2.669302900313901\n",
      "train loss:2.668800033255277\n",
      "train loss:2.6596634770981638\n",
      "train loss:2.6665462108770144\n",
      "train loss:2.6706098377067584\n",
      "train loss:2.6687227278569994\n",
      "train loss:2.6633256210207197\n",
      "train loss:2.658553018896414\n",
      "train loss:2.6670280898486283\n",
      "train loss:2.6679967760928083\n",
      "train loss:2.659395134034164\n",
      "train loss:2.664858263564266\n",
      "train loss:2.6631615016297667\n",
      "train loss:2.659739895471578\n",
      "train loss:2.6630096356931974\n",
      "train loss:2.651431710912337\n",
      "train loss:2.6565734028489048\n",
      "train loss:2.6576999350924537\n",
      "train loss:2.6515265274202067\n",
      "train loss:2.662504306175827\n",
      "train loss:2.6507087485100014\n",
      "train loss:2.663464637839045\n",
      "train loss:2.6537809359936415\n",
      "train loss:2.6525348652647827\n",
      "train loss:2.651738478181854\n",
      "train loss:2.652792716434721\n",
      "train loss:2.651575356360889\n",
      "train loss:2.6457013147543638\n",
      "train loss:2.6503112749073163\n",
      "train loss:2.641032121930678\n",
      "train loss:2.647220023196013\n",
      "train loss:2.6474842449005602\n",
      "train loss:2.6417856817368137\n",
      "train loss:2.643852125284821\n",
      "train loss:2.641956647609638\n",
      "train loss:2.6385263680483773\n",
      "train loss:2.63909051154254\n",
      "train loss:2.641711079030352\n",
      "train loss:2.638383406158769\n",
      "train loss:2.63638193361978\n",
      "train loss:2.644886770073748\n",
      "train loss:2.6432115166196017\n",
      "train loss:2.6442999174194166\n",
      "train loss:2.6467125621265617\n",
      "train loss:2.640714057501597\n",
      "train loss:2.639257858340004\n",
      "train loss:2.6314910475893885\n",
      "train loss:2.6379796715229107\n",
      "train loss:2.6359071813378434\n",
      "train loss:2.6332606712753925\n",
      "train loss:2.637300054554475\n",
      "train loss:2.634206643048784\n",
      "=== epoch:14, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.6351418246180374\n",
      "train loss:2.6334268219725807\n",
      "train loss:2.6352030056204856\n",
      "train loss:2.626268952486597\n",
      "train loss:2.642355714189958\n",
      "train loss:2.6269602088511053\n",
      "train loss:2.626897368831926\n",
      "train loss:2.627364293366114\n",
      "train loss:2.631349111958297\n",
      "train loss:2.6242572808464404\n",
      "train loss:2.6250485923496663\n",
      "train loss:2.630878561972799\n",
      "train loss:2.6289936524033215\n",
      "train loss:2.619295772190426\n",
      "train loss:2.6216944214385354\n",
      "train loss:2.6214111775529774\n",
      "train loss:2.618450557437046\n",
      "train loss:2.6276645216560763\n",
      "train loss:2.632210142966202\n",
      "train loss:2.6221305196289744\n",
      "train loss:2.619522226846947\n",
      "train loss:2.6162768813951542\n",
      "train loss:2.6247187493441926\n",
      "train loss:2.6177037643360537\n",
      "train loss:2.6244108449599337\n",
      "train loss:2.617357544628611\n",
      "train loss:2.6169963713266813\n",
      "train loss:2.615250376351642\n",
      "train loss:2.6119794920449957\n",
      "train loss:2.611231763569927\n",
      "train loss:2.61482463876469\n",
      "train loss:2.6175876976598484\n",
      "train loss:2.612064018762254\n",
      "train loss:2.612383896656529\n",
      "train loss:2.6115102118326576\n",
      "train loss:2.6152042762479355\n",
      "train loss:2.609239192177018\n",
      "train loss:2.612282160353435\n",
      "train loss:2.615025887571724\n",
      "train loss:2.6110067060711177\n",
      "train loss:2.6016888649010492\n",
      "train loss:2.6091121610838646\n",
      "train loss:2.614014269251849\n",
      "train loss:2.602556412734968\n",
      "train loss:2.6061323027859187\n",
      "train loss:2.612032175477426\n",
      "train loss:2.6081082637727926\n",
      "train loss:2.602587410558861\n",
      "train loss:2.60313155706414\n",
      "train loss:2.612559781230993\n",
      "train loss:2.597131082490334\n",
      "train loss:2.6026765613136145\n",
      "train loss:2.602771299762787\n",
      "train loss:2.603686009527427\n",
      "train loss:2.6005012282794415\n",
      "train loss:2.602576021844616\n",
      "train loss:2.5969271494222\n",
      "train loss:2.6033074929343645\n",
      "train loss:2.601229422587916\n",
      "train loss:2.5969905797316453\n",
      "train loss:2.591658655707683\n",
      "train loss:2.5942203041564866\n",
      "train loss:2.6032771538219848\n",
      "train loss:2.588180125590124\n",
      "train loss:2.5994264262916884\n",
      "train loss:2.597218053539421\n",
      "train loss:2.5874610612768096\n",
      "train loss:2.5949615361448846\n",
      "train loss:2.5905684562116646\n",
      "train loss:2.595677421160118\n",
      "train loss:2.5789101360671385\n",
      "train loss:2.591672425450964\n",
      "train loss:2.599681713122187\n",
      "train loss:2.578991982527248\n",
      "train loss:2.5938136833075993\n",
      "train loss:2.5885058321265824\n",
      "train loss:2.5901059434993634\n",
      "train loss:2.575356938315191\n",
      "train loss:2.587204151597244\n",
      "train loss:2.578576397036382\n",
      "train loss:2.5858260053574664\n",
      "train loss:2.5812111524781693\n",
      "train loss:2.579756564501362\n",
      "train loss:2.582502500038408\n",
      "train loss:2.5804964783416664\n",
      "train loss:2.5765176503724363\n",
      "train loss:2.5918823715779284\n",
      "train loss:2.5862332642676007\n",
      "train loss:2.5780425012744574\n",
      "train loss:2.5766493603951206\n",
      "train loss:2.5776037554495996\n",
      "train loss:2.5783158495004925\n",
      "train loss:2.5680354553578844\n",
      "train loss:2.570690645509062\n",
      "train loss:2.5746725948813918\n",
      "train loss:2.5797861076466537\n",
      "train loss:2.578380178816029\n",
      "train loss:2.5683264441268836\n",
      "train loss:2.578126593082926\n",
      "train loss:2.5801493133814843\n",
      "train loss:2.563614076322449\n",
      "train loss:2.5805982866163046\n",
      "train loss:2.5734492057444567\n",
      "train loss:2.574825754408925\n",
      "train loss:2.561116269979572\n",
      "train loss:2.5660023989498275\n",
      "train loss:2.57493777200036\n",
      "train loss:2.574173188834794\n",
      "train loss:2.56416737736754\n",
      "train loss:2.577270372806621\n",
      "train loss:2.5686340614252456\n",
      "train loss:2.566552100026469\n",
      "train loss:2.564186949742259\n",
      "train loss:2.571824613238328\n",
      "train loss:2.5782220896419674\n",
      "train loss:2.5673657494941056\n",
      "train loss:2.5651559984850247\n",
      "train loss:2.5657115812485953\n",
      "train loss:2.5643045848283603\n",
      "train loss:2.5663073707417845\n",
      "train loss:2.5628250484184063\n",
      "train loss:2.5620941335641936\n",
      "train loss:2.5639614237457042\n",
      "train loss:2.5569064682153417\n",
      "train loss:2.5540271252112974\n",
      "train loss:2.5685483374888456\n",
      "train loss:2.5505334399574227\n",
      "train loss:2.548668893355294\n",
      "train loss:2.559533262452892\n",
      "train loss:2.557854347537014\n",
      "train loss:2.551840457197694\n",
      "train loss:2.5509448525434895\n",
      "train loss:2.5545463424051755\n",
      "train loss:2.56073331358776\n",
      "train loss:2.5516492940511215\n",
      "train loss:2.5503620462554806\n",
      "train loss:2.5544607926574807\n",
      "train loss:2.5547020069774327\n",
      "train loss:2.557794412852781\n",
      "train loss:2.545434922025147\n",
      "train loss:2.547073923728076\n",
      "train loss:2.549274016226683\n",
      "train loss:2.5526860464781445\n",
      "train loss:2.551088335200149\n",
      "train loss:2.5495348700153286\n",
      "train loss:2.5545116209486203\n",
      "train loss:2.5479750917433157\n",
      "train loss:2.5491421041016027\n",
      "train loss:2.5431803698303894\n",
      "train loss:2.5482744180071153\n",
      "train loss:2.545550739308993\n",
      "train loss:2.544619846085915\n",
      "train loss:2.5381525088435635\n",
      "train loss:2.551011820176743\n",
      "train loss:2.5397782067115657\n",
      "train loss:2.549543712921568\n",
      "train loss:2.5429231776875287\n",
      "train loss:2.537864702440031\n",
      "train loss:2.5421334004366303\n",
      "train loss:2.535481455750958\n",
      "train loss:2.548530287451071\n",
      "train loss:2.5431148853088352\n",
      "train loss:2.540526957743301\n",
      "train loss:2.5488465281846704\n",
      "train loss:2.542908924428956\n",
      "train loss:2.5373552451093393\n",
      "train loss:2.540765955002205\n",
      "train loss:2.5398049499711695\n",
      "train loss:2.540544311138654\n",
      "train loss:2.5417056135003415\n",
      "train loss:2.5384940109083405\n",
      "train loss:2.542689708994429\n",
      "train loss:2.535975954083749\n",
      "train loss:2.5380823260532974\n",
      "train loss:2.5451934321809495\n",
      "train loss:2.526874158603384\n",
      "train loss:2.5369893826153183\n",
      "train loss:2.53158743039268\n",
      "train loss:2.532605733121872\n",
      "train loss:2.5345865863656263\n",
      "train loss:2.5325908436264464\n",
      "train loss:2.5356818606500298\n",
      "train loss:2.533980879171619\n",
      "train loss:2.5232705755745806\n",
      "train loss:2.530035945575333\n",
      "train loss:2.52374973989594\n",
      "train loss:2.5356729600750314\n",
      "train loss:2.524798324972178\n",
      "train loss:2.529331268845302\n",
      "train loss:2.5294237501492485\n",
      "train loss:2.5318234894352725\n",
      "train loss:2.5390690831302645\n",
      "train loss:2.531087428110302\n",
      "train loss:2.5295986676301485\n",
      "train loss:2.529747753353672\n",
      "train loss:2.5363483211696893\n",
      "train loss:2.5259161275064796\n",
      "train loss:2.53559459477209\n",
      "train loss:2.523459721940336\n",
      "train loss:2.528515258345177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:15, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.513842117105434\n",
      "train loss:2.5171871940125885\n",
      "train loss:2.5289568392525474\n",
      "train loss:2.5201811954591555\n",
      "train loss:2.5139863368492112\n",
      "train loss:2.524819115708654\n",
      "train loss:2.513741182883042\n",
      "train loss:2.523015286518033\n",
      "train loss:2.523635141394774\n",
      "train loss:2.5244320370426676\n",
      "train loss:2.52079672039044\n",
      "train loss:2.5169137590574384\n",
      "train loss:2.5217783223439305\n",
      "train loss:2.523493423095575\n",
      "train loss:2.5130811353376687\n",
      "train loss:2.5144329660530933\n",
      "train loss:2.5144213873966854\n",
      "train loss:2.5245816843850015\n",
      "train loss:2.5139584485286046\n",
      "train loss:2.5166753143096035\n",
      "train loss:2.515252561458298\n",
      "train loss:2.510654170709952\n",
      "train loss:2.5155269046714337\n",
      "train loss:2.5086727036915386\n",
      "train loss:2.516528136389025\n",
      "train loss:2.5182290517985635\n",
      "train loss:2.515351508974511\n",
      "train loss:2.5135069935416547\n",
      "train loss:2.5103342459149314\n",
      "train loss:2.516171556989727\n",
      "train loss:2.509269313846136\n",
      "train loss:2.5187456609654113\n",
      "train loss:2.510869871933813\n",
      "train loss:2.5143923713352203\n",
      "train loss:2.5114194325162638\n",
      "train loss:2.5125749308390444\n",
      "train loss:2.5084371143218522\n",
      "train loss:2.5083140804375144\n",
      "train loss:2.5039109612145407\n",
      "train loss:2.506920175805654\n",
      "train loss:2.5059867072040305\n",
      "train loss:2.5018662309601245\n",
      "train loss:2.5103428858355423\n",
      "train loss:2.507468222598361\n",
      "train loss:2.501181525167568\n",
      "train loss:2.509221630989843\n",
      "train loss:2.5040138944090296\n",
      "train loss:2.500045236243367\n",
      "train loss:2.5045021814961146\n",
      "train loss:2.4963007224429252\n",
      "train loss:2.5029748293210163\n",
      "train loss:2.5023959996207172\n",
      "train loss:2.5014069580910685\n",
      "train loss:2.5032778217098537\n",
      "train loss:2.497770128724842\n",
      "train loss:2.5010516598560972\n",
      "train loss:2.505268737745064\n",
      "train loss:2.5093421931219067\n",
      "train loss:2.496448325451841\n",
      "train loss:2.500905529738741\n",
      "train loss:2.500665464875505\n",
      "train loss:2.4965574228179226\n",
      "train loss:2.498795132097918\n",
      "train loss:2.5003703757577\n",
      "train loss:2.493649793645861\n",
      "train loss:2.4982895117913655\n",
      "train loss:2.4974571740554605\n",
      "train loss:2.4975326300060745\n",
      "train loss:2.4891873009112073\n",
      "train loss:2.4944634308804305\n",
      "train loss:2.4913220252118626\n",
      "train loss:2.4948647281738547\n",
      "train loss:2.487769389095513\n",
      "train loss:2.488487531069215\n",
      "train loss:2.4989888865562286\n",
      "train loss:2.4910779348594194\n",
      "train loss:2.4923828075776493\n",
      "train loss:2.494376874878061\n",
      "train loss:2.489253563850084\n",
      "train loss:2.492481915191383\n",
      "train loss:2.4858892416653284\n",
      "train loss:2.4878707027551834\n",
      "train loss:2.486992050109248\n",
      "train loss:2.4825649855153413\n",
      "train loss:2.4953576842873013\n",
      "train loss:2.4840986744265194\n",
      "train loss:2.4832830214638317\n",
      "train loss:2.490498744822772\n",
      "train loss:2.47957513649464\n",
      "train loss:2.48637686276495\n",
      "train loss:2.4796910147156788\n",
      "train loss:2.482039114413977\n",
      "train loss:2.4829276559904336\n",
      "train loss:2.4918115282528506\n",
      "train loss:2.481922187652096\n",
      "train loss:2.4823580048998592\n",
      "train loss:2.489425412873441\n",
      "train loss:2.485701161778155\n",
      "train loss:2.4901465082570557\n",
      "train loss:2.481856611323625\n",
      "train loss:2.4853325662093844\n",
      "train loss:2.4798434358786134\n",
      "train loss:2.485064676698058\n",
      "train loss:2.4751898919133506\n",
      "train loss:2.4790843364098922\n",
      "train loss:2.4740353504583124\n",
      "train loss:2.4720106850917576\n",
      "train loss:2.4770842774655804\n",
      "train loss:2.4891570379575247\n",
      "train loss:2.4893603183599122\n",
      "train loss:2.479091058523849\n",
      "train loss:2.482371924129312\n",
      "train loss:2.4738897797077524\n",
      "train loss:2.4832515318403288\n",
      "train loss:2.4697816502739305\n",
      "train loss:2.4825057051961963\n",
      "train loss:2.4727946939944117\n",
      "train loss:2.4803919316988186\n",
      "train loss:2.4675956655425573\n",
      "train loss:2.4809897410289654\n",
      "train loss:2.476732345552509\n",
      "train loss:2.4802037911318835\n",
      "train loss:2.475487058098874\n",
      "train loss:2.4710091302814714\n",
      "train loss:2.470836354642521\n",
      "train loss:2.472782078574161\n",
      "train loss:2.4669918214640565\n",
      "train loss:2.468343010539323\n",
      "train loss:2.476211930676908\n",
      "train loss:2.4707212201467126\n",
      "train loss:2.4724879909711475\n",
      "train loss:2.4732022889574443\n",
      "train loss:2.473191521976665\n",
      "train loss:2.481275163183154\n",
      "train loss:2.473707337023309\n",
      "train loss:2.4718482573437495\n",
      "train loss:2.4758805831871475\n",
      "train loss:2.4647500946153102\n",
      "train loss:2.4701210962902125\n",
      "train loss:2.46812523497116\n",
      "train loss:2.4695169263404764\n",
      "train loss:2.473654650926512\n",
      "train loss:2.464775761288652\n",
      "train loss:2.4598117709259477\n",
      "train loss:2.467136342135099\n",
      "train loss:2.4580971809753636\n",
      "train loss:2.4710826645752606\n",
      "train loss:2.4709040760765943\n",
      "train loss:2.4633053275993584\n",
      "train loss:2.4612956776740433\n",
      "train loss:2.4672315435328946\n",
      "train loss:2.468923809492977\n",
      "train loss:2.463436485814536\n",
      "train loss:2.4585748721697636\n",
      "train loss:2.4660846413778095\n",
      "train loss:2.4695897071761013\n",
      "train loss:2.456374716561688\n",
      "train loss:2.458838395964302\n",
      "train loss:2.4520405402750303\n",
      "train loss:2.458562768614637\n",
      "train loss:2.4642744406696995\n",
      "train loss:2.4692372275329606\n",
      "train loss:2.459222430115015\n",
      "train loss:2.4573944345851975\n",
      "train loss:2.4653704880153358\n",
      "train loss:2.4550145087788753\n",
      "train loss:2.4595679184171546\n",
      "train loss:2.458780940338695\n",
      "train loss:2.4628949422095796\n",
      "train loss:2.457845348357997\n",
      "train loss:2.463452688479339\n",
      "train loss:2.4698310831609427\n",
      "train loss:2.4593251449148403\n",
      "train loss:2.458564546496227\n",
      "train loss:2.457336769456485\n",
      "train loss:2.4609777408418942\n",
      "train loss:2.4522587688352617\n",
      "train loss:2.457617661785112\n",
      "train loss:2.4601203827024287\n",
      "train loss:2.451124463395022\n",
      "train loss:2.457598639950262\n",
      "train loss:2.451152904319682\n",
      "train loss:2.4615768349393274\n",
      "train loss:2.453891923252012\n",
      "train loss:2.4544476123461703\n",
      "train loss:2.456098724882635\n",
      "train loss:2.4564182809355857\n",
      "train loss:2.4516526175468214\n",
      "train loss:2.4534752327208067\n",
      "train loss:2.456311696973481\n",
      "train loss:2.453935460980469\n",
      "train loss:2.4465865344792954\n",
      "train loss:2.4456830168278754\n",
      "train loss:2.4514678168961703\n",
      "train loss:2.4556088115993604\n",
      "train loss:2.4448980172802544\n",
      "train loss:2.4576279842582824\n",
      "train loss:2.4525148990776415\n",
      "train loss:2.451326528303224\n",
      "train loss:2.45952578089955\n",
      "=== epoch:16, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.446520986002167\n",
      "train loss:2.4496171161862685\n",
      "train loss:2.444460156577208\n",
      "train loss:2.4401945922293815\n",
      "train loss:2.4337139359463165\n",
      "train loss:2.457795577744461\n",
      "train loss:2.4445967261546544\n",
      "train loss:2.4491083902472366\n",
      "train loss:2.44531461457546\n",
      "train loss:2.4455376869197156\n",
      "train loss:2.4588836860350716\n",
      "train loss:2.4449020293484853\n",
      "train loss:2.439710120904248\n",
      "train loss:2.444654928351155\n",
      "train loss:2.4474876535705996\n",
      "train loss:2.4412271542228727\n",
      "train loss:2.4506260015969974\n",
      "train loss:2.440782046925207\n",
      "train loss:2.455801467349103\n",
      "train loss:2.445189566203924\n",
      "train loss:2.443634270611954\n",
      "train loss:2.44382400896815\n",
      "train loss:2.4521179282067416\n",
      "train loss:2.4482160094034473\n",
      "train loss:2.446606554743912\n",
      "train loss:2.443858606843521\n",
      "train loss:2.4463190474825915\n",
      "train loss:2.4391040632593435\n",
      "train loss:2.4404068394083342\n",
      "train loss:2.447078299630262\n",
      "train loss:2.433158335982627\n",
      "train loss:2.443126350114864\n",
      "train loss:2.4429448546948\n",
      "train loss:2.4397899075836196\n",
      "train loss:2.4431928802172957\n",
      "train loss:2.4350366170101516\n",
      "train loss:2.4426122480155756\n",
      "train loss:2.437528021133593\n",
      "train loss:2.439562456068899\n",
      "train loss:2.439840702003344\n",
      "train loss:2.4407804367697556\n",
      "train loss:2.4256191869610353\n",
      "train loss:2.436212153161982\n",
      "train loss:2.4359741232020067\n",
      "train loss:2.437173465717531\n",
      "train loss:2.429560599316052\n",
      "train loss:2.444280525268467\n",
      "train loss:2.4414925001626404\n",
      "train loss:2.4410621819508402\n",
      "train loss:2.429421403517519\n",
      "train loss:2.4412875988334597\n",
      "train loss:2.434599592248394\n",
      "train loss:2.4375477578319606\n",
      "train loss:2.429936430447044\n",
      "train loss:2.432239676172519\n",
      "train loss:2.4390762075414116\n",
      "train loss:2.4300728937636134\n",
      "train loss:2.4327768317099117\n",
      "train loss:2.433715038306431\n",
      "train loss:2.4364455649242545\n",
      "train loss:2.434942868197161\n",
      "train loss:2.438169173721709\n",
      "train loss:2.430807340567686\n",
      "train loss:2.430494889330651\n",
      "train loss:2.4240716243010483\n",
      "train loss:2.4298087164867037\n",
      "train loss:2.4419303563534305\n",
      "train loss:2.431555620702824\n",
      "train loss:2.4295537703912835\n",
      "train loss:2.4282902137875064\n",
      "train loss:2.4331873049407164\n",
      "train loss:2.4283402314913953\n",
      "train loss:2.421935565179968\n",
      "train loss:2.4320107100431048\n",
      "train loss:2.4285805512678973\n",
      "train loss:2.4378479017463457\n",
      "train loss:2.4239798174788896\n",
      "train loss:2.4203014938631986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.4252010936733503\n",
      "train loss:2.4303818359213087\n",
      "train loss:2.432842491697305\n",
      "train loss:2.4263035672087856\n",
      "train loss:2.4178570978037994\n",
      "train loss:2.4276160509699602\n",
      "train loss:2.4222722579128844\n",
      "train loss:2.4273379461669276\n",
      "train loss:2.4209371720041286\n",
      "train loss:2.428498200733453\n",
      "train loss:2.428032978112804\n",
      "train loss:2.4256598043780118\n",
      "train loss:2.419056232089167\n",
      "train loss:2.4278308684030616\n",
      "train loss:2.419200106504039\n",
      "train loss:2.4231591454672143\n",
      "train loss:2.4275570959308483\n",
      "train loss:2.4196027409761593\n",
      "train loss:2.4337571146005357\n",
      "train loss:2.4250285628533037\n",
      "train loss:2.4172103889262937\n",
      "train loss:2.4347615370920925\n",
      "train loss:2.423105766373376\n",
      "train loss:2.4291390953945324\n",
      "train loss:2.4159391007970443\n",
      "train loss:2.421751479408103\n",
      "train loss:2.418428485512001\n",
      "train loss:2.4155137799099364\n",
      "train loss:2.4239092677523675\n",
      "train loss:2.4197249770174603\n",
      "train loss:2.4222310799822586\n",
      "train loss:2.416730117205452\n",
      "train loss:2.4107241822458185\n",
      "train loss:2.4248150155185426\n",
      "train loss:2.4205835583394006\n",
      "train loss:2.4273676301553793\n",
      "train loss:2.4237208615148234\n",
      "train loss:2.420421595061164\n",
      "train loss:2.429658938903985\n",
      "train loss:2.4261604616793977\n",
      "train loss:2.4183887605865\n",
      "train loss:2.4180580331465276\n",
      "train loss:2.41652731762548\n",
      "train loss:2.418377660468935\n",
      "train loss:2.4177493116739908\n",
      "train loss:2.42030150681749\n",
      "train loss:2.432115268734334\n",
      "train loss:2.4136473788502832\n",
      "train loss:2.4105481980054493\n",
      "train loss:2.413189961489909\n",
      "train loss:2.418315462875817\n",
      "train loss:2.407827199513174\n",
      "train loss:2.4203178637204394\n",
      "train loss:2.4238026085727746\n",
      "train loss:2.4248799830975214\n",
      "train loss:2.4217782016557705\n",
      "train loss:2.416562123422583\n",
      "train loss:2.421957088376788\n",
      "train loss:2.417174780531139\n",
      "train loss:2.4168610726804425\n",
      "train loss:2.4131103441520168\n",
      "train loss:2.4143170550948665\n",
      "train loss:2.4097367972239194\n",
      "train loss:2.4111347646273287\n",
      "train loss:2.426614186992119\n",
      "train loss:2.4132927155333044\n",
      "train loss:2.4101110784442765\n",
      "train loss:2.4125657964206173\n",
      "train loss:2.4096318172014315\n",
      "train loss:2.4115233269715137\n",
      "train loss:2.410981440984804\n",
      "train loss:2.4016358520360095\n",
      "train loss:2.4051345808853206\n",
      "train loss:2.4199012832950424\n",
      "train loss:2.4073938613601897\n",
      "train loss:2.408332424477941\n",
      "train loss:2.399365465777964\n",
      "train loss:2.4080516723281997\n",
      "train loss:2.4067097503752994\n",
      "train loss:2.416404756116148\n",
      "train loss:2.4034209646309166\n",
      "train loss:2.401770332146286\n",
      "train loss:2.4111362096088516\n",
      "train loss:2.4043815429535824\n",
      "train loss:2.417760003781667\n",
      "train loss:2.404984849211003\n",
      "train loss:2.4139987521828288\n",
      "train loss:2.4018697388759915\n",
      "train loss:2.403559245498839\n",
      "train loss:2.4147804400796637\n",
      "train loss:2.399539255982901\n",
      "train loss:2.405597673820543\n",
      "train loss:2.4036762654102697\n",
      "train loss:2.4136967831533873\n",
      "train loss:2.409783367026542\n",
      "train loss:2.4058907860592265\n",
      "train loss:2.4040095588119517\n",
      "train loss:2.4056403775767756\n",
      "train loss:2.412022843957235\n",
      "train loss:2.4029303319171116\n",
      "train loss:2.407422381555158\n",
      "train loss:2.397772996971194\n",
      "train loss:2.404463768742905\n",
      "train loss:2.409609643325201\n",
      "train loss:2.4119767270381347\n",
      "train loss:2.3999330518095547\n",
      "train loss:2.3938566780885795\n",
      "train loss:2.401976560895951\n",
      "train loss:2.4090254334945325\n",
      "train loss:2.40422435566653\n",
      "train loss:2.402253946456007\n",
      "train loss:2.389647969181416\n",
      "train loss:2.3985790973019627\n",
      "train loss:2.403376709824113\n",
      "train loss:2.4105335180090672\n",
      "train loss:2.400917976676188\n",
      "train loss:2.403409040640614\n",
      "train loss:2.4016702812597397\n",
      "train loss:2.3975464623837133\n",
      "train loss:2.3991746023714056\n",
      "train loss:2.4038977012034466\n",
      "train loss:2.4005958706589903\n",
      "=== epoch:17, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.402692402822585\n",
      "train loss:2.403147653083352\n",
      "train loss:2.410812856757757\n",
      "train loss:2.3979153104475555\n",
      "train loss:2.4010473550325813\n",
      "train loss:2.3991259932503572\n",
      "train loss:2.39917234303437\n",
      "train loss:2.401537191251849\n",
      "train loss:2.395720874492558\n",
      "train loss:2.406758422288858\n",
      "train loss:2.396329016769875\n",
      "train loss:2.4019212039938624\n",
      "train loss:2.3993470394356207\n",
      "train loss:2.399364986493214\n",
      "train loss:2.396257005750475\n",
      "train loss:2.3958155068143268\n",
      "train loss:2.393560007939564\n",
      "train loss:2.3975416127475935\n",
      "train loss:2.402564497020562\n",
      "train loss:2.397830577413295\n",
      "train loss:2.393474123783505\n",
      "train loss:2.3941568015361585\n",
      "train loss:2.4000358584036645\n",
      "train loss:2.400029847299892\n",
      "train loss:2.3904125164357337\n",
      "train loss:2.3931592663374612\n",
      "train loss:2.3930767255957313\n",
      "train loss:2.398889697189191\n",
      "train loss:2.3979574614052583\n",
      "train loss:2.3981593411318753\n",
      "train loss:2.3965029329672887\n",
      "train loss:2.3926495506232364\n",
      "train loss:2.39976412003903\n",
      "train loss:2.3936666329693375\n",
      "train loss:2.3965242745612003\n",
      "train loss:2.384667347604426\n",
      "train loss:2.391640418445168\n",
      "train loss:2.397587844576037\n",
      "train loss:2.393685393074326\n",
      "train loss:2.3860688305555358\n",
      "train loss:2.405015217425554\n",
      "train loss:2.393644415138471\n",
      "train loss:2.3982402347997898\n",
      "train loss:2.4009837409806276\n",
      "train loss:2.397773080692575\n",
      "train loss:2.3898631500762346\n",
      "train loss:2.3951850686187774\n",
      "train loss:2.386938679591606\n",
      "train loss:2.389281196882684\n",
      "train loss:2.3990610015343834\n",
      "train loss:2.386519246202465\n",
      "train loss:2.3900888812610988\n",
      "train loss:2.3929743953718807\n",
      "train loss:2.394902044478191\n",
      "train loss:2.396145954414869\n",
      "train loss:2.3953968206214578\n",
      "train loss:2.3860010117735513\n",
      "train loss:2.385042199749243\n",
      "train loss:2.395764619334752\n",
      "train loss:2.3946586078298586\n",
      "train loss:2.3964948697602044\n",
      "train loss:2.3892243856553312\n",
      "train loss:2.391397327744265\n",
      "train loss:2.3929177849893426\n",
      "train loss:2.395923946343151\n",
      "train loss:2.391000232719425\n",
      "train loss:2.3962827369854622\n",
      "train loss:2.3860262397504295\n",
      "train loss:2.3852025465516586\n",
      "train loss:2.392192822869858\n",
      "train loss:2.3907641138201683\n",
      "train loss:2.3806707144236317\n",
      "train loss:2.3983995618435356\n",
      "train loss:2.383544265459733\n",
      "train loss:2.3872674466921997\n",
      "train loss:2.387218218872057\n",
      "train loss:2.393168537259143\n",
      "train loss:2.3946436481275515\n",
      "train loss:2.387033923702799\n",
      "train loss:2.3826277379911316\n",
      "train loss:2.3929718626704877\n",
      "train loss:2.3844842794898846\n",
      "train loss:2.383574598663127\n",
      "train loss:2.3777406913752697\n",
      "train loss:2.384711293358893\n",
      "train loss:2.3798167213488\n",
      "train loss:2.378807518866447\n",
      "train loss:2.398240356716099\n",
      "train loss:2.3845016493026727\n",
      "train loss:2.3767754317629977\n",
      "train loss:2.392301506651251\n",
      "train loss:2.3885783204759625\n",
      "train loss:2.3859327465421245\n",
      "train loss:2.374704883679732\n",
      "train loss:2.368047309258935\n",
      "train loss:2.3780830269597875\n",
      "train loss:2.388043590203886\n",
      "train loss:2.3759437421007616\n",
      "train loss:2.387687465322494\n",
      "train loss:2.392185096129091\n",
      "train loss:2.3861126747236696\n",
      "train loss:2.3857781601421713\n",
      "train loss:2.381851704474829\n",
      "train loss:2.383002634595732\n",
      "train loss:2.3837881033841026\n",
      "train loss:2.3805458697193247\n",
      "train loss:2.3834695825922645\n",
      "train loss:2.383379662242384\n",
      "train loss:2.370652935270361\n",
      "train loss:2.381256024554786\n",
      "train loss:2.3847147715909225\n",
      "train loss:2.381752994987682\n",
      "train loss:2.376191353212852\n",
      "train loss:2.382534586406703\n",
      "train loss:2.383936362369672\n",
      "train loss:2.3826835711168384\n",
      "train loss:2.3808542355763826\n",
      "train loss:2.3747787907491222\n",
      "train loss:2.3750972640496326\n",
      "train loss:2.3827266418451445\n",
      "train loss:2.381176797913445\n",
      "train loss:2.381795072903354\n",
      "train loss:2.3795890920172624\n",
      "train loss:2.3693450296315346\n",
      "train loss:2.378322347084614\n",
      "train loss:2.382556491164643\n",
      "train loss:2.3788006123710645\n",
      "train loss:2.378777006289088\n",
      "train loss:2.3819623941965444\n",
      "train loss:2.3781970247355866\n",
      "train loss:2.379026112636096\n",
      "train loss:2.375036572383253\n",
      "train loss:2.382435935525044\n",
      "train loss:2.3738688389302425\n",
      "train loss:2.3708231308457823\n",
      "train loss:2.3860913386210942\n",
      "train loss:2.3819492461135625\n",
      "train loss:2.3737469164020446\n",
      "train loss:2.3827516122424814\n",
      "train loss:2.3747294356241806\n",
      "train loss:2.3694803702305216\n",
      "train loss:2.3755423463158185\n",
      "train loss:2.38385083483502\n",
      "train loss:2.377869104898301\n",
      "train loss:2.374034023520813\n",
      "train loss:2.3767941663774645\n",
      "train loss:2.3789893707373433\n",
      "train loss:2.3726530059445925\n",
      "train loss:2.381034901577162\n",
      "train loss:2.3650036256517124\n",
      "train loss:2.3820243085443353\n",
      "train loss:2.379607852847499\n",
      "train loss:2.3701195327998708\n",
      "train loss:2.3739665940167036\n",
      "train loss:2.376279557927304\n",
      "train loss:2.379224654033195\n",
      "train loss:2.362120812900476\n",
      "train loss:2.3796426537754254\n",
      "train loss:2.377228424654256\n",
      "train loss:2.377640420330124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3748166372060697\n",
      "train loss:2.3694776263138104\n",
      "train loss:2.366114867222958\n",
      "train loss:2.373714250910107\n",
      "train loss:2.3650736724108135\n",
      "train loss:2.372214738667972\n",
      "train loss:2.371221506399641\n",
      "train loss:2.371712741958341\n",
      "train loss:2.3772777677582893\n",
      "train loss:2.370385332748823\n",
      "train loss:2.3617159826996073\n",
      "train loss:2.3710902768968203\n",
      "train loss:2.3786434606407125\n",
      "train loss:2.367372833573186\n",
      "train loss:2.362489119834957\n",
      "train loss:2.38102245444698\n",
      "train loss:2.368815574483356\n",
      "train loss:2.367873576546218\n",
      "train loss:2.376711728471036\n",
      "train loss:2.375038911906552\n",
      "train loss:2.373588811092647\n",
      "train loss:2.374054447506567\n",
      "train loss:2.371778836447702\n",
      "train loss:2.364926815676978\n",
      "train loss:2.3735880347934817\n",
      "train loss:2.366286293380673\n",
      "train loss:2.378006017606157\n",
      "train loss:2.3749008304794286\n",
      "train loss:2.3641533867831326\n",
      "train loss:2.362031927400837\n",
      "train loss:2.367973336397139\n",
      "train loss:2.3654627850889636\n",
      "train loss:2.3656795572833818\n",
      "train loss:2.366225425659541\n",
      "train loss:2.363154819907623\n",
      "train loss:2.3672234177597526\n",
      "train loss:2.3681927785270216\n",
      "train loss:2.370740907280131\n",
      "train loss:2.372889955524757\n",
      "train loss:2.3613195658122317\n",
      "=== epoch:18, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3739863974925672\n",
      "train loss:2.3727389125131815\n",
      "train loss:2.3700488818545624\n",
      "train loss:2.370345891960287\n",
      "train loss:2.3645396082432213\n",
      "train loss:2.3681935264478677\n",
      "train loss:2.369773784229735\n",
      "train loss:2.358456492379906\n",
      "train loss:2.365641995262204\n",
      "train loss:2.357966427613634\n",
      "train loss:2.3636605730947746\n",
      "train loss:2.362540603783025\n",
      "train loss:2.373130776775568\n",
      "train loss:2.364093915332953\n",
      "train loss:2.366302134861724\n",
      "train loss:2.366649615917277\n",
      "train loss:2.368660631666586\n",
      "train loss:2.3685859880067475\n",
      "train loss:2.366264280708305\n",
      "train loss:2.378617856800722\n",
      "train loss:2.3675043425520372\n",
      "train loss:2.372089590421792\n",
      "train loss:2.368240017891449\n",
      "train loss:2.3670353108305813\n",
      "train loss:2.3675087044459526\n",
      "train loss:2.3561001491717857\n",
      "train loss:2.357359860316564\n",
      "train loss:2.3696647490953597\n",
      "train loss:2.3624598485778288\n",
      "train loss:2.368128738591219\n",
      "train loss:2.356481387382075\n",
      "train loss:2.364472755060513\n",
      "train loss:2.3683377067826514\n",
      "train loss:2.35791668360371\n",
      "train loss:2.3669231871385974\n",
      "train loss:2.3633685773386666\n",
      "train loss:2.3555882237382746\n",
      "train loss:2.3517650915332933\n",
      "train loss:2.360247029215828\n",
      "train loss:2.3484319990060136\n",
      "train loss:2.3600682243335265\n",
      "train loss:2.360054358962057\n",
      "train loss:2.367704132236145\n",
      "train loss:2.365398065808961\n",
      "train loss:2.3648449317288733\n",
      "train loss:2.3625819907654684\n",
      "train loss:2.3698670343952264\n",
      "train loss:2.3645145859065217\n",
      "train loss:2.3583282260422487\n",
      "train loss:2.357456788608884\n",
      "train loss:2.3698328323405895\n",
      "train loss:2.363717971267363\n",
      "train loss:2.3614493336596905\n",
      "train loss:2.3573674172737564\n",
      "train loss:2.36019030545759\n",
      "train loss:2.365460515524862\n",
      "train loss:2.355988850654556\n",
      "train loss:2.360853524679774\n",
      "train loss:2.361694066565074\n",
      "train loss:2.355960040313773\n",
      "train loss:2.3666006160819046\n",
      "train loss:2.362490237025759\n",
      "train loss:2.35541006415763\n",
      "train loss:2.3606490548567516\n",
      "train loss:2.3553799206030126\n",
      "train loss:2.354937293968797\n",
      "train loss:2.349762491226927\n",
      "train loss:2.353933759915038\n",
      "train loss:2.367009599478507\n",
      "train loss:2.360523074425539\n",
      "train loss:2.355910738559511\n",
      "train loss:2.356502977031514\n",
      "train loss:2.3580046343785526\n",
      "train loss:2.348307341146543\n",
      "train loss:2.36452760891341\n",
      "train loss:2.364144614604668\n",
      "train loss:2.3584741723044136\n",
      "train loss:2.350522963383182\n",
      "train loss:2.3682232468677684\n",
      "train loss:2.3620118299462125\n",
      "train loss:2.3608880827890135\n",
      "train loss:2.3580092131858224\n",
      "train loss:2.3618026925066853\n",
      "train loss:2.3594994690848403\n",
      "train loss:2.3535156764968734\n",
      "train loss:2.34432779215122\n",
      "train loss:2.357577195996924\n",
      "train loss:2.3592688381617264\n",
      "train loss:2.355517822126972\n",
      "train loss:2.35656401570457\n",
      "train loss:2.3537558302597392\n",
      "train loss:2.355699312385595\n",
      "train loss:2.3589116506320185\n",
      "train loss:2.3640614993588365\n",
      "train loss:2.361384412671128\n",
      "train loss:2.3438745596727504\n",
      "train loss:2.3547787446812816\n",
      "train loss:2.3579710178953905\n",
      "train loss:2.3543838150998386\n",
      "train loss:2.356479041247605\n",
      "train loss:2.3573828996599575\n",
      "train loss:2.3545094333246883\n",
      "train loss:2.3474621031271057\n",
      "train loss:2.3468796837519794\n",
      "train loss:2.3560098698200425\n",
      "train loss:2.3446381773277283\n",
      "train loss:2.3524695915472265\n",
      "train loss:2.3580123722269106\n",
      "train loss:2.3609773468443818\n",
      "train loss:2.346318348745951\n",
      "train loss:2.353597195619773\n",
      "train loss:2.3630702958404686\n",
      "train loss:2.360684997836806\n",
      "train loss:2.3478680165924137\n",
      "train loss:2.361739005627167\n",
      "train loss:2.3585956073651233\n",
      "train loss:2.3485282241035788\n",
      "train loss:2.3545024899503812\n",
      "train loss:2.344462040966275\n",
      "train loss:2.3583791956578795\n",
      "train loss:2.3576265514469594\n",
      "train loss:2.3515604353947412\n",
      "train loss:2.3551265471825436\n",
      "train loss:2.3448786741368335\n",
      "train loss:2.356077441507175\n",
      "train loss:2.353032651020514\n",
      "train loss:2.3561436280775494\n",
      "train loss:2.3438221266545916\n",
      "train loss:2.361158426207113\n",
      "train loss:2.3518693807124658\n",
      "train loss:2.3571440311372513\n",
      "train loss:2.344810726095616\n",
      "train loss:2.351130737876637\n",
      "train loss:2.3584565112678755\n",
      "train loss:2.3549435306161466\n",
      "train loss:2.3511706792488924\n",
      "train loss:2.3512881220228525\n",
      "train loss:2.357777500731786\n",
      "train loss:2.350919168599219\n",
      "train loss:2.3571336479786265\n",
      "train loss:2.3513484974993526\n",
      "train loss:2.346349944434053\n",
      "train loss:2.3518018281979267\n",
      "train loss:2.3506490422232456\n",
      "train loss:2.3485806541441447\n",
      "train loss:2.355206857059527\n",
      "train loss:2.3417316960733605\n",
      "train loss:2.345861712199334\n",
      "train loss:2.349230656677037\n",
      "train loss:2.3518800082554514\n",
      "train loss:2.349000721278501\n",
      "train loss:2.3416162119394484\n",
      "train loss:2.339748860350511\n",
      "train loss:2.358830862406962\n",
      "train loss:2.3542667462009863\n",
      "train loss:2.337545093949152\n",
      "train loss:2.3586064746860576\n",
      "train loss:2.3516568451839026\n",
      "train loss:2.3443245385085847\n",
      "train loss:2.354470314725456\n",
      "train loss:2.3468319410774017\n",
      "train loss:2.356453512174216\n",
      "train loss:2.35249771139942\n",
      "train loss:2.3524423089526074\n",
      "train loss:2.3578782322102603\n",
      "train loss:2.3402078484984727\n",
      "train loss:2.3487154706815887\n",
      "train loss:2.3518991476609337\n",
      "train loss:2.3458634800711944\n",
      "train loss:2.351232861419445\n",
      "train loss:2.361490063945916\n",
      "train loss:2.3489397164665053\n",
      "train loss:2.351733396276158\n",
      "train loss:2.352134665821182\n",
      "train loss:2.3615253267839154\n",
      "train loss:2.3418250073247373\n",
      "train loss:2.36043114746379\n",
      "train loss:2.350685222656252\n",
      "train loss:2.353170293748504\n",
      "train loss:2.351967226017258\n",
      "train loss:2.35719061492145\n",
      "train loss:2.340135681046703\n",
      "train loss:2.345569695128629\n",
      "train loss:2.3420980242877634\n",
      "train loss:2.338604606045851\n",
      "train loss:2.3456061870712053\n",
      "train loss:2.3440687686694326\n",
      "train loss:2.3493100685891277\n",
      "train loss:2.3505256140121253\n",
      "train loss:2.3429700109898284\n",
      "train loss:2.3391817094681966\n",
      "train loss:2.349904705996862\n",
      "train loss:2.351821669141441\n",
      "train loss:2.3523800025275228\n",
      "train loss:2.3527934672989446\n",
      "train loss:2.34713672936583\n",
      "train loss:2.3418137204457494\n",
      "train loss:2.352565071068618\n",
      "train loss:2.349045858172091\n",
      "train loss:2.3454036760882793\n",
      "=== epoch:19, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3476300780395527\n",
      "train loss:2.348874446263885\n",
      "train loss:2.3419363790185064\n",
      "train loss:2.3472494191963715\n",
      "train loss:2.3386643788321253\n",
      "train loss:2.3538719243570716\n",
      "train loss:2.351383949505874\n",
      "train loss:2.3458603384021197\n",
      "train loss:2.3470839074613554\n",
      "train loss:2.336230269948643\n",
      "train loss:2.336991353507266\n",
      "train loss:2.3437436103469906\n",
      "train loss:2.3554621865895835\n",
      "train loss:2.3419208738469335\n",
      "train loss:2.3481593908818352\n",
      "train loss:2.3414234151096442\n",
      "train loss:2.353456159570132\n",
      "train loss:2.343455726828083\n",
      "train loss:2.348707817785796\n",
      "train loss:2.350158953185447\n",
      "train loss:2.3355460044236604\n",
      "train loss:2.351837926208327\n",
      "train loss:2.3349334368857986\n",
      "train loss:2.3416406366398963\n",
      "train loss:2.3367889192116236\n",
      "train loss:2.3495868074531066\n",
      "train loss:2.334709604657448\n",
      "train loss:2.34174946581103\n",
      "train loss:2.3388238789587503\n",
      "train loss:2.328347268098472\n",
      "train loss:2.3506439480129457\n",
      "train loss:2.348780608979196\n",
      "train loss:2.345182602088919\n",
      "train loss:2.3450448041902456\n",
      "train loss:2.3409282407413246\n",
      "train loss:2.3456668581875473\n",
      "train loss:2.3456097091981234\n",
      "train loss:2.341277992517956\n",
      "train loss:2.341953178210652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.343621821689632\n",
      "train loss:2.3430709528423437\n",
      "train loss:2.344607408891151\n",
      "train loss:2.350763373717784\n",
      "train loss:2.3352743992765452\n",
      "train loss:2.352769449474593\n",
      "train loss:2.3502770829155466\n",
      "train loss:2.344180993032282\n",
      "train loss:2.3294201530614647\n",
      "train loss:2.352634199249802\n",
      "train loss:2.3446387959467816\n",
      "train loss:2.3392608768735426\n",
      "train loss:2.3316649145546604\n",
      "train loss:2.3379941926427725\n",
      "train loss:2.336805545385568\n",
      "train loss:2.3391966153803847\n",
      "train loss:2.3463673993879315\n",
      "train loss:2.3386629931656446\n",
      "train loss:2.34214402310915\n",
      "train loss:2.344388522228585\n",
      "train loss:2.348657132793486\n",
      "train loss:2.340835054570804\n",
      "train loss:2.341748104086433\n",
      "train loss:2.3357762164573015\n",
      "train loss:2.3410264165516605\n",
      "train loss:2.3408932212363203\n",
      "train loss:2.3457567360140787\n",
      "train loss:2.3428037271487336\n",
      "train loss:2.3452503632101713\n",
      "train loss:2.3480322991614213\n",
      "train loss:2.3461103793342435\n",
      "train loss:2.343774639752967\n",
      "train loss:2.3383737671837794\n",
      "train loss:2.334816452799521\n",
      "train loss:2.32938544724352\n",
      "train loss:2.3349776787145284\n",
      "train loss:2.3426704978791726\n",
      "train loss:2.3396593861738686\n",
      "train loss:2.3443154829448876\n",
      "train loss:2.337999846656932\n",
      "train loss:2.3398378768433656\n",
      "train loss:2.3372099337784205\n",
      "train loss:2.339228776835729\n",
      "train loss:2.339215127270885\n",
      "train loss:2.340617657037342\n",
      "train loss:2.3374134896398915\n",
      "train loss:2.3380174274716583\n",
      "train loss:2.3297830533084642\n",
      "train loss:2.3203483342366287\n",
      "train loss:2.340267189569523\n",
      "train loss:2.345227477073036\n",
      "train loss:2.333347903362061\n",
      "train loss:2.3402278252867688\n",
      "train loss:2.3400800864693836\n",
      "train loss:2.3385715708021815\n",
      "train loss:2.3374441627651574\n",
      "train loss:2.333343310925549\n",
      "train loss:2.332297428787301\n",
      "train loss:2.333660513904811\n",
      "train loss:2.331740578967833\n",
      "train loss:2.3387528003696643\n",
      "train loss:2.335850097363783\n",
      "train loss:2.3293772646003768\n",
      "train loss:2.339815861507067\n",
      "train loss:2.3361954279083244\n",
      "train loss:2.346309702751938\n",
      "train loss:2.344583231769071\n",
      "train loss:2.346126305633111\n",
      "train loss:2.344075704867325\n",
      "train loss:2.328928229489248\n",
      "train loss:2.332684567300994\n",
      "train loss:2.3350343116618855\n",
      "train loss:2.336500455159682\n",
      "train loss:2.3395700897213607\n",
      "train loss:2.3411303583051652\n",
      "train loss:2.3431412373939087\n",
      "train loss:2.3462844625139114\n",
      "train loss:2.337416285395132\n",
      "train loss:2.337791147318228\n",
      "train loss:2.3495738205960435\n",
      "train loss:2.335722969179836\n",
      "train loss:2.3301311120280395\n",
      "train loss:2.345871344830014\n",
      "train loss:2.335955374615154\n",
      "train loss:2.345833772067416\n",
      "train loss:2.335790165383579\n",
      "train loss:2.3463897261194093\n",
      "train loss:2.3394763698864653\n",
      "train loss:2.3431175914431335\n",
      "train loss:2.338957466115653\n",
      "train loss:2.3338405238469475\n",
      "train loss:2.3398591581685375\n",
      "train loss:2.336446558895867\n",
      "train loss:2.3342321211687413\n",
      "train loss:2.3292943800352157\n",
      "train loss:2.3444648312613716\n",
      "train loss:2.3331285712968883\n",
      "train loss:2.335644255300273\n",
      "train loss:2.339638917285167\n",
      "train loss:2.3301819375174584\n",
      "train loss:2.334163378651061\n",
      "train loss:2.3362515441911174\n",
      "train loss:2.342453180971376\n",
      "train loss:2.339065083491929\n",
      "train loss:2.3354494957173952\n",
      "train loss:2.338037581382654\n",
      "train loss:2.3412272440141466\n",
      "train loss:2.326920917933405\n",
      "train loss:2.3325517667960187\n",
      "train loss:2.3297809443954085\n",
      "train loss:2.334110939849222\n",
      "train loss:2.3308246126731658\n",
      "train loss:2.331621038687078\n",
      "train loss:2.3299267717997134\n",
      "train loss:2.3364207426841066\n",
      "train loss:2.3321444326065244\n",
      "train loss:2.3358684040551085\n",
      "train loss:2.342111569024087\n",
      "train loss:2.334108031161656\n",
      "train loss:2.3276627212796646\n",
      "train loss:2.3411449654681675\n",
      "train loss:2.3318912714057207\n",
      "train loss:2.3365876717838643\n",
      "train loss:2.332381533643558\n",
      "train loss:2.3393708196128182\n",
      "train loss:2.3364544309933923\n",
      "train loss:2.3259233376697073\n",
      "train loss:2.3309407159405504\n",
      "train loss:2.3356067248625307\n",
      "train loss:2.337891967070217\n",
      "train loss:2.3317439834763105\n",
      "train loss:2.336557705999123\n",
      "train loss:2.3277919316324813\n",
      "train loss:2.3388086390756526\n",
      "train loss:2.3394943598638034\n",
      "train loss:2.338937319018264\n",
      "train loss:2.3413515543092176\n",
      "train loss:2.3198084159844266\n",
      "train loss:2.3296340148239487\n",
      "train loss:2.3350414114470475\n",
      "train loss:2.324049485887114\n",
      "train loss:2.3378328584558736\n",
      "train loss:2.334242693898513\n",
      "train loss:2.33092420703195\n",
      "train loss:2.3270863987789294\n",
      "train loss:2.3300447186646718\n",
      "train loss:2.327369337662917\n",
      "train loss:2.3397048791805397\n",
      "train loss:2.338350282223363\n",
      "train loss:2.329018369556217\n",
      "train loss:2.3424016321507004\n",
      "train loss:2.331394948678591\n",
      "train loss:2.331479231051715\n",
      "train loss:2.331690642202638\n",
      "train loss:2.3234953622295116\n",
      "train loss:2.326114448747016\n",
      "train loss:2.3314333725267615\n",
      "train loss:2.332985980250045\n",
      "train loss:2.334035729413871\n",
      "train loss:2.3324993269911967\n",
      "train loss:2.3387637489515654\n",
      "=== epoch:20, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.333578803255121\n",
      "train loss:2.3301072300675374\n",
      "train loss:2.32680111993871\n",
      "train loss:2.3241051027814503\n",
      "train loss:2.3279824358942127\n",
      "train loss:2.334024703930865\n",
      "train loss:2.3259754729387137\n",
      "train loss:2.330044951334847\n",
      "train loss:2.328384130776128\n",
      "train loss:2.335312807702286\n",
      "train loss:2.3365595651599835\n",
      "train loss:2.3286207698303567\n",
      "train loss:2.3290645904721665\n",
      "train loss:2.3256587903219725\n",
      "train loss:2.328731392417589\n",
      "train loss:2.3333765143241285\n",
      "train loss:2.338502954234811\n",
      "train loss:2.33291328317493\n",
      "train loss:2.3299302313378534\n",
      "train loss:2.327014919240589\n",
      "train loss:2.321712659620151\n",
      "train loss:2.3272183233202095\n",
      "train loss:2.330730524262138\n",
      "train loss:2.3278255412788638\n",
      "train loss:2.324353639842455\n",
      "train loss:2.333849267343933\n",
      "train loss:2.3252443646919945\n",
      "train loss:2.339673667340907\n",
      "train loss:2.3326899394576617\n",
      "train loss:2.330194055646987\n",
      "train loss:2.321683017741726\n",
      "train loss:2.3270056525585883\n",
      "train loss:2.3284523825880052\n",
      "train loss:2.3284786984235555\n",
      "train loss:2.3289093467677255\n",
      "train loss:2.338419044261545\n",
      "train loss:2.318230144088947\n",
      "train loss:2.334765484439383\n",
      "train loss:2.322482474749741\n",
      "train loss:2.328953135232459\n",
      "train loss:2.332134937904494\n",
      "train loss:2.3270568163978544\n",
      "train loss:2.331747283796155\n",
      "train loss:2.3314165346576616\n",
      "train loss:2.328001779780505\n",
      "train loss:2.3323945726320945\n",
      "train loss:2.334926390005561\n",
      "train loss:2.3312104779600293\n",
      "train loss:2.326340852954425\n",
      "train loss:2.3224576662972507\n",
      "train loss:2.330813607951347\n",
      "train loss:2.3363260921018334\n",
      "train loss:2.3207751172435898\n",
      "train loss:2.334391765915659\n",
      "train loss:2.326050595746968\n",
      "train loss:2.3369920265782564\n",
      "train loss:2.325191306053369\n",
      "train loss:2.3212348032703187\n",
      "train loss:2.323933694975231\n",
      "train loss:2.324678275064838\n",
      "train loss:2.328073004021309\n",
      "train loss:2.3319982128684686\n",
      "train loss:2.3234167582390426\n",
      "train loss:2.323465905357794\n",
      "train loss:2.3334070778342606\n",
      "train loss:2.3315715456246395\n",
      "train loss:2.326687606417262\n",
      "train loss:2.3243118432947902\n",
      "train loss:2.329749418841\n",
      "train loss:2.3206225732455152\n",
      "train loss:2.32860253366098\n",
      "train loss:2.329223819448526\n",
      "train loss:2.3200896509759574\n",
      "train loss:2.3241626434485734\n",
      "train loss:2.332672219885609\n",
      "train loss:2.334895955567959\n",
      "train loss:2.327109457804347\n",
      "train loss:2.3282033013193586\n",
      "train loss:2.3294854248840378\n",
      "train loss:2.32811510878553\n",
      "train loss:2.3235684158210392\n",
      "train loss:2.3314367307213226\n",
      "train loss:2.325174728683552\n",
      "train loss:2.32340476770766\n",
      "train loss:2.3275597274215047\n",
      "train loss:2.3202831464239173\n",
      "train loss:2.3239273615092815\n",
      "train loss:2.317932124126495\n",
      "train loss:2.3272684003293738\n",
      "train loss:2.3247851894734715\n",
      "train loss:2.3256244551044905\n",
      "train loss:2.322873173218294\n",
      "train loss:2.326372858252386\n",
      "train loss:2.331735428648611\n",
      "train loss:2.337084065563985\n",
      "train loss:2.327663039192709\n",
      "train loss:2.320784922599893\n",
      "train loss:2.3243008425635394\n",
      "train loss:2.324216102342665\n",
      "train loss:2.3181713632731507\n",
      "train loss:2.324633092068109\n",
      "train loss:2.3214280061752044\n",
      "train loss:2.319990837896179\n",
      "train loss:2.3203180896744264\n",
      "train loss:2.3230632705361107\n",
      "train loss:2.3254665705611144\n",
      "train loss:2.3245626357336993\n",
      "train loss:2.3229138807809755\n",
      "train loss:2.329296983768628\n",
      "train loss:2.319195902950756\n",
      "train loss:2.3248850236940117\n",
      "train loss:2.3325075535549176\n",
      "train loss:2.3195690052823474\n",
      "train loss:2.3270167330708604\n",
      "train loss:2.330757282695372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3259859924747546\n",
      "train loss:2.3295957228671624\n",
      "train loss:2.325378360081498\n",
      "train loss:2.3225903398213505\n",
      "train loss:2.3215424302546483\n",
      "train loss:2.317673027142617\n",
      "train loss:2.3234612204352305\n",
      "train loss:2.3356008996472073\n",
      "train loss:2.324978323602678\n",
      "train loss:2.321431373375548\n",
      "train loss:2.3159271991404413\n",
      "train loss:2.336122551957229\n",
      "train loss:2.3332933359905863\n",
      "train loss:2.3261831903981576\n",
      "train loss:2.3292983208242317\n",
      "train loss:2.317776022105545\n",
      "train loss:2.3212886933269736\n",
      "train loss:2.31036716491506\n",
      "train loss:2.3289813001895636\n",
      "train loss:2.3297923522859945\n",
      "train loss:2.3199196296384503\n",
      "train loss:2.325214662270092\n",
      "train loss:2.3275652508658986\n",
      "train loss:2.321283440763516\n",
      "train loss:2.330300612362387\n",
      "train loss:2.330990449653711\n",
      "train loss:2.321334805568397\n",
      "train loss:2.3264726437367367\n",
      "train loss:2.3313065512863944\n",
      "train loss:2.319732392406744\n",
      "train loss:2.3300430240664185\n",
      "train loss:2.318365593891748\n",
      "train loss:2.3219900891143572\n",
      "train loss:2.3243395963968836\n",
      "train loss:2.3141746288359157\n",
      "train loss:2.314265546738644\n",
      "train loss:2.324184147912781\n",
      "train loss:2.322589407817421\n",
      "train loss:2.3221230582912433\n",
      "train loss:2.3266154795607115\n",
      "train loss:2.3197408263551775\n",
      "train loss:2.326352156790658\n",
      "train loss:2.330141448917225\n",
      "train loss:2.3252777109937144\n",
      "train loss:2.3201893196497507\n",
      "train loss:2.3222787924246675\n",
      "train loss:2.326578534272833\n",
      "train loss:2.3269627480627997\n",
      "train loss:2.3251679906815874\n",
      "train loss:2.3239061525678597\n",
      "train loss:2.3205478902881187\n",
      "train loss:2.3167497108329558\n",
      "train loss:2.3219294156080763\n",
      "train loss:2.3266328596595827\n",
      "train loss:2.322249220367726\n",
      "train loss:2.3257989129264622\n",
      "train loss:2.3215903337512347\n",
      "train loss:2.3249036834692047\n",
      "train loss:2.324236502095353\n",
      "train loss:2.3313206721898516\n",
      "train loss:2.321419829255958\n",
      "train loss:2.315387333893977\n",
      "train loss:2.3227560909085696\n",
      "train loss:2.328244835411348\n",
      "train loss:2.325234556667408\n",
      "train loss:2.3168304616622715\n",
      "train loss:2.3228295688971268\n",
      "train loss:2.316332353056782\n",
      "train loss:2.323897067118209\n",
      "train loss:2.326382333744542\n",
      "train loss:2.320421699200723\n",
      "train loss:2.321046156625786\n",
      "train loss:2.323603137670045\n",
      "train loss:2.3151342414429856\n",
      "train loss:2.32217339595045\n",
      "train loss:2.3110782082302572\n",
      "train loss:2.323731560650835\n",
      "train loss:2.3203519943559514\n",
      "train loss:2.3224588099238255\n",
      "train loss:2.321606988755463\n",
      "train loss:2.323481102502439\n",
      "train loss:2.320990002011193\n",
      "train loss:2.3216018516820887\n",
      "train loss:2.320949635306052\n",
      "train loss:2.3232646969473363\n",
      "=== epoch:21, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3119075653417394\n",
      "train loss:2.315356918921472\n",
      "train loss:2.3157241515311484\n",
      "train loss:2.321352438491474\n",
      "train loss:2.316316351788598\n",
      "train loss:2.3216041828383625\n",
      "train loss:2.323283639432354\n",
      "train loss:2.326636081788635\n",
      "train loss:2.3240969223312753\n",
      "train loss:2.3152105268482353\n",
      "train loss:2.327832516625379\n",
      "train loss:2.3215210303549263\n",
      "train loss:2.3202177775410555\n",
      "train loss:2.3270598856758196\n",
      "train loss:2.3204521946498833\n",
      "train loss:2.3071857381014036\n",
      "train loss:2.319803319019708\n",
      "train loss:2.32273040202172\n",
      "train loss:2.3140232764631854\n",
      "train loss:2.321054254343552\n",
      "train loss:2.3248167601838596\n",
      "train loss:2.320530300716445\n",
      "train loss:2.318338319537152\n",
      "train loss:2.3231159073774785\n",
      "train loss:2.3203321580789997\n",
      "train loss:2.322617466087402\n",
      "train loss:2.321243322430858\n",
      "train loss:2.315096923303063\n",
      "train loss:2.322477951169407\n",
      "train loss:2.3227385686548105\n",
      "train loss:2.31522040391871\n",
      "train loss:2.3179527174120955\n",
      "train loss:2.326561974124743\n",
      "train loss:2.325721520895046\n",
      "train loss:2.327280645542722\n",
      "train loss:2.3199682269994817\n",
      "train loss:2.3139064820457165\n",
      "train loss:2.317348027312277\n",
      "train loss:2.3219040160652176\n",
      "train loss:2.3259328366746956\n",
      "train loss:2.3219266124011173\n",
      "train loss:2.316724666335468\n",
      "train loss:2.307926601357433\n",
      "train loss:2.318034073023212\n",
      "train loss:2.31700126412239\n",
      "train loss:2.3170051411388215\n",
      "train loss:2.320029529318592\n",
      "train loss:2.3187193859994326\n",
      "train loss:2.3260911311000396\n",
      "train loss:2.3107230683268516\n",
      "train loss:2.3118825939476815\n",
      "train loss:2.3145580759812416\n",
      "train loss:2.3188491307140957\n",
      "train loss:2.3218942093529154\n",
      "train loss:2.3213040732547863\n",
      "train loss:2.31105174674637\n",
      "train loss:2.3173491231479586\n",
      "train loss:2.3140957366397865\n",
      "train loss:2.3173915476847227\n",
      "train loss:2.32348383997032\n",
      "train loss:2.3061809175680414\n",
      "train loss:2.3232175445435512\n",
      "train loss:2.3298664242390896\n",
      "train loss:2.3252386885452623\n",
      "train loss:2.311268589091895\n",
      "train loss:2.309504106484907\n",
      "train loss:2.3212178897938474\n",
      "train loss:2.314219564078162\n",
      "train loss:2.3251917047450372\n",
      "train loss:2.3154678603165935\n",
      "train loss:2.3316222120149837\n",
      "train loss:2.3151200736106197\n",
      "train loss:2.3243624241218575\n",
      "train loss:2.317533212181585\n",
      "train loss:2.324786911619517\n",
      "train loss:2.314111012554132\n",
      "train loss:2.3171221529595853\n",
      "train loss:2.314443413013119\n",
      "train loss:2.319672844514634\n",
      "train loss:2.3286460451310895\n",
      "train loss:2.3096747580369237\n",
      "train loss:2.319311201556296\n",
      "train loss:2.321864972519946\n",
      "train loss:2.317698556170216\n",
      "train loss:2.312131708298352\n",
      "train loss:2.322533816480218\n",
      "train loss:2.3159444057586938\n",
      "train loss:2.3049413370111913\n",
      "train loss:2.3202039147631686\n",
      "train loss:2.31217700713035\n",
      "train loss:2.3253003701953343\n",
      "train loss:2.312936775592579\n",
      "train loss:2.3246350320271145\n",
      "train loss:2.3201776118981727\n",
      "train loss:2.317752171428948\n",
      "train loss:2.3161239616625555\n",
      "train loss:2.3218172556531256\n",
      "train loss:2.3223222327698876\n",
      "train loss:2.319704719257847\n",
      "train loss:2.3255180158318054\n",
      "train loss:2.3165297077822133\n",
      "train loss:2.3185993012110293\n",
      "train loss:2.3169406634085132\n",
      "train loss:2.319768573213374\n",
      "train loss:2.313184669695805\n",
      "train loss:2.3043579432678762\n",
      "train loss:2.312910876116104\n",
      "train loss:2.31156056601019\n",
      "train loss:2.329403223116697\n",
      "train loss:2.317764651299516\n",
      "train loss:2.314399043187626\n",
      "train loss:2.3247833074181035\n",
      "train loss:2.3206659286466578\n",
      "train loss:2.3213236391603886\n",
      "train loss:2.3173011315320404\n",
      "train loss:2.3134729295266165\n",
      "train loss:2.3195388690155543\n",
      "train loss:2.318406265072638\n",
      "train loss:2.3195327105443764\n",
      "train loss:2.309727832909114\n",
      "train loss:2.311257342415424\n",
      "train loss:2.320328759544635\n",
      "train loss:2.3191762788696098\n",
      "train loss:2.317387343377674\n",
      "train loss:2.3182414326280334\n",
      "train loss:2.3141762560120687\n",
      "train loss:2.3202954388834818\n",
      "train loss:2.3112303251539243\n",
      "train loss:2.330107207269127\n",
      "train loss:2.3187241812829202\n",
      "train loss:2.321402162764619\n",
      "train loss:2.3086256719605376\n",
      "train loss:2.3232747747702795\n",
      "train loss:2.3093041058313917\n",
      "train loss:2.3123929969875996\n",
      "train loss:2.3222047699127035\n",
      "train loss:2.3113851169901567\n",
      "train loss:2.3146698079612342\n",
      "train loss:2.316656545090554\n",
      "train loss:2.3185960737082065\n",
      "train loss:2.324504042046981\n",
      "train loss:2.3136909774933736\n",
      "train loss:2.316102422318755\n",
      "train loss:2.309585982241998\n",
      "train loss:2.319838677953497\n",
      "train loss:2.3133206800347974\n",
      "train loss:2.3152456770820105\n",
      "train loss:2.3188724460455177\n",
      "train loss:2.31673775389895\n",
      "train loss:2.313052515411735\n",
      "train loss:2.3154830980025394\n",
      "train loss:2.325593973806967\n",
      "train loss:2.328676656497584\n",
      "train loss:2.3217828375048537\n",
      "train loss:2.3251806236357697\n",
      "train loss:2.3249878506968327\n",
      "train loss:2.320599221310673\n",
      "train loss:2.3112132559106735\n",
      "train loss:2.3198143148211696\n",
      "train loss:2.322869000358013\n",
      "train loss:2.31803778740031\n",
      "train loss:2.320507916370265\n",
      "train loss:2.3150879415498395\n",
      "train loss:2.3150876955956843\n",
      "train loss:2.3124947211737696\n",
      "train loss:2.3187415093544796\n",
      "train loss:2.3175620203714704\n",
      "train loss:2.306618527491267\n",
      "train loss:2.3142978454630474\n",
      "train loss:2.3154568939616316\n",
      "train loss:2.324216131971273\n",
      "train loss:2.3103181829464803\n",
      "train loss:2.318901214716956\n",
      "train loss:2.3063815412480504\n",
      "train loss:2.309092221357581\n",
      "train loss:2.3124017065016385\n",
      "train loss:2.3157519032619565\n",
      "train loss:2.306989364679444\n",
      "train loss:2.3060645172392156\n",
      "train loss:2.3159253438389618\n",
      "train loss:2.3133849892918796\n",
      "train loss:2.3155481812698127\n",
      "train loss:2.31332924953928\n",
      "train loss:2.307202122987048\n",
      "train loss:2.3170235296585746\n",
      "train loss:2.316370205925333\n",
      "train loss:2.3092286088098954\n",
      "train loss:2.312001574763767\n",
      "train loss:2.3132962285349827\n",
      "train loss:2.3193391031399013\n",
      "train loss:2.3077561289753614\n",
      "train loss:2.3123184127905603\n",
      "train loss:2.323627171251203\n",
      "train loss:2.3132476479584696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3160769378879142\n",
      "train loss:2.3162163254931296\n",
      "train loss:2.314202828164135\n",
      "train loss:2.319151344262763\n",
      "train loss:2.3205260605368907\n",
      "train loss:2.3063173200818805\n",
      "=== epoch:22, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.315139505641401\n",
      "train loss:2.313442543154131\n",
      "train loss:2.3276479886512647\n",
      "train loss:2.3172338275380286\n",
      "train loss:2.3091213090384906\n",
      "train loss:2.3211955683064676\n",
      "train loss:2.3093907340202686\n",
      "train loss:2.3132439474938407\n",
      "train loss:2.3265069483665424\n",
      "train loss:2.3181061491716455\n",
      "train loss:2.3167539847940417\n",
      "train loss:2.3068467078522827\n",
      "train loss:2.31190626775235\n",
      "train loss:2.3130396509437605\n",
      "train loss:2.3166421102923116\n",
      "train loss:2.3062694639484436\n",
      "train loss:2.3219498755597896\n",
      "train loss:2.3097606246654103\n",
      "train loss:2.3166674198651886\n",
      "train loss:2.3103334053628224\n",
      "train loss:2.3095336033925293\n",
      "train loss:2.317381069884117\n",
      "train loss:2.3207203441976243\n",
      "train loss:2.3165930874164062\n",
      "train loss:2.319272461690766\n",
      "train loss:2.310975792165392\n",
      "train loss:2.3223865180983907\n",
      "train loss:2.3088755031745616\n",
      "train loss:2.314332183512578\n",
      "train loss:2.316706144139963\n",
      "train loss:2.318923230390768\n",
      "train loss:2.3226950100152663\n",
      "train loss:2.308573356473544\n",
      "train loss:2.3126153475673643\n",
      "train loss:2.3145614664345304\n",
      "train loss:2.311437735451004\n",
      "train loss:2.319894075907609\n",
      "train loss:2.3027247189287006\n",
      "train loss:2.3141558684642596\n",
      "train loss:2.3076176856924606\n",
      "train loss:2.315001592159125\n",
      "train loss:2.318246362069463\n",
      "train loss:2.303787203800047\n",
      "train loss:2.3158831190412523\n",
      "train loss:2.31977217805758\n",
      "train loss:2.308424854079309\n",
      "train loss:2.318514919071348\n",
      "train loss:2.3034047701468636\n",
      "train loss:2.3111034722181167\n",
      "train loss:2.3189485683469506\n",
      "train loss:2.3131827885880045\n",
      "train loss:2.3080398975229124\n",
      "train loss:2.3113204784825014\n",
      "train loss:2.3184857963478156\n",
      "train loss:2.323275564402215\n",
      "train loss:2.3138088759719446\n",
      "train loss:2.321072127665514\n",
      "train loss:2.3141683767150325\n",
      "train loss:2.308486440405266\n",
      "train loss:2.3189536424184176\n",
      "train loss:2.3169638691406176\n",
      "train loss:2.31481501728216\n",
      "train loss:2.3152509834275934\n",
      "train loss:2.3231170235863616\n",
      "train loss:2.308218019254316\n",
      "train loss:2.310711524275529\n",
      "train loss:2.311579974345031\n",
      "train loss:2.3084282347879204\n",
      "train loss:2.3180489595010734\n",
      "train loss:2.317050296384552\n",
      "train loss:2.3116506019729846\n",
      "train loss:2.3107069005748393\n",
      "train loss:2.3132289671752346\n",
      "train loss:2.3138358242946095\n",
      "train loss:2.3152742659491374\n",
      "train loss:2.3143332878131555\n",
      "train loss:2.32060222528392\n",
      "train loss:2.3105686975641127\n",
      "train loss:2.3127669630122245\n",
      "train loss:2.3182172068456395\n",
      "train loss:2.3183159700442455\n",
      "train loss:2.3176768318515353\n",
      "train loss:2.3130404554337414\n",
      "train loss:2.311795778677669\n",
      "train loss:2.321243110270143\n",
      "train loss:2.31922381011319\n",
      "train loss:2.3000706247224763\n",
      "train loss:2.3075096583830494\n",
      "train loss:2.3167111285359043\n",
      "train loss:2.305807162233392\n",
      "train loss:2.3156762000038684\n",
      "train loss:2.3196272696811056\n",
      "train loss:2.3132728829841986\n",
      "train loss:2.317773712879682\n",
      "train loss:2.309376123148443\n",
      "train loss:2.314775669662012\n",
      "train loss:2.305854901189163\n",
      "train loss:2.3049342147266727\n",
      "train loss:2.3108534281103017\n",
      "train loss:2.306462254067723\n",
      "train loss:2.3099908287094526\n",
      "train loss:2.3107327047553117\n",
      "train loss:2.3172830627074505\n",
      "train loss:2.316047222835933\n",
      "train loss:2.309001033767382\n",
      "train loss:2.315860043674512\n",
      "train loss:2.3125912298567264\n",
      "train loss:2.3102948373755807\n",
      "train loss:2.3041202106703658\n",
      "train loss:2.3161543329889303\n",
      "train loss:2.3124776656700963\n",
      "train loss:2.3061942160591005\n",
      "train loss:2.309053073866143\n",
      "train loss:2.298213717058332\n",
      "train loss:2.309230763724799\n",
      "train loss:2.3155481051516458\n",
      "train loss:2.3106535765036273\n",
      "train loss:2.3001647476705784\n",
      "train loss:2.3062509208998345\n",
      "train loss:2.3168040467926487\n",
      "train loss:2.30784701106289\n",
      "train loss:2.3167726403076134\n",
      "train loss:2.312135792391844\n",
      "train loss:2.305900241851268\n",
      "train loss:2.3124669943690894\n",
      "train loss:2.315761350486704\n",
      "train loss:2.3063662564824523\n",
      "train loss:2.3031451346560776\n",
      "train loss:2.3147734713633232\n",
      "train loss:2.3127109726376744\n",
      "train loss:2.3198132219450085\n",
      "train loss:2.3061162086410327\n",
      "train loss:2.3105815133987075\n",
      "train loss:2.298599353031797\n",
      "train loss:2.3027888626165307\n",
      "train loss:2.303500125948238\n",
      "train loss:2.308889123403402\n",
      "train loss:2.314562342847838\n",
      "train loss:2.3143825009401153\n",
      "train loss:2.315600600145807\n",
      "train loss:2.31053708946716\n",
      "train loss:2.303743218855287\n",
      "train loss:2.3131404137695806\n",
      "train loss:2.312893642639407\n",
      "train loss:2.31488651606041\n",
      "train loss:2.3081820055505866\n",
      "train loss:2.315139227089662\n",
      "train loss:2.3082498739202433\n",
      "train loss:2.32108299195031\n",
      "train loss:2.310176437944466\n",
      "train loss:2.2990840043957923\n",
      "train loss:2.311805327677831\n",
      "train loss:2.305831771758729\n",
      "train loss:2.3118010870823733\n",
      "train loss:2.316345629966075\n",
      "train loss:2.3154666021116133\n",
      "train loss:2.320560326740259\n",
      "train loss:2.308824062002347\n",
      "train loss:2.3058825958597033\n",
      "train loss:2.3111489862574603\n",
      "train loss:2.31325662895833\n",
      "train loss:2.3154758949509304\n",
      "train loss:2.3129194266833357\n",
      "train loss:2.315538923458828\n",
      "train loss:2.3142704809009618\n",
      "train loss:2.310849722519206\n",
      "train loss:2.3073748360412027\n",
      "train loss:2.3102720197838087\n",
      "train loss:2.3122577613611504\n",
      "train loss:2.3043868108660623\n",
      "train loss:2.3080285669937117\n",
      "train loss:2.3067191677298045\n",
      "train loss:2.3105005588114675\n",
      "train loss:2.311174160100767\n",
      "train loss:2.3171233119461254\n",
      "train loss:2.3134128244868433\n",
      "train loss:2.312378522279182\n",
      "train loss:2.3134611063204464\n",
      "train loss:2.308213641664339\n",
      "train loss:2.3138180705399516\n",
      "train loss:2.3117829094443847\n",
      "train loss:2.3087334814325673\n",
      "train loss:2.318590338793263\n",
      "train loss:2.32160993189177\n",
      "train loss:2.3038296223054573\n",
      "train loss:2.317046128813366\n",
      "train loss:2.3141613392814597\n",
      "train loss:2.315126166823055\n",
      "train loss:2.3109534583570386\n",
      "train loss:2.3089860108589195\n",
      "train loss:2.3068776608310824\n",
      "train loss:2.3058796531415915\n",
      "train loss:2.310889908104654\n",
      "train loss:2.3147710022508416\n",
      "train loss:2.321261379072656\n",
      "train loss:2.3026196337507185\n",
      "train loss:2.312089030022243\n",
      "train loss:2.3111632678546354\n",
      "train loss:2.306028652154886\n",
      "train loss:2.305681041089773\n",
      "=== epoch:23, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3145481576376157\n",
      "train loss:2.3119689470392966\n",
      "train loss:2.3085302924481588\n",
      "train loss:2.3135907004169503\n",
      "train loss:2.308140597655761\n",
      "train loss:2.3072171917098445\n",
      "train loss:2.3071917445018606\n",
      "train loss:2.307979268963753\n",
      "train loss:2.311286890568795\n",
      "train loss:2.310742741153103\n",
      "train loss:2.3211908481205294\n",
      "train loss:2.312956166432338\n",
      "train loss:2.3102218411198234\n",
      "train loss:2.305716319809005\n",
      "train loss:2.3098268962563786\n",
      "train loss:2.3149372235158334\n",
      "train loss:2.304100916665078\n",
      "train loss:2.3082127572636946\n",
      "train loss:2.31543528425153\n",
      "train loss:2.3103590229305007\n",
      "train loss:2.314160537560017\n",
      "train loss:2.3081573055819264\n",
      "train loss:2.3109108590166416\n",
      "train loss:2.3081484229360982\n",
      "train loss:2.30762421607564\n",
      "train loss:2.3024203864843926\n",
      "train loss:2.310867728257231\n",
      "train loss:2.304560389041\n",
      "train loss:2.322196900909116\n",
      "train loss:2.3137735633842422\n",
      "train loss:2.3164107729839616\n",
      "train loss:2.3144646918352523\n",
      "train loss:2.31349354481368\n",
      "train loss:2.3085940723584817\n",
      "train loss:2.3052604796895086\n",
      "train loss:2.309470443711482\n",
      "train loss:2.303905719086137\n",
      "train loss:2.3063191816303235\n",
      "train loss:2.3002348649539934\n",
      "train loss:2.3097499043356096\n",
      "train loss:2.3021193508936126\n",
      "train loss:2.318593105685298\n",
      "train loss:2.307124067245434\n",
      "train loss:2.314753411956387\n",
      "train loss:2.315848956726878\n",
      "train loss:2.3137264793757737\n",
      "train loss:2.3061253652847946\n",
      "train loss:2.3177984253009236\n",
      "train loss:2.308803783290466\n",
      "train loss:2.3058044556595574\n",
      "train loss:2.3125400845817374\n",
      "train loss:2.307887434162995\n",
      "train loss:2.311945956738244\n",
      "train loss:2.3075450826422004\n",
      "train loss:2.308899899075495\n",
      "train loss:2.3189015335289462\n",
      "train loss:2.300116145904601\n",
      "train loss:2.3166621775744276\n",
      "train loss:2.318823649247389\n",
      "train loss:2.3104453247729877\n",
      "train loss:2.32017049749571\n",
      "train loss:2.307374519738493\n",
      "train loss:2.3072502464421194\n",
      "train loss:2.314892483951098\n",
      "train loss:2.3081813994720983\n",
      "train loss:2.3041145023610916\n",
      "train loss:2.3094531867000927\n",
      "train loss:2.31373019692443\n",
      "train loss:2.3011528629946247\n",
      "train loss:2.3068135995320116\n",
      "train loss:2.3088070176261626\n",
      "train loss:2.2992560091126464\n",
      "train loss:2.3129083658518863\n",
      "train loss:2.309735304521389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.307287405233683\n",
      "train loss:2.3042320450510756\n",
      "train loss:2.303319068398903\n",
      "train loss:2.3148178468515797\n",
      "train loss:2.3057248307203633\n",
      "train loss:2.3189312992447153\n",
      "train loss:2.314717392116383\n",
      "train loss:2.3079190225798647\n",
      "train loss:2.3076888209461237\n",
      "train loss:2.312349810689324\n",
      "train loss:2.3070791415101306\n",
      "train loss:2.311163976474576\n",
      "train loss:2.3126297393756214\n",
      "train loss:2.305644907116881\n",
      "train loss:2.3198829812532145\n",
      "train loss:2.3110135576963193\n",
      "train loss:2.3092359590381033\n",
      "train loss:2.3119194432966474\n",
      "train loss:2.306561526454285\n",
      "train loss:2.303059827423733\n",
      "train loss:2.3034009380102183\n",
      "train loss:2.3124333384829683\n",
      "train loss:2.303871361578259\n",
      "train loss:2.3105293624133436\n",
      "train loss:2.302597203554207\n",
      "train loss:2.3130320609057935\n",
      "train loss:2.305578337071769\n",
      "train loss:2.3002656601834754\n",
      "train loss:2.3069582115996163\n",
      "train loss:2.3117088146341516\n",
      "train loss:2.313193940094198\n",
      "train loss:2.306044140602015\n",
      "train loss:2.299792653720476\n",
      "train loss:2.295829084451279\n",
      "train loss:2.3083227031897495\n",
      "train loss:2.3059299992521667\n",
      "train loss:2.309580674277098\n",
      "train loss:2.3110830638789355\n",
      "train loss:2.3066098130754598\n",
      "train loss:2.311539293002246\n",
      "train loss:2.314142342502246\n",
      "train loss:2.3150707657541987\n",
      "train loss:2.312947606824454\n",
      "train loss:2.3106072610903334\n",
      "train loss:2.3084346936829774\n",
      "train loss:2.3017595714688848\n",
      "train loss:2.3114870499937696\n",
      "train loss:2.307156343289348\n",
      "train loss:2.303284045419952\n",
      "train loss:2.3100691560354885\n",
      "train loss:2.3042509026255495\n",
      "train loss:2.298769748190805\n",
      "train loss:2.314024173119715\n",
      "train loss:2.310025305477125\n",
      "train loss:2.32067212570105\n",
      "train loss:2.30261508945842\n",
      "train loss:2.3021992689240682\n",
      "train loss:2.3126017248550146\n",
      "train loss:2.3145274591885214\n",
      "train loss:2.322929567779426\n",
      "train loss:2.314099301117685\n",
      "train loss:2.3133830057590816\n",
      "train loss:2.3070611943980475\n",
      "train loss:2.3092112454282345\n",
      "train loss:2.310784977605563\n",
      "train loss:2.3160188024076107\n",
      "train loss:2.310260339686289\n",
      "train loss:2.309555784500431\n",
      "train loss:2.3113056674668435\n",
      "train loss:2.30315454290987\n",
      "train loss:2.3150568369011246\n",
      "train loss:2.301481092042262\n",
      "train loss:2.29835561771353\n",
      "train loss:2.305879211176875\n",
      "train loss:2.3128373836147635\n",
      "train loss:2.3055451608773705\n",
      "train loss:2.3071889903582785\n",
      "train loss:2.309408799985242\n",
      "train loss:2.3111433543824385\n",
      "train loss:2.3026392376284748\n",
      "train loss:2.315277838660369\n",
      "train loss:2.3064064539993385\n",
      "train loss:2.313133348893552\n",
      "train loss:2.3072617972009093\n",
      "train loss:2.314439995964708\n",
      "train loss:2.30401533534113\n",
      "train loss:2.3134627433402906\n",
      "train loss:2.2992902945133853\n",
      "train loss:2.3014494967279546\n",
      "train loss:2.3094418039346243\n",
      "train loss:2.317958298495576\n",
      "train loss:2.3133412779670617\n",
      "train loss:2.309163972471664\n",
      "train loss:2.3093672389199495\n",
      "train loss:2.3143894805236975\n",
      "train loss:2.2990746214989795\n",
      "train loss:2.3064654326547878\n",
      "train loss:2.3002661794932493\n",
      "train loss:2.309492064718253\n",
      "train loss:2.299437435969461\n",
      "train loss:2.3100158417175765\n",
      "train loss:2.314630743674256\n",
      "train loss:2.302695411698296\n",
      "train loss:2.306930554599962\n",
      "train loss:2.3062138319241097\n",
      "train loss:2.3081511826459504\n",
      "train loss:2.309608578753819\n",
      "train loss:2.3088344881539773\n",
      "train loss:2.3104212813824607\n",
      "train loss:2.30098148476985\n",
      "train loss:2.3039824249910494\n",
      "train loss:2.3032231471189237\n",
      "train loss:2.30315165492869\n",
      "train loss:2.305466289129708\n",
      "train loss:2.3076742459936077\n",
      "train loss:2.3145360725397275\n",
      "train loss:2.303139901227596\n",
      "train loss:2.314007269186849\n",
      "train loss:2.2992940425218027\n",
      "train loss:2.31016969077231\n",
      "train loss:2.3150242758606674\n",
      "train loss:2.3160904599160905\n",
      "train loss:2.3014907570157677\n",
      "train loss:2.302658726534717\n",
      "train loss:2.315123258905459\n",
      "train loss:2.301166577969174\n",
      "=== epoch:24, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3092987502283955\n",
      "train loss:2.302281524489272\n",
      "train loss:2.310523386929595\n",
      "train loss:2.3133541585434614\n",
      "train loss:2.312622119115967\n",
      "train loss:2.306235469474655\n",
      "train loss:2.3019178230084925\n",
      "train loss:2.304178383515971\n",
      "train loss:2.3029462714968205\n",
      "train loss:2.2952948139336655\n",
      "train loss:2.310614549826981\n",
      "train loss:2.3059673547091526\n",
      "train loss:2.2921084039437067\n",
      "train loss:2.297923642205293\n",
      "train loss:2.316933828775161\n",
      "train loss:2.305917810748056\n",
      "train loss:2.2990691476832725\n",
      "train loss:2.297409601112349\n",
      "train loss:2.3115235302548722\n",
      "train loss:2.3039213495431197\n",
      "train loss:2.3100732189406243\n",
      "train loss:2.3055786456656078\n",
      "train loss:2.3043774192076376\n",
      "train loss:2.313169817552622\n",
      "train loss:2.3121040574510916\n",
      "train loss:2.306216585152737\n",
      "train loss:2.2961050541515213\n",
      "train loss:2.3119005733065467\n",
      "train loss:2.303373295606893\n",
      "train loss:2.3033676649630936\n",
      "train loss:2.3038734153588787\n",
      "train loss:2.306847497672166\n",
      "train loss:2.299083176794734\n",
      "train loss:2.3185414593652034\n",
      "train loss:2.3107129641190207\n",
      "train loss:2.309377470083941\n",
      "train loss:2.3089288183083436\n",
      "train loss:2.308960976484643\n",
      "train loss:2.311082449562308\n",
      "train loss:2.3107755250073754\n",
      "train loss:2.294045075464223\n",
      "train loss:2.3042353764606744\n",
      "train loss:2.302408696143675\n",
      "train loss:2.3042150893356976\n",
      "train loss:2.3132618745566242\n",
      "train loss:2.3029329208449387\n",
      "train loss:2.3161275816054534\n",
      "train loss:2.3064141225465544\n",
      "train loss:2.3154370686446595\n",
      "train loss:2.316843255669708\n",
      "train loss:2.3050086696865257\n",
      "train loss:2.3089496566448005\n",
      "train loss:2.298943511963687\n",
      "train loss:2.302278219134248\n",
      "train loss:2.3072811038359644\n",
      "train loss:2.311819092170496\n",
      "train loss:2.3085540493550107\n",
      "train loss:2.3122140410025356\n",
      "train loss:2.3135510099057495\n",
      "train loss:2.3069150086319965\n",
      "train loss:2.3139614131805937\n",
      "train loss:2.3011811635374846\n",
      "train loss:2.2990927970233517\n",
      "train loss:2.303292676222217\n",
      "train loss:2.303093387900183\n",
      "train loss:2.30999030395918\n",
      "train loss:2.3031504924986215\n",
      "train loss:2.3090048086782624\n",
      "train loss:2.310992291438508\n",
      "train loss:2.3076425298745398\n",
      "train loss:2.3075969155838694\n",
      "train loss:2.2992895463251295\n",
      "train loss:2.304645111636077\n",
      "train loss:2.3077012074104726\n",
      "train loss:2.3011753503203263\n",
      "train loss:2.2993951460994815\n",
      "train loss:2.3082334242207385\n",
      "train loss:2.3096196414685592\n",
      "train loss:2.3109390813357367\n",
      "train loss:2.3113541569460616\n",
      "train loss:2.3080877303021756\n",
      "train loss:2.3162193487157916\n",
      "train loss:2.3048770068568554\n",
      "train loss:2.303909540849812\n",
      "train loss:2.3101914841561637\n",
      "train loss:2.2961789676641287\n",
      "train loss:2.3049823174284514\n",
      "train loss:2.300266753294032\n",
      "train loss:2.306337149658215\n",
      "train loss:2.3022852820503927\n",
      "train loss:2.304205094523396\n",
      "train loss:2.3085693785642007\n",
      "train loss:2.310313964934865\n",
      "train loss:2.301823402638302\n",
      "train loss:2.3029355931080784\n",
      "train loss:2.3093548198373854\n",
      "train loss:2.297969872848625\n",
      "train loss:2.306666781564812\n",
      "train loss:2.317197248377035\n",
      "train loss:2.3090922808657908\n",
      "train loss:2.30059643719284\n",
      "train loss:2.307234334182102\n",
      "train loss:2.3074400040300542\n",
      "train loss:2.3136293530623866\n",
      "train loss:2.3015826647724604\n",
      "train loss:2.305733560523947\n",
      "train loss:2.3038847640160895\n",
      "train loss:2.298138654818423\n",
      "train loss:2.2948346848879373\n",
      "train loss:2.3100757542134933\n",
      "train loss:2.302403212444218\n",
      "train loss:2.313067461183733\n",
      "train loss:2.314693353794363\n",
      "train loss:2.3002187578957134\n",
      "train loss:2.3121429587541322\n",
      "train loss:2.304691537075173\n",
      "train loss:2.29934121643241\n",
      "train loss:2.3055840489087687\n",
      "train loss:2.302077106208299\n",
      "train loss:2.301694928456995\n",
      "train loss:2.3050031377850377\n",
      "train loss:2.3128850909861853\n",
      "train loss:2.3072846817669848\n",
      "train loss:2.3114219030591765\n",
      "train loss:2.3105888582886704\n",
      "train loss:2.3122540726198397\n",
      "train loss:2.3053511894887952\n",
      "train loss:2.303716256108405\n",
      "train loss:2.300754581628736\n",
      "train loss:2.302992439730964\n",
      "train loss:2.3085424205376426\n",
      "train loss:2.3044945557191125\n",
      "train loss:2.310074748823974\n",
      "train loss:2.3031620839056703\n",
      "train loss:2.31628667293243\n",
      "train loss:2.3107335888894385\n",
      "train loss:2.3119549032411615\n",
      "train loss:2.311423628806273\n",
      "train loss:2.308460767037381\n",
      "train loss:2.308369891700458\n",
      "train loss:2.298503698832726\n",
      "train loss:2.2978304076140725\n",
      "train loss:2.309614282068272\n",
      "train loss:2.316591847524766\n",
      "train loss:2.3100696112936547\n",
      "train loss:2.3065570302913536\n",
      "train loss:2.302028449052305\n",
      "train loss:2.3069176254154367\n",
      "train loss:2.313928695491412\n",
      "train loss:2.303916730628217\n",
      "train loss:2.3061356227209524\n",
      "train loss:2.2996113290564115\n",
      "train loss:2.3054911287989523\n",
      "train loss:2.314358656781821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3070998838346997\n",
      "train loss:2.3033339668863047\n",
      "train loss:2.3011003602059006\n",
      "train loss:2.3006620007036584\n",
      "train loss:2.307580638265519\n",
      "train loss:2.2982280504115185\n",
      "train loss:2.306240985106583\n",
      "train loss:2.3100527432068207\n",
      "train loss:2.3081276405658375\n",
      "train loss:2.301473491376687\n",
      "train loss:2.3034717002607024\n",
      "train loss:2.313955664996359\n",
      "train loss:2.305538432431756\n",
      "train loss:2.311652801620685\n",
      "train loss:2.3002264464071835\n",
      "train loss:2.3043457520715256\n",
      "train loss:2.3046484474090794\n",
      "train loss:2.3064278294113687\n",
      "train loss:2.305832295961119\n",
      "train loss:2.300949444850927\n",
      "train loss:2.3135610785079628\n",
      "train loss:2.302265515208599\n",
      "train loss:2.3017773558792305\n",
      "train loss:2.2988337819285167\n",
      "train loss:2.29972807782438\n",
      "train loss:2.3097871783040014\n",
      "train loss:2.310037844624333\n",
      "train loss:2.3079209752431598\n",
      "train loss:2.3127623106021984\n",
      "train loss:2.3047921457410485\n",
      "train loss:2.3003833225045476\n",
      "train loss:2.3118351532430768\n",
      "train loss:2.3090085683594403\n",
      "train loss:2.3094813784057546\n",
      "train loss:2.3117771457197445\n",
      "train loss:2.3097929717834798\n",
      "train loss:2.302198640228707\n",
      "train loss:2.304237158574918\n",
      "train loss:2.2943467846434844\n",
      "train loss:2.3027763350645816\n",
      "train loss:2.2943704292143017\n",
      "train loss:2.3133883544296245\n",
      "train loss:2.307550333965573\n",
      "train loss:2.3028280669202736\n",
      "train loss:2.303267363996247\n",
      "train loss:2.310906978819401\n",
      "=== epoch:25, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3033298942643805\n",
      "train loss:2.3031798862907165\n",
      "train loss:2.2988861613126668\n",
      "train loss:2.302981385249507\n",
      "train loss:2.303630557449496\n",
      "train loss:2.3020080891366352\n",
      "train loss:2.2925370102631364\n",
      "train loss:2.306799299000652\n",
      "train loss:2.3032572808322347\n",
      "train loss:2.303744016293092\n",
      "train loss:2.2978859412650743\n",
      "train loss:2.303211004151315\n",
      "train loss:2.304101463237102\n",
      "train loss:2.310292558127414\n",
      "train loss:2.313986275580221\n",
      "train loss:2.307747238372434\n",
      "train loss:2.3152282559961295\n",
      "train loss:2.307671838495961\n",
      "train loss:2.311306641457362\n",
      "train loss:2.2985793891124633\n",
      "train loss:2.304259969264475\n",
      "train loss:2.3053986978125063\n",
      "train loss:2.300880247079342\n",
      "train loss:2.30481511282216\n",
      "train loss:2.309530296025187\n",
      "train loss:2.308069058835539\n",
      "train loss:2.3090779716673735\n",
      "train loss:2.305032391758784\n",
      "train loss:2.3041533623277095\n",
      "train loss:2.302270791729835\n",
      "train loss:2.314378300992369\n",
      "train loss:2.310154811833906\n",
      "train loss:2.3109602642553577\n",
      "train loss:2.2945967214757017\n",
      "train loss:2.293561842221614\n",
      "train loss:2.308931863008742\n",
      "train loss:2.302824010597175\n",
      "train loss:2.3138637511916382\n",
      "train loss:2.3018952411723927\n",
      "train loss:2.3075368671399024\n",
      "train loss:2.3008259620449856\n",
      "train loss:2.3105060819728975\n",
      "train loss:2.304114910861536\n",
      "train loss:2.3026718018779206\n",
      "train loss:2.3049660361133926\n",
      "train loss:2.3038691950092356\n",
      "train loss:2.302996319518542\n",
      "train loss:2.3038192064742526\n",
      "train loss:2.3024841260828546\n",
      "train loss:2.3091646545226627\n",
      "train loss:2.3121702095791927\n",
      "train loss:2.312515767984619\n",
      "train loss:2.3047660785642434\n",
      "train loss:2.308297336911281\n",
      "train loss:2.301130727409488\n",
      "train loss:2.304850829131033\n",
      "train loss:2.300813007528289\n",
      "train loss:2.2991777077851587\n",
      "train loss:2.302721052724181\n",
      "train loss:2.3040388670715184\n",
      "train loss:2.309441010130028\n",
      "train loss:2.3064461772905744\n",
      "train loss:2.301487430252444\n",
      "train loss:2.304799223984342\n",
      "train loss:2.30537854181404\n",
      "train loss:2.3073060637371783\n",
      "train loss:2.3040490787969587\n",
      "train loss:2.3064089990395558\n",
      "train loss:2.2989842151007487\n",
      "train loss:2.293967398190238\n",
      "train loss:2.3014776955377445\n",
      "train loss:2.306925873742985\n",
      "train loss:2.304240775589479\n",
      "train loss:2.3192373480706587\n",
      "train loss:2.3066390981441254\n",
      "train loss:2.304221314949885\n",
      "train loss:2.305890300792257\n",
      "train loss:2.3011719231960073\n",
      "train loss:2.294761541277636\n",
      "train loss:2.303537563766638\n",
      "train loss:2.3071660571955417\n",
      "train loss:2.3038591221302354\n",
      "train loss:2.300566588148304\n",
      "train loss:2.3075908234452935\n",
      "train loss:2.301780943592705\n",
      "train loss:2.297338686189765\n",
      "train loss:2.3010377102122535\n",
      "train loss:2.307552601211996\n",
      "train loss:2.30748449721293\n",
      "train loss:2.3090700274763356\n",
      "train loss:2.303763029586665\n",
      "train loss:2.3041115432239856\n",
      "train loss:2.3061578064226924\n",
      "train loss:2.305031751020928\n",
      "train loss:2.3069735794294743\n",
      "train loss:2.2979058195083018\n",
      "train loss:2.3108628346089506\n",
      "train loss:2.3098947181628806\n",
      "train loss:2.306907829377039\n",
      "train loss:2.3030753668995545\n",
      "train loss:2.311537774770383\n",
      "train loss:2.3085245328320223\n",
      "train loss:2.2995855192813393\n",
      "train loss:2.3112412773715545\n",
      "train loss:2.307856178838759\n",
      "train loss:2.3012778690396103\n",
      "train loss:2.3082956324266397\n",
      "train loss:2.306683883988299\n",
      "train loss:2.301764754815359\n",
      "train loss:2.3073741014165856\n",
      "train loss:2.309825601596227\n",
      "train loss:2.3034482822891915\n",
      "train loss:2.308975007061052\n",
      "train loss:2.3019860262521785\n",
      "train loss:2.3081387926151598\n",
      "train loss:2.2978421722964555\n",
      "train loss:2.3091468702832794\n",
      "train loss:2.2983473766954208\n",
      "train loss:2.3005197471135514\n",
      "train loss:2.301073823095856\n",
      "train loss:2.3014032993009\n",
      "train loss:2.297462206763586\n",
      "train loss:2.312082525266127\n",
      "train loss:2.3055374182063666\n",
      "train loss:2.3122970524093884\n",
      "train loss:2.3086849623207595\n",
      "train loss:2.305398459902537\n",
      "train loss:2.3109236437036826\n",
      "train loss:2.3054541641282964\n",
      "train loss:2.300134722462505\n",
      "train loss:2.298857174758533\n",
      "train loss:2.3064990369989333\n",
      "train loss:2.30715708224575\n",
      "train loss:2.3013834393919974\n",
      "train loss:2.3064441628325714\n",
      "train loss:2.303167219217742\n",
      "train loss:2.3013867570174664\n",
      "train loss:2.3113510851458945\n",
      "train loss:2.3057077844438463\n",
      "train loss:2.3006668608207814\n",
      "train loss:2.301285951138064\n",
      "train loss:2.296562356798623\n",
      "train loss:2.31118868676179\n",
      "train loss:2.3043926302897737\n",
      "train loss:2.298557525911914\n",
      "train loss:2.294723019347777\n",
      "train loss:2.302173925377205\n",
      "train loss:2.299894832323623\n",
      "train loss:2.3032323925381113\n",
      "train loss:2.2968877839009942\n",
      "train loss:2.3057976023163618\n",
      "train loss:2.3026858453730377\n",
      "train loss:2.3047934444421627\n",
      "train loss:2.3166276665607186\n",
      "train loss:2.3052729233552056\n",
      "train loss:2.3021073710573345\n",
      "train loss:2.3137643922989573\n",
      "train loss:2.308301784032972\n",
      "train loss:2.305133017068425\n",
      "train loss:2.3063018116982295\n",
      "train loss:2.299349206791348\n",
      "train loss:2.3044030868652507\n",
      "train loss:2.310629470433338\n",
      "train loss:2.3055972751866225\n",
      "train loss:2.308410050071051\n",
      "train loss:2.3094854561979528\n",
      "train loss:2.310959334999284\n",
      "train loss:2.3137935412472275\n",
      "train loss:2.3033549350668125\n",
      "train loss:2.3073236408164974\n",
      "train loss:2.3022484102608245\n",
      "train loss:2.303094408404359\n",
      "train loss:2.301343217265469\n",
      "train loss:2.311612950574762\n",
      "train loss:2.3105150206631793\n",
      "train loss:2.3082124598662332\n",
      "train loss:2.3084570919941587\n",
      "train loss:2.3025821838698586\n",
      "train loss:2.29774709313544\n",
      "train loss:2.300175388935342\n",
      "train loss:2.3167586059457497\n",
      "train loss:2.303293588630917\n",
      "train loss:2.3070517951293095\n",
      "train loss:2.306076039388989\n",
      "train loss:2.3007503362179396\n",
      "train loss:2.303099554306042\n",
      "train loss:2.315394157247276\n",
      "train loss:2.3141832290454283\n",
      "train loss:2.301308472837839\n",
      "train loss:2.305438964164772\n",
      "train loss:2.3068318524300366\n",
      "train loss:2.314548531932542\n",
      "train loss:2.301368589082162\n",
      "train loss:2.2987243285613457\n",
      "train loss:2.3118299637429587\n",
      "train loss:2.306132087774767\n",
      "train loss:2.311155743922814\n",
      "train loss:2.3074326975899484\n",
      "train loss:2.315149966984892\n",
      "train loss:2.293818165522631\n",
      "=== epoch:26, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2947605423730355\n",
      "train loss:2.312741654877771\n",
      "train loss:2.306345060180923\n",
      "train loss:2.3035935605326725\n",
      "train loss:2.305209771938982\n",
      "train loss:2.299910471085119\n",
      "train loss:2.2961911622349223\n",
      "train loss:2.3066423433176637\n",
      "train loss:2.3067747043468323\n",
      "train loss:2.3019581491675734\n",
      "train loss:2.3058774534731654\n",
      "train loss:2.307687178345722\n",
      "train loss:2.301566531864825\n",
      "train loss:2.3031284497150812\n",
      "train loss:2.3039361966467484\n",
      "train loss:2.3123758020557528\n",
      "train loss:2.3078400476919474\n",
      "train loss:2.3047345028785524\n",
      "train loss:2.301289531783298\n",
      "train loss:2.3086615582816004\n",
      "train loss:2.2973652699812677\n",
      "train loss:2.301545636513545\n",
      "train loss:2.3086408132442813\n",
      "train loss:2.298975054061797\n",
      "train loss:2.3048753112974287\n",
      "train loss:2.3076505945981483\n",
      "train loss:2.303056140433267\n",
      "train loss:2.3010552745163544\n",
      "train loss:2.311166139024335\n",
      "train loss:2.3008716387929558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.295316528166316\n",
      "train loss:2.3073155180803244\n",
      "train loss:2.3009226655571875\n",
      "train loss:2.311647141202254\n",
      "train loss:2.306701907790781\n",
      "train loss:2.2993853290928\n",
      "train loss:2.308476672845778\n",
      "train loss:2.3040095386087405\n",
      "train loss:2.303091767738381\n",
      "train loss:2.307944203087981\n",
      "train loss:2.3068881449458947\n",
      "train loss:2.3068255181144908\n",
      "train loss:2.3033119235956865\n",
      "train loss:2.2987507044714524\n",
      "train loss:2.3011576948228356\n",
      "train loss:2.3024819301706763\n",
      "train loss:2.2924782806588153\n",
      "train loss:2.31119442808452\n",
      "train loss:2.3095653729186307\n",
      "train loss:2.306022484606597\n",
      "train loss:2.3021872776216097\n",
      "train loss:2.3026781142222426\n",
      "train loss:2.307955813748486\n",
      "train loss:2.312832344031546\n",
      "train loss:2.2985480058060985\n",
      "train loss:2.305205601658895\n",
      "train loss:2.303169977123659\n",
      "train loss:2.307922719265571\n",
      "train loss:2.3032146837908485\n",
      "train loss:2.3068012918499705\n",
      "train loss:2.2963748577239023\n",
      "train loss:2.303322834573487\n",
      "train loss:2.3037393027643653\n",
      "train loss:2.3030113806485266\n",
      "train loss:2.306242831189208\n",
      "train loss:2.2987826382057355\n",
      "train loss:2.3022229418541147\n",
      "train loss:2.2946595178694484\n",
      "train loss:2.3076029905183315\n",
      "train loss:2.309410759180791\n",
      "train loss:2.3070290560761024\n",
      "train loss:2.3112883834590097\n",
      "train loss:2.3067215298216923\n",
      "train loss:2.3021551543390437\n",
      "train loss:2.3027055742524807\n",
      "train loss:2.304056757990903\n",
      "train loss:2.3118602882652532\n",
      "train loss:2.2978890506780627\n",
      "train loss:2.309972252855454\n",
      "train loss:2.3075980411254378\n",
      "train loss:2.2971223949300152\n",
      "train loss:2.295026729890564\n",
      "train loss:2.3041272328622364\n",
      "train loss:2.307622974105922\n",
      "train loss:2.3115845747778634\n",
      "train loss:2.3075487785592506\n",
      "train loss:2.3032081125540147\n",
      "train loss:2.3047811902070383\n",
      "train loss:2.3090845199693733\n",
      "train loss:2.2965102359131917\n",
      "train loss:2.2973663544927776\n",
      "train loss:2.2905535696325887\n",
      "train loss:2.303969836819537\n",
      "train loss:2.2886966430910465\n",
      "train loss:2.302207140624316\n",
      "train loss:2.3074509395409315\n",
      "train loss:2.2930118890677433\n",
      "train loss:2.305549509414537\n",
      "train loss:2.30504014724353\n",
      "train loss:2.306009560923484\n",
      "train loss:2.3087571652954395\n",
      "train loss:2.3055604794046927\n",
      "train loss:2.3088927271972173\n",
      "train loss:2.297628275276586\n",
      "train loss:2.2993727660872842\n",
      "train loss:2.2991614543748917\n",
      "train loss:2.3086426791227423\n",
      "train loss:2.3039475671880547\n",
      "train loss:2.3029522115320558\n",
      "train loss:2.3021607012096794\n",
      "train loss:2.297346858613511\n",
      "train loss:2.304423599302624\n",
      "train loss:2.299516597117975\n",
      "train loss:2.301566811257862\n",
      "train loss:2.3068412700260192\n",
      "train loss:2.2996631981940463\n",
      "train loss:2.2983988781546567\n",
      "train loss:2.301374423118973\n",
      "train loss:2.2994137257385794\n",
      "train loss:2.3072690743046067\n",
      "train loss:2.3004830605254636\n",
      "train loss:2.307459366903888\n",
      "train loss:2.298025514701254\n",
      "train loss:2.296240726669561\n",
      "train loss:2.298653842358293\n",
      "train loss:2.298388314766036\n",
      "train loss:2.3014658405282065\n",
      "train loss:2.3108971594109344\n",
      "train loss:2.297473771872106\n",
      "train loss:2.300967105209207\n",
      "train loss:2.298999257273671\n",
      "train loss:2.309457690090747\n",
      "train loss:2.3051147899697213\n",
      "train loss:2.3055247674943344\n",
      "train loss:2.2943291687580754\n",
      "train loss:2.3047994034931065\n",
      "train loss:2.3020325199969984\n",
      "train loss:2.29726759453816\n",
      "train loss:2.3099725076150057\n",
      "train loss:2.301131252353481\n",
      "train loss:2.307115765818468\n",
      "train loss:2.302248947972809\n",
      "train loss:2.30360981843967\n",
      "train loss:2.3019261344385216\n",
      "train loss:2.3074322355300874\n",
      "train loss:2.299920800399331\n",
      "train loss:2.3067789580951708\n",
      "train loss:2.304720073544806\n",
      "train loss:2.307448376871301\n",
      "train loss:2.3039635533158753\n",
      "train loss:2.3067732407321224\n",
      "train loss:2.3023628873594832\n",
      "train loss:2.299504831419895\n",
      "train loss:2.3085636636099833\n",
      "train loss:2.2988269579272385\n",
      "train loss:2.3111186318180788\n",
      "train loss:2.2972555921659787\n",
      "train loss:2.30141017814698\n",
      "train loss:2.3114146127694943\n",
      "train loss:2.3009783561408264\n",
      "train loss:2.300997242273682\n",
      "train loss:2.3073392647457025\n",
      "train loss:2.3024954337012047\n",
      "train loss:2.297625973020555\n",
      "train loss:2.298508404477178\n",
      "train loss:2.3148865534517067\n",
      "train loss:2.3054218671923987\n",
      "train loss:2.3074543372941116\n",
      "train loss:2.299036851732168\n",
      "train loss:2.299413754038629\n",
      "train loss:2.3085979721070435\n",
      "train loss:2.3012803712821466\n",
      "train loss:2.288513113344708\n",
      "train loss:2.3075888729225955\n",
      "train loss:2.301300600947731\n",
      "train loss:2.3053277893815394\n",
      "train loss:2.30563585713619\n",
      "train loss:2.29968315367869\n",
      "train loss:2.304224252419119\n",
      "train loss:2.31178482678952\n",
      "train loss:2.3064977333329257\n",
      "train loss:2.297839595954881\n",
      "train loss:2.3048451427615855\n",
      "train loss:2.3096050243216855\n",
      "train loss:2.302136927137463\n",
      "train loss:2.3061254978715295\n",
      "train loss:2.298007060736803\n",
      "train loss:2.307835623257957\n",
      "train loss:2.3059710305087666\n",
      "train loss:2.305003158174537\n",
      "train loss:2.3075481877260846\n",
      "train loss:2.3051421169585207\n",
      "train loss:2.299661742886229\n",
      "train loss:2.3005464958801194\n",
      "train loss:2.30662343479423\n",
      "train loss:2.3019917441984648\n",
      "train loss:2.301613683829017\n",
      "train loss:2.298884382703297\n",
      "train loss:2.3069792112588505\n",
      "train loss:2.2999662943494568\n",
      "=== epoch:27, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3072170158445147\n",
      "train loss:2.3111084405731797\n",
      "train loss:2.309174684612792\n",
      "train loss:2.3000190811067283\n",
      "train loss:2.297823935750408\n",
      "train loss:2.3081553463831668\n",
      "train loss:2.302467333572478\n",
      "train loss:2.302452499812125\n",
      "train loss:2.3002185220610887\n",
      "train loss:2.3089758849682545\n",
      "train loss:2.313604846330699\n",
      "train loss:2.3002900827241026\n",
      "train loss:2.2949940955927577\n",
      "train loss:2.2965893374061377\n",
      "train loss:2.308666185583002\n",
      "train loss:2.3118451617594027\n",
      "train loss:2.310075639036077\n",
      "train loss:2.3108660150193576\n",
      "train loss:2.304974074547956\n",
      "train loss:2.301845410828871\n",
      "train loss:2.304989617540652\n",
      "train loss:2.299120621548548\n",
      "train loss:2.3083779360794754\n",
      "train loss:2.294380868846809\n",
      "train loss:2.2976375860103464\n",
      "train loss:2.302646126877193\n",
      "train loss:2.3066483018230075\n",
      "train loss:2.307032840556142\n",
      "train loss:2.3096998358111036\n",
      "train loss:2.3096563460320603\n",
      "train loss:2.3056900722439573\n",
      "train loss:2.310330051244263\n",
      "train loss:2.3003663786752644\n",
      "train loss:2.2983832661076637\n",
      "train loss:2.2999647649282244\n",
      "train loss:2.3032730737345832\n",
      "train loss:2.300351797865602\n",
      "train loss:2.301629774714308\n",
      "train loss:2.3140735525620086\n",
      "train loss:2.3031736785984274\n",
      "train loss:2.3052771192204107\n",
      "train loss:2.2999508387979364\n",
      "train loss:2.3073716061074805\n",
      "train loss:2.3075558710119854\n",
      "train loss:2.298169941477622\n",
      "train loss:2.287492316414639\n",
      "train loss:2.3044347712014934\n",
      "train loss:2.2997025652330705\n",
      "train loss:2.3047592009777853\n",
      "train loss:2.303043228889601\n",
      "train loss:2.305897634466108\n",
      "train loss:2.297824548152597\n",
      "train loss:2.3083142277295705\n",
      "train loss:2.3020107837299815\n",
      "train loss:2.3093958129709033\n",
      "train loss:2.2924435658613675\n",
      "train loss:2.302374500054795\n",
      "train loss:2.3055453180456853\n",
      "train loss:2.2961691148153682\n",
      "train loss:2.3004309094203657\n",
      "train loss:2.2985677131198528\n",
      "train loss:2.3072059058637695\n",
      "train loss:2.301799993307949\n",
      "train loss:2.302328999579067\n",
      "train loss:2.3058129236020135\n",
      "train loss:2.3037720264115626\n",
      "train loss:2.3016328206819634\n",
      "train loss:2.306704211814534\n",
      "train loss:2.3046880457001735\n",
      "train loss:2.297240322808458\n",
      "train loss:2.310923562329426\n",
      "train loss:2.301729564854362\n",
      "train loss:2.305866852933946\n",
      "train loss:2.2973083164341497\n",
      "train loss:2.307764496077122\n",
      "train loss:2.297986231031824\n",
      "train loss:2.302011581842115\n",
      "train loss:2.2973481638837643\n",
      "train loss:2.3087920471835846\n",
      "train loss:2.3130562482908723\n",
      "train loss:2.305058885186089\n",
      "train loss:2.3101938342330293\n",
      "train loss:2.3042759968651354\n",
      "train loss:2.308355340779441\n",
      "train loss:2.2994409607873654\n",
      "train loss:2.307370785314695\n",
      "train loss:2.302408342478498\n",
      "train loss:2.3009583023319267\n",
      "train loss:2.301466879875632\n",
      "train loss:2.305446362113039\n",
      "train loss:2.3030442383334075\n",
      "train loss:2.3037109326567906\n",
      "train loss:2.2991547180463674\n",
      "train loss:2.2984765427217306\n",
      "train loss:2.299035896341794\n",
      "train loss:2.2936929157333554\n",
      "train loss:2.2993338199952165\n",
      "train loss:2.3067640436221946\n",
      "train loss:2.308070259605606\n",
      "train loss:2.305243579157642\n",
      "train loss:2.301270286826189\n",
      "train loss:2.3106799797683326\n",
      "train loss:2.298656686017522\n",
      "train loss:2.291183325348665\n",
      "train loss:2.300558528602363\n",
      "train loss:2.3037513038440127\n",
      "train loss:2.3057367218893106\n",
      "train loss:2.303255038987308\n",
      "train loss:2.293165311640898\n",
      "train loss:2.3081845455436687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3021151520454906\n",
      "train loss:2.3083855848315173\n",
      "train loss:2.298020122197251\n",
      "train loss:2.2970877196346273\n",
      "train loss:2.299715799282616\n",
      "train loss:2.291711571270831\n",
      "train loss:2.3003411635066136\n",
      "train loss:2.303538543409165\n",
      "train loss:2.3055129244906514\n",
      "train loss:2.303746129923204\n",
      "train loss:2.29627972481867\n",
      "train loss:2.302754618165701\n",
      "train loss:2.304263378068274\n",
      "train loss:2.3041502343060722\n",
      "train loss:2.3077256847695873\n",
      "train loss:2.302390256997415\n",
      "train loss:2.3018949844230256\n",
      "train loss:2.295068044220519\n",
      "train loss:2.302459546355678\n",
      "train loss:2.3011974851204755\n",
      "train loss:2.3099339544433475\n",
      "train loss:2.3059369960139176\n",
      "train loss:2.303625663278385\n",
      "train loss:2.3068604441916127\n",
      "train loss:2.2982850574424325\n",
      "train loss:2.3002442625911357\n",
      "train loss:2.301649042604708\n",
      "train loss:2.302478380859693\n",
      "train loss:2.2998910810854793\n",
      "train loss:2.307580820692612\n",
      "train loss:2.297338626268404\n",
      "train loss:2.290947714032937\n",
      "train loss:2.300737704040922\n",
      "train loss:2.3009607130300336\n",
      "train loss:2.30725331813312\n",
      "train loss:2.2990359745039637\n",
      "train loss:2.298571128498393\n",
      "train loss:2.30315671989893\n",
      "train loss:2.2999058106094576\n",
      "train loss:2.3064203325888846\n",
      "train loss:2.3100584338196066\n",
      "train loss:2.2998531011524364\n",
      "train loss:2.3022500588730868\n",
      "train loss:2.3020726382626786\n",
      "train loss:2.295921367967034\n",
      "train loss:2.2982939668464897\n",
      "train loss:2.302683316715959\n",
      "train loss:2.2968570149063092\n",
      "train loss:2.296580405403015\n",
      "train loss:2.2959430345845213\n",
      "train loss:2.2952049315088603\n",
      "train loss:2.299826158874826\n",
      "train loss:2.30164598678053\n",
      "train loss:2.299055477018148\n",
      "train loss:2.304049374597286\n",
      "train loss:2.300778752191039\n",
      "train loss:2.3089053634136167\n",
      "train loss:2.301443948999481\n",
      "train loss:2.3139164869829534\n",
      "train loss:2.30145915431904\n",
      "train loss:2.3090000683405028\n",
      "train loss:2.2991255950536846\n",
      "train loss:2.2994040614539104\n",
      "train loss:2.3035490208863774\n",
      "train loss:2.29483411598378\n",
      "train loss:2.2970247320642367\n",
      "train loss:2.30086672840641\n",
      "train loss:2.308620599103879\n",
      "train loss:2.3085017835387753\n",
      "train loss:2.297122857347859\n",
      "train loss:2.3083111555482554\n",
      "train loss:2.3021246115508895\n",
      "train loss:2.303032067540136\n",
      "train loss:2.3073807234973254\n",
      "train loss:2.2995870528317255\n",
      "train loss:2.307052604809119\n",
      "train loss:2.299586612027194\n",
      "train loss:2.3038587709295437\n",
      "train loss:2.303003919161634\n",
      "train loss:2.302660092648262\n",
      "train loss:2.302122238304583\n",
      "train loss:2.3072139546651576\n",
      "train loss:2.3051448775648407\n",
      "train loss:2.3077401051396316\n",
      "train loss:2.3034677602796934\n",
      "train loss:2.3083009465253594\n",
      "train loss:2.299697421505792\n",
      "train loss:2.298133191026798\n",
      "train loss:2.3067363658312066\n",
      "train loss:2.304617895122159\n",
      "=== epoch:28, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.307878415730945\n",
      "train loss:2.309961355384975\n",
      "train loss:2.307891262328482\n",
      "train loss:2.298349039643733\n",
      "train loss:2.294952094099514\n",
      "train loss:2.31045428765663\n",
      "train loss:2.3059815509484025\n",
      "train loss:2.3065648723705827\n",
      "train loss:2.30355924308027\n",
      "train loss:2.305424164768302\n",
      "train loss:2.3079383692375854\n",
      "train loss:2.306642129534425\n",
      "train loss:2.306792359607167\n",
      "train loss:2.301130663994097\n",
      "train loss:2.300616933465419\n",
      "train loss:2.302687019360942\n",
      "train loss:2.3109283534774727\n",
      "train loss:2.3049595551659303\n",
      "train loss:2.297424891482262\n",
      "train loss:2.306314567961812\n",
      "train loss:2.3021591853299186\n",
      "train loss:2.298675327853813\n",
      "train loss:2.3010252226928674\n",
      "train loss:2.310697269158869\n",
      "train loss:2.3019680751767\n",
      "train loss:2.305067452818774\n",
      "train loss:2.3118830547445226\n",
      "train loss:2.297715261151082\n",
      "train loss:2.3015945265892825\n",
      "train loss:2.299381868444171\n",
      "train loss:2.302263468346751\n",
      "train loss:2.304034452111854\n",
      "train loss:2.311410340081128\n",
      "train loss:2.303896346899428\n",
      "train loss:2.305783350524887\n",
      "train loss:2.3024869106329757\n",
      "train loss:2.305575155937146\n",
      "train loss:2.306466167225849\n",
      "train loss:2.298509814610812\n",
      "train loss:2.3011140272188952\n",
      "train loss:2.305073857763153\n",
      "train loss:2.3051202753918205\n",
      "train loss:2.30329501450649\n",
      "train loss:2.307975622277072\n",
      "train loss:2.302130715198796\n",
      "train loss:2.300476124824206\n",
      "train loss:2.2933897138511576\n",
      "train loss:2.3040610528018752\n",
      "train loss:2.3006522102849862\n",
      "train loss:2.3044439630682816\n",
      "train loss:2.305496881510613\n",
      "train loss:2.3121929701089567\n",
      "train loss:2.2969239659633454\n",
      "train loss:2.3085851153418258\n",
      "train loss:2.3047580153152616\n",
      "train loss:2.305759021508745\n",
      "train loss:2.30576336246086\n",
      "train loss:2.3076602031080053\n",
      "train loss:2.3061478671855835\n",
      "train loss:2.301443312360957\n",
      "train loss:2.3028257578399622\n",
      "train loss:2.2999167546830472\n",
      "train loss:2.301806924697652\n",
      "train loss:2.3000019852816433\n",
      "train loss:2.3050109827877137\n",
      "train loss:2.3056440657177353\n",
      "train loss:2.291235107909534\n",
      "train loss:2.3001275621656236\n",
      "train loss:2.3071315528870397\n",
      "train loss:2.300238480575407\n",
      "train loss:2.303726697895484\n",
      "train loss:2.310964994777811\n",
      "train loss:2.300291459973208\n",
      "train loss:2.2997219179265436\n",
      "train loss:2.3013824491018675\n",
      "train loss:2.3018261802800923\n",
      "train loss:2.309555272832911\n",
      "train loss:2.3036146307610488\n",
      "train loss:2.304949790373529\n",
      "train loss:2.2964929989625977\n",
      "train loss:2.301719418112675\n",
      "train loss:2.3018627079785556\n",
      "train loss:2.3066214430843637\n",
      "train loss:2.3015922683985606\n",
      "train loss:2.303629580025073\n",
      "train loss:2.30329547236528\n",
      "train loss:2.301010025798217\n",
      "train loss:2.3031614566745366\n",
      "train loss:2.3062509574726255\n",
      "train loss:2.2971917317275894\n",
      "train loss:2.3043016117751414\n",
      "train loss:2.3044140636277604\n",
      "train loss:2.305360060208351\n",
      "train loss:2.2960177800619084\n",
      "train loss:2.305050091247776\n",
      "train loss:2.2936772005768695\n",
      "train loss:2.2968494767057988\n",
      "train loss:2.3005753477164355\n",
      "train loss:2.298142519446835\n",
      "train loss:2.2999947477043854\n",
      "train loss:2.2999273537744114\n",
      "train loss:2.306251352054\n",
      "train loss:2.301202592904621\n",
      "train loss:2.308741698475332\n",
      "train loss:2.3019239497773323\n",
      "train loss:2.3014637972723055\n",
      "train loss:2.3067305803506417\n",
      "train loss:2.3035815210086366\n",
      "train loss:2.3051117152887257\n",
      "train loss:2.3048256137661594\n",
      "train loss:2.2979300327257124\n",
      "train loss:2.30004796260638\n",
      "train loss:2.30652211181951\n",
      "train loss:2.2952515441834604\n",
      "train loss:2.3000253388028136\n",
      "train loss:2.2925649214100967\n",
      "train loss:2.312369462729305\n",
      "train loss:2.3086071539244553\n",
      "train loss:2.3125799584465376\n",
      "train loss:2.303369838372148\n",
      "train loss:2.3099647025680015\n",
      "train loss:2.295999099041778\n",
      "train loss:2.3058745237319624\n",
      "train loss:2.3052420863053347\n",
      "train loss:2.3084852491757264\n",
      "train loss:2.3019451401484994\n",
      "train loss:2.3066535013987806\n",
      "train loss:2.304525626298726\n",
      "train loss:2.2974123450442887\n",
      "train loss:2.3049937138157977\n",
      "train loss:2.2928624897074923\n",
      "train loss:2.306540402203279\n",
      "train loss:2.300892592489021\n",
      "train loss:2.3029361668099617\n",
      "train loss:2.293992455160299\n",
      "train loss:2.2978967747879686\n",
      "train loss:2.29235544986768\n",
      "train loss:2.3088115728192835\n",
      "train loss:2.30357140976944\n",
      "train loss:2.305017346632676\n",
      "train loss:2.305799523578172\n",
      "train loss:2.2970442366676678\n",
      "train loss:2.3034750691144255\n",
      "train loss:2.294530490194889\n",
      "train loss:2.3097664770966517\n",
      "train loss:2.300702519145074\n",
      "train loss:2.3076120628955916\n",
      "train loss:2.301610637179499\n",
      "train loss:2.3029823868977197\n",
      "train loss:2.3013814651315436\n",
      "train loss:2.308031370837779\n",
      "train loss:2.307519055485244\n",
      "train loss:2.3014997880463754\n",
      "train loss:2.3072662583221164\n",
      "train loss:2.297348560430932\n",
      "train loss:2.299152484830491\n",
      "train loss:2.300453944945949\n",
      "train loss:2.2995232620268022\n",
      "train loss:2.3025907682434945\n",
      "train loss:2.300163326411737\n",
      "train loss:2.302904801729879\n",
      "train loss:2.3042546590698954\n",
      "train loss:2.295850316013408\n",
      "train loss:2.3026914233389397\n",
      "train loss:2.3057621507409514\n",
      "train loss:2.294667124788998\n",
      "train loss:2.3013072340910536\n",
      "train loss:2.3104887323981065\n",
      "train loss:2.312864838694421\n",
      "train loss:2.303050296980269\n",
      "train loss:2.2940880439076707\n",
      "train loss:2.3053133453277113\n",
      "train loss:2.3092129722036376\n",
      "train loss:2.3131885888294654\n",
      "train loss:2.2906228995975257\n",
      "train loss:2.3048359204841047\n",
      "train loss:2.3041095612217264\n",
      "train loss:2.3014401430946947\n",
      "train loss:2.299729523248393\n",
      "train loss:2.3010994332741683\n",
      "train loss:2.3014623342845013\n",
      "train loss:2.29730002769987\n",
      "train loss:2.294773782565517\n",
      "train loss:2.2891497763946558\n",
      "train loss:2.2968671276586425\n",
      "train loss:2.3084686144866318\n",
      "train loss:2.299880718520993\n",
      "train loss:2.299977090210655\n",
      "train loss:2.302170541440146\n",
      "train loss:2.3076067846195376\n",
      "train loss:2.302044256722948\n",
      "train loss:2.3033665017847604\n",
      "train loss:2.302080729523457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3030708621114826\n",
      "train loss:2.3061768053483\n",
      "train loss:2.3054020080524444\n",
      "train loss:2.292996654237069\n",
      "train loss:2.3106855519990663\n",
      "train loss:2.29784121553769\n",
      "train loss:2.3007788025812066\n",
      "=== epoch:29, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.299506014747133\n",
      "train loss:2.308782080615911\n",
      "train loss:2.3036019443008184\n",
      "train loss:2.300321158098311\n",
      "train loss:2.3024327531313977\n",
      "train loss:2.307896906858793\n",
      "train loss:2.3065659297358305\n",
      "train loss:2.3001313655641997\n",
      "train loss:2.2987788433837406\n",
      "train loss:2.3087209075651463\n",
      "train loss:2.288990987522713\n",
      "train loss:2.3023471957721213\n",
      "train loss:2.3040515363747747\n",
      "train loss:2.30181635123973\n",
      "train loss:2.304459517444959\n",
      "train loss:2.3028501839288302\n",
      "train loss:2.302799668118365\n",
      "train loss:2.3076255307812357\n",
      "train loss:2.301404940398911\n",
      "train loss:2.3110763391235793\n",
      "train loss:2.3001731479335157\n",
      "train loss:2.3018448137376417\n",
      "train loss:2.2953244007726807\n",
      "train loss:2.2977437026430763\n",
      "train loss:2.3047678278847052\n",
      "train loss:2.3000575380308956\n",
      "train loss:2.3035533482561483\n",
      "train loss:2.302651885907857\n",
      "train loss:2.3042128836576246\n",
      "train loss:2.301886197883998\n",
      "train loss:2.301939940809187\n",
      "train loss:2.309938426313184\n",
      "train loss:2.2974717231084014\n",
      "train loss:2.303061377241902\n",
      "train loss:2.3038479733943324\n",
      "train loss:2.3035803649730924\n",
      "train loss:2.3086586106174156\n",
      "train loss:2.3048812807154535\n",
      "train loss:2.308825326148827\n",
      "train loss:2.2970303461552346\n",
      "train loss:2.3027170540149386\n",
      "train loss:2.3049037320098127\n",
      "train loss:2.2923398765866416\n",
      "train loss:2.3009995433756902\n",
      "train loss:2.2878185883606634\n",
      "train loss:2.299801037855075\n",
      "train loss:2.3048970071196733\n",
      "train loss:2.3014679305822456\n",
      "train loss:2.2990593526972694\n",
      "train loss:2.30414208171764\n",
      "train loss:2.29670426083098\n",
      "train loss:2.3075537786360982\n",
      "train loss:2.3086985949470944\n",
      "train loss:2.300136991396002\n",
      "train loss:2.301019724636108\n",
      "train loss:2.2976758109022497\n",
      "train loss:2.2886605405461364\n",
      "train loss:2.307338303345833\n",
      "train loss:2.3029671928501205\n",
      "train loss:2.299047880193836\n",
      "train loss:2.307322049185625\n",
      "train loss:2.2997466519758287\n",
      "train loss:2.305541403001934\n",
      "train loss:2.303118333466555\n",
      "train loss:2.30440734405958\n",
      "train loss:2.299572438332374\n",
      "train loss:2.3006287404336847\n",
      "train loss:2.300178487415544\n",
      "train loss:2.298644008965596\n",
      "train loss:2.3015411373653065\n",
      "train loss:2.305427761877473\n",
      "train loss:2.299492603405021\n",
      "train loss:2.304321653511828\n",
      "train loss:2.3076504593184413\n",
      "train loss:2.3046657685707213\n",
      "train loss:2.2996482398873987\n",
      "train loss:2.300508619850868\n",
      "train loss:2.3027009102783595\n",
      "train loss:2.3028123237871374\n",
      "train loss:2.3097148012816153\n",
      "train loss:2.2935991607616844\n",
      "train loss:2.3070276484905192\n",
      "train loss:2.2898200178199546\n",
      "train loss:2.288193899835424\n",
      "train loss:2.30429112165863\n",
      "train loss:2.3059557219842612\n",
      "train loss:2.3077923990353075\n",
      "train loss:2.303279418999278\n",
      "train loss:2.2959956065997376\n",
      "train loss:2.2968965393807568\n",
      "train loss:2.3095127861813314\n",
      "train loss:2.3056314049487763\n",
      "train loss:2.3014574390803735\n",
      "train loss:2.307366751317388\n",
      "train loss:2.3018447038216827\n",
      "train loss:2.3103981589502167\n",
      "train loss:2.307493447862588\n",
      "train loss:2.2927634878309577\n",
      "train loss:2.3035206841884137\n",
      "train loss:2.290996580194006\n",
      "train loss:2.303264234245285\n",
      "train loss:2.2976538203595984\n",
      "train loss:2.3073883291092647\n",
      "train loss:2.296934562846505\n",
      "train loss:2.3002426829981326\n",
      "train loss:2.302645814154627\n",
      "train loss:2.304966780503496\n",
      "train loss:2.302295021847348\n",
      "train loss:2.304643803562837\n",
      "train loss:2.2962869563006407\n",
      "train loss:2.295289946816038\n",
      "train loss:2.304922371381127\n",
      "train loss:2.3083806771050424\n",
      "train loss:2.299299038514701\n",
      "train loss:2.3061812639451285\n",
      "train loss:2.3049843190907766\n",
      "train loss:2.299090681700159\n",
      "train loss:2.302207173556037\n",
      "train loss:2.307950822045421\n",
      "train loss:2.296834770388052\n",
      "train loss:2.3022843930362957\n",
      "train loss:2.3029507168126484\n",
      "train loss:2.295506451137373\n",
      "train loss:2.308997036043273\n",
      "train loss:2.3090800708809756\n",
      "train loss:2.301698334774529\n",
      "train loss:2.306013225027389\n",
      "train loss:2.3023815972622943\n",
      "train loss:2.295865106096197\n",
      "train loss:2.292374856661119\n",
      "train loss:2.303728859199223\n",
      "train loss:2.299651207894864\n",
      "train loss:2.301565047124078\n",
      "train loss:2.302418826848598\n",
      "train loss:2.303051210727218\n",
      "train loss:2.30922591920615\n",
      "train loss:2.2989664607134785\n",
      "train loss:2.307109790963971\n",
      "train loss:2.3101486001692253\n",
      "train loss:2.3038717535884508\n",
      "train loss:2.3021058972454482\n",
      "train loss:2.2978101625198075\n",
      "train loss:2.306207108568147\n",
      "train loss:2.2993804186698434\n",
      "train loss:2.3009862978517557\n",
      "train loss:2.303695995237808\n",
      "train loss:2.3059793808354283\n",
      "train loss:2.3032778774424565\n",
      "train loss:2.3129930452790464\n",
      "train loss:2.303024149021048\n",
      "train loss:2.2997827969449984\n",
      "train loss:2.303197555737668\n",
      "train loss:2.302739296433192\n",
      "train loss:2.3059776268482377\n",
      "train loss:2.299790370321398\n",
      "train loss:2.3036036504064787\n",
      "train loss:2.3008280980310425\n",
      "train loss:2.297519074915933\n",
      "train loss:2.3130439862762837\n",
      "train loss:2.2956119202830765\n",
      "train loss:2.3122797344769186\n",
      "train loss:2.3052826919924927\n",
      "train loss:2.303637905332446\n",
      "train loss:2.2913947088902775\n",
      "train loss:2.305943742072359\n",
      "train loss:2.2964840224936216\n",
      "train loss:2.3001267981333426\n",
      "train loss:2.2985277458046003\n",
      "train loss:2.3061678881767804\n",
      "train loss:2.3139852492871342\n",
      "train loss:2.302187313452462\n",
      "train loss:2.30431716204835\n",
      "train loss:2.297118996038467\n",
      "train loss:2.2980429105082307\n",
      "train loss:2.3081560306010394\n",
      "train loss:2.2979463376498646\n",
      "train loss:2.2957205634136133\n",
      "train loss:2.294347555975833\n",
      "train loss:2.303253978670105\n",
      "train loss:2.30406508105165\n",
      "train loss:2.299147641620304\n",
      "train loss:2.3075832548486623\n",
      "train loss:2.3007152503789805\n",
      "train loss:2.3022691310693264\n",
      "train loss:2.304923058830735\n",
      "train loss:2.301890083356631\n",
      "train loss:2.3022142354345627\n",
      "train loss:2.303798737703818\n",
      "train loss:2.312236345205089\n",
      "train loss:2.304231871708954\n",
      "train loss:2.305864484165619\n",
      "train loss:2.297869582676816\n",
      "train loss:2.303275371890653\n",
      "train loss:2.300444730480463\n",
      "train loss:2.3018405588048525\n",
      "train loss:2.2985765869896913\n",
      "train loss:2.2916346690494676\n",
      "train loss:2.3049966394317813\n",
      "train loss:2.299376232552705\n",
      "train loss:2.297671367441649\n",
      "=== epoch:30, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3020733198528283\n",
      "train loss:2.3047305634425443\n",
      "train loss:2.303902632786384\n",
      "train loss:2.3072320522765835\n",
      "train loss:2.304536057731437\n",
      "train loss:2.3060129092023467\n",
      "train loss:2.301851612284501\n",
      "train loss:2.3062060512354563\n",
      "train loss:2.2978773827460413\n",
      "train loss:2.3037628242832384\n",
      "train loss:2.312093745148134\n",
      "train loss:2.2983428768263883\n",
      "train loss:2.3069697265484272\n",
      "train loss:2.3057042359910707\n",
      "train loss:2.2981588703117\n",
      "train loss:2.3016872283669803\n",
      "train loss:2.3012150063399175\n",
      "train loss:2.306606686031254\n",
      "train loss:2.3020217458536094\n",
      "train loss:2.294266678132841\n",
      "train loss:2.2986346252221344\n",
      "train loss:2.2990422110473476\n",
      "train loss:2.3118496175325727\n",
      "train loss:2.3071779474834266\n",
      "train loss:2.3081996861435825\n",
      "train loss:2.3032733145644424\n",
      "train loss:2.300879289276311\n",
      "train loss:2.302480918475201\n",
      "train loss:2.2931997730205698\n",
      "train loss:2.306984354163006\n",
      "train loss:2.3006169917365424\n",
      "train loss:2.3015615364434376\n",
      "train loss:2.2895579460572875\n",
      "train loss:2.2986422128825126\n",
      "train loss:2.3022562425235336\n",
      "train loss:2.305329184137284\n",
      "train loss:2.313552169242466\n",
      "train loss:2.3016167872725135\n",
      "train loss:2.296674779299453\n",
      "train loss:2.2963689562941476\n",
      "train loss:2.2991286169047553\n",
      "train loss:2.300843279208237\n",
      "train loss:2.300539924034524\n",
      "train loss:2.303313543815205\n",
      "train loss:2.306422008534837\n",
      "train loss:2.302086974019421\n",
      "train loss:2.305518996453664\n",
      "train loss:2.3010149090081415\n",
      "train loss:2.3046233847249025\n",
      "train loss:2.3055273550390085\n",
      "train loss:2.3005195363162256\n",
      "train loss:2.303623971179842\n",
      "train loss:2.298795124153826\n",
      "train loss:2.3123625717768626\n",
      "train loss:2.305125133398714\n",
      "train loss:2.3061045611240294\n",
      "train loss:2.301225956176779\n",
      "train loss:2.3021801228002925\n",
      "train loss:2.302103888147755\n",
      "train loss:2.296729091143963\n",
      "train loss:2.2982854255893956\n",
      "train loss:2.2928849612535247\n",
      "train loss:2.304548070460804\n",
      "train loss:2.2972543698988557\n",
      "train loss:2.30529176225515\n",
      "train loss:2.291403169214549\n",
      "train loss:2.3024821368680373\n",
      "train loss:2.3047911548066065\n",
      "train loss:2.3037139445018884\n",
      "train loss:2.313806899902865\n",
      "train loss:2.306758114365764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3003014656228156\n",
      "train loss:2.3027047551360904\n",
      "train loss:2.3011481903846356\n",
      "train loss:2.3040975092349605\n",
      "train loss:2.3069995305103927\n",
      "train loss:2.310858604020663\n",
      "train loss:2.307454515363339\n",
      "train loss:2.301669530882661\n",
      "train loss:2.290855793232074\n",
      "train loss:2.2985926596519226\n",
      "train loss:2.307584010652694\n",
      "train loss:2.303378973864335\n",
      "train loss:2.2994413673577894\n",
      "train loss:2.3004436884643584\n",
      "train loss:2.293464975180541\n",
      "train loss:2.297982554602406\n",
      "train loss:2.29915431949369\n",
      "train loss:2.306883899382991\n",
      "train loss:2.3060638890105336\n",
      "train loss:2.2927413078298\n",
      "train loss:2.3038803846843696\n",
      "train loss:2.29550342402691\n",
      "train loss:2.3048732929494498\n",
      "train loss:2.302653844537707\n",
      "train loss:2.2987697160285894\n",
      "train loss:2.295469647333353\n",
      "train loss:2.302029595008219\n",
      "train loss:2.301002041892938\n",
      "train loss:2.3001169191429534\n",
      "train loss:2.303158069604441\n",
      "train loss:2.2937888751529885\n",
      "train loss:2.29502320401049\n",
      "train loss:2.305487345825754\n",
      "train loss:2.3051667756827072\n",
      "train loss:2.295665203703428\n",
      "train loss:2.301423910865848\n",
      "train loss:2.303873130814538\n",
      "train loss:2.2991969706849735\n",
      "train loss:2.3031044060797106\n",
      "train loss:2.30046265692903\n",
      "train loss:2.304459241552687\n",
      "train loss:2.3036262052838397\n",
      "train loss:2.303826037496463\n",
      "train loss:2.304268995914933\n",
      "train loss:2.290658415710469\n",
      "train loss:2.2975205308196323\n",
      "train loss:2.2974148778688273\n",
      "train loss:2.307522776622744\n",
      "train loss:2.3030888243777623\n",
      "train loss:2.301210042529072\n",
      "train loss:2.2993254068339413\n",
      "train loss:2.3013818844780314\n",
      "train loss:2.3039715048482363\n",
      "train loss:2.295916674269856\n",
      "train loss:2.302417475792441\n",
      "train loss:2.2971587727466014\n",
      "train loss:2.295350382090753\n",
      "train loss:2.296890646707166\n",
      "train loss:2.2994796666102624\n",
      "train loss:2.2981848523723225\n",
      "train loss:2.293956662743519\n",
      "train loss:2.3082445879016626\n",
      "train loss:2.302743251723982\n",
      "train loss:2.2940881339646544\n",
      "train loss:2.2931830862630727\n",
      "train loss:2.3089759128145197\n",
      "train loss:2.2995264766444983\n",
      "train loss:2.3001466014107153\n",
      "train loss:2.30570135464875\n",
      "train loss:2.2918782757580294\n",
      "train loss:2.3058443801398685\n",
      "train loss:2.3071172580941175\n",
      "train loss:2.303811546018621\n",
      "train loss:2.2979101634545884\n",
      "train loss:2.305905691245911\n",
      "train loss:2.2994485953883683\n",
      "train loss:2.3015068580174995\n",
      "train loss:2.2982128251042866\n",
      "train loss:2.308047092202679\n",
      "train loss:2.3026221371464066\n",
      "train loss:2.2972136746792335\n",
      "train loss:2.2980632490902044\n",
      "train loss:2.3034881592772534\n",
      "train loss:2.297512746029426\n",
      "train loss:2.3032697209406594\n",
      "train loss:2.2962593821236013\n",
      "train loss:2.308378775755831\n",
      "train loss:2.291699964208391\n",
      "train loss:2.3059986800765\n",
      "train loss:2.296645468898848\n",
      "train loss:2.3056184693569026\n",
      "train loss:2.300251725090045\n",
      "train loss:2.3014194756491886\n",
      "train loss:2.304132504418409\n",
      "train loss:2.308761118180004\n",
      "train loss:2.3072244914108544\n",
      "train loss:2.3050769642400715\n",
      "train loss:2.3134128819525923\n",
      "train loss:2.3047422265193847\n",
      "train loss:2.3058151990646514\n",
      "train loss:2.2938786749350406\n",
      "train loss:2.298842230639737\n",
      "train loss:2.300474884711074\n",
      "train loss:2.300797899838021\n",
      "train loss:2.3084634843186005\n",
      "train loss:2.2956164988155816\n",
      "train loss:2.308222925464542\n",
      "train loss:2.304158266762114\n",
      "train loss:2.310763989497823\n",
      "train loss:2.2990712728302602\n",
      "train loss:2.2919696925130153\n",
      "train loss:2.2956269356216272\n",
      "train loss:2.3051916997884034\n",
      "train loss:2.293999916820616\n",
      "train loss:2.298289991318676\n",
      "train loss:2.303551893717877\n",
      "train loss:2.3001617696489376\n",
      "train loss:2.295207075528603\n",
      "train loss:2.30184893160106\n",
      "train loss:2.310440600016111\n",
      "train loss:2.303133494801913\n",
      "train loss:2.3060918221119038\n",
      "train loss:2.30475307799071\n",
      "train loss:2.3032678695548974\n",
      "train loss:2.3065821925126198\n",
      "train loss:2.2983803756499257\n",
      "train loss:2.3015625764283576\n",
      "train loss:2.2997252720747925\n",
      "train loss:2.296320454146682\n",
      "=== epoch:31, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2951457755900164\n",
      "train loss:2.30835598293751\n",
      "train loss:2.293939362156785\n",
      "train loss:2.294483363531317\n",
      "train loss:2.301713230291615\n",
      "train loss:2.302924567910564\n",
      "train loss:2.2973360251761386\n",
      "train loss:2.305436723880071\n",
      "train loss:2.3031370202138013\n",
      "train loss:2.3015367567010108\n",
      "train loss:2.303335663310252\n",
      "train loss:2.2986538536432866\n",
      "train loss:2.300328449485452\n",
      "train loss:2.2935248914922783\n",
      "train loss:2.2998169450541712\n",
      "train loss:2.301068283012377\n",
      "train loss:2.307798477126262\n",
      "train loss:2.301070498720155\n",
      "train loss:2.2969360847792544\n",
      "train loss:2.2943551167263405\n",
      "train loss:2.2995056583295783\n",
      "train loss:2.2992718277820097\n",
      "train loss:2.303758092905651\n",
      "train loss:2.2986269884256596\n",
      "train loss:2.303916499199004\n",
      "train loss:2.306849340287191\n",
      "train loss:2.2990816225561153\n",
      "train loss:2.2967058751318303\n",
      "train loss:2.306932865210169\n",
      "train loss:2.3012849035067253\n",
      "train loss:2.3094497870777633\n",
      "train loss:2.302198522526008\n",
      "train loss:2.297175869808611\n",
      "train loss:2.3067714032264792\n",
      "train loss:2.2874091034287196\n",
      "train loss:2.2964508053349215\n",
      "train loss:2.2964158999482898\n",
      "train loss:2.294120832950624\n",
      "train loss:2.3034573286847553\n",
      "train loss:2.302624681960854\n",
      "train loss:2.297874181739978\n",
      "train loss:2.3048347307080275\n",
      "train loss:2.306215009519282\n",
      "train loss:2.3107415633209283\n",
      "train loss:2.3084276834702355\n",
      "train loss:2.307450402357602\n",
      "train loss:2.3019832614666575\n",
      "train loss:2.2990867102074866\n",
      "train loss:2.2984398784171853\n",
      "train loss:2.295389424510135\n",
      "train loss:2.304582905179059\n",
      "train loss:2.302888373435791\n",
      "train loss:2.3032534972481242\n",
      "train loss:2.306502960451073\n",
      "train loss:2.30841211837177\n",
      "train loss:2.3025766137581236\n",
      "train loss:2.3008552063301373\n",
      "train loss:2.2969531885179473\n",
      "train loss:2.3088817951110925\n",
      "train loss:2.305267993181929\n",
      "train loss:2.3073412941565445\n",
      "train loss:2.3053910729449387\n",
      "train loss:2.301305674266889\n",
      "train loss:2.288229114809348\n",
      "train loss:2.296755926670257\n",
      "train loss:2.298442880519707\n",
      "train loss:2.30240457955053\n",
      "train loss:2.314102485255532\n",
      "train loss:2.29905882690443\n",
      "train loss:2.300483496534424\n",
      "train loss:2.3033731115065494\n",
      "train loss:2.300340942331766\n",
      "train loss:2.3027393599023265\n",
      "train loss:2.3034508947469177\n",
      "train loss:2.3018358583983036\n",
      "train loss:2.3046722302186042\n",
      "train loss:2.3023842887774215\n",
      "train loss:2.3054705111360656\n",
      "train loss:2.300730645379085\n",
      "train loss:2.2970691657393707\n",
      "train loss:2.2937331928774847\n",
      "train loss:2.2978475540279355\n",
      "train loss:2.3010083878147825\n",
      "train loss:2.2959569925445225\n",
      "train loss:2.294269298663341\n",
      "train loss:2.2978961893977083\n",
      "train loss:2.301232676881153\n",
      "train loss:2.3024034540665426\n",
      "train loss:2.306811422744265\n",
      "train loss:2.300749220444927\n",
      "train loss:2.3010894101376422\n",
      "train loss:2.3062848665880233\n",
      "train loss:2.310935518914306\n",
      "train loss:2.3015721928341057\n",
      "train loss:2.300474851879933\n",
      "train loss:2.2999483395675595\n",
      "train loss:2.307435340970494\n",
      "train loss:2.311242240828169\n",
      "train loss:2.3025890416453803\n",
      "train loss:2.3048745169874296\n",
      "train loss:2.302585056214486\n",
      "train loss:2.3085465228578945\n",
      "train loss:2.3054926087159338\n",
      "train loss:2.303621443615265\n",
      "train loss:2.3004190229085033\n",
      "train loss:2.302410376038699\n",
      "train loss:2.304347975347329\n",
      "train loss:2.304669701516189\n",
      "train loss:2.302883263967203\n",
      "train loss:2.30311896538837\n",
      "train loss:2.2994719554742353\n",
      "train loss:2.299544527607871\n",
      "train loss:2.307855375923465\n",
      "train loss:2.2984416085394788\n",
      "train loss:2.2990614020818256\n",
      "train loss:2.2983326164717046\n",
      "train loss:2.307618156460359\n",
      "train loss:2.2990577189983115\n",
      "train loss:2.2945637877995497\n",
      "train loss:2.299545016072567\n",
      "train loss:2.30798641480664\n",
      "train loss:2.3079409995444244\n",
      "train loss:2.303729587656598\n",
      "train loss:2.3016210680219675\n",
      "train loss:2.3036974688502156\n",
      "train loss:2.3030820689221447\n",
      "train loss:2.3001196695428603\n",
      "train loss:2.3038988101313382\n",
      "train loss:2.3052923131379965\n",
      "train loss:2.301061864671586\n",
      "train loss:2.305341066883415\n",
      "train loss:2.30730559813564\n",
      "train loss:2.30810441172485\n",
      "train loss:2.2978268280747915\n",
      "train loss:2.299416120412044\n",
      "train loss:2.3001219089808433\n",
      "train loss:2.3162419555000735\n",
      "train loss:2.302545603018665\n",
      "train loss:2.304181120194324\n",
      "train loss:2.305155904429759\n",
      "train loss:2.3066622246126074\n",
      "train loss:2.2911771549388167\n",
      "train loss:2.2982315925738703\n",
      "train loss:2.2961606212671\n",
      "train loss:2.302858619188234\n",
      "train loss:2.3018729248719025\n",
      "train loss:2.304341957784359\n",
      "train loss:2.2981738303508488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3045246693229684\n",
      "train loss:2.2994061567219464\n",
      "train loss:2.3131728098589894\n",
      "train loss:2.3025020695655245\n",
      "train loss:2.3014096994342346\n",
      "train loss:2.2990620509767528\n",
      "train loss:2.3020250500948105\n",
      "train loss:2.304273570100749\n",
      "train loss:2.306465814930088\n",
      "train loss:2.2974807372489523\n",
      "train loss:2.3073826992442443\n",
      "train loss:2.3045014439757288\n",
      "train loss:2.3010993923988985\n",
      "train loss:2.301819655512102\n",
      "train loss:2.305229092573021\n",
      "train loss:2.304553163643033\n",
      "train loss:2.2983469523490205\n",
      "train loss:2.2977298555514616\n",
      "train loss:2.296000440697333\n",
      "train loss:2.2951699068969282\n",
      "train loss:2.2977432099136923\n",
      "train loss:2.3026212297034894\n",
      "train loss:2.2962598800513816\n",
      "train loss:2.306329343419942\n",
      "train loss:2.3054879285531897\n",
      "train loss:2.2974080384455378\n",
      "train loss:2.2976916722428116\n",
      "train loss:2.302027398973557\n",
      "train loss:2.296147211909284\n",
      "train loss:2.300617121846143\n",
      "train loss:2.2954000142590756\n",
      "train loss:2.2973366698814095\n",
      "train loss:2.3086307254057052\n",
      "train loss:2.297501018379776\n",
      "train loss:2.3056702685021526\n",
      "train loss:2.28861166040182\n",
      "train loss:2.2982339007621997\n",
      "train loss:2.3060490340736037\n",
      "train loss:2.3007278328613476\n",
      "train loss:2.304176451441298\n",
      "train loss:2.2983760868495273\n",
      "train loss:2.2987308109250963\n",
      "train loss:2.308768970680675\n",
      "train loss:2.298170147926209\n",
      "train loss:2.3077281856104572\n",
      "train loss:2.3073197370471092\n",
      "train loss:2.29828675309926\n",
      "train loss:2.3017554798516047\n",
      "train loss:2.3029413976320643\n",
      "train loss:2.3042620330469856\n",
      "train loss:2.3036237654823366\n",
      "train loss:2.297612383589303\n",
      "=== epoch:32, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2990469216887295\n",
      "train loss:2.2947845143660928\n",
      "train loss:2.299927188261807\n",
      "train loss:2.3039950361722163\n",
      "train loss:2.2993001198873984\n",
      "train loss:2.296623655720536\n",
      "train loss:2.2949431913196667\n",
      "train loss:2.3030862736797437\n",
      "train loss:2.297768628287574\n",
      "train loss:2.3028375324895842\n",
      "train loss:2.301053888115458\n",
      "train loss:2.2944872266919174\n",
      "train loss:2.300686537345588\n",
      "train loss:2.297226496206328\n",
      "train loss:2.3016836271894596\n",
      "train loss:2.3054012839060536\n",
      "train loss:2.306607461560164\n",
      "train loss:2.3036523329559064\n",
      "train loss:2.3031640005966483\n",
      "train loss:2.305549801404969\n",
      "train loss:2.2952825604926788\n",
      "train loss:2.2987462303064765\n",
      "train loss:2.306083309894696\n",
      "train loss:2.29169969138478\n",
      "train loss:2.3032975578832455\n",
      "train loss:2.3005316649150744\n",
      "train loss:2.302896360735415\n",
      "train loss:2.2948423414568024\n",
      "train loss:2.3096855502968636\n",
      "train loss:2.2962411406438377\n",
      "train loss:2.290390716683455\n",
      "train loss:2.30886072330674\n",
      "train loss:2.305330233780676\n",
      "train loss:2.3105194115347243\n",
      "train loss:2.3001995090126903\n",
      "train loss:2.3040020867192927\n",
      "train loss:2.298156240642889\n",
      "train loss:2.3033344084644614\n",
      "train loss:2.303211012570595\n",
      "train loss:2.3051842537925804\n",
      "train loss:2.3047400185994684\n",
      "train loss:2.307990355200185\n",
      "train loss:2.303774647250052\n",
      "train loss:2.3032569025028553\n",
      "train loss:2.296755854829573\n",
      "train loss:2.293853778624497\n",
      "train loss:2.290406189480384\n",
      "train loss:2.303894388232647\n",
      "train loss:2.298445625276266\n",
      "train loss:2.305305606471888\n",
      "train loss:2.2975465164865287\n",
      "train loss:2.2987867566224605\n",
      "train loss:2.2974724987322337\n",
      "train loss:2.3007924783946976\n",
      "train loss:2.2961525967180094\n",
      "train loss:2.2999306426376753\n",
      "train loss:2.298888656881226\n",
      "train loss:2.3030481037222326\n",
      "train loss:2.30931075673392\n",
      "train loss:2.3035738249007447\n",
      "train loss:2.3048662395120454\n",
      "train loss:2.307925249662433\n",
      "train loss:2.3108766623057697\n",
      "train loss:2.300587054214372\n",
      "train loss:2.300888679307371\n",
      "train loss:2.305828107947464\n",
      "train loss:2.297696562315846\n",
      "train loss:2.302757331007423\n",
      "train loss:2.301710814121218\n",
      "train loss:2.2998074443464067\n",
      "train loss:2.3057695939356\n",
      "train loss:2.3080151864021614\n",
      "train loss:2.3018453849788827\n",
      "train loss:2.310382214446902\n",
      "train loss:2.3064773400403507\n",
      "train loss:2.296301257502442\n",
      "train loss:2.308442971894534\n",
      "train loss:2.305501793369064\n",
      "train loss:2.3041016397952507\n",
      "train loss:2.2955158305454155\n",
      "train loss:2.301833750668065\n",
      "train loss:2.2975727049922767\n",
      "train loss:2.3041761844226802\n",
      "train loss:2.3032591582693427\n",
      "train loss:2.3045738813140444\n",
      "train loss:2.3083243331985983\n",
      "train loss:2.2993721108792378\n",
      "train loss:2.307677972884091\n",
      "train loss:2.3082358666141487\n",
      "train loss:2.2979914000133754\n",
      "train loss:2.2997740529067805\n",
      "train loss:2.302438319405968\n",
      "train loss:2.301549275560124\n",
      "train loss:2.3088214091389108\n",
      "train loss:2.300087712918837\n",
      "train loss:2.294986367512624\n",
      "train loss:2.294101540880133\n",
      "train loss:2.3125139143163898\n",
      "train loss:2.3052898000053985\n",
      "train loss:2.305309777880927\n",
      "train loss:2.3003670385816637\n",
      "train loss:2.3001288616710074\n",
      "train loss:2.302851786067346\n",
      "train loss:2.311989594308865\n",
      "train loss:2.292064055274302\n",
      "train loss:2.306365535370459\n",
      "train loss:2.30692119973799\n",
      "train loss:2.306365440435716\n",
      "train loss:2.3024282292521514\n",
      "train loss:2.3013994197682353\n",
      "train loss:2.2987113141093083\n",
      "train loss:2.3044661954090997\n",
      "train loss:2.302440037534526\n",
      "train loss:2.3114254400240224\n",
      "train loss:2.3094107710311294\n",
      "train loss:2.299038710593812\n",
      "train loss:2.3075844401671204\n",
      "train loss:2.298671502813604\n",
      "train loss:2.3074564404624005\n",
      "train loss:2.310585256973649\n",
      "train loss:2.2921449728862924\n",
      "train loss:2.3100596826355164\n",
      "train loss:2.2946744161087134\n",
      "train loss:2.2959818522962294\n",
      "train loss:2.305651539655751\n",
      "train loss:2.306992029758994\n",
      "train loss:2.300941327694381\n",
      "train loss:2.2972644773273294\n",
      "train loss:2.300919358732731\n",
      "train loss:2.2982813448870294\n",
      "train loss:2.301950170948086\n",
      "train loss:2.3071380249499303\n",
      "train loss:2.3005702158738264\n",
      "train loss:2.3012463283862004\n",
      "train loss:2.3066086664020817\n",
      "train loss:2.2986403498088777\n",
      "train loss:2.298424737160122\n",
      "train loss:2.3001569680932015\n",
      "train loss:2.306023219965141\n",
      "train loss:2.294916438399919\n",
      "train loss:2.303793711923808\n",
      "train loss:2.301315521657547\n",
      "train loss:2.2941217005951664\n",
      "train loss:2.299894060275058\n",
      "train loss:2.305053897257075\n",
      "train loss:2.3055090895737598\n",
      "train loss:2.304913998985565\n",
      "train loss:2.3071850073253755\n",
      "train loss:2.3048104401406633\n",
      "train loss:2.3032453375383812\n",
      "train loss:2.3092089595857703\n",
      "train loss:2.301546900402033\n",
      "train loss:2.2979636139638546\n",
      "train loss:2.3074906965746442\n",
      "train loss:2.2981809681740213\n",
      "train loss:2.3004223635390635\n",
      "train loss:2.299194432477782\n",
      "train loss:2.2993720389400423\n",
      "train loss:2.302606543719593\n",
      "train loss:2.3137340807987528\n",
      "train loss:2.2985786474287226\n",
      "train loss:2.3086401012460427\n",
      "train loss:2.3055401120374355\n",
      "train loss:2.3030231754870036\n",
      "train loss:2.3029058332433276\n",
      "train loss:2.303501937847777\n",
      "train loss:2.3028204409886532\n",
      "train loss:2.2995162583332025\n",
      "train loss:2.309351097051878\n",
      "train loss:2.3086194667586843\n",
      "train loss:2.2911376351827477\n",
      "train loss:2.303725778536969\n",
      "train loss:2.2923309140221675\n",
      "train loss:2.3077224881129275\n",
      "train loss:2.300264244504083\n",
      "train loss:2.301722947970394\n",
      "train loss:2.300577366275244\n",
      "train loss:2.301070001484127\n",
      "train loss:2.299610610178425\n",
      "train loss:2.303635349845528\n",
      "train loss:2.3066983696810253\n",
      "train loss:2.3054130162454167\n",
      "train loss:2.3067172774003146\n",
      "train loss:2.301710165396469\n",
      "train loss:2.304223317315932\n",
      "train loss:2.303686914999283\n",
      "train loss:2.3012325345936615\n",
      "train loss:2.3047576185499925\n",
      "train loss:2.303699778413443\n",
      "train loss:2.2958252834058435\n",
      "train loss:2.300568504787311\n",
      "train loss:2.3008323617924793\n",
      "train loss:2.2937899302956373\n",
      "train loss:2.3051738443252066\n",
      "train loss:2.301234131586126\n",
      "train loss:2.295359933592623\n",
      "train loss:2.3047289465657097\n",
      "train loss:2.302612919577394\n",
      "train loss:2.31435602151619\n",
      "train loss:2.295153629801661\n",
      "=== epoch:33, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3064194932606856\n",
      "train loss:2.2950008198468237\n",
      "train loss:2.2926896333808506\n",
      "train loss:2.3024969987377863\n",
      "train loss:2.3002317312579135\n",
      "train loss:2.2977666916999984\n",
      "train loss:2.3003788075156755\n",
      "train loss:2.2991170097161495\n",
      "train loss:2.297312612465245\n",
      "train loss:2.303098396977705\n",
      "train loss:2.302066893139662\n",
      "train loss:2.301645227433059\n",
      "train loss:2.3039342617724032\n",
      "train loss:2.295984545731733\n",
      "train loss:2.300508442862721\n",
      "train loss:2.3000369857608653\n",
      "train loss:2.303570729579347\n",
      "train loss:2.302932535368514\n",
      "train loss:2.3006833530918573\n",
      "train loss:2.3050408740009676\n",
      "train loss:2.3042163754465803\n",
      "train loss:2.3033261110528818\n",
      "train loss:2.3012430961192476\n",
      "train loss:2.2980955260369726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3038243315244102\n",
      "train loss:2.3028377394814723\n",
      "train loss:2.3038785535314625\n",
      "train loss:2.301194414639023\n",
      "train loss:2.3027738148157657\n",
      "train loss:2.302956218617061\n",
      "train loss:2.3074145363258527\n",
      "train loss:2.3017968625435317\n",
      "train loss:2.300069250378737\n",
      "train loss:2.3025861231623073\n",
      "train loss:2.3047417179638057\n",
      "train loss:2.298163109107248\n",
      "train loss:2.306964026897933\n",
      "train loss:2.3085145201049153\n",
      "train loss:2.3031517085445934\n",
      "train loss:2.294003888647709\n",
      "train loss:2.2983816627485383\n",
      "train loss:2.2968535867003688\n",
      "train loss:2.301332571152201\n",
      "train loss:2.2987726832585795\n",
      "train loss:2.303138121497356\n",
      "train loss:2.3100887330138633\n",
      "train loss:2.299834488807878\n",
      "train loss:2.302672235401986\n",
      "train loss:2.3043929729533654\n",
      "train loss:2.302639093497215\n",
      "train loss:2.2979708308965017\n",
      "train loss:2.3030204554634786\n",
      "train loss:2.3068305332331076\n",
      "train loss:2.297619327451962\n",
      "train loss:2.3009865366536717\n",
      "train loss:2.30123926755415\n",
      "train loss:2.302422257724599\n",
      "train loss:2.3023533458359093\n",
      "train loss:2.2979637503091146\n",
      "train loss:2.3000859761625088\n",
      "train loss:2.2974911714371027\n",
      "train loss:2.301570728278185\n",
      "train loss:2.2875118097970506\n",
      "train loss:2.2968263878086046\n",
      "train loss:2.3000770903787586\n",
      "train loss:2.2977817592533434\n",
      "train loss:2.2995404211958044\n",
      "train loss:2.302218200804906\n",
      "train loss:2.309226113872615\n",
      "train loss:2.306221030936599\n",
      "train loss:2.3017654493765773\n",
      "train loss:2.2995679703019785\n",
      "train loss:2.2992346071738856\n",
      "train loss:2.303062413244781\n",
      "train loss:2.3031030389503306\n",
      "train loss:2.297729848985754\n",
      "train loss:2.300247263012015\n",
      "train loss:2.3003759357420317\n",
      "train loss:2.3059026512614094\n",
      "train loss:2.2930482600933377\n",
      "train loss:2.3039679641152437\n",
      "train loss:2.2992464232988303\n",
      "train loss:2.3040448896474084\n",
      "train loss:2.2976547685531505\n",
      "train loss:2.3048471093722713\n",
      "train loss:2.3072609376295192\n",
      "train loss:2.295773248871125\n",
      "train loss:2.3003019835340845\n",
      "train loss:2.2992687595239802\n",
      "train loss:2.303162796612863\n",
      "train loss:2.298904803489053\n",
      "train loss:2.304392842108185\n",
      "train loss:2.3038607613863413\n",
      "train loss:2.3046786199498084\n",
      "train loss:2.306100755066679\n",
      "train loss:2.3077047162643702\n",
      "train loss:2.3072929293039617\n",
      "train loss:2.3101723753745453\n",
      "train loss:2.3036969725326153\n",
      "train loss:2.2942126510474465\n",
      "train loss:2.2995703452956926\n",
      "train loss:2.3034098087231443\n",
      "train loss:2.2958958559111178\n",
      "train loss:2.294635079799476\n",
      "train loss:2.2996330635151923\n",
      "train loss:2.299867809835387\n",
      "train loss:2.303527400372258\n",
      "train loss:2.2970685852707615\n",
      "train loss:2.3020561963856374\n",
      "train loss:2.306379753529831\n",
      "train loss:2.304007829966089\n",
      "train loss:2.302352029434628\n",
      "train loss:2.308907837190858\n",
      "train loss:2.301461345556665\n",
      "train loss:2.3012848060962536\n",
      "train loss:2.299293701557362\n",
      "train loss:2.3065977626105845\n",
      "train loss:2.297733597806288\n",
      "train loss:2.3001766229829603\n",
      "train loss:2.2982323577317136\n",
      "train loss:2.3084101184703756\n",
      "train loss:2.3029673467185305\n",
      "train loss:2.3036997376201596\n",
      "train loss:2.3031569201370927\n",
      "train loss:2.3010611089280952\n",
      "train loss:2.3025268373878416\n",
      "train loss:2.307845429559434\n",
      "train loss:2.3066215523277176\n",
      "train loss:2.305547279959675\n",
      "train loss:2.2974358946608913\n",
      "train loss:2.3086602297608185\n",
      "train loss:2.2961602471113123\n",
      "train loss:2.2956175022601673\n",
      "train loss:2.2994026157752856\n",
      "train loss:2.2984082150150287\n",
      "train loss:2.2970658336725216\n",
      "train loss:2.299730671464819\n",
      "train loss:2.2994612809832846\n",
      "train loss:2.2999131939775164\n",
      "train loss:2.301586808466177\n",
      "train loss:2.3030282771448505\n",
      "train loss:2.3018835708312575\n",
      "train loss:2.2993913235297843\n",
      "train loss:2.298175297453778\n",
      "train loss:2.300538089371086\n",
      "train loss:2.3008975085083683\n",
      "train loss:2.2986480905829882\n",
      "train loss:2.3087765009112413\n",
      "train loss:2.2986133405363893\n",
      "train loss:2.29917928609349\n",
      "train loss:2.305498301502356\n",
      "train loss:2.304883475757039\n",
      "train loss:2.296600920973997\n",
      "train loss:2.3054467734744293\n",
      "train loss:2.2969778106056875\n",
      "train loss:2.2877755501057853\n",
      "train loss:2.293038129380567\n",
      "train loss:2.2983545802326644\n",
      "train loss:2.3015411596588775\n",
      "train loss:2.303055369515729\n",
      "train loss:2.3048644162598064\n",
      "train loss:2.296766057431605\n",
      "train loss:2.3085139667979555\n",
      "train loss:2.306661873183643\n",
      "train loss:2.295958337867525\n",
      "train loss:2.301332967290617\n",
      "train loss:2.303333080174029\n",
      "train loss:2.3010753715332064\n",
      "train loss:2.3027599510156858\n",
      "train loss:2.2940962043045823\n",
      "train loss:2.2908786828347747\n",
      "train loss:2.3002461669118244\n",
      "train loss:2.2980856452861707\n",
      "train loss:2.302054009428954\n",
      "train loss:2.307324964267782\n",
      "train loss:2.299678071320919\n",
      "train loss:2.3018958622713552\n",
      "train loss:2.2999686073170587\n",
      "train loss:2.300201561818766\n",
      "train loss:2.29951210251767\n",
      "train loss:2.2991837684128518\n",
      "train loss:2.294972101652398\n",
      "train loss:2.3068996007849267\n",
      "train loss:2.2915989797196716\n",
      "train loss:2.3035061028494153\n",
      "train loss:2.3016617477626777\n",
      "train loss:2.3004600974080867\n",
      "train loss:2.296228523844736\n",
      "train loss:2.3089690634656392\n",
      "train loss:2.3008763484061077\n",
      "train loss:2.2933886611173353\n",
      "train loss:2.3021267055540546\n",
      "train loss:2.2917019173072695\n",
      "train loss:2.301294790879091\n",
      "train loss:2.3059376791241437\n",
      "train loss:2.30323885945984\n",
      "train loss:2.295201271565853\n",
      "train loss:2.299358057658617\n",
      "train loss:2.306867242791562\n",
      "train loss:2.306322548598952\n",
      "=== epoch:34, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3047881555769902\n",
      "train loss:2.300528517359624\n",
      "train loss:2.2926995154548733\n",
      "train loss:2.2982736357580498\n",
      "train loss:2.305651347942043\n",
      "train loss:2.3004651199837416\n",
      "train loss:2.3030693210402897\n",
      "train loss:2.3012903747552786\n",
      "train loss:2.305090241443613\n",
      "train loss:2.299500662686101\n",
      "train loss:2.3082847368917268\n",
      "train loss:2.304864388988801\n",
      "train loss:2.29852011500007\n",
      "train loss:2.3037057167983783\n",
      "train loss:2.3039305667559113\n",
      "train loss:2.301761287023522\n",
      "train loss:2.3048171985500963\n",
      "train loss:2.2977182506141265\n",
      "train loss:2.2934954082537167\n",
      "train loss:2.300593527717578\n",
      "train loss:2.2986276852483782\n",
      "train loss:2.302530111201783\n",
      "train loss:2.311412015451088\n",
      "train loss:2.306977157568049\n",
      "train loss:2.295812871273028\n",
      "train loss:2.30162765602494\n",
      "train loss:2.3022607538546307\n",
      "train loss:2.301751785857431\n",
      "train loss:2.2962387308769645\n",
      "train loss:2.300185558331589\n",
      "train loss:2.304035953236521\n",
      "train loss:2.31188290905029\n",
      "train loss:2.3013697711463768\n",
      "train loss:2.300176358927579\n",
      "train loss:2.301075983307395\n",
      "train loss:2.295116802741126\n",
      "train loss:2.309116010860685\n",
      "train loss:2.3019424421350627\n",
      "train loss:2.2981779378288016\n",
      "train loss:2.3066715577627503\n",
      "train loss:2.290130849884705\n",
      "train loss:2.301583630632636\n",
      "train loss:2.2980451149220387\n",
      "train loss:2.3062234465290072\n",
      "train loss:2.300936280062858\n",
      "train loss:2.300400286138291\n",
      "train loss:2.298948421544387\n",
      "train loss:2.304121637752081\n",
      "train loss:2.303199915623554\n",
      "train loss:2.2879710310542984\n",
      "train loss:2.3072120407124816\n",
      "train loss:2.3047588097314873\n",
      "train loss:2.3060200346465094\n",
      "train loss:2.3001665048649116\n",
      "train loss:2.3008961523264606\n",
      "train loss:2.3044592941471382\n",
      "train loss:2.29296224535712\n",
      "train loss:2.2992281290855376\n",
      "train loss:2.30525453715903\n",
      "train loss:2.2981543766828145\n",
      "train loss:2.3077915212622875\n",
      "train loss:2.303850597060048\n",
      "train loss:2.2970105153365963\n",
      "train loss:2.300193124349523\n",
      "train loss:2.305056754257313\n",
      "train loss:2.293178479668859\n",
      "train loss:2.301117750174856\n",
      "train loss:2.305520767243647\n",
      "train loss:2.296644681826113\n",
      "train loss:2.295898003024713\n",
      "train loss:2.297355117982187\n",
      "train loss:2.3055451490263437\n",
      "train loss:2.3101660357831646\n",
      "train loss:2.3087683839937236\n",
      "train loss:2.3044898152131488\n",
      "train loss:2.2963395987515733\n",
      "train loss:2.297372659854398\n",
      "train loss:2.3024210809787693\n",
      "train loss:2.300984742981421\n",
      "train loss:2.3021949576197684\n",
      "train loss:2.3015128862372416\n",
      "train loss:2.303992730875742\n",
      "train loss:2.3076998563969493\n",
      "train loss:2.3000782118404364\n",
      "train loss:2.300050707229281\n",
      "train loss:2.304210043510617\n",
      "train loss:2.3029001592749725\n",
      "train loss:2.3041081240645855\n",
      "train loss:2.2955422090206206\n",
      "train loss:2.3063146559811116\n",
      "train loss:2.3042321306390168\n",
      "train loss:2.3034311740831206\n",
      "train loss:2.302845381567767\n",
      "train loss:2.2986816732470228\n",
      "train loss:2.29962832875997\n",
      "train loss:2.3044745355413228\n",
      "train loss:2.3051398604150553\n",
      "train loss:2.302409663254284\n",
      "train loss:2.3007947186869293\n",
      "train loss:2.304656155931878\n",
      "train loss:2.3036862339289175\n",
      "train loss:2.2992527465818178\n",
      "train loss:2.306432160274092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299171741372967\n",
      "train loss:2.2999914568554782\n",
      "train loss:2.3003225102658846\n",
      "train loss:2.308405659288764\n",
      "train loss:2.3009891193429515\n",
      "train loss:2.296181117061631\n",
      "train loss:2.3102602687346625\n",
      "train loss:2.307281454034005\n",
      "train loss:2.3112320868125313\n",
      "train loss:2.3096021152389903\n",
      "train loss:2.309080249646192\n",
      "train loss:2.302862256636187\n",
      "train loss:2.2927688963726456\n",
      "train loss:2.2948047552916\n",
      "train loss:2.2990768865152984\n",
      "train loss:2.3010737228035634\n",
      "train loss:2.3006060992323\n",
      "train loss:2.3036608576266966\n",
      "train loss:2.2967089356903125\n",
      "train loss:2.2910043970332143\n",
      "train loss:2.3008883280495342\n",
      "train loss:2.2981285550196207\n",
      "train loss:2.289461500334299\n",
      "train loss:2.302892347587361\n",
      "train loss:2.3000691467048653\n",
      "train loss:2.297678228900098\n",
      "train loss:2.3046256836710954\n",
      "train loss:2.300286419371727\n",
      "train loss:2.301175041058244\n",
      "train loss:2.3002179211537443\n",
      "train loss:2.300535679749118\n",
      "train loss:2.3016072448955556\n",
      "train loss:2.3005570077111437\n",
      "train loss:2.303307720684358\n",
      "train loss:2.299390028935056\n",
      "train loss:2.2997299912282005\n",
      "train loss:2.297008139106916\n",
      "train loss:2.302289850795287\n",
      "train loss:2.302085804393408\n",
      "train loss:2.289064083957041\n",
      "train loss:2.297862341786619\n",
      "train loss:2.2912389451544883\n",
      "train loss:2.298508268114771\n",
      "train loss:2.305880430212254\n",
      "train loss:2.294465433170018\n",
      "train loss:2.2993603835708436\n",
      "train loss:2.2958758932008254\n",
      "train loss:2.301408758327848\n",
      "train loss:2.306133432969464\n",
      "train loss:2.3051208084736925\n",
      "train loss:2.300188891022407\n",
      "train loss:2.2985386033700874\n",
      "train loss:2.290993584567398\n",
      "train loss:2.29862178753442\n",
      "train loss:2.3037471071829265\n",
      "train loss:2.313001613816139\n",
      "train loss:2.3031312972388496\n",
      "train loss:2.2996489810822096\n",
      "train loss:2.3083718553263\n",
      "train loss:2.307515706158605\n",
      "train loss:2.2956893793667157\n",
      "train loss:2.306893407601923\n",
      "train loss:2.310306411118662\n",
      "train loss:2.300882590701622\n",
      "train loss:2.3013366500679875\n",
      "train loss:2.3047454204551445\n",
      "train loss:2.298776776490059\n",
      "train loss:2.2964341073372805\n",
      "train loss:2.3068047990698433\n",
      "train loss:2.3089808330139068\n",
      "train loss:2.3074489824107105\n",
      "train loss:2.296481875559235\n",
      "train loss:2.295837509147596\n",
      "train loss:2.3059186071999167\n",
      "train loss:2.3023182072990886\n",
      "train loss:2.302710095285241\n",
      "train loss:2.304608763248192\n",
      "train loss:2.301176800842588\n",
      "train loss:2.3089670317188262\n",
      "train loss:2.3006458951982873\n",
      "train loss:2.3104642333095806\n",
      "train loss:2.305090183386795\n",
      "train loss:2.3051627190783064\n",
      "train loss:2.2963936876017454\n",
      "train loss:2.2936785653877414\n",
      "train loss:2.2914980228176374\n",
      "train loss:2.3106546581382528\n",
      "train loss:2.2988290321617253\n",
      "train loss:2.2957806108358896\n",
      "train loss:2.299168395370237\n",
      "train loss:2.3012304048695587\n",
      "train loss:2.301291452731602\n",
      "train loss:2.2985453528828423\n",
      "train loss:2.3017410837235266\n",
      "train loss:2.302358617976464\n",
      "train loss:2.3015315786326425\n",
      "train loss:2.297628684816329\n",
      "=== epoch:35, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3049577661686373\n",
      "train loss:2.2961073157497864\n",
      "train loss:2.2984750436165635\n",
      "train loss:2.302102081248291\n",
      "train loss:2.300542245930637\n",
      "train loss:2.304285807355704\n",
      "train loss:2.3075072623454562\n",
      "train loss:2.3045632976287025\n",
      "train loss:2.304054397352427\n",
      "train loss:2.300671506073794\n",
      "train loss:2.3013212703920085\n",
      "train loss:2.3065746340436393\n",
      "train loss:2.3042764023647075\n",
      "train loss:2.2994270485130275\n",
      "train loss:2.30318427426005\n",
      "train loss:2.303720060551243\n",
      "train loss:2.304008556471379\n",
      "train loss:2.2992176897100514\n",
      "train loss:2.2949555948131573\n",
      "train loss:2.304748523526961\n",
      "train loss:2.3050472846613195\n",
      "train loss:2.3018832460550183\n",
      "train loss:2.2999928841626067\n",
      "train loss:2.2945417523395144\n",
      "train loss:2.2989493800709755\n",
      "train loss:2.2941319906565574\n",
      "train loss:2.301855472991484\n",
      "train loss:2.2993701349351414\n",
      "train loss:2.3005722189282953\n",
      "train loss:2.3025671202804423\n",
      "train loss:2.308180885576784\n",
      "train loss:2.2974464779340917\n",
      "train loss:2.3053926890968253\n",
      "train loss:2.303327856493205\n",
      "train loss:2.2999565860825597\n",
      "train loss:2.3021549162373076\n",
      "train loss:2.300785335767796\n",
      "train loss:2.297163728230045\n",
      "train loss:2.3032984117632953\n",
      "train loss:2.2935929582540235\n",
      "train loss:2.309677133842598\n",
      "train loss:2.2923109426613175\n",
      "train loss:2.29826782779491\n",
      "train loss:2.29866817771715\n",
      "train loss:2.31127575366728\n",
      "train loss:2.306000046627376\n",
      "train loss:2.305557109002865\n",
      "train loss:2.304586577395714\n",
      "train loss:2.2949763952790794\n",
      "train loss:2.296816096767509\n",
      "train loss:2.299664573547894\n",
      "train loss:2.299073568813912\n",
      "train loss:2.300380986625119\n",
      "train loss:2.3035254497664277\n",
      "train loss:2.302369877654426\n",
      "train loss:2.3020196670819555\n",
      "train loss:2.300791759833096\n",
      "train loss:2.298023846547109\n",
      "train loss:2.3061860781894716\n",
      "train loss:2.2988464801314756\n",
      "train loss:2.289565386449477\n",
      "train loss:2.307708991887068\n",
      "train loss:2.2914815480473365\n",
      "train loss:2.301834799279602\n",
      "train loss:2.2953389357607143\n",
      "train loss:2.30500732327552\n",
      "train loss:2.2999999838813014\n",
      "train loss:2.3049125502491874\n",
      "train loss:2.292607732616964\n",
      "train loss:2.3132591090516517\n",
      "train loss:2.3073093967309406\n",
      "train loss:2.30217473995766\n",
      "train loss:2.2982618353697504\n",
      "train loss:2.304539322521612\n",
      "train loss:2.2919187457124535\n",
      "train loss:2.3007056172927416\n",
      "train loss:2.300015477952335\n",
      "train loss:2.3032271880048327\n",
      "train loss:2.3032469587403805\n",
      "train loss:2.293737963447566\n",
      "train loss:2.305608968701223\n",
      "train loss:2.30226757759483\n",
      "train loss:2.294429066625496\n",
      "train loss:2.2937286768471488\n",
      "train loss:2.299889982075151\n",
      "train loss:2.3044554603893452\n",
      "train loss:2.294083522697021\n",
      "train loss:2.3007018711527976\n",
      "train loss:2.294264043088216\n",
      "train loss:2.305664145297981\n",
      "train loss:2.298660875444068\n",
      "train loss:2.3082735163778434\n",
      "train loss:2.2961102047465833\n",
      "train loss:2.3033770722344493\n",
      "train loss:2.3032954542995148\n",
      "train loss:2.3024179832654723\n",
      "train loss:2.3025859133338265\n",
      "train loss:2.3021655497812237\n",
      "train loss:2.299668469946307\n",
      "train loss:2.304142459887324\n",
      "train loss:2.301679511015547\n",
      "train loss:2.301598592663624\n",
      "train loss:2.299854314577659\n",
      "train loss:2.302644646747493\n",
      "train loss:2.304532555093352\n",
      "train loss:2.298330317352456\n",
      "train loss:2.306533562929756\n",
      "train loss:2.299970896748581\n",
      "train loss:2.295355044642058\n",
      "train loss:2.3079694949621055\n",
      "train loss:2.300126078239406\n",
      "train loss:2.297802244682396\n",
      "train loss:2.298977584319798\n",
      "train loss:2.3037697956000645\n",
      "train loss:2.295400437972952\n",
      "train loss:2.2994594227592557\n",
      "train loss:2.2992762249306917\n",
      "train loss:2.296655442832206\n",
      "train loss:2.3108935097866112\n",
      "train loss:2.292559257781814\n",
      "train loss:2.3064099415036097\n",
      "train loss:2.302734448465404\n",
      "train loss:2.3064418769728525\n",
      "train loss:2.304189720247276\n",
      "train loss:2.303558457530549\n",
      "train loss:2.2998287924288086\n",
      "train loss:2.3062697449433087\n",
      "train loss:2.2971838138578935\n",
      "train loss:2.304579291810744\n",
      "train loss:2.3081220667873708\n",
      "train loss:2.2965388482104294\n",
      "train loss:2.3059414714879924\n",
      "train loss:2.291792818365891\n",
      "train loss:2.298473056178952\n",
      "train loss:2.2999607405382476\n",
      "train loss:2.3020435213951504\n",
      "train loss:2.300344282030094\n",
      "train loss:2.304564342624234\n",
      "train loss:2.304996001844508\n",
      "train loss:2.295188543530354\n",
      "train loss:2.296854477181404\n",
      "train loss:2.2992271956573345\n",
      "train loss:2.2954186151468585\n",
      "train loss:2.304501447135311\n",
      "train loss:2.303093010035956\n",
      "train loss:2.3026488909482317\n",
      "train loss:2.3045937288527347\n",
      "train loss:2.29617263329496\n",
      "train loss:2.2964452651207803\n",
      "train loss:2.3045343364452924\n",
      "train loss:2.301747708973494\n",
      "train loss:2.290107563295016\n",
      "train loss:2.306904211499799\n",
      "train loss:2.299846520683576\n",
      "train loss:2.3077518617505492\n",
      "train loss:2.2979936974907025\n",
      "train loss:2.3052250389015927\n",
      "train loss:2.302182804385675\n",
      "train loss:2.306821357987106\n",
      "train loss:2.2994136579484\n",
      "train loss:2.2965956272603028\n",
      "train loss:2.299075325521793\n",
      "train loss:2.296437488167238\n",
      "train loss:2.303902344176048\n",
      "train loss:2.3090047130771643\n",
      "train loss:2.304455788668614\n",
      "train loss:2.309393506855246\n",
      "train loss:2.3045036071678107\n",
      "train loss:2.2960363243045836\n",
      "train loss:2.298634923064254\n",
      "train loss:2.2998055120000855\n",
      "train loss:2.313819423506851\n",
      "train loss:2.3058423913671473\n",
      "train loss:2.2998285078736487\n",
      "train loss:2.298737623775869\n",
      "train loss:2.3061809548155123\n",
      "train loss:2.3022924240077196\n",
      "train loss:2.2943472183148494\n",
      "train loss:2.296256085691319\n",
      "train loss:2.2999667676111075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.30019675369127\n",
      "train loss:2.299455801745824\n",
      "train loss:2.299088853834027\n",
      "train loss:2.3046373902453445\n",
      "train loss:2.29852694316334\n",
      "train loss:2.295148259874583\n",
      "train loss:2.297725957405369\n",
      "train loss:2.3016009670285347\n",
      "train loss:2.3072441852253505\n",
      "train loss:2.3050305991861486\n",
      "train loss:2.3024783742399144\n",
      "train loss:2.304119696955891\n",
      "train loss:2.2979596022428903\n",
      "train loss:2.308367180364348\n",
      "train loss:2.295556472510163\n",
      "train loss:2.29956917063599\n",
      "train loss:2.300526878440089\n",
      "train loss:2.2929433177006255\n",
      "train loss:2.2958815326262387\n",
      "train loss:2.2964500887374695\n",
      "=== epoch:36, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.315346039879531\n",
      "train loss:2.2998535535640126\n",
      "train loss:2.304189273985227\n",
      "train loss:2.3007427717354982\n",
      "train loss:2.306818466031622\n",
      "train loss:2.3076388400204824\n",
      "train loss:2.3011064637715473\n",
      "train loss:2.306406100175557\n",
      "train loss:2.289653399833143\n",
      "train loss:2.3060143791157577\n",
      "train loss:2.3080237422345586\n",
      "train loss:2.3012967692861137\n",
      "train loss:2.290162934585159\n",
      "train loss:2.2983134555682025\n",
      "train loss:2.2951573023401535\n",
      "train loss:2.3040008097107356\n",
      "train loss:2.300954311872196\n",
      "train loss:2.304432665779343\n",
      "train loss:2.301786730247792\n",
      "train loss:2.2962442951833135\n",
      "train loss:2.3073915944098404\n",
      "train loss:2.28714379669416\n",
      "train loss:2.3093006992793943\n",
      "train loss:2.2976171952127493\n",
      "train loss:2.310193730732002\n",
      "train loss:2.299124355036287\n",
      "train loss:2.303842753179246\n",
      "train loss:2.3050887704141276\n",
      "train loss:2.299035827523457\n",
      "train loss:2.298748253914631\n",
      "train loss:2.301574583112018\n",
      "train loss:2.3045519283077196\n",
      "train loss:2.3047570218390168\n",
      "train loss:2.3020610368628702\n",
      "train loss:2.3064426877189645\n",
      "train loss:2.3036521219534247\n",
      "train loss:2.309735297966038\n",
      "train loss:2.304571199004595\n",
      "train loss:2.3072750112479437\n",
      "train loss:2.295070784733248\n",
      "train loss:2.3084552632857944\n",
      "train loss:2.300025919070623\n",
      "train loss:2.3039372040847867\n",
      "train loss:2.2973206308535232\n",
      "train loss:2.301170677362754\n",
      "train loss:2.3026482734095164\n",
      "train loss:2.2981616093252883\n",
      "train loss:2.302846927264196\n",
      "train loss:2.307729941952605\n",
      "train loss:2.2992636952478716\n",
      "train loss:2.297527823444524\n",
      "train loss:2.2970238731313066\n",
      "train loss:2.3013072343117837\n",
      "train loss:2.2929281800591155\n",
      "train loss:2.299375310399448\n",
      "train loss:2.3033140963918095\n",
      "train loss:2.3019722709005848\n",
      "train loss:2.299664626594789\n",
      "train loss:2.3009215848732234\n",
      "train loss:2.3001630147510475\n",
      "train loss:2.2946353951543275\n",
      "train loss:2.3047404755550964\n",
      "train loss:2.299453731092847\n",
      "train loss:2.3094124093926807\n",
      "train loss:2.3071427566020613\n",
      "train loss:2.298695605830278\n",
      "train loss:2.295586639013752\n",
      "train loss:2.2964789973491646\n",
      "train loss:2.3052046647366256\n",
      "train loss:2.3013485389799713\n",
      "train loss:2.309853125879529\n",
      "train loss:2.29924625300395\n",
      "train loss:2.303723585700681\n",
      "train loss:2.2922953252906075\n",
      "train loss:2.2951667357946537\n",
      "train loss:2.3048229569798426\n",
      "train loss:2.292458452574366\n",
      "train loss:2.3013397390261043\n",
      "train loss:2.309446499567467\n",
      "train loss:2.299145896470763\n",
      "train loss:2.2979029502207995\n",
      "train loss:2.297617847710163\n",
      "train loss:2.2985632188888516\n",
      "train loss:2.303789766092215\n",
      "train loss:2.3010724808230743\n",
      "train loss:2.3009089107886194\n",
      "train loss:2.3044638028484807\n",
      "train loss:2.303757988413887\n",
      "train loss:2.3032235916458665\n",
      "train loss:2.3040583922009397\n",
      "train loss:2.298190160163222\n",
      "train loss:2.3009442351519818\n",
      "train loss:2.3115619769065847\n",
      "train loss:2.2932380831502024\n",
      "train loss:2.296018229002923\n",
      "train loss:2.3037901563748213\n",
      "train loss:2.292936487685817\n",
      "train loss:2.3019629594548934\n",
      "train loss:2.302906292789268\n",
      "train loss:2.294119420405813\n",
      "train loss:2.300671278884472\n",
      "train loss:2.298510654074249\n",
      "train loss:2.29971078507136\n",
      "train loss:2.2998893149296955\n",
      "train loss:2.3017443867677176\n",
      "train loss:2.3038646160724046\n",
      "train loss:2.3049980401183405\n",
      "train loss:2.3001224153062725\n",
      "train loss:2.303007572768345\n",
      "train loss:2.3040915072568606\n",
      "train loss:2.300801473104739\n",
      "train loss:2.2971918995434635\n",
      "train loss:2.300044768632145\n",
      "train loss:2.3081110997745564\n",
      "train loss:2.2942790533401043\n",
      "train loss:2.3013483816219833\n",
      "train loss:2.303758097815986\n",
      "train loss:2.3018069918795727\n",
      "train loss:2.297315883908377\n",
      "train loss:2.3118892204406145\n",
      "train loss:2.296923705910123\n",
      "train loss:2.3029499887931157\n",
      "train loss:2.309374865137723\n",
      "train loss:2.302795304126019\n",
      "train loss:2.3081385230060434\n",
      "train loss:2.2956619675737477\n",
      "train loss:2.3023442106511816\n",
      "train loss:2.3065610601650497\n",
      "train loss:2.29980250577963\n",
      "train loss:2.2976851541317673\n",
      "train loss:2.3027833658534487\n",
      "train loss:2.295111295390952\n",
      "train loss:2.3017418848917415\n",
      "train loss:2.301311667173984\n",
      "train loss:2.3067376273312545\n",
      "train loss:2.3073598752038973\n",
      "train loss:2.305993832353349\n",
      "train loss:2.309776958708657\n",
      "train loss:2.2970476359861225\n",
      "train loss:2.3061556632655886\n",
      "train loss:2.296993071952813\n",
      "train loss:2.3000855777046527\n",
      "train loss:2.3021400789813393\n",
      "train loss:2.3023342176467514\n",
      "train loss:2.3048983662093607\n",
      "train loss:2.2985606254207562\n",
      "train loss:2.312060337583244\n",
      "train loss:2.301481160764453\n",
      "train loss:2.2949634603730455\n",
      "train loss:2.3035766480795195\n",
      "train loss:2.298873566841119\n",
      "train loss:2.2961184928202063\n",
      "train loss:2.2993176955579115\n",
      "train loss:2.310827627372535\n",
      "train loss:2.2995979735687215\n",
      "train loss:2.296981966190936\n",
      "train loss:2.306477763560358\n",
      "train loss:2.301764917924611\n",
      "train loss:2.307308752765772\n",
      "train loss:2.3034721048858957\n",
      "train loss:2.3126899020623917\n",
      "train loss:2.296446284323052\n",
      "train loss:2.2965250769705627\n",
      "train loss:2.30321750539433\n",
      "train loss:2.3001886318693083\n",
      "train loss:2.3040844603021506\n",
      "train loss:2.3019327973359154\n",
      "train loss:2.299999013935273\n",
      "train loss:2.297940735495691\n",
      "train loss:2.306723602972418\n",
      "train loss:2.305328094007586\n",
      "train loss:2.3063520867533245\n",
      "train loss:2.306814707989955\n",
      "train loss:2.300126862120314\n",
      "train loss:2.3049141457699287\n",
      "train loss:2.304911491239358\n",
      "train loss:2.3037614263742223\n",
      "train loss:2.3015792213929718\n",
      "train loss:2.303000598359208\n",
      "train loss:2.2921767108827007\n",
      "train loss:2.2970359969084395\n",
      "train loss:2.294717791685705\n",
      "train loss:2.301121735772086\n",
      "train loss:2.3000936690187794\n",
      "train loss:2.3032363398636377\n",
      "train loss:2.3147491642514075\n",
      "train loss:2.304864720445588\n",
      "train loss:2.316901461359531\n",
      "train loss:2.301280867452032\n",
      "train loss:2.2998310136702735\n",
      "train loss:2.300433421639444\n",
      "train loss:2.301121334872696\n",
      "train loss:2.2975529538964534\n",
      "train loss:2.304178898500691\n",
      "train loss:2.300427469471143\n",
      "train loss:2.3019579512928536\n",
      "train loss:2.2976204355060457\n",
      "train loss:2.3030847296055725\n",
      "train loss:2.3039033301924063\n",
      "train loss:2.3041116347490362\n",
      "=== epoch:37, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3044814971013796\n",
      "train loss:2.3003731223262824\n",
      "train loss:2.308161253422392\n",
      "train loss:2.308364493833254\n",
      "train loss:2.3014552255811\n",
      "train loss:2.291896116472995\n",
      "train loss:2.295375435848905\n",
      "train loss:2.306971876616907\n",
      "train loss:2.297838091976567\n",
      "train loss:2.305330166013509\n",
      "train loss:2.3047309043106106\n",
      "train loss:2.306597085218272\n",
      "train loss:2.304160114281211\n",
      "train loss:2.306490065766427\n",
      "train loss:2.3034295312080153\n",
      "train loss:2.295898847524423\n",
      "train loss:2.2973881341044753\n",
      "train loss:2.30107945540771\n",
      "train loss:2.3045008081353253\n",
      "train loss:2.3067137598937\n",
      "train loss:2.307363784393795\n",
      "train loss:2.301845175485362\n",
      "train loss:2.2988284091695217\n",
      "train loss:2.302704757092686\n",
      "train loss:2.303544923533157\n",
      "train loss:2.3031711364534213\n",
      "train loss:2.295330694099406\n",
      "train loss:2.300768235667553\n",
      "train loss:2.3033435237233615\n",
      "train loss:2.2987417646539936\n",
      "train loss:2.300864458688219\n",
      "train loss:2.3007620623712546\n",
      "train loss:2.2964691068807492\n",
      "train loss:2.3054008145260507\n",
      "train loss:2.298877017829662\n",
      "train loss:2.299748592346414\n",
      "train loss:2.2977023823373166\n",
      "train loss:2.307203279874557\n",
      "train loss:2.302815023467706\n",
      "train loss:2.301941670714557\n",
      "train loss:2.301800982405499\n",
      "train loss:2.2907602546697454\n",
      "train loss:2.3013082074805715\n",
      "train loss:2.3093163608651825\n",
      "train loss:2.3046396111731515\n",
      "train loss:2.297774936808836\n",
      "train loss:2.293803053817521\n",
      "train loss:2.3031087993456256\n",
      "train loss:2.300607467710034\n",
      "train loss:2.2891611364686413\n",
      "train loss:2.3051759336667716\n",
      "train loss:2.3011942622233974\n",
      "train loss:2.299959242065318\n",
      "train loss:2.2979884067450134\n",
      "train loss:2.300055559813325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301917650075036\n",
      "train loss:2.3015567201090894\n",
      "train loss:2.305326338901165\n",
      "train loss:2.3014180221839378\n",
      "train loss:2.3042726414995545\n",
      "train loss:2.3031438942552995\n",
      "train loss:2.303876087304869\n",
      "train loss:2.3025411799695474\n",
      "train loss:2.3022521002130443\n",
      "train loss:2.3012805357477593\n",
      "train loss:2.298923443237036\n",
      "train loss:2.2974984806426115\n",
      "train loss:2.297560499538222\n",
      "train loss:2.305041723776268\n",
      "train loss:2.303471616824145\n",
      "train loss:2.299105531595853\n",
      "train loss:2.3051997690976633\n",
      "train loss:2.3089020822821262\n",
      "train loss:2.3027202147012518\n",
      "train loss:2.3014303022833293\n",
      "train loss:2.3038693286508374\n",
      "train loss:2.3011260201968105\n",
      "train loss:2.2949872345112436\n",
      "train loss:2.2901662592102707\n",
      "train loss:2.292003989782482\n",
      "train loss:2.302927961931753\n",
      "train loss:2.3032610619192058\n",
      "train loss:2.307828961956918\n",
      "train loss:2.2963352695928854\n",
      "train loss:2.3083657857834416\n",
      "train loss:2.3037174108184404\n",
      "train loss:2.30938444858717\n",
      "train loss:2.3141555746152536\n",
      "train loss:2.300761488826544\n",
      "train loss:2.293899394529114\n",
      "train loss:2.298549653339642\n",
      "train loss:2.3001920793940704\n",
      "train loss:2.3003033192059195\n",
      "train loss:2.29994517266113\n",
      "train loss:2.3046018909132897\n",
      "train loss:2.303373065830517\n",
      "train loss:2.3073664616183214\n",
      "train loss:2.303357760471661\n",
      "train loss:2.302907957001256\n",
      "train loss:2.3024052051520343\n",
      "train loss:2.3025983953799813\n",
      "train loss:2.3063661744377573\n",
      "train loss:2.3099428133732753\n",
      "train loss:2.299023501571747\n",
      "train loss:2.3046514853892472\n",
      "train loss:2.300617782276312\n",
      "train loss:2.302265823701165\n",
      "train loss:2.302554529828929\n",
      "train loss:2.3031701746895585\n",
      "train loss:2.3068911229829356\n",
      "train loss:2.2996553530493657\n",
      "train loss:2.3030673999713756\n",
      "train loss:2.3075276039830035\n",
      "train loss:2.304271320165251\n",
      "train loss:2.3002241386162625\n",
      "train loss:2.3009613743320187\n",
      "train loss:2.2954782050224076\n",
      "train loss:2.293552380365915\n",
      "train loss:2.307988333311864\n",
      "train loss:2.2999368001028704\n",
      "train loss:2.2903910379551604\n",
      "train loss:2.3029075659650324\n",
      "train loss:2.3066002457868766\n",
      "train loss:2.3008041510779105\n",
      "train loss:2.2949244299268887\n",
      "train loss:2.2935730211806136\n",
      "train loss:2.2920204625875895\n",
      "train loss:2.294145484603827\n",
      "train loss:2.2968301842067023\n",
      "train loss:2.310699221199209\n",
      "train loss:2.3026250458282966\n",
      "train loss:2.2972688538456336\n",
      "train loss:2.3092693056006133\n",
      "train loss:2.295982530820645\n",
      "train loss:2.2945855725395155\n",
      "train loss:2.306746206992419\n",
      "train loss:2.2971042768591965\n",
      "train loss:2.302812792001583\n",
      "train loss:2.302499364210368\n",
      "train loss:2.3039301656361015\n",
      "train loss:2.305856854210583\n",
      "train loss:2.2988292266264243\n",
      "train loss:2.3080508504315875\n",
      "train loss:2.3051399921555307\n",
      "train loss:2.3031945315374984\n",
      "train loss:2.305563771789222\n",
      "train loss:2.31044896689206\n",
      "train loss:2.301280918263584\n",
      "train loss:2.2906291556997385\n",
      "train loss:2.3003477249003645\n",
      "train loss:2.2973167621560764\n",
      "train loss:2.2970077859157594\n",
      "train loss:2.3055862027234713\n",
      "train loss:2.301703250436752\n",
      "train loss:2.2906812092541298\n",
      "train loss:2.3001888381766316\n",
      "train loss:2.2974327886460615\n",
      "train loss:2.3020336276815634\n",
      "train loss:2.3070947388329093\n",
      "train loss:2.309432818624947\n",
      "train loss:2.299662819397267\n",
      "train loss:2.2960422010701698\n",
      "train loss:2.3057971829671247\n",
      "train loss:2.3138576592888427\n",
      "train loss:2.2936052438987176\n",
      "train loss:2.308026320250814\n",
      "train loss:2.2982630529896944\n",
      "train loss:2.3015350442956146\n",
      "train loss:2.2993023556292407\n",
      "train loss:2.300142481004078\n",
      "train loss:2.307748640566055\n",
      "train loss:2.3054613345334642\n",
      "train loss:2.301735367155536\n",
      "train loss:2.306155817701873\n",
      "train loss:2.3001359943082957\n",
      "train loss:2.3019998558310233\n",
      "train loss:2.2981653402612876\n",
      "train loss:2.3034644856462374\n",
      "train loss:2.2981752962847355\n",
      "train loss:2.304568464183216\n",
      "train loss:2.3031422383961515\n",
      "train loss:2.3075640926876906\n",
      "train loss:2.301436029549725\n",
      "train loss:2.2954490678435686\n",
      "train loss:2.2992506102275123\n",
      "train loss:2.303231106836924\n",
      "train loss:2.3003878101161828\n",
      "train loss:2.298222970775999\n",
      "train loss:2.297051138395518\n",
      "train loss:2.3015863223749857\n",
      "train loss:2.3056851757485806\n",
      "train loss:2.296865330015791\n",
      "train loss:2.297158723064899\n",
      "train loss:2.3005765889722114\n",
      "train loss:2.30274862192773\n",
      "train loss:2.3135557965739624\n",
      "train loss:2.3044219863079225\n",
      "train loss:2.3048119428650558\n",
      "train loss:2.3062311983233394\n",
      "train loss:2.293187291563401\n",
      "=== epoch:38, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3097170334406805\n",
      "train loss:2.312640567021664\n",
      "train loss:2.2999006297146036\n",
      "train loss:2.3028455608117278\n",
      "train loss:2.2935090879897335\n",
      "train loss:2.3012092593213023\n",
      "train loss:2.3023368775144495\n",
      "train loss:2.300091959716726\n",
      "train loss:2.297254975033841\n",
      "train loss:2.3110181477056626\n",
      "train loss:2.30038850068328\n",
      "train loss:2.3019044229772643\n",
      "train loss:2.300651307473164\n",
      "train loss:2.3024182907352317\n",
      "train loss:2.297188230620685\n",
      "train loss:2.300862476675049\n",
      "train loss:2.302117891636887\n",
      "train loss:2.307285545251919\n",
      "train loss:2.3003409015763814\n",
      "train loss:2.2966437896618053\n",
      "train loss:2.308361349206359\n",
      "train loss:2.301227874068993\n",
      "train loss:2.294997374025538\n",
      "train loss:2.3043493356887073\n",
      "train loss:2.3019098529095334\n",
      "train loss:2.3068695153852707\n",
      "train loss:2.3032985872335794\n",
      "train loss:2.3059225133096817\n",
      "train loss:2.302957716550749\n",
      "train loss:2.2986149094319837\n",
      "train loss:2.293096839729945\n",
      "train loss:2.306141932117429\n",
      "train loss:2.2861918193946056\n",
      "train loss:2.2914767008595054\n",
      "train loss:2.298890954436314\n",
      "train loss:2.302906787426018\n",
      "train loss:2.29889992218125\n",
      "train loss:2.293968983778341\n",
      "train loss:2.2988903766307254\n",
      "train loss:2.3001160046202154\n",
      "train loss:2.3052543952590883\n",
      "train loss:2.2955832992212506\n",
      "train loss:2.297832758745248\n",
      "train loss:2.303596573762824\n",
      "train loss:2.296691228072961\n",
      "train loss:2.293634966934218\n",
      "train loss:2.298365082240105\n",
      "train loss:2.305989067086532\n",
      "train loss:2.29313831948829\n",
      "train loss:2.2953901016284233\n",
      "train loss:2.2974326045427764\n",
      "train loss:2.3082097324325894\n",
      "train loss:2.304553068239428\n",
      "train loss:2.3015394690761766\n",
      "train loss:2.307264597805592\n",
      "train loss:2.298394377202815\n",
      "train loss:2.301793306832968\n",
      "train loss:2.2973412305308103\n",
      "train loss:2.302864598738741\n",
      "train loss:2.299712355343918\n",
      "train loss:2.303218358332894\n",
      "train loss:2.306915983444197\n",
      "train loss:2.301516957923064\n",
      "train loss:2.3045882083187657\n",
      "train loss:2.2956737730104413\n",
      "train loss:2.3102284242104387\n",
      "train loss:2.3044132933544654\n",
      "train loss:2.2935689564521677\n",
      "train loss:2.30189610987712\n",
      "train loss:2.3035372244649586\n",
      "train loss:2.301336033744914\n",
      "train loss:2.304527090418881\n",
      "train loss:2.3114755191347087\n",
      "train loss:2.2838558786344456\n",
      "train loss:2.29783588319597\n",
      "train loss:2.304282567157382\n",
      "train loss:2.2988765376026166\n",
      "train loss:2.302943920680962\n",
      "train loss:2.3042518239198335\n",
      "train loss:2.299969866984702\n",
      "train loss:2.293723651906237\n",
      "train loss:2.2996897006310753\n",
      "train loss:2.309439527097927\n",
      "train loss:2.3048134279851205\n",
      "train loss:2.2973303494649486\n",
      "train loss:2.2984417472805383\n",
      "train loss:2.2897083295725627\n",
      "train loss:2.2962770155923895\n",
      "train loss:2.3007445044409676\n",
      "train loss:2.29304830787022\n",
      "train loss:2.299400054013751\n",
      "train loss:2.2865741928122825\n",
      "train loss:2.301165616897514\n",
      "train loss:2.3014698004436513\n",
      "train loss:2.2998955209687963\n",
      "train loss:2.3062576971186712\n",
      "train loss:2.3062768449937487\n",
      "train loss:2.2967258941239335\n",
      "train loss:2.2968468073840174\n",
      "train loss:2.293238076691638\n",
      "train loss:2.3001939039159804\n",
      "train loss:2.303029467187532\n",
      "train loss:2.297784702572839\n",
      "train loss:2.2993860236399066\n",
      "train loss:2.301367338534949\n",
      "train loss:2.2950337088949215\n",
      "train loss:2.305706810172925\n",
      "train loss:2.2960767570227727\n",
      "train loss:2.310235057864012\n",
      "train loss:2.3005170829085806\n",
      "train loss:2.3017343209071024\n",
      "train loss:2.3007053881233483\n",
      "train loss:2.2959878192049143\n",
      "train loss:2.2978074906854196\n",
      "train loss:2.290930822932151\n",
      "train loss:2.303135450421352\n",
      "train loss:2.305632192411945\n",
      "train loss:2.2999221662579092\n",
      "train loss:2.2993274727087267\n",
      "train loss:2.288580416043232\n",
      "train loss:2.2982963315744844\n",
      "train loss:2.3057885187429674\n",
      "train loss:2.291763304431068\n",
      "train loss:2.300088711120901\n",
      "train loss:2.3057611175396224\n",
      "train loss:2.3072148160859705\n",
      "train loss:2.3042121913561906\n",
      "train loss:2.3043737024268025\n",
      "train loss:2.3082186937410993\n",
      "train loss:2.2975146017978862\n",
      "train loss:2.302236835968181\n",
      "train loss:2.301705004909863\n",
      "train loss:2.3030007373648713\n",
      "train loss:2.3004260673379218\n",
      "train loss:2.312465386059797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299197565297558\n",
      "train loss:2.2905443161244974\n",
      "train loss:2.3114786872678823\n",
      "train loss:2.3022987019367087\n",
      "train loss:2.2936314827059556\n",
      "train loss:2.305352177214029\n",
      "train loss:2.29720373692913\n",
      "train loss:2.3026156310779706\n",
      "train loss:2.294904025369333\n",
      "train loss:2.3036679117554066\n",
      "train loss:2.299659612455877\n",
      "train loss:2.302414870084502\n",
      "train loss:2.3032469667764106\n",
      "train loss:2.295115836639237\n",
      "train loss:2.3048679591708394\n",
      "train loss:2.294365596156066\n",
      "train loss:2.308136475863582\n",
      "train loss:2.297676520791289\n",
      "train loss:2.2950934847677886\n",
      "train loss:2.3043198297002037\n",
      "train loss:2.3057419684983294\n",
      "train loss:2.3099218340308014\n",
      "train loss:2.300306786349668\n",
      "train loss:2.3036817039561384\n",
      "train loss:2.3053892615301512\n",
      "train loss:2.3073373086960816\n",
      "train loss:2.301280622099662\n",
      "train loss:2.302531225530262\n",
      "train loss:2.306842180414322\n",
      "train loss:2.300732350134384\n",
      "train loss:2.295360048719748\n",
      "train loss:2.294632466468587\n",
      "train loss:2.3104264999618573\n",
      "train loss:2.2992339109366258\n",
      "train loss:2.2982292733281957\n",
      "train loss:2.297439748703112\n",
      "train loss:2.2980124593225923\n",
      "train loss:2.3049512028157615\n",
      "train loss:2.2984244684807016\n",
      "train loss:2.2997618929985615\n",
      "train loss:2.3021952480849657\n",
      "train loss:2.30418792092741\n",
      "train loss:2.3058200190930433\n",
      "train loss:2.305814917190695\n",
      "train loss:2.306585305693895\n",
      "train loss:2.301855577552878\n",
      "train loss:2.302200899223016\n",
      "train loss:2.3010047668286036\n",
      "train loss:2.2902126790562454\n",
      "train loss:2.303946318609707\n",
      "train loss:2.299628519391503\n",
      "train loss:2.303838733093334\n",
      "train loss:2.30384111224338\n",
      "train loss:2.295994334092911\n",
      "train loss:2.3032254435003954\n",
      "train loss:2.2950733015350657\n",
      "train loss:2.3002999824467407\n",
      "train loss:2.2979391919098\n",
      "train loss:2.3037346283997766\n",
      "train loss:2.295645157036747\n",
      "train loss:2.3001940879749565\n",
      "train loss:2.2979823852918573\n",
      "train loss:2.305797679826071\n",
      "train loss:2.3067531009502917\n",
      "train loss:2.308106865654061\n",
      "=== epoch:39, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301411322286742\n",
      "train loss:2.301619185071505\n",
      "train loss:2.2987878063276357\n",
      "train loss:2.3091193830528116\n",
      "train loss:2.3034101825133613\n",
      "train loss:2.3006173144177886\n",
      "train loss:2.2991666940172757\n",
      "train loss:2.302908554793593\n",
      "train loss:2.296437904533667\n",
      "train loss:2.2962030105184223\n",
      "train loss:2.299636393059563\n",
      "train loss:2.309998430387655\n",
      "train loss:2.2977227143443617\n",
      "train loss:2.3015350753496553\n",
      "train loss:2.306923614745558\n",
      "train loss:2.298511083085588\n",
      "train loss:2.295193625623618\n",
      "train loss:2.2955330266322953\n",
      "train loss:2.2985019544733376\n",
      "train loss:2.298750059303801\n",
      "train loss:2.3065297703963528\n",
      "train loss:2.301308689412373\n",
      "train loss:2.3009212853014116\n",
      "train loss:2.302356073518891\n",
      "train loss:2.310421365342072\n",
      "train loss:2.29695910488393\n",
      "train loss:2.3117455659706523\n",
      "train loss:2.2986428949755946\n",
      "train loss:2.3085073110307985\n",
      "train loss:2.2978326458996277\n",
      "train loss:2.306616020092381\n",
      "train loss:2.306550910409491\n",
      "train loss:2.297526375805916\n",
      "train loss:2.308448975951191\n",
      "train loss:2.3033530666053057\n",
      "train loss:2.308234083078025\n",
      "train loss:2.305887342695108\n",
      "train loss:2.2989862273901336\n",
      "train loss:2.2973182610051985\n",
      "train loss:2.3012869353792147\n",
      "train loss:2.3077124294632743\n",
      "train loss:2.307636351442554\n",
      "train loss:2.301509635585697\n",
      "train loss:2.302860032462875\n",
      "train loss:2.3024917967910388\n",
      "train loss:2.3007762533918634\n",
      "train loss:2.302535089886489\n",
      "train loss:2.3048623576087572\n",
      "train loss:2.3030256030963514\n",
      "train loss:2.3056843794898256\n",
      "train loss:2.2986880949551205\n",
      "train loss:2.294311201073853\n",
      "train loss:2.3041513118753407\n",
      "train loss:2.3000205070664954\n",
      "train loss:2.297899498810958\n",
      "train loss:2.2964093524410423\n",
      "train loss:2.308209232203336\n",
      "train loss:2.3085703409661176\n",
      "train loss:2.3000444033869996\n",
      "train loss:2.300509557174583\n",
      "train loss:2.298289525008183\n",
      "train loss:2.2915051716599915\n",
      "train loss:2.302448009198588\n",
      "train loss:2.291961142381075\n",
      "train loss:2.2958767285521415\n",
      "train loss:2.311486321289042\n",
      "train loss:2.2923221438331667\n",
      "train loss:2.296634729722903\n",
      "train loss:2.296530416399588\n",
      "train loss:2.2973811453316046\n",
      "train loss:2.3002230572858013\n",
      "train loss:2.3012020287180577\n",
      "train loss:2.291446166179569\n",
      "train loss:2.300818048882758\n",
      "train loss:2.3044398818874265\n",
      "train loss:2.301967876096353\n",
      "train loss:2.299246317589824\n",
      "train loss:2.3055517889980517\n",
      "train loss:2.2983389596517605\n",
      "train loss:2.2921894598253263\n",
      "train loss:2.3079620914860124\n",
      "train loss:2.2982928023776674\n",
      "train loss:2.2953583934609094\n",
      "train loss:2.2955720225580163\n",
      "train loss:2.3061477626241977\n",
      "train loss:2.309155593231451\n",
      "train loss:2.299901912682928\n",
      "train loss:2.30208473816413\n",
      "train loss:2.3033285823803973\n",
      "train loss:2.299788745764825\n",
      "train loss:2.301837680166144\n",
      "train loss:2.2979534786960327\n",
      "train loss:2.3069583570887113\n",
      "train loss:2.2992224532704975\n",
      "train loss:2.297233822209051\n",
      "train loss:2.2906639660639114\n",
      "train loss:2.2998594266329575\n",
      "train loss:2.3020550696090636\n",
      "train loss:2.308385494646292\n",
      "train loss:2.296082136940627\n",
      "train loss:2.2957039419375858\n",
      "train loss:2.2956381854644494\n",
      "train loss:2.292153810591467\n",
      "train loss:2.296129449574985\n",
      "train loss:2.3039865365594125\n",
      "train loss:2.3043883560707297\n",
      "train loss:2.30187019368403\n",
      "train loss:2.298667829672436\n",
      "train loss:2.303677212232592\n",
      "train loss:2.3060075138956466\n",
      "train loss:2.299483731740782\n",
      "train loss:2.303191071873376\n",
      "train loss:2.308581416145959\n",
      "train loss:2.301976502318147\n",
      "train loss:2.2986646006819416\n",
      "train loss:2.3112003269922763\n",
      "train loss:2.30791759218203\n",
      "train loss:2.2955670035298636\n",
      "train loss:2.2972926819208057\n",
      "train loss:2.3093427250740053\n",
      "train loss:2.298261573606475\n",
      "train loss:2.2920419439329898\n",
      "train loss:2.303375327884388\n",
      "train loss:2.2999950877124955\n",
      "train loss:2.304602104774159\n",
      "train loss:2.305065238864377\n",
      "train loss:2.2954189722922007\n",
      "train loss:2.3050629764784993\n",
      "train loss:2.2962444641427036\n",
      "train loss:2.289919195213221\n",
      "train loss:2.3002181790390916\n",
      "train loss:2.3007002786916053\n",
      "train loss:2.2959326737121093\n",
      "train loss:2.3056048597196077\n",
      "train loss:2.3005707036204663\n",
      "train loss:2.303531694818724\n",
      "train loss:2.2950945223482577\n",
      "train loss:2.3039885528775885\n",
      "train loss:2.294282700498259\n",
      "train loss:2.304769528030668\n",
      "train loss:2.297387757821451\n",
      "train loss:2.298558031097243\n",
      "train loss:2.2968595917344303\n",
      "train loss:2.2983458188499224\n",
      "train loss:2.30463470089932\n",
      "train loss:2.297324662122464\n",
      "train loss:2.2977757545172572\n",
      "train loss:2.3000314702857914\n",
      "train loss:2.294662479356246\n",
      "train loss:2.296536307045501\n",
      "train loss:2.2976005489717326\n",
      "train loss:2.3055245286678487\n",
      "train loss:2.2988123099749362\n",
      "train loss:2.301660794778048\n",
      "train loss:2.305607146998935\n",
      "train loss:2.3041627789152828\n",
      "train loss:2.2945594188010277\n",
      "train loss:2.2961594297405763\n",
      "train loss:2.3042088278301427\n",
      "train loss:2.300413773088854\n",
      "train loss:2.30486490817638\n",
      "train loss:2.3033222963413817\n",
      "train loss:2.298817566543321\n",
      "train loss:2.3039047831328716\n",
      "train loss:2.295713814088268\n",
      "train loss:2.3045703035995886\n",
      "train loss:2.295087078849274\n",
      "train loss:2.2946779019875128\n",
      "train loss:2.3046580575103\n",
      "train loss:2.300549319635846\n",
      "train loss:2.298002024051087\n",
      "train loss:2.3049628123432204\n",
      "train loss:2.2951055285436763\n",
      "train loss:2.302339703001115\n",
      "train loss:2.2943819687888296\n",
      "train loss:2.2932105897421033\n",
      "train loss:2.3004796846421307\n",
      "train loss:2.3053013515148497\n",
      "train loss:2.297094906247493\n",
      "train loss:2.296851658898005\n",
      "train loss:2.2966599090848003\n",
      "train loss:2.2988389748443767\n",
      "train loss:2.300165275904206\n",
      "train loss:2.298267486801614\n",
      "train loss:2.3020083383320866\n",
      "train loss:2.3015758203421526\n",
      "train loss:2.3000535286800234\n",
      "train loss:2.2966795306263954\n",
      "train loss:2.3026870163877216\n",
      "train loss:2.303444351910209\n",
      "train loss:2.301068335262458\n",
      "train loss:2.2941778477967993\n",
      "train loss:2.2940014127298194\n",
      "train loss:2.294423013445411\n",
      "train loss:2.3008995176229172\n",
      "train loss:2.301431214975066\n",
      "train loss:2.2970472023894644\n",
      "train loss:2.304322518493178\n",
      "train loss:2.306024417109988\n",
      "train loss:2.303865526108901\n",
      "=== epoch:40, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308938634975848\n",
      "train loss:2.2942835866031794\n",
      "train loss:2.310842074390338\n",
      "train loss:2.3031823371879163\n",
      "train loss:2.2982553744281744\n",
      "train loss:2.289474488787039\n",
      "train loss:2.3079114382395045\n",
      "train loss:2.2950834574218604\n",
      "train loss:2.298715113559271\n",
      "train loss:2.296984138731089\n",
      "train loss:2.3056232698401624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300569645111435\n",
      "train loss:2.307210727988884\n",
      "train loss:2.303981717575991\n",
      "train loss:2.303886440679251\n",
      "train loss:2.2976822000567276\n",
      "train loss:2.2993858333222224\n",
      "train loss:2.298390579272616\n",
      "train loss:2.2981918404119486\n",
      "train loss:2.295398125392313\n",
      "train loss:2.300652487069133\n",
      "train loss:2.2965922864963906\n",
      "train loss:2.303306535848236\n",
      "train loss:2.308578490095316\n",
      "train loss:2.3040943694391345\n",
      "train loss:2.2994630913640925\n",
      "train loss:2.301664148787254\n",
      "train loss:2.300423003590175\n",
      "train loss:2.3050938694235943\n",
      "train loss:2.3013280611427254\n",
      "train loss:2.2977885440260124\n",
      "train loss:2.3033357211579433\n",
      "train loss:2.3093828574077624\n",
      "train loss:2.298997895361667\n",
      "train loss:2.2946286169900376\n",
      "train loss:2.3061192399717725\n",
      "train loss:2.297091244196499\n",
      "train loss:2.30403824795435\n",
      "train loss:2.3069698133732133\n",
      "train loss:2.3011378566753025\n",
      "train loss:2.30096519210818\n",
      "train loss:2.300534157272487\n",
      "train loss:2.3013168548603455\n",
      "train loss:2.294727425127491\n",
      "train loss:2.2933896324351353\n",
      "train loss:2.2991216774067706\n",
      "train loss:2.2999745856951885\n",
      "train loss:2.3084450959658387\n",
      "train loss:2.3015599958138973\n",
      "train loss:2.306691710618261\n",
      "train loss:2.2973941763707564\n",
      "train loss:2.296837450312143\n",
      "train loss:2.2934121010130593\n",
      "train loss:2.298570557845443\n",
      "train loss:2.2916769704036875\n",
      "train loss:2.303039874350639\n",
      "train loss:2.300486984550281\n",
      "train loss:2.302967924547147\n",
      "train loss:2.3091958258446055\n",
      "train loss:2.2962976832316406\n",
      "train loss:2.307125443534859\n",
      "train loss:2.2990582422925137\n",
      "train loss:2.2964475558381627\n",
      "train loss:2.29884358511644\n",
      "train loss:2.292666857366823\n",
      "train loss:2.296678753466845\n",
      "train loss:2.30329679389709\n",
      "train loss:2.299270704281831\n",
      "train loss:2.301578992768565\n",
      "train loss:2.3048349027196906\n",
      "train loss:2.3057015752568395\n",
      "train loss:2.3121966789551247\n",
      "train loss:2.3010170820082867\n",
      "train loss:2.2930099358263303\n",
      "train loss:2.304388216543318\n",
      "train loss:2.3020415725361874\n",
      "train loss:2.306688368933313\n",
      "train loss:2.309609491469565\n",
      "train loss:2.3056236796323026\n",
      "train loss:2.3064083340498382\n",
      "train loss:2.314844755904979\n",
      "train loss:2.306213688973981\n",
      "train loss:2.29910502449489\n",
      "train loss:2.303974633114387\n",
      "train loss:2.302882348879901\n",
      "train loss:2.3065943692939843\n",
      "train loss:2.3061568653804767\n",
      "train loss:2.292986128907695\n",
      "train loss:2.2958650044051674\n",
      "train loss:2.3001948422217002\n",
      "train loss:2.298590556648078\n",
      "train loss:2.3039074926953385\n",
      "train loss:2.305282971158017\n",
      "train loss:2.2970664621306605\n",
      "train loss:2.2929107113352316\n",
      "train loss:2.301084435565486\n",
      "train loss:2.300287975164651\n",
      "train loss:2.297741721069742\n",
      "train loss:2.299691554563267\n",
      "train loss:2.30205467061584\n",
      "train loss:2.3042613305374617\n",
      "train loss:2.3015807513336\n",
      "train loss:2.3037844240299914\n",
      "train loss:2.302847977789545\n",
      "train loss:2.298694521423008\n",
      "train loss:2.301043216697926\n",
      "train loss:2.3023730520261627\n",
      "train loss:2.2954338613258143\n",
      "train loss:2.301461416009287\n",
      "train loss:2.292867398053806\n",
      "train loss:2.3106028168267825\n",
      "train loss:2.3036483832521126\n",
      "train loss:2.2966397240030503\n",
      "train loss:2.3021372249344254\n",
      "train loss:2.296756807254594\n",
      "train loss:2.292886503398888\n",
      "train loss:2.3041301515441286\n",
      "train loss:2.309381133142437\n",
      "train loss:2.2935804331441076\n",
      "train loss:2.3066297376252574\n",
      "train loss:2.30039865166685\n",
      "train loss:2.3054547604672684\n",
      "train loss:2.3054084994938178\n",
      "train loss:2.2972160606433487\n",
      "train loss:2.298706325922629\n",
      "train loss:2.300408437525039\n",
      "train loss:2.295740173940953\n",
      "train loss:2.3026687168344626\n",
      "train loss:2.3022079311304577\n",
      "train loss:2.303187660692166\n",
      "train loss:2.306280073426436\n",
      "train loss:2.300346839428235\n",
      "train loss:2.3000821758629506\n",
      "train loss:2.298349826884337\n",
      "train loss:2.306364708624932\n",
      "train loss:2.3031425511475296\n",
      "train loss:2.306337060921175\n",
      "train loss:2.291948658662226\n",
      "train loss:2.308709297026373\n",
      "train loss:2.299130138177903\n",
      "train loss:2.301356271776842\n",
      "train loss:2.3067797244581145\n",
      "train loss:2.293815586495949\n",
      "train loss:2.3051642529607275\n",
      "train loss:2.3009586833313644\n",
      "train loss:2.2967118219432576\n",
      "train loss:2.2967267788243575\n",
      "train loss:2.296420736945211\n",
      "train loss:2.3015885654865196\n",
      "train loss:2.3028202332110888\n",
      "train loss:2.3007982890671586\n",
      "train loss:2.3014521886132324\n",
      "train loss:2.296800979985089\n",
      "train loss:2.3007912116275464\n",
      "train loss:2.3005321209994474\n",
      "train loss:2.3060941207768164\n",
      "train loss:2.3062007446573816\n",
      "train loss:2.301054748771654\n",
      "train loss:2.304295429598117\n",
      "train loss:2.3080672675938523\n",
      "train loss:2.299704886999662\n",
      "train loss:2.2999024278645828\n",
      "train loss:2.301581052137109\n",
      "train loss:2.305247848640012\n",
      "train loss:2.3024146726095727\n",
      "train loss:2.307268690125319\n",
      "train loss:2.308360675116388\n",
      "train loss:2.300676390600261\n",
      "train loss:2.2987093110844787\n",
      "train loss:2.3027245148770876\n",
      "train loss:2.2967607324981425\n",
      "train loss:2.301121472413797\n",
      "train loss:2.306041527852111\n",
      "train loss:2.294533083157746\n",
      "train loss:2.3026442080252036\n",
      "train loss:2.308379637182052\n",
      "train loss:2.2853530517405964\n",
      "train loss:2.2960333058656763\n",
      "train loss:2.302460475923173\n",
      "train loss:2.301260831572083\n",
      "train loss:2.305025645452454\n",
      "train loss:2.3052049360361213\n",
      "train loss:2.299642722535619\n",
      "train loss:2.303815494807453\n",
      "train loss:2.299433033821179\n",
      "train loss:2.310516806521799\n",
      "train loss:2.3054452320595313\n",
      "train loss:2.2942420891795234\n",
      "train loss:2.2998508901819967\n",
      "train loss:2.3079993955924154\n",
      "train loss:2.305580632485246\n",
      "train loss:2.3005819672028536\n",
      "train loss:2.287749049163346\n",
      "train loss:2.3082671359763705\n",
      "train loss:2.3095228867303583\n",
      "train loss:2.302440352390944\n",
      "train loss:2.301501362874982\n",
      "train loss:2.294014302684306\n",
      "train loss:2.300412858204964\n",
      "train loss:2.2981831903268803\n",
      "=== epoch:41, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3087917787322376\n",
      "train loss:2.295133411630669\n",
      "train loss:2.3062621063460456\n",
      "train loss:2.301266136233201\n",
      "train loss:2.295813899199277\n",
      "train loss:2.308791047983429\n",
      "train loss:2.307150078506095\n",
      "train loss:2.2946308460009908\n",
      "train loss:2.297082157526326\n",
      "train loss:2.3102380298986933\n",
      "train loss:2.3032386118289594\n",
      "train loss:2.3112774393958397\n",
      "train loss:2.2940953200295784\n",
      "train loss:2.2937029672309395\n",
      "train loss:2.3073388850486407\n",
      "train loss:2.30998502307165\n",
      "train loss:2.3055307445796864\n",
      "train loss:2.3020343037060274\n",
      "train loss:2.3038001894555262\n",
      "train loss:2.2989283362900483\n",
      "train loss:2.298012731746249\n",
      "train loss:2.304820088039671\n",
      "train loss:2.2962532463488285\n",
      "train loss:2.30721427297423\n",
      "train loss:2.298091102692418\n",
      "train loss:2.303851050355311\n",
      "train loss:2.2976484719226336\n",
      "train loss:2.291853714343817\n",
      "train loss:2.2998669700565038\n",
      "train loss:2.3039478968935976\n",
      "train loss:2.3031142286689885\n",
      "train loss:2.3054315154547944\n",
      "train loss:2.2897056834386365\n",
      "train loss:2.2945429730482982\n",
      "train loss:2.2951074985176954\n",
      "train loss:2.2967980373630303\n",
      "train loss:2.302249740154092\n",
      "train loss:2.3042743206360616\n",
      "train loss:2.3063667456424284\n",
      "train loss:2.2974675466476913\n",
      "train loss:2.310446466670892\n",
      "train loss:2.291342974803619\n",
      "train loss:2.3057023595081265\n",
      "train loss:2.3083197305241723\n",
      "train loss:2.307903335884688\n",
      "train loss:2.306419344561185\n",
      "train loss:2.3009454569220833\n",
      "train loss:2.2990914693881304\n",
      "train loss:2.2995853259587804\n",
      "train loss:2.3031731118552035\n",
      "train loss:2.3002036934112957\n",
      "train loss:2.3005877877198166\n",
      "train loss:2.303236818833368\n",
      "train loss:2.2958921813188193\n",
      "train loss:2.3002760594397396\n",
      "train loss:2.30978878229594\n",
      "train loss:2.307618646652612\n",
      "train loss:2.3025806698501974\n",
      "train loss:2.297934377368608\n",
      "train loss:2.288414017692547\n",
      "train loss:2.3001446598716626\n",
      "train loss:2.30493159959244\n",
      "train loss:2.3103150219371216\n",
      "train loss:2.311433319650878\n",
      "train loss:2.300477161683274\n",
      "train loss:2.2981290154118708\n",
      "train loss:2.2993167542822976\n",
      "train loss:2.3055035949857325\n",
      "train loss:2.3010583537460634\n",
      "train loss:2.3032688768157996\n",
      "train loss:2.3013580366875526\n",
      "train loss:2.304638762288628\n",
      "train loss:2.2987161692736797\n",
      "train loss:2.3019481155009736\n",
      "train loss:2.308212121105584\n",
      "train loss:2.298126279023289\n",
      "train loss:2.3020529954734994\n",
      "train loss:2.308041175037062\n",
      "train loss:2.2975003194769394\n",
      "train loss:2.297184309810862\n",
      "train loss:2.3015253356128644\n",
      "train loss:2.299028716299847\n",
      "train loss:2.2993006877308706\n",
      "train loss:2.3049309367292516\n",
      "train loss:2.3066285104265494\n",
      "train loss:2.299585659108235\n",
      "train loss:2.308166924574977\n",
      "train loss:2.297415266505107\n",
      "train loss:2.307572672892731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306670099433444\n",
      "train loss:2.3019038254286017\n",
      "train loss:2.2976321532401234\n",
      "train loss:2.303981947097955\n",
      "train loss:2.2964011735502234\n",
      "train loss:2.299247254657408\n",
      "train loss:2.3015728035650818\n",
      "train loss:2.3045620427438727\n",
      "train loss:2.2985961708473317\n",
      "train loss:2.3017248068472007\n",
      "train loss:2.2910941995716607\n",
      "train loss:2.2976214347796198\n",
      "train loss:2.307068580098696\n",
      "train loss:2.3036358242299895\n",
      "train loss:2.301384841226713\n",
      "train loss:2.3047772989245785\n",
      "train loss:2.307211648985166\n",
      "train loss:2.310443932693309\n",
      "train loss:2.2995640985986086\n",
      "train loss:2.309328393576286\n",
      "train loss:2.291681272232069\n",
      "train loss:2.299230142178067\n",
      "train loss:2.2989117866708733\n",
      "train loss:2.2991723903972794\n",
      "train loss:2.3071186712923617\n",
      "train loss:2.295415779473291\n",
      "train loss:2.2932310251491264\n",
      "train loss:2.307386226187617\n",
      "train loss:2.308560942252517\n",
      "train loss:2.3002099492745938\n",
      "train loss:2.3007672086380344\n",
      "train loss:2.3018271758482713\n",
      "train loss:2.3091291222672914\n",
      "train loss:2.304876406062406\n",
      "train loss:2.3102277713756862\n",
      "train loss:2.298637552570371\n",
      "train loss:2.3061996811073247\n",
      "train loss:2.2968911504006377\n",
      "train loss:2.300628321871268\n",
      "train loss:2.30124591068361\n",
      "train loss:2.2984653793357612\n",
      "train loss:2.3024488298761523\n",
      "train loss:2.2997140219174663\n",
      "train loss:2.306117147475418\n",
      "train loss:2.303709991268359\n",
      "train loss:2.3051439217081344\n",
      "train loss:2.299305122963907\n",
      "train loss:2.3026760378362483\n",
      "train loss:2.2981264604973197\n",
      "train loss:2.2992454566734675\n",
      "train loss:2.300294022909723\n",
      "train loss:2.299135702629356\n",
      "train loss:2.301636942280097\n",
      "train loss:2.3027068352632476\n",
      "train loss:2.3003136445303047\n",
      "train loss:2.300754950882187\n",
      "train loss:2.3050744734579314\n",
      "train loss:2.299062667889858\n",
      "train loss:2.2975955500344476\n",
      "train loss:2.2970695801527423\n",
      "train loss:2.3099941246933082\n",
      "train loss:2.306438867479274\n",
      "train loss:2.2956731932964605\n",
      "train loss:2.303866729379535\n",
      "train loss:2.291386202154328\n",
      "train loss:2.304170932364917\n",
      "train loss:2.3010876114539354\n",
      "train loss:2.308522994053222\n",
      "train loss:2.3045722020525288\n",
      "train loss:2.307537501426874\n",
      "train loss:2.292960607111715\n",
      "train loss:2.3021341033790206\n",
      "train loss:2.298788147612456\n",
      "train loss:2.2974978482657606\n",
      "train loss:2.2966801084938235\n",
      "train loss:2.3059731006996054\n",
      "train loss:2.296515654583044\n",
      "train loss:2.3019229309002123\n",
      "train loss:2.308100764300981\n",
      "train loss:2.301883167996506\n",
      "train loss:2.296338730836746\n",
      "train loss:2.295298728118114\n",
      "train loss:2.3005195136642738\n",
      "train loss:2.3000385340924954\n",
      "train loss:2.30614162094682\n",
      "train loss:2.3106382197345594\n",
      "train loss:2.2991086975857598\n",
      "train loss:2.3018893122102897\n",
      "train loss:2.300064712490698\n",
      "train loss:2.3006636857115823\n",
      "train loss:2.3081950891950815\n",
      "train loss:2.3046409105958063\n",
      "train loss:2.305945294253092\n",
      "train loss:2.3037541067421987\n",
      "train loss:2.3099604839696215\n",
      "train loss:2.2963240487135645\n",
      "train loss:2.297029047323472\n",
      "train loss:2.3046806712589447\n",
      "train loss:2.3000979181064776\n",
      "train loss:2.29696696416314\n",
      "train loss:2.303396478025149\n",
      "train loss:2.2940497823711405\n",
      "train loss:2.304411522943587\n",
      "train loss:2.303093033740503\n",
      "train loss:2.3004828979898293\n",
      "train loss:2.3011601265116024\n",
      "train loss:2.296015362178215\n",
      "train loss:2.2967820555833436\n",
      "train loss:2.2987945167939805\n",
      "train loss:2.305264606210123\n",
      "train loss:2.300041012926067\n",
      "=== epoch:42, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3010522687590726\n",
      "train loss:2.2983404214035676\n",
      "train loss:2.314787171169843\n",
      "train loss:2.3068410390815295\n",
      "train loss:2.2921642962873907\n",
      "train loss:2.3006784513838388\n",
      "train loss:2.3050199239943243\n",
      "train loss:2.304305781029446\n",
      "train loss:2.2963489279882214\n",
      "train loss:2.3050315858924666\n",
      "train loss:2.3089596894617466\n",
      "train loss:2.291145267633879\n",
      "train loss:2.3062577402825397\n",
      "train loss:2.3007160758950813\n",
      "train loss:2.2919879931082883\n",
      "train loss:2.2980963067440516\n",
      "train loss:2.2979565922432075\n",
      "train loss:2.3003312438973134\n",
      "train loss:2.303624993774456\n",
      "train loss:2.3035636322256985\n",
      "train loss:2.3046005117211723\n",
      "train loss:2.302480760283304\n",
      "train loss:2.305812666199605\n",
      "train loss:2.3038165088860607\n",
      "train loss:2.297126221709833\n",
      "train loss:2.3083464521912282\n",
      "train loss:2.2945911189379666\n",
      "train loss:2.2982171802814997\n",
      "train loss:2.305110033189573\n",
      "train loss:2.304915099183684\n",
      "train loss:2.2896524718178006\n",
      "train loss:2.302995775853942\n",
      "train loss:2.30876723413479\n",
      "train loss:2.3027535645122876\n",
      "train loss:2.3052489962080527\n",
      "train loss:2.2959262008093564\n",
      "train loss:2.3027438423111883\n",
      "train loss:2.300323502297422\n",
      "train loss:2.2897746030807657\n",
      "train loss:2.296287505336168\n",
      "train loss:2.3088428390432996\n",
      "train loss:2.309793970312834\n",
      "train loss:2.2945363284689604\n",
      "train loss:2.307631806138524\n",
      "train loss:2.2926560997231107\n",
      "train loss:2.3037997609036234\n",
      "train loss:2.3026658415942634\n",
      "train loss:2.295900513690285\n",
      "train loss:2.2992121127845007\n",
      "train loss:2.2853810035831295\n",
      "train loss:2.301426084207573\n",
      "train loss:2.3027816289297207\n",
      "train loss:2.2985900020799\n",
      "train loss:2.3004672205775845\n",
      "train loss:2.304295804100256\n",
      "train loss:2.297356108116728\n",
      "train loss:2.2962908942706903\n",
      "train loss:2.304104961387967\n",
      "train loss:2.305310462776937\n",
      "train loss:2.303558220701899\n",
      "train loss:2.3012385672828977\n",
      "train loss:2.3015311913144103\n",
      "train loss:2.2996742660404093\n",
      "train loss:2.295035496766547\n",
      "train loss:2.300256347585026\n",
      "train loss:2.3037470054467284\n",
      "train loss:2.3009929323158187\n",
      "train loss:2.303888678672087\n",
      "train loss:2.2921913831933836\n",
      "train loss:2.3059504339544032\n",
      "train loss:2.303726498427649\n",
      "train loss:2.300887918104045\n",
      "train loss:2.297049566164324\n",
      "train loss:2.30938400765194\n",
      "train loss:2.298546398541358\n",
      "train loss:2.297878595743696\n",
      "train loss:2.2995630660879116\n",
      "train loss:2.2946634588856756\n",
      "train loss:2.299067420529808\n",
      "train loss:2.2988909271188334\n",
      "train loss:2.301834167642782\n",
      "train loss:2.30555578577936\n",
      "train loss:2.3069906810868956\n",
      "train loss:2.3049749087056366\n",
      "train loss:2.3033778926069317\n",
      "train loss:2.3075290071209014\n",
      "train loss:2.3097555965019123\n",
      "train loss:2.2961779415044075\n",
      "train loss:2.300020737934354\n",
      "train loss:2.296545845076485\n",
      "train loss:2.296888573249002\n",
      "train loss:2.2989418240358814\n",
      "train loss:2.2952990922665326\n",
      "train loss:2.309503053679027\n",
      "train loss:2.298816711900283\n",
      "train loss:2.3005984802707284\n",
      "train loss:2.303663026378409\n",
      "train loss:2.3062969461150886\n",
      "train loss:2.3065392115985297\n",
      "train loss:2.2967036939014656\n",
      "train loss:2.301708824025944\n",
      "train loss:2.2980537784355892\n",
      "train loss:2.2951808070319\n",
      "train loss:2.3052814914685795\n",
      "train loss:2.29440751678741\n",
      "train loss:2.302761941838134\n",
      "train loss:2.2938958548480852\n",
      "train loss:2.298269097741561\n",
      "train loss:2.292455647932685\n",
      "train loss:2.3058323367348073\n",
      "train loss:2.3031824750523304\n",
      "train loss:2.298098195972443\n",
      "train loss:2.305485199807693\n",
      "train loss:2.2997677735142217\n",
      "train loss:2.311079792290558\n",
      "train loss:2.2995198346323145\n",
      "train loss:2.310883868567179\n",
      "train loss:2.304386503163277\n",
      "train loss:2.3085608773765287\n",
      "train loss:2.307268690112371\n",
      "train loss:2.3048256983556823\n",
      "train loss:2.3047002068224964\n",
      "train loss:2.300417432187217\n",
      "train loss:2.3109978931762094\n",
      "train loss:2.3019971755131214\n",
      "train loss:2.302255548038657\n",
      "train loss:2.3044033580934786\n",
      "train loss:2.289809585439685\n",
      "train loss:2.294389513400487\n",
      "train loss:2.299040892589824\n",
      "train loss:2.2995712694216635\n",
      "train loss:2.3029768298351527\n",
      "train loss:2.2983614995221466\n",
      "train loss:2.293372116358829\n",
      "train loss:2.3020193474596025\n",
      "train loss:2.3007035128868925\n",
      "train loss:2.3119302593104156\n",
      "train loss:2.3079672977990096\n",
      "train loss:2.298470820890731\n",
      "train loss:2.2982958801047944\n",
      "train loss:2.297501544086041\n",
      "train loss:2.3097250485113636\n",
      "train loss:2.296755229734185\n",
      "train loss:2.2957704713776863\n",
      "train loss:2.2944850036937425\n",
      "train loss:2.301635330811131\n",
      "train loss:2.302789617169715\n",
      "train loss:2.304181355793204\n",
      "train loss:2.2997244707760833\n",
      "train loss:2.299851882155821\n",
      "train loss:2.3093681846138656\n",
      "train loss:2.303313493611928\n",
      "train loss:2.3042697136000925\n",
      "train loss:2.299385460612229\n",
      "train loss:2.305911065110133\n",
      "train loss:2.302928627620321\n",
      "train loss:2.2994795891754447\n",
      "train loss:2.3005844497421344\n",
      "train loss:2.3034854171491608\n",
      "train loss:2.298883943087375\n",
      "train loss:2.30356366280619\n",
      "train loss:2.309550579156222\n",
      "train loss:2.305855353665703\n",
      "train loss:2.297769331874798\n",
      "train loss:2.3031514260951265\n",
      "train loss:2.3004323776657065\n",
      "train loss:2.3017226256235706\n",
      "train loss:2.3033663595592744\n",
      "train loss:2.299454350173399\n",
      "train loss:2.299182428752129\n",
      "train loss:2.3030618583858162\n",
      "train loss:2.2958881742351718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2969013455663942\n",
      "train loss:2.295152787471767\n",
      "train loss:2.2974358948587628\n",
      "train loss:2.3025193935883936\n",
      "train loss:2.3053326388803446\n",
      "train loss:2.302177705096635\n",
      "train loss:2.3050291175068787\n",
      "train loss:2.3058874737095945\n",
      "train loss:2.2986058010414725\n",
      "train loss:2.294701712418056\n",
      "train loss:2.300928331352748\n",
      "train loss:2.2966369064217838\n",
      "train loss:2.2977590612147627\n",
      "train loss:2.3061911546840883\n",
      "train loss:2.300932745415204\n",
      "train loss:2.3011878471170886\n",
      "train loss:2.295684363624489\n",
      "train loss:2.302939941460938\n",
      "train loss:2.3024518151394635\n",
      "train loss:2.2928468315956354\n",
      "train loss:2.3105642785049576\n",
      "train loss:2.3023173946581434\n",
      "train loss:2.2964175408678122\n",
      "train loss:2.302668583355055\n",
      "train loss:2.3040318094822183\n",
      "train loss:2.3046819289893996\n",
      "train loss:2.298602404597767\n",
      "train loss:2.3008183726969174\n",
      "=== epoch:43, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2934995938999134\n",
      "train loss:2.2996739518536793\n",
      "train loss:2.2948652138429186\n",
      "train loss:2.30132176457674\n",
      "train loss:2.301681491518696\n",
      "train loss:2.3098830012331053\n",
      "train loss:2.2930804820028774\n",
      "train loss:2.302649045297926\n",
      "train loss:2.2959569362419114\n",
      "train loss:2.3018517778811916\n",
      "train loss:2.3036211321570694\n",
      "train loss:2.30636917233891\n",
      "train loss:2.3005616279681984\n",
      "train loss:2.3055979799304733\n",
      "train loss:2.3073743152222295\n",
      "train loss:2.3010781738376105\n",
      "train loss:2.3064237165132018\n",
      "train loss:2.3082621437978945\n",
      "train loss:2.2997897754006575\n",
      "train loss:2.299971088344183\n",
      "train loss:2.29740455061812\n",
      "train loss:2.2999098940107343\n",
      "train loss:2.302254849555143\n",
      "train loss:2.3021489219245064\n",
      "train loss:2.299719987273897\n",
      "train loss:2.2919755123099357\n",
      "train loss:2.3136763968793157\n",
      "train loss:2.298090701588354\n",
      "train loss:2.3028477208886535\n",
      "train loss:2.294938692065239\n",
      "train loss:2.3019498133137346\n",
      "train loss:2.308533124092842\n",
      "train loss:2.303945804556276\n",
      "train loss:2.290862166841388\n",
      "train loss:2.2920726867598007\n",
      "train loss:2.2951860724530335\n",
      "train loss:2.303514948843878\n",
      "train loss:2.2996529763443956\n",
      "train loss:2.3041620740314164\n",
      "train loss:2.2975051128128747\n",
      "train loss:2.305244148794314\n",
      "train loss:2.3035180557027455\n",
      "train loss:2.2994642114031634\n",
      "train loss:2.30512799138115\n",
      "train loss:2.2982356185854926\n",
      "train loss:2.30544131027569\n",
      "train loss:2.3035798236572647\n",
      "train loss:2.309221214690958\n",
      "train loss:2.3018911931821915\n",
      "train loss:2.298764454744107\n",
      "train loss:2.2985128141030833\n",
      "train loss:2.3003449729312195\n",
      "train loss:2.2954569519788453\n",
      "train loss:2.301212540139409\n",
      "train loss:2.299820536277275\n",
      "train loss:2.301025526251826\n",
      "train loss:2.2958839789298526\n",
      "train loss:2.2997723679677007\n",
      "train loss:2.298161735198467\n",
      "train loss:2.3039363204928787\n",
      "train loss:2.301639898157805\n",
      "train loss:2.3100372780280547\n",
      "train loss:2.3069798954631784\n",
      "train loss:2.3031206509308437\n",
      "train loss:2.2994484282228793\n",
      "train loss:2.2961267499168097\n",
      "train loss:2.2997351654950218\n",
      "train loss:2.2944172098007667\n",
      "train loss:2.2976221804351273\n",
      "train loss:2.302510808661468\n",
      "train loss:2.300438455169953\n",
      "train loss:2.305289981095621\n",
      "train loss:2.3067654028765157\n",
      "train loss:2.2925868913413874\n",
      "train loss:2.2979373714609808\n",
      "train loss:2.301741654988463\n",
      "train loss:2.296189200759203\n",
      "train loss:2.299007088440456\n",
      "train loss:2.3008411299636684\n",
      "train loss:2.2947760184575094\n",
      "train loss:2.300325751524935\n",
      "train loss:2.2975469325887157\n",
      "train loss:2.298271319765629\n",
      "train loss:2.2975764081576626\n",
      "train loss:2.2939284558035666\n",
      "train loss:2.3053440750579886\n",
      "train loss:2.300273723131965\n",
      "train loss:2.295091248050836\n",
      "train loss:2.298455454527879\n",
      "train loss:2.306466009128939\n",
      "train loss:2.2914047997765907\n",
      "train loss:2.3017629680065324\n",
      "train loss:2.302447956592241\n",
      "train loss:2.2977149592768034\n",
      "train loss:2.304676372217371\n",
      "train loss:2.304646276119327\n",
      "train loss:2.306072785350871\n",
      "train loss:2.295817830691768\n",
      "train loss:2.297787077072012\n",
      "train loss:2.3057851942737084\n",
      "train loss:2.303963023188049\n",
      "train loss:2.2987885680005324\n",
      "train loss:2.30192533754671\n",
      "train loss:2.2992498350408748\n",
      "train loss:2.301929514455964\n",
      "train loss:2.3047717782009562\n",
      "train loss:2.2994773230574563\n",
      "train loss:2.2965426094143844\n",
      "train loss:2.2969835779920023\n",
      "train loss:2.301384265917865\n",
      "train loss:2.304773710810027\n",
      "train loss:2.3039095476311444\n",
      "train loss:2.3060196770893606\n",
      "train loss:2.2999279569828155\n",
      "train loss:2.3088460679268525\n",
      "train loss:2.3035939089289648\n",
      "train loss:2.302316997501213\n",
      "train loss:2.3050457107856834\n",
      "train loss:2.3071295848559523\n",
      "train loss:2.303121338847019\n",
      "train loss:2.308923243797417\n",
      "train loss:2.296023074192236\n",
      "train loss:2.3042539912888236\n",
      "train loss:2.3048321034595616\n",
      "train loss:2.2947080722995077\n",
      "train loss:2.2885130806998135\n",
      "train loss:2.301857927839462\n",
      "train loss:2.296948623741264\n",
      "train loss:2.2900984035341345\n",
      "train loss:2.2988759280933886\n",
      "train loss:2.304032675277169\n",
      "train loss:2.2977488270495083\n",
      "train loss:2.3019719349355654\n",
      "train loss:2.3083800138052815\n",
      "train loss:2.3021140356033833\n",
      "train loss:2.298392872596821\n",
      "train loss:2.3002521764748654\n",
      "train loss:2.2924880054646986\n",
      "train loss:2.3104724030156025\n",
      "train loss:2.2978726587940246\n",
      "train loss:2.2986848191587455\n",
      "train loss:2.3001784228285045\n",
      "train loss:2.2955538981903723\n",
      "train loss:2.3046573707526767\n",
      "train loss:2.301687372279278\n",
      "train loss:2.301026244658536\n",
      "train loss:2.3018602282586937\n",
      "train loss:2.296497568997563\n",
      "train loss:2.309027240429798\n",
      "train loss:2.297998386319127\n",
      "train loss:2.30059159834592\n",
      "train loss:2.29955346321165\n",
      "train loss:2.3017587158939827\n",
      "train loss:2.2982730852682107\n",
      "train loss:2.292573362738967\n",
      "train loss:2.294916381342997\n",
      "train loss:2.301621941980904\n",
      "train loss:2.29725297956909\n",
      "train loss:2.2981051204424476\n",
      "train loss:2.297963176986565\n",
      "train loss:2.305571547402325\n",
      "train loss:2.29899023367262\n",
      "train loss:2.289392952136386\n",
      "train loss:2.2964253353226933\n",
      "train loss:2.3110856755828255\n",
      "train loss:2.3064588867149194\n",
      "train loss:2.2994569283647257\n",
      "train loss:2.3027836969511934\n",
      "train loss:2.3050190857232375\n",
      "train loss:2.2983340345528065\n",
      "train loss:2.3065432419078378\n",
      "train loss:2.296702123930015\n",
      "train loss:2.303535650963371\n",
      "train loss:2.3033969882476573\n",
      "train loss:2.291241607149243\n",
      "train loss:2.298735164106326\n",
      "train loss:2.290496669350258\n",
      "train loss:2.298880459007463\n",
      "train loss:2.2969400672830087\n",
      "train loss:2.293779677399157\n",
      "train loss:2.3057475338056914\n",
      "train loss:2.297872023736693\n",
      "train loss:2.300581212540023\n",
      "train loss:2.297933609855135\n",
      "train loss:2.307306293429243\n",
      "train loss:2.296864718904319\n",
      "train loss:2.3002260876112146\n",
      "train loss:2.3048126459104097\n",
      "train loss:2.306511963569251\n",
      "train loss:2.2980166429769437\n",
      "train loss:2.2935196534576145\n",
      "train loss:2.2954922950946064\n",
      "train loss:2.3010785833169356\n",
      "train loss:2.2979725951303083\n",
      "train loss:2.3069802322284265\n",
      "train loss:2.299721435723384\n",
      "train loss:2.294954116644274\n",
      "train loss:2.303723601977034\n",
      "train loss:2.308145844969165\n",
      "train loss:2.3014630666743754\n",
      "=== epoch:44, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3077463779973675\n",
      "train loss:2.301586948131684\n",
      "train loss:2.2927249168090147\n",
      "train loss:2.2931360210512852\n",
      "train loss:2.303929333346138\n",
      "train loss:2.3069983917399837\n",
      "train loss:2.293309859016339\n",
      "train loss:2.302513201095692\n",
      "train loss:2.2947278641509996\n",
      "train loss:2.3052984846947573\n",
      "train loss:2.2996899758366802\n",
      "train loss:2.3031042152529535\n",
      "train loss:2.3170257776123537\n",
      "train loss:2.303007173686573\n",
      "train loss:2.298256710774844\n",
      "train loss:2.286164208845751\n",
      "train loss:2.30700801401121\n",
      "train loss:2.3017728214346977\n",
      "train loss:2.308482738286802\n",
      "train loss:2.304179215472145\n",
      "train loss:2.2954480389232685\n",
      "train loss:2.306237047472608\n",
      "train loss:2.2993279645496076\n",
      "train loss:2.2994471228948115\n",
      "train loss:2.3021060017473256\n",
      "train loss:2.2994801283838955\n",
      "train loss:2.3056868894338316\n",
      "train loss:2.3047926981905533\n",
      "train loss:2.3096068576110604\n",
      "train loss:2.3045771033683367\n",
      "train loss:2.3001854542588744\n",
      "train loss:2.3146307819920917\n",
      "train loss:2.3099595963239836\n",
      "train loss:2.3050357802685557\n",
      "train loss:2.298305597267815\n",
      "train loss:2.292504433986899\n",
      "train loss:2.302322531361582\n",
      "train loss:2.3015958754791424\n",
      "train loss:2.3008317274297574\n",
      "train loss:2.2968100687808026\n",
      "train loss:2.2958163720915925\n",
      "train loss:2.2915208480472873\n",
      "train loss:2.301445175245335\n",
      "train loss:2.308504426550457\n",
      "train loss:2.309431350014507\n",
      "train loss:2.2955328321934743\n",
      "train loss:2.292959497349101\n",
      "train loss:2.3038813870497896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2968250801359025\n",
      "train loss:2.309528229468457\n",
      "train loss:2.300048426524904\n",
      "train loss:2.2979212990314637\n",
      "train loss:2.305347407130691\n",
      "train loss:2.3044438686665334\n",
      "train loss:2.297964919003533\n",
      "train loss:2.298817847611062\n",
      "train loss:2.297397306333744\n",
      "train loss:2.290254767153445\n",
      "train loss:2.3016370916836775\n",
      "train loss:2.3126308986228694\n",
      "train loss:2.2964194404752396\n",
      "train loss:2.2992962409344924\n",
      "train loss:2.2863372202471455\n",
      "train loss:2.30409848640361\n",
      "train loss:2.2968272250974584\n",
      "train loss:2.3016596539933794\n",
      "train loss:2.309607687922553\n",
      "train loss:2.2973039441538647\n",
      "train loss:2.299301889488101\n",
      "train loss:2.3016208425682696\n",
      "train loss:2.3070470524512605\n",
      "train loss:2.307737805234429\n",
      "train loss:2.299933246984615\n",
      "train loss:2.300314317822392\n",
      "train loss:2.294770232804231\n",
      "train loss:2.300495805097518\n",
      "train loss:2.300535938332414\n",
      "train loss:2.3046937990379925\n",
      "train loss:2.2970277471107186\n",
      "train loss:2.291990236053483\n",
      "train loss:2.3002458345122276\n",
      "train loss:2.3060424105050514\n",
      "train loss:2.3033450696359807\n",
      "train loss:2.3132318865628854\n",
      "train loss:2.2955562748400795\n",
      "train loss:2.30499020261067\n",
      "train loss:2.2971181026978877\n",
      "train loss:2.3103079568440616\n",
      "train loss:2.2973279941072042\n",
      "train loss:2.3010179860901347\n",
      "train loss:2.3047891687748843\n",
      "train loss:2.3072982010121756\n",
      "train loss:2.2925751848533467\n",
      "train loss:2.2980706534069015\n",
      "train loss:2.3028106592799604\n",
      "train loss:2.30248583182747\n",
      "train loss:2.2983643264744096\n",
      "train loss:2.2999986015044063\n",
      "train loss:2.2986396291203373\n",
      "train loss:2.3067058953786503\n",
      "train loss:2.3050337634619713\n",
      "train loss:2.3090648450248836\n",
      "train loss:2.2989646011191334\n",
      "train loss:2.3031057275304314\n",
      "train loss:2.3009425145377387\n",
      "train loss:2.3020788305157986\n",
      "train loss:2.299346149392305\n",
      "train loss:2.311545293600441\n",
      "train loss:2.304898311231347\n",
      "train loss:2.288955866797676\n",
      "train loss:2.3028606647853547\n",
      "train loss:2.2948599073720626\n",
      "train loss:2.306986203667645\n",
      "train loss:2.3081137337749227\n",
      "train loss:2.308941087003659\n",
      "train loss:2.303443402353727\n",
      "train loss:2.301743589681733\n",
      "train loss:2.317551138812106\n",
      "train loss:2.2937656244033016\n",
      "train loss:2.298822458164319\n",
      "train loss:2.308529703844276\n",
      "train loss:2.3021659520671296\n",
      "train loss:2.3087495280212385\n",
      "train loss:2.2990351329065843\n",
      "train loss:2.297340342673339\n",
      "train loss:2.299303005882008\n",
      "train loss:2.304792673503847\n",
      "train loss:2.3011497580635867\n",
      "train loss:2.297248661037121\n",
      "train loss:2.3022840943954823\n",
      "train loss:2.297528398341065\n",
      "train loss:2.309156571030318\n",
      "train loss:2.3021927518975027\n",
      "train loss:2.2986436893615294\n",
      "train loss:2.304942829634062\n",
      "train loss:2.302779103742251\n",
      "train loss:2.3007988380461604\n",
      "train loss:2.302783131996415\n",
      "train loss:2.2970828416809836\n",
      "train loss:2.309134397309494\n",
      "train loss:2.297388315331834\n",
      "train loss:2.3046405423210605\n",
      "train loss:2.3062233693364615\n",
      "train loss:2.301393331893594\n",
      "train loss:2.2917694888117714\n",
      "train loss:2.2989544359417096\n",
      "train loss:2.30269105145781\n",
      "train loss:2.3006721042047777\n",
      "train loss:2.300326087935435\n",
      "train loss:2.302725349225205\n",
      "train loss:2.303299010329462\n",
      "train loss:2.293780055542365\n",
      "train loss:2.3057705389044147\n",
      "train loss:2.3056486006340324\n",
      "train loss:2.3089801874536824\n",
      "train loss:2.3062626708813156\n",
      "train loss:2.298427721875622\n",
      "train loss:2.2990433529177907\n",
      "train loss:2.307497746993958\n",
      "train loss:2.303014390026132\n",
      "train loss:2.302112917632257\n",
      "train loss:2.303349065821062\n",
      "train loss:2.30281433523948\n",
      "train loss:2.311191788817454\n",
      "train loss:2.2960495142031667\n",
      "train loss:2.303280105197914\n",
      "train loss:2.2976626715593547\n",
      "train loss:2.302045649904104\n",
      "train loss:2.298630646674184\n",
      "train loss:2.304494203280621\n",
      "train loss:2.3083954375640054\n",
      "train loss:2.31305885940527\n",
      "train loss:2.3106039640518965\n",
      "train loss:2.297395204363573\n",
      "train loss:2.3035757420644076\n",
      "train loss:2.3010230132418497\n",
      "train loss:2.301661103629801\n",
      "train loss:2.296055601919963\n",
      "train loss:2.3094413268072\n",
      "train loss:2.2999055807616235\n",
      "train loss:2.301694645779024\n",
      "train loss:2.295525424104559\n",
      "train loss:2.2959962855571012\n",
      "train loss:2.30507585275134\n",
      "train loss:2.3053738786975746\n",
      "train loss:2.2974399854428387\n",
      "train loss:2.305919615067185\n",
      "train loss:2.3089470407190316\n",
      "train loss:2.300183412923205\n",
      "train loss:2.303627093190582\n",
      "train loss:2.2950198576375835\n",
      "train loss:2.303560824792748\n",
      "train loss:2.298320809667307\n",
      "train loss:2.3049645565464596\n",
      "train loss:2.3004222665787277\n",
      "train loss:2.3058167324697467\n",
      "train loss:2.3050501821357035\n",
      "train loss:2.296395266317136\n",
      "train loss:2.291526337607392\n",
      "train loss:2.2939971694395176\n",
      "=== epoch:45, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.309838403386166\n",
      "train loss:2.3085265349619934\n",
      "train loss:2.3135637550898203\n",
      "train loss:2.3023398275937885\n",
      "train loss:2.306906342631197\n",
      "train loss:2.3045274287987785\n",
      "train loss:2.2964196560833448\n",
      "train loss:2.2940176702850956\n",
      "train loss:2.3006498851979167\n",
      "train loss:2.295913780178576\n",
      "train loss:2.3024213414306254\n",
      "train loss:2.3053773870141785\n",
      "train loss:2.2956401464264307\n",
      "train loss:2.3023330968120796\n",
      "train loss:2.294753485162838\n",
      "train loss:2.3051988414472446\n",
      "train loss:2.295399909720124\n",
      "train loss:2.3051930669415364\n",
      "train loss:2.3027770193873756\n",
      "train loss:2.297498176864835\n",
      "train loss:2.298743574447396\n",
      "train loss:2.297580723129728\n",
      "train loss:2.308407269259648\n",
      "train loss:2.3046932155455946\n",
      "train loss:2.3020560861645816\n",
      "train loss:2.291270855571866\n",
      "train loss:2.3014279534876825\n",
      "train loss:2.2950378317270967\n",
      "train loss:2.3031259222689364\n",
      "train loss:2.297143415990187\n",
      "train loss:2.3143236470185253\n",
      "train loss:2.2948843120701885\n",
      "train loss:2.301733217514562\n",
      "train loss:2.301553990442545\n",
      "train loss:2.3023064703177947\n",
      "train loss:2.295369018617426\n",
      "train loss:2.2959940983520104\n",
      "train loss:2.3003319607022195\n",
      "train loss:2.2933302787363106\n",
      "train loss:2.297839541130367\n",
      "train loss:2.2932567614833843\n",
      "train loss:2.3056328310602914\n",
      "train loss:2.2986821045209447\n",
      "train loss:2.2994926672526086\n",
      "train loss:2.301258191997324\n",
      "train loss:2.2985447690225165\n",
      "train loss:2.3067641841211532\n",
      "train loss:2.3080698307179337\n",
      "train loss:2.3080117449359325\n",
      "train loss:2.301547045652044\n",
      "train loss:2.3003826420023157\n",
      "train loss:2.304487344975471\n",
      "train loss:2.3168796493316153\n",
      "train loss:2.302326361011228\n",
      "train loss:2.3093147883454037\n",
      "train loss:2.300606915286726\n",
      "train loss:2.301706639509005\n",
      "train loss:2.306690769641859\n",
      "train loss:2.3014922885086033\n",
      "train loss:2.303636979719787\n",
      "train loss:2.3058544583894744\n",
      "train loss:2.297521626361335\n",
      "train loss:2.3048299201246896\n",
      "train loss:2.2941541409498405\n",
      "train loss:2.298378485441723\n",
      "train loss:2.309146720063696\n",
      "train loss:2.299338538169785\n",
      "train loss:2.29834464906687\n",
      "train loss:2.3102962360062698\n",
      "train loss:2.2985171710215746\n",
      "train loss:2.2949287673135457\n",
      "train loss:2.288676355940788\n",
      "train loss:2.3007530264640343\n",
      "train loss:2.29904499134641\n",
      "train loss:2.2963203516806994\n",
      "train loss:2.2899715292132634\n",
      "train loss:2.2924356617662642\n",
      "train loss:2.3065042011533397\n",
      "train loss:2.302560436561551\n",
      "train loss:2.2954947737324596\n",
      "train loss:2.2998509574392787\n",
      "train loss:2.3050642541950555\n",
      "train loss:2.3130801396069756\n",
      "train loss:2.3040764988448243\n",
      "train loss:2.2970280100885963\n",
      "train loss:2.3023980964580715\n",
      "train loss:2.301767613435406\n",
      "train loss:2.2908998709497417\n",
      "train loss:2.306901452705826\n",
      "train loss:2.2924656407176456\n",
      "train loss:2.298645596234229\n",
      "train loss:2.3017563363903686\n",
      "train loss:2.3041405368730143\n",
      "train loss:2.2981315784847816\n",
      "train loss:2.3056924572809177\n",
      "train loss:2.2960743006027275\n",
      "train loss:2.3053859591436447\n",
      "train loss:2.297805937307947\n",
      "train loss:2.302752895499486\n",
      "train loss:2.3038469478618606\n",
      "train loss:2.295082797632866\n",
      "train loss:2.3041583874488656\n",
      "train loss:2.301795994198545\n",
      "train loss:2.313304845712825\n",
      "train loss:2.2984923999336404\n",
      "train loss:2.297941632416174\n",
      "train loss:2.3053705712711414\n",
      "train loss:2.2969341401624366\n",
      "train loss:2.2949980987631076\n",
      "train loss:2.3053518482362003\n",
      "train loss:2.303882914208719\n",
      "train loss:2.3051015182858476\n",
      "train loss:2.3096566989807377\n",
      "train loss:2.29383121108777\n",
      "train loss:2.3079264814120486\n",
      "train loss:2.2898150703565667\n",
      "train loss:2.3097210645119293\n",
      "train loss:2.2987064068482987\n",
      "train loss:2.3060738615480254\n",
      "train loss:2.3051055843193424\n",
      "train loss:2.298975404577725\n",
      "train loss:2.3029679228446325\n",
      "train loss:2.299404109058248\n",
      "train loss:2.2964144838263834\n",
      "train loss:2.3028363638828915\n",
      "train loss:2.292792409779755\n",
      "train loss:2.296616997990998\n",
      "train loss:2.3111830139567764\n",
      "train loss:2.302411032316228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3008867246702005\n",
      "train loss:2.309147995549848\n",
      "train loss:2.3009888505829084\n",
      "train loss:2.300196723356674\n",
      "train loss:2.3062640258841567\n",
      "train loss:2.3031383705029485\n",
      "train loss:2.304213051622592\n",
      "train loss:2.3080632265222087\n",
      "train loss:2.3038580106245115\n",
      "train loss:2.294914341203654\n",
      "train loss:2.304427901161804\n",
      "train loss:2.3053629200973007\n",
      "train loss:2.303422109347981\n",
      "train loss:2.3068858488581907\n",
      "train loss:2.312018911238368\n",
      "train loss:2.3051336176743726\n",
      "train loss:2.309094770435565\n",
      "train loss:2.2926926624691126\n",
      "train loss:2.3014157082563074\n",
      "train loss:2.304519356354829\n",
      "train loss:2.3096039512314785\n",
      "train loss:2.3017454738143424\n",
      "train loss:2.3080151666071926\n",
      "train loss:2.304968009598831\n",
      "train loss:2.3027834029852157\n",
      "train loss:2.2941465213642043\n",
      "train loss:2.3098713239666306\n",
      "train loss:2.3030120841306276\n",
      "train loss:2.298870782779422\n",
      "train loss:2.3065542747484757\n",
      "train loss:2.3049860965803197\n",
      "train loss:2.29577874988549\n",
      "train loss:2.297949509430225\n",
      "train loss:2.3023131088389683\n",
      "train loss:2.306599676801006\n",
      "train loss:2.299112612702521\n",
      "train loss:2.2953869379646634\n",
      "train loss:2.295985247136959\n",
      "train loss:2.3041454031170554\n",
      "train loss:2.3028024667440445\n",
      "train loss:2.3009635327250204\n",
      "train loss:2.2971227866859945\n",
      "train loss:2.322852872886805\n",
      "train loss:2.2924410788492917\n",
      "train loss:2.312142352532764\n",
      "train loss:2.3056054230502343\n",
      "train loss:2.3022676588013025\n",
      "train loss:2.3031313987197737\n",
      "train loss:2.297110708349296\n",
      "train loss:2.300615652146417\n",
      "train loss:2.2999116970404603\n",
      "train loss:2.309817757489839\n",
      "train loss:2.3096088184053087\n",
      "train loss:2.304453454839624\n",
      "train loss:2.304155937989445\n",
      "train loss:2.2931239291793037\n",
      "train loss:2.2926371534145558\n",
      "train loss:2.296522887644464\n",
      "train loss:2.304803461743107\n",
      "train loss:2.3020672994820806\n",
      "train loss:2.2958225158458836\n",
      "train loss:2.3015288384569823\n",
      "train loss:2.300056197980542\n",
      "train loss:2.3042521772504356\n",
      "train loss:2.293664309421221\n",
      "train loss:2.3019847744183783\n",
      "train loss:2.3003834643842076\n",
      "train loss:2.3008672442109237\n",
      "train loss:2.3030797109443406\n",
      "train loss:2.296827569416802\n",
      "train loss:2.292196771155877\n",
      "=== epoch:46, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3004067462649536\n",
      "train loss:2.3061475749205043\n",
      "train loss:2.2949796673444878\n",
      "train loss:2.304891865273451\n",
      "train loss:2.2987867347134574\n",
      "train loss:2.3005513693368878\n",
      "train loss:2.3043282922037926\n",
      "train loss:2.3022474718488404\n",
      "train loss:2.2978789680912897\n",
      "train loss:2.2976455139892034\n",
      "train loss:2.296638281434108\n",
      "train loss:2.2936143814266114\n",
      "train loss:2.305128836323706\n",
      "train loss:2.3019198528210274\n",
      "train loss:2.301758886351555\n",
      "train loss:2.2976810025488708\n",
      "train loss:2.304883033792482\n",
      "train loss:2.297968139291342\n",
      "train loss:2.3006757200898154\n",
      "train loss:2.2983884022718493\n",
      "train loss:2.3036018913844307\n",
      "train loss:2.303508464898628\n",
      "train loss:2.2983372815486978\n",
      "train loss:2.3047144231342713\n",
      "train loss:2.2924983707593687\n",
      "train loss:2.295194744881109\n",
      "train loss:2.309927867978972\n",
      "train loss:2.299490563723284\n",
      "train loss:2.304136601483798\n",
      "train loss:2.2991237092549914\n",
      "train loss:2.303712840685384\n",
      "train loss:2.295360744702976\n",
      "train loss:2.3009490619095883\n",
      "train loss:2.2994160938599637\n",
      "train loss:2.3029779641428485\n",
      "train loss:2.299086927447595\n",
      "train loss:2.306484996228255\n",
      "train loss:2.303315801296244\n",
      "train loss:2.2954805328912617\n",
      "train loss:2.295098020547216\n",
      "train loss:2.2980991896940908\n",
      "train loss:2.294116767399962\n",
      "train loss:2.304578698161632\n",
      "train loss:2.2950320463727976\n",
      "train loss:2.297737794807833\n",
      "train loss:2.30879683487529\n",
      "train loss:2.308863102605726\n",
      "train loss:2.3025521962583726\n",
      "train loss:2.3062226064576024\n",
      "train loss:2.303638891603115\n",
      "train loss:2.298742753758862\n",
      "train loss:2.30086776408056\n",
      "train loss:2.3047241777738945\n",
      "train loss:2.302929323644079\n",
      "train loss:2.3001570767175012\n",
      "train loss:2.3090419883916993\n",
      "train loss:2.2926372242136948\n",
      "train loss:2.299838252336582\n",
      "train loss:2.2918351813727416\n",
      "train loss:2.3085062888496792\n",
      "train loss:2.300739203924046\n",
      "train loss:2.3024843737861627\n",
      "train loss:2.299202360024336\n",
      "train loss:2.304421241287526\n",
      "train loss:2.2986052370319032\n",
      "train loss:2.3087336433833228\n",
      "train loss:2.302118674068701\n",
      "train loss:2.3003185034018685\n",
      "train loss:2.3034751639890754\n",
      "train loss:2.308833276060948\n",
      "train loss:2.2974235829640137\n",
      "train loss:2.304189066664405\n",
      "train loss:2.3068667567992525\n",
      "train loss:2.3120394201319603\n",
      "train loss:2.303879896340583\n",
      "train loss:2.2931654106165036\n",
      "train loss:2.307376177824722\n",
      "train loss:2.3014480371397172\n",
      "train loss:2.3085408754855954\n",
      "train loss:2.2946082059088813\n",
      "train loss:2.3072983012133146\n",
      "train loss:2.309457721326444\n",
      "train loss:2.3000714940274833\n",
      "train loss:2.3060238813857468\n",
      "train loss:2.2958721849671258\n",
      "train loss:2.297462431303306\n",
      "train loss:2.2989430467618037\n",
      "train loss:2.294532150991396\n",
      "train loss:2.3041772676175536\n",
      "train loss:2.307735995604835\n",
      "train loss:2.3023788923584387\n",
      "train loss:2.302430791706827\n",
      "train loss:2.3019699155635442\n",
      "train loss:2.3030440365076608\n",
      "train loss:2.301330012118471\n",
      "train loss:2.302858792103667\n",
      "train loss:2.305126640830927\n",
      "train loss:2.2979027430712833\n",
      "train loss:2.301425327026523\n",
      "train loss:2.303367355082692\n",
      "train loss:2.3057461756589483\n",
      "train loss:2.3072266759914233\n",
      "train loss:2.2988416687795428\n",
      "train loss:2.3023419468730344\n",
      "train loss:2.296092660470695\n",
      "train loss:2.3038479972957395\n",
      "train loss:2.308933283541643\n",
      "train loss:2.306299501398313\n",
      "train loss:2.2949947402156052\n",
      "train loss:2.298169934202468\n",
      "train loss:2.297945838268933\n",
      "train loss:2.306253509264384\n",
      "train loss:2.3090436637273477\n",
      "train loss:2.2995004925291878\n",
      "train loss:2.3018489566977562\n",
      "train loss:2.310483088790235\n",
      "train loss:2.304543353504642\n",
      "train loss:2.301791590158704\n",
      "train loss:2.2977094717394233\n",
      "train loss:2.3025543541996254\n",
      "train loss:2.3004003611149737\n",
      "train loss:2.298964554343409\n",
      "train loss:2.2953550679968435\n",
      "train loss:2.2996017321670794\n",
      "train loss:2.2917935019067572\n",
      "train loss:2.299102925554857\n",
      "train loss:2.3025719944320984\n",
      "train loss:2.3066596687650516\n",
      "train loss:2.300895268609568\n",
      "train loss:2.303959343436188\n",
      "train loss:2.305299466355266\n",
      "train loss:2.3135416062664613\n",
      "train loss:2.298080256626847\n",
      "train loss:2.29858605929185\n",
      "train loss:2.2926471178892913\n",
      "train loss:2.300007224411473\n",
      "train loss:2.3050670570232286\n",
      "train loss:2.3037893937849936\n",
      "train loss:2.304490559846689\n",
      "train loss:2.3010915273432566\n",
      "train loss:2.3004646833854467\n",
      "train loss:2.30318082472529\n",
      "train loss:2.3008039924475345\n",
      "train loss:2.2954991803482474\n",
      "train loss:2.299098250901311\n",
      "train loss:2.30146838839438\n",
      "train loss:2.3078836912937994\n",
      "train loss:2.2978589220745507\n",
      "train loss:2.294285808349832\n",
      "train loss:2.2938007742440996\n",
      "train loss:2.3055408384991303\n",
      "train loss:2.3069701514758347\n",
      "train loss:2.2970300932339947\n",
      "train loss:2.308541910256329\n",
      "train loss:2.303821696050104\n",
      "train loss:2.3086964987520546\n",
      "train loss:2.298419400264695\n",
      "train loss:2.298198023435422\n",
      "train loss:2.301826041237393\n",
      "train loss:2.2979952739088056\n",
      "train loss:2.3042299478488313\n",
      "train loss:2.3028930675806754\n",
      "train loss:2.2987852726275646\n",
      "train loss:2.299186011942314\n",
      "train loss:2.2961390290217887\n",
      "train loss:2.2883767460423003\n",
      "train loss:2.2865424943070507\n",
      "train loss:2.3010796520390797\n",
      "train loss:2.2979122404787\n",
      "train loss:2.2962907275534397\n",
      "train loss:2.2970902040784815\n",
      "train loss:2.3095110984511265\n",
      "train loss:2.290623497008552\n",
      "train loss:2.305706636574236\n",
      "train loss:2.302054608173162\n",
      "train loss:2.3047631407021907\n",
      "train loss:2.2999643111485506\n",
      "train loss:2.304970181569209\n",
      "train loss:2.307981678052153\n",
      "train loss:2.2951570613225614\n",
      "train loss:2.3021607244731372\n",
      "train loss:2.295665200854008\n",
      "train loss:2.304940219603962\n",
      "train loss:2.3044570288414477\n",
      "train loss:2.307041298611602\n",
      "train loss:2.296978473466154\n",
      "train loss:2.3004085628692525\n",
      "train loss:2.302558270120765\n",
      "train loss:2.2925285535215214\n",
      "train loss:2.3007304247191427\n",
      "train loss:2.305906364460015\n",
      "train loss:2.299843021069095\n",
      "train loss:2.2983572028530705\n",
      "train loss:2.297582877987236\n",
      "train loss:2.3018508014172343\n",
      "train loss:2.2951762877435913\n",
      "train loss:2.2929377130801645\n",
      "train loss:2.2900746814444717\n",
      "train loss:2.301055851630096\n",
      "train loss:2.312781377158993\n",
      "=== epoch:47, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2961923032289384\n",
      "train loss:2.3058029847621944\n",
      "train loss:2.2968825026026893\n",
      "train loss:2.302801168043536\n",
      "train loss:2.301913147571054\n",
      "train loss:2.3059406403101166\n",
      "train loss:2.3015572998424525\n",
      "train loss:2.304844361239437\n",
      "train loss:2.304873473412066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2957093140712255\n",
      "train loss:2.2948701645651948\n",
      "train loss:2.299907748812976\n",
      "train loss:2.299751915735208\n",
      "train loss:2.2980603678820364\n",
      "train loss:2.3102503392333205\n",
      "train loss:2.2996036153274044\n",
      "train loss:2.310241438598967\n",
      "train loss:2.305960896734338\n",
      "train loss:2.3110451245260366\n",
      "train loss:2.3050539648354875\n",
      "train loss:2.301716425733866\n",
      "train loss:2.303059814419573\n",
      "train loss:2.2903477368097303\n",
      "train loss:2.309469529881611\n",
      "train loss:2.3045739551974993\n",
      "train loss:2.301687266833592\n",
      "train loss:2.299007530108311\n",
      "train loss:2.301701197767143\n",
      "train loss:2.3026528910121984\n",
      "train loss:2.298760232670708\n",
      "train loss:2.301619725892621\n",
      "train loss:2.2968063231591413\n",
      "train loss:2.305148820835996\n",
      "train loss:2.2946264390845017\n",
      "train loss:2.307360058446852\n",
      "train loss:2.302880233339074\n",
      "train loss:2.3001637872814458\n",
      "train loss:2.300970286166134\n",
      "train loss:2.301708214247088\n",
      "train loss:2.297445830114855\n",
      "train loss:2.301940274823065\n",
      "train loss:2.3092427292489695\n",
      "train loss:2.3055002037168775\n",
      "train loss:2.2980411319900846\n",
      "train loss:2.305233719026835\n",
      "train loss:2.2970866585199\n",
      "train loss:2.31024554594327\n",
      "train loss:2.3021872253653166\n",
      "train loss:2.3003664633878036\n",
      "train loss:2.2963061648934744\n",
      "train loss:2.2963199886463572\n",
      "train loss:2.305765196657055\n",
      "train loss:2.308001486920062\n",
      "train loss:2.29781354840606\n",
      "train loss:2.3064473653239865\n",
      "train loss:2.3011598326992724\n",
      "train loss:2.3008460556325003\n",
      "train loss:2.2956845596253195\n",
      "train loss:2.3138886216457295\n",
      "train loss:2.3038171940254712\n",
      "train loss:2.2982615076264303\n",
      "train loss:2.3048280831292276\n",
      "train loss:2.293231101381369\n",
      "train loss:2.3074306033132017\n",
      "train loss:2.294020730570741\n",
      "train loss:2.291190524339841\n",
      "train loss:2.3001098469678496\n",
      "train loss:2.301672599285775\n",
      "train loss:2.3021905159741167\n",
      "train loss:2.3058899429485593\n",
      "train loss:2.2941955127353757\n",
      "train loss:2.308720502610212\n",
      "train loss:2.2999271336703844\n",
      "train loss:2.2913404037207976\n",
      "train loss:2.3105500148986104\n",
      "train loss:2.292144840692164\n",
      "train loss:2.3101237662176906\n",
      "train loss:2.2934293408975974\n",
      "train loss:2.3102730880576607\n",
      "train loss:2.2942026556236925\n",
      "train loss:2.299501600404388\n",
      "train loss:2.3024922715555936\n",
      "train loss:2.3050526733659895\n",
      "train loss:2.306031735274935\n",
      "train loss:2.307032167514463\n",
      "train loss:2.301817004468864\n",
      "train loss:2.3043244409425134\n",
      "train loss:2.3144010774427666\n",
      "train loss:2.3053327001036683\n",
      "train loss:2.3042043700767216\n",
      "train loss:2.3016708110748807\n",
      "train loss:2.3000698262019568\n",
      "train loss:2.3048242987857677\n",
      "train loss:2.3079937305934606\n",
      "train loss:2.301913730919969\n",
      "train loss:2.3036325506199478\n",
      "train loss:2.299347059400307\n",
      "train loss:2.311829729810946\n",
      "train loss:2.299603596843349\n",
      "train loss:2.305529638611261\n",
      "train loss:2.306157217249466\n",
      "train loss:2.2970672843027273\n",
      "train loss:2.2977375328337604\n",
      "train loss:2.3014922809051104\n",
      "train loss:2.3032313554462176\n",
      "train loss:2.30596601277071\n",
      "train loss:2.3065341683082186\n",
      "train loss:2.300665575901715\n",
      "train loss:2.29972459023777\n",
      "train loss:2.3031865842192296\n",
      "train loss:2.304441717910787\n",
      "train loss:2.3058091579783\n",
      "train loss:2.297497607645234\n",
      "train loss:2.3027334764807432\n",
      "train loss:2.3036288529346027\n",
      "train loss:2.303210881266635\n",
      "train loss:2.311650069224607\n",
      "train loss:2.298442277186471\n",
      "train loss:2.2991282701316336\n",
      "train loss:2.3035422098629823\n",
      "train loss:2.3101001261435363\n",
      "train loss:2.3007548409195233\n",
      "train loss:2.30445258377797\n",
      "train loss:2.2931628574834946\n",
      "train loss:2.298740335665998\n",
      "train loss:2.2994870232154994\n",
      "train loss:2.2999419564660193\n",
      "train loss:2.295527587002876\n",
      "train loss:2.2922035561054455\n",
      "train loss:2.3040743379933475\n",
      "train loss:2.2985324479656293\n",
      "train loss:2.3170544817210565\n",
      "train loss:2.3024320005259584\n",
      "train loss:2.3032644063915715\n",
      "train loss:2.3072316225473384\n",
      "train loss:2.3048121902150536\n",
      "train loss:2.302826362539739\n",
      "train loss:2.3049661539384196\n",
      "train loss:2.303500879428612\n",
      "train loss:2.3065245251711195\n",
      "train loss:2.2991178600018536\n",
      "train loss:2.3013737570523487\n",
      "train loss:2.3038824188306934\n",
      "train loss:2.2993053463080524\n",
      "train loss:2.293257880804214\n",
      "train loss:2.3071556227106096\n",
      "train loss:2.3017922909064987\n",
      "train loss:2.3057620622499724\n",
      "train loss:2.2958694030915185\n",
      "train loss:2.2969380041722887\n",
      "train loss:2.302411897113729\n",
      "train loss:2.3088385291354747\n",
      "train loss:2.301885098148787\n",
      "train loss:2.30434893507462\n",
      "train loss:2.3069437110218174\n",
      "train loss:2.2980313507829693\n",
      "train loss:2.3023541131349097\n",
      "train loss:2.304787595945189\n",
      "train loss:2.293750266228937\n",
      "train loss:2.2962424602406033\n",
      "train loss:2.2985839316503918\n",
      "train loss:2.299710791231245\n",
      "train loss:2.302649161630933\n",
      "train loss:2.2998974530306087\n",
      "train loss:2.3006424636611738\n",
      "train loss:2.299313069877227\n",
      "train loss:2.3037404997933137\n",
      "train loss:2.3037751612034434\n",
      "train loss:2.286732475865773\n",
      "train loss:2.308753430549778\n",
      "train loss:2.294721842002576\n",
      "train loss:2.2961302130869106\n",
      "train loss:2.306434768558766\n",
      "train loss:2.295840361351865\n",
      "train loss:2.3094440576115165\n",
      "train loss:2.2992231814044417\n",
      "train loss:2.299238534941905\n",
      "train loss:2.305323888917496\n",
      "train loss:2.304819396413569\n",
      "train loss:2.3092784148327525\n",
      "train loss:2.3050924773782966\n",
      "train loss:2.297802544331689\n",
      "train loss:2.2972616903536145\n",
      "train loss:2.29408282061046\n",
      "train loss:2.3052893762461686\n",
      "train loss:2.2920094520596512\n",
      "train loss:2.3023331057494114\n",
      "train loss:2.29564323068313\n",
      "train loss:2.302421123324631\n",
      "train loss:2.298531600056804\n",
      "train loss:2.298187678279893\n",
      "train loss:2.299551232140664\n",
      "train loss:2.2985030317073787\n",
      "train loss:2.3091896507387046\n",
      "train loss:2.301922395161284\n",
      "train loss:2.29467959519482\n",
      "train loss:2.3001986079203562\n",
      "train loss:2.298541703438768\n",
      "train loss:2.311987953769439\n",
      "train loss:2.30408356307277\n",
      "=== epoch:48, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.306383132507025\n",
      "train loss:2.309936954651131\n",
      "train loss:2.3116903390081722\n",
      "train loss:2.305283798692916\n",
      "train loss:2.3027578910966096\n",
      "train loss:2.3083324023367293\n",
      "train loss:2.289932966328329\n",
      "train loss:2.2951282350645403\n",
      "train loss:2.3001750545688195\n",
      "train loss:2.303706590211541\n",
      "train loss:2.3054084685647815\n",
      "train loss:2.297702038567541\n",
      "train loss:2.2934905008935154\n",
      "train loss:2.307320310032403\n",
      "train loss:2.2985327299126608\n",
      "train loss:2.3028501832550443\n",
      "train loss:2.29846345386801\n",
      "train loss:2.302880437487442\n",
      "train loss:2.3025927491053606\n",
      "train loss:2.2978666320433625\n",
      "train loss:2.298063889958015\n",
      "train loss:2.296372071523854\n",
      "train loss:2.3071085277531433\n",
      "train loss:2.3070115652990513\n",
      "train loss:2.299122625495169\n",
      "train loss:2.3003662375103264\n",
      "train loss:2.298436868812639\n",
      "train loss:2.3093833395736776\n",
      "train loss:2.2979350812231116\n",
      "train loss:2.293319429429565\n",
      "train loss:2.2975282662097025\n",
      "train loss:2.301503552980606\n",
      "train loss:2.3042934675676983\n",
      "train loss:2.304753412470476\n",
      "train loss:2.293769680881169\n",
      "train loss:2.2968631002752056\n",
      "train loss:2.297753259668175\n",
      "train loss:2.307523168761022\n",
      "train loss:2.3059708574729534\n",
      "train loss:2.3021070264308703\n",
      "train loss:2.3143317137527766\n",
      "train loss:2.2993265192252457\n",
      "train loss:2.299930478512151\n",
      "train loss:2.3029818782924036\n",
      "train loss:2.308072480078051\n",
      "train loss:2.2994386569860623\n",
      "train loss:2.2921668349917046\n",
      "train loss:2.300884754499633\n",
      "train loss:2.304114700926784\n",
      "train loss:2.3064911538755712\n",
      "train loss:2.3108610773472735\n",
      "train loss:2.2987290962026306\n",
      "train loss:2.299504821053655\n",
      "train loss:2.292416467736891\n",
      "train loss:2.301453890176353\n",
      "train loss:2.3033706803916485\n",
      "train loss:2.2945612192310842\n",
      "train loss:2.295487407890199\n",
      "train loss:2.3015242112665715\n",
      "train loss:2.298678840385741\n",
      "train loss:2.3039607172832395\n",
      "train loss:2.313551162248338\n",
      "train loss:2.3078476438934774\n",
      "train loss:2.297641038226786\n",
      "train loss:2.2987111952964847\n",
      "train loss:2.297280796637984\n",
      "train loss:2.3066733146346543\n",
      "train loss:2.2994676673167564\n",
      "train loss:2.2952134408516622\n",
      "train loss:2.2905420708110107\n",
      "train loss:2.2991317631029644\n",
      "train loss:2.3008793821955775\n",
      "train loss:2.2924621194256\n",
      "train loss:2.303704686768685\n",
      "train loss:2.298597859063438\n",
      "train loss:2.294487677937055\n",
      "train loss:2.2960444082078344\n",
      "train loss:2.3087738333549006\n",
      "train loss:2.304471590490099\n",
      "train loss:2.3036673515956996\n",
      "train loss:2.30391572745015\n",
      "train loss:2.3062401470635785\n",
      "train loss:2.304697487248975\n",
      "train loss:2.2974522771883983\n",
      "train loss:2.3050128576353885\n",
      "train loss:2.29993274137097\n",
      "train loss:2.2958204124558095\n",
      "train loss:2.3054438751019477\n",
      "train loss:2.2922828342377235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.305320428068317\n",
      "train loss:2.301054123620936\n",
      "train loss:2.3012635494234774\n",
      "train loss:2.303840586276294\n",
      "train loss:2.2974727806862822\n",
      "train loss:2.298888251755289\n",
      "train loss:2.2954663727606714\n",
      "train loss:2.301957863507075\n",
      "train loss:2.2973480166481473\n",
      "train loss:2.296574983938203\n",
      "train loss:2.29795235288719\n",
      "train loss:2.2970857955165984\n",
      "train loss:2.3000200917453832\n",
      "train loss:2.3065329102603287\n",
      "train loss:2.2937469245690334\n",
      "train loss:2.297848810823645\n",
      "train loss:2.30158263693401\n",
      "train loss:2.2902244578426614\n",
      "train loss:2.296420564775912\n",
      "train loss:2.3058693519039455\n",
      "train loss:2.2982941299662083\n",
      "train loss:2.3001501709382643\n",
      "train loss:2.29404489433588\n",
      "train loss:2.303681553145454\n",
      "train loss:2.3011942064491118\n",
      "train loss:2.304014206518316\n",
      "train loss:2.305967280320283\n",
      "train loss:2.302417741773652\n",
      "train loss:2.3043081253647983\n",
      "train loss:2.2948627956156065\n",
      "train loss:2.3060500935569315\n",
      "train loss:2.296742314992191\n",
      "train loss:2.30160059944037\n",
      "train loss:2.3044913128100912\n",
      "train loss:2.3054835573242225\n",
      "train loss:2.3009949266041656\n",
      "train loss:2.2945218143638217\n",
      "train loss:2.297402189515773\n",
      "train loss:2.308258697897841\n",
      "train loss:2.303406348970867\n",
      "train loss:2.3001760372957163\n",
      "train loss:2.302900196785029\n",
      "train loss:2.2994562035446364\n",
      "train loss:2.3051555332536546\n",
      "train loss:2.300531061945463\n",
      "train loss:2.303509616901527\n",
      "train loss:2.293797938355457\n",
      "train loss:2.301576222476638\n",
      "train loss:2.3078997869583886\n",
      "train loss:2.300716279615706\n",
      "train loss:2.301818661205065\n",
      "train loss:2.306340383778743\n",
      "train loss:2.2996656369978288\n",
      "train loss:2.3031981572447795\n",
      "train loss:2.2992351645731675\n",
      "train loss:2.3065787501229176\n",
      "train loss:2.2968779239563077\n",
      "train loss:2.3104644272165578\n",
      "train loss:2.296578647260375\n",
      "train loss:2.301063490036261\n",
      "train loss:2.3064402495274474\n",
      "train loss:2.288873867793549\n",
      "train loss:2.301194448591288\n",
      "train loss:2.301123056857525\n",
      "train loss:2.308173115322024\n",
      "train loss:2.305884938274155\n",
      "train loss:2.295392686182013\n",
      "train loss:2.2982729678753406\n",
      "train loss:2.312288476082857\n",
      "train loss:2.2989850264210157\n",
      "train loss:2.2983322080044677\n",
      "train loss:2.301742651652277\n",
      "train loss:2.3095998658843757\n",
      "train loss:2.3079944261890244\n",
      "train loss:2.3039699534320697\n",
      "train loss:2.29931476125325\n",
      "train loss:2.3016779063162947\n",
      "train loss:2.3033755788002215\n",
      "train loss:2.3042726678691823\n",
      "train loss:2.2924389972766304\n",
      "train loss:2.2992009853885826\n",
      "train loss:2.307662974282081\n",
      "train loss:2.3037981714330993\n",
      "train loss:2.295770723040207\n",
      "train loss:2.3030950824238303\n",
      "train loss:2.295679039651221\n",
      "train loss:2.296456653375027\n",
      "train loss:2.3028131573403314\n",
      "train loss:2.299871888152173\n",
      "train loss:2.291790471064776\n",
      "train loss:2.302103519660241\n",
      "train loss:2.301589374721381\n",
      "train loss:2.300929099337355\n",
      "train loss:2.2932674631766745\n",
      "train loss:2.2930902917598828\n",
      "train loss:2.2986095793896273\n",
      "train loss:2.3035344438467136\n",
      "train loss:2.2925055071038845\n",
      "train loss:2.3137094096539212\n",
      "train loss:2.3046706354862905\n",
      "train loss:2.2994435165611242\n",
      "train loss:2.3064161780484373\n",
      "train loss:2.305537002637762\n",
      "train loss:2.3022863668109275\n",
      "train loss:2.3043477446359204\n",
      "train loss:2.2984472184033455\n",
      "train loss:2.3138768347507086\n",
      "train loss:2.309746240282045\n",
      "train loss:2.2949127638619395\n",
      "train loss:2.3002077640706866\n",
      "train loss:2.2988625785173515\n",
      "=== epoch:49, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2995206790084985\n",
      "train loss:2.2942159168831453\n",
      "train loss:2.293921062268554\n",
      "train loss:2.302445615869703\n",
      "train loss:2.305967399966174\n",
      "train loss:2.305848447529049\n",
      "train loss:2.2976387969426755\n",
      "train loss:2.307107042485963\n",
      "train loss:2.2965211688198663\n",
      "train loss:2.308450581074312\n",
      "train loss:2.302177999621789\n",
      "train loss:2.301898281990388\n",
      "train loss:2.299600287905202\n",
      "train loss:2.3021846070468794\n",
      "train loss:2.3026746914238725\n",
      "train loss:2.304780061969142\n",
      "train loss:2.2958960936330874\n",
      "train loss:2.300958928408978\n",
      "train loss:2.3039347623912505\n",
      "train loss:2.2941412588101193\n",
      "train loss:2.306302518133068\n",
      "train loss:2.300897221800195\n",
      "train loss:2.301235316057417\n",
      "train loss:2.3053491203565426\n",
      "train loss:2.3089280140566197\n",
      "train loss:2.296033272286276\n",
      "train loss:2.2972252082213425\n",
      "train loss:2.3034528000316903\n",
      "train loss:2.3028329298271215\n",
      "train loss:2.294022340078356\n",
      "train loss:2.2960940887709613\n",
      "train loss:2.3061931436078007\n",
      "train loss:2.3030506214744344\n",
      "train loss:2.2974402850237556\n",
      "train loss:2.3011981681711613\n",
      "train loss:2.3008087960497883\n",
      "train loss:2.306616276736597\n",
      "train loss:2.2967324378799567\n",
      "train loss:2.298066461214612\n",
      "train loss:2.3085019819423818\n",
      "train loss:2.3063663345419214\n",
      "train loss:2.3034453971594515\n",
      "train loss:2.2935655189001802\n",
      "train loss:2.2968824986840484\n",
      "train loss:2.2995699923565365\n",
      "train loss:2.298689617668765\n",
      "train loss:2.3003506960023796\n",
      "train loss:2.3055938169751578\n",
      "train loss:2.302159506389354\n",
      "train loss:2.3033240807314734\n",
      "train loss:2.30113701790271\n",
      "train loss:2.3050761509149313\n",
      "train loss:2.3048511210994866\n",
      "train loss:2.3010020856921125\n",
      "train loss:2.3049650620500954\n",
      "train loss:2.3012716164701605\n",
      "train loss:2.2967511406213625\n",
      "train loss:2.3030827009414874\n",
      "train loss:2.302344186402838\n",
      "train loss:2.3026201621257787\n",
      "train loss:2.3000436787677687\n",
      "train loss:2.299747391665417\n",
      "train loss:2.3019262201503525\n",
      "train loss:2.304114793841317\n",
      "train loss:2.302162236150429\n",
      "train loss:2.297560441379551\n",
      "train loss:2.2935701491722926\n",
      "train loss:2.302669177111891\n",
      "train loss:2.3019949132419817\n",
      "train loss:2.296497526240726\n",
      "train loss:2.2998008015382605\n",
      "train loss:2.308408680987862\n",
      "train loss:2.3012834624042426\n",
      "train loss:2.296467186367459\n",
      "train loss:2.294957936753625\n",
      "train loss:2.3099371410635583\n",
      "train loss:2.2954587134886286\n",
      "train loss:2.297787438286526\n",
      "train loss:2.3093126457214126\n",
      "train loss:2.303366690865357\n",
      "train loss:2.297721983313413\n",
      "train loss:2.2939091552086914\n",
      "train loss:2.30679925368074\n",
      "train loss:2.292411630827566\n",
      "train loss:2.3042504593626942\n",
      "train loss:2.297461103560891\n",
      "train loss:2.3054999315139804\n",
      "train loss:2.3059989211395076\n",
      "train loss:2.3006360390927103\n",
      "train loss:2.294851437244549\n",
      "train loss:2.30728264440617\n",
      "train loss:2.3068560142932375\n",
      "train loss:2.3110571978408236\n",
      "train loss:2.3049637295823056\n",
      "train loss:2.2951739815516263\n",
      "train loss:2.2975946689376525\n",
      "train loss:2.295618218643892\n",
      "train loss:2.2960529908951104\n",
      "train loss:2.3046444129442203\n",
      "train loss:2.293767580698139\n",
      "train loss:2.304120896506246\n",
      "train loss:2.293172609830735\n",
      "train loss:2.304007244166318\n",
      "train loss:2.303128989564069\n",
      "train loss:2.29817907314498\n",
      "train loss:2.3054761817577902\n",
      "train loss:2.2948505508327455\n",
      "train loss:2.3040059654847966\n",
      "train loss:2.301882335405357\n",
      "train loss:2.308730731961105\n",
      "train loss:2.3053381807101845\n",
      "train loss:2.3089071717817062\n",
      "train loss:2.301925918276739\n",
      "train loss:2.2978025317029536\n",
      "train loss:2.3018583062447324\n",
      "train loss:2.3039559617331102\n",
      "train loss:2.2982974568144625\n",
      "train loss:2.309975684340743\n",
      "train loss:2.301022802679174\n",
      "train loss:2.300769372297132\n",
      "train loss:2.305062608646645\n",
      "train loss:2.2994595824133905\n",
      "train loss:2.2964299465588693\n",
      "train loss:2.304886052341596\n",
      "train loss:2.2994995320445475\n",
      "train loss:2.2911265586830427\n",
      "train loss:2.314937638895047\n",
      "train loss:2.3008528254513143\n",
      "train loss:2.2942763809619247\n",
      "train loss:2.3030837197064087\n",
      "train loss:2.3035110737094424\n",
      "train loss:2.285403490760731\n",
      "train loss:2.2974358513015516\n",
      "train loss:2.2996131132524393\n",
      "train loss:2.29703050168535\n",
      "train loss:2.305356633782953\n",
      "train loss:2.293975361179499\n",
      "train loss:2.304979925357839\n",
      "train loss:2.2972520724963763\n",
      "train loss:2.3043107853204923\n",
      "train loss:2.291600803695739\n",
      "train loss:2.296291376491033\n",
      "train loss:2.3021188748044223\n",
      "train loss:2.3067527802887207\n",
      "train loss:2.308503275011289\n",
      "train loss:2.2999485532974195\n",
      "train loss:2.296675972599304\n",
      "train loss:2.304897162750352\n",
      "train loss:2.302269374108154\n",
      "train loss:2.299284275589122\n",
      "train loss:2.3065996102447266\n",
      "train loss:2.3019175549288047\n",
      "train loss:2.305443597120369\n",
      "train loss:2.3102785540620507\n",
      "train loss:2.3095348209061894\n",
      "train loss:2.3011455102874407\n",
      "train loss:2.30208296962461\n",
      "train loss:2.301468372533654\n",
      "train loss:2.307480481038652\n",
      "train loss:2.3040341636913855\n",
      "train loss:2.2972655655697904\n",
      "train loss:2.299390370554774\n",
      "train loss:2.3016224499814544\n",
      "train loss:2.308290420765856\n",
      "train loss:2.296794824671412\n",
      "train loss:2.3155804485088254\n",
      "train loss:2.3051694286706437\n",
      "train loss:2.301989086982683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3014928490571505\n",
      "train loss:2.3058214788154507\n",
      "train loss:2.3116914764270846\n",
      "train loss:2.30253203042585\n",
      "train loss:2.3002721300261504\n",
      "train loss:2.3015242278793058\n",
      "train loss:2.2944878841726086\n",
      "train loss:2.305676960601118\n",
      "train loss:2.297736972940652\n",
      "train loss:2.305700342211338\n",
      "train loss:2.2934864858968913\n",
      "train loss:2.3071145095317096\n",
      "train loss:2.295546147930992\n",
      "train loss:2.299108554347801\n",
      "train loss:2.305049463972373\n",
      "train loss:2.2996482996821173\n",
      "train loss:2.297313230016765\n",
      "train loss:2.3036357173139512\n",
      "train loss:2.2981387760964305\n",
      "train loss:2.2979031811419124\n",
      "train loss:2.2948641251015682\n",
      "train loss:2.3099703754344416\n",
      "train loss:2.3008274296731126\n",
      "train loss:2.288560641507756\n",
      "train loss:2.299989573927172\n",
      "train loss:2.3048811292307696\n",
      "train loss:2.3009440702477417\n",
      "train loss:2.3044826398042786\n",
      "train loss:2.301418621566549\n",
      "train loss:2.297850945648777\n",
      "train loss:2.3005967493126582\n",
      "train loss:2.306621663226603\n",
      "=== epoch:50, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3056397531494297\n",
      "train loss:2.297461653271256\n",
      "train loss:2.297227643758916\n",
      "train loss:2.3006277900176144\n",
      "train loss:2.302706891005268\n",
      "train loss:2.3147622513403796\n",
      "train loss:2.3018401890057034\n",
      "train loss:2.306825023221311\n",
      "train loss:2.3003691239162882\n",
      "train loss:2.301825301009036\n",
      "train loss:2.301184670547541\n",
      "train loss:2.301225407096142\n",
      "train loss:2.3024260621308157\n",
      "train loss:2.3039006334502954\n",
      "train loss:2.304519122909419\n",
      "train loss:2.294321682817429\n",
      "train loss:2.3139925038583242\n",
      "train loss:2.302845197711847\n",
      "train loss:2.3003970949079187\n",
      "train loss:2.3018864461836426\n",
      "train loss:2.304012401219188\n",
      "train loss:2.300589943394577\n",
      "train loss:2.302318327984959\n",
      "train loss:2.2963197990209103\n",
      "train loss:2.2990054933444304\n",
      "train loss:2.301455376114271\n",
      "train loss:2.3065328859085223\n",
      "train loss:2.3007796702230507\n",
      "train loss:2.308346336664765\n",
      "train loss:2.3075237156369535\n",
      "train loss:2.306763418559339\n",
      "train loss:2.3080513507670397\n",
      "train loss:2.3037601875045155\n",
      "train loss:2.295193424865958\n",
      "train loss:2.296088133909309\n",
      "train loss:2.3049090137297963\n",
      "train loss:2.292441805855959\n",
      "train loss:2.299727285112597\n",
      "train loss:2.305496618138513\n",
      "train loss:2.29779803412917\n",
      "train loss:2.296190352561543\n",
      "train loss:2.2972012334367276\n",
      "train loss:2.30174298258935\n",
      "train loss:2.3061882729542478\n",
      "train loss:2.295029783930543\n",
      "train loss:2.3019763112860208\n",
      "train loss:2.3063711147273285\n",
      "train loss:2.3133811335074634\n",
      "train loss:2.298807962703002\n",
      "train loss:2.2991663133432523\n",
      "train loss:2.310515363293244\n",
      "train loss:2.3067624162079055\n",
      "train loss:2.3083999626098124\n",
      "train loss:2.291427157128127\n",
      "train loss:2.3008922139782015\n",
      "train loss:2.2908685664737165\n",
      "train loss:2.3099389397090313\n",
      "train loss:2.2987017972590453\n",
      "train loss:2.3003732770510568\n",
      "train loss:2.2997528415395947\n",
      "train loss:2.290431790883973\n",
      "train loss:2.294707820095495\n",
      "train loss:2.3096169282629964\n",
      "train loss:2.2966058876858217\n",
      "train loss:2.3013908848527365\n",
      "train loss:2.296395631801382\n",
      "train loss:2.3030694223373596\n",
      "train loss:2.3065105687384326\n",
      "train loss:2.300405905717653\n",
      "train loss:2.299702163873699\n",
      "train loss:2.3029451945575805\n",
      "train loss:2.3112663081338978\n",
      "train loss:2.3107893927791356\n",
      "train loss:2.306713067465501\n",
      "train loss:2.3004277127390305\n",
      "train loss:2.3110838158756124\n",
      "train loss:2.3004376412378074\n",
      "train loss:2.3016310907885207\n",
      "train loss:2.3084561929807257\n",
      "train loss:2.302977493670894\n",
      "train loss:2.3044990743662086\n",
      "train loss:2.3078024671371273\n",
      "train loss:2.2998276995209386\n",
      "train loss:2.3044780160611866\n",
      "train loss:2.2986714478605847\n",
      "train loss:2.2959561280053165\n",
      "train loss:2.3002384755845213\n",
      "train loss:2.299898190545737\n",
      "train loss:2.301497062715501\n",
      "train loss:2.3011372227179465\n",
      "train loss:2.300250942974912\n",
      "train loss:2.301606851900208\n",
      "train loss:2.3074057268175565\n",
      "train loss:2.3042825369219426\n",
      "train loss:2.3005994150549967\n",
      "train loss:2.2983807307321653\n",
      "train loss:2.303984768713269\n",
      "train loss:2.3036987763224066\n",
      "train loss:2.308998507593211\n",
      "train loss:2.303108885387533\n",
      "train loss:2.310981277220332\n",
      "train loss:2.3042132658135923\n",
      "train loss:2.3019968661950543\n",
      "train loss:2.3012065785843454\n",
      "train loss:2.3026408389266013\n",
      "train loss:2.303817852920735\n",
      "train loss:2.3057241499283645\n",
      "train loss:2.3033373552679968\n",
      "train loss:2.310017720042053\n",
      "train loss:2.2958864593043558\n",
      "train loss:2.303974219966632\n",
      "train loss:2.297769792136647\n",
      "train loss:2.3004323356864558\n",
      "train loss:2.303480517206098\n",
      "train loss:2.288893884511141\n",
      "train loss:2.3015878598563773\n",
      "train loss:2.303439327043372\n",
      "train loss:2.302595932927233\n",
      "train loss:2.2957102843596915\n",
      "train loss:2.302289861335946\n",
      "train loss:2.2966265215586064\n",
      "train loss:2.3032386942510614\n",
      "train loss:2.300429879866776\n",
      "train loss:2.295152102847013\n",
      "train loss:2.2894990244562274\n",
      "train loss:2.3064350023280693\n",
      "train loss:2.299031710101708\n",
      "train loss:2.3011827177175035\n",
      "train loss:2.304955062562009\n",
      "train loss:2.2949370852113247\n",
      "train loss:2.298440666542608\n",
      "train loss:2.2908022772457994\n",
      "train loss:2.307780065245658\n",
      "train loss:2.296683574364355\n",
      "train loss:2.3026716959287405\n",
      "train loss:2.3017671917436417\n",
      "train loss:2.290463456699252\n",
      "train loss:2.2987120566514543\n",
      "train loss:2.3033060692687983\n",
      "train loss:2.3030062913173395\n",
      "train loss:2.301819982237357\n",
      "train loss:2.3069985247751896\n",
      "train loss:2.3021370746343495\n",
      "train loss:2.2859091042995985\n",
      "train loss:2.309321108682136\n",
      "train loss:2.3025542863655493\n",
      "train loss:2.312943496571077\n",
      "train loss:2.3022520930575037\n",
      "train loss:2.305508646981575\n",
      "train loss:2.293804962331141\n",
      "train loss:2.3021715779874503\n",
      "train loss:2.303691343191326\n",
      "train loss:2.304460382198858\n",
      "train loss:2.3040887022967516\n",
      "train loss:2.3008387785181443\n",
      "train loss:2.304480971305717\n",
      "train loss:2.2960842781070205\n",
      "train loss:2.300119033849918\n",
      "train loss:2.302347799706654\n",
      "train loss:2.3076815985316275\n",
      "train loss:2.297448565940422\n",
      "train loss:2.30176359029984\n",
      "train loss:2.305166102206478\n",
      "train loss:2.300909191946128\n",
      "train loss:2.306655939342619\n",
      "train loss:2.300512428906064\n",
      "train loss:2.3042054855627097\n",
      "train loss:2.3001778639143535\n",
      "train loss:2.3116490620254657\n",
      "train loss:2.3026796558965543\n",
      "train loss:2.3072091192510666\n",
      "train loss:2.301954123278084\n",
      "train loss:2.305584603136872\n",
      "train loss:2.3059579330276514\n",
      "train loss:2.304514522532322\n",
      "train loss:2.302587152722713\n",
      "train loss:2.3047970988600586\n",
      "train loss:2.3039791203714834\n",
      "train loss:2.3024727202978763\n",
      "train loss:2.305029774509682\n",
      "train loss:2.2993797276778167\n",
      "train loss:2.30563030794749\n",
      "train loss:2.3007729669811083\n",
      "train loss:2.3021729968089093\n",
      "train loss:2.2987685935357076\n",
      "train loss:2.298406465155911\n",
      "train loss:2.303221177483829\n",
      "train loss:2.2963216997866707\n",
      "train loss:2.298228059468666\n",
      "train loss:2.308733166995554\n",
      "train loss:2.3021850232616052\n",
      "train loss:2.30602147328312\n",
      "train loss:2.3034494027136487\n",
      "train loss:2.3033927255825826\n",
      "train loss:2.3102498747275257\n",
      "train loss:2.3007898527178114\n",
      "train loss:2.3034722578383504\n",
      "train loss:2.3068940331460985\n",
      "train loss:2.303641247765337\n",
      "train loss:2.2995033232101685\n",
      "=== epoch:51, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.303640321448005\n",
      "train loss:2.297166901939096\n",
      "train loss:2.3077511296844704\n",
      "train loss:2.3072052325296024\n",
      "train loss:2.300081363585702\n",
      "train loss:2.299942569216981\n",
      "train loss:2.29879678919514\n",
      "train loss:2.297883715741335\n",
      "train loss:2.3049166123699667\n",
      "train loss:2.3069275988425155\n",
      "train loss:2.2935188381569658\n",
      "train loss:2.3028341919147177\n",
      "train loss:2.3010934276919794\n",
      "train loss:2.303934305520079\n",
      "train loss:2.3003393936917513\n",
      "train loss:2.296918430135338\n",
      "train loss:2.290304936024553\n",
      "train loss:2.3064548512001326\n",
      "train loss:2.2964779900433316\n",
      "train loss:2.297248765947334\n",
      "train loss:2.2969897781908326\n",
      "train loss:2.3084970321777973\n",
      "train loss:2.302722587729446\n",
      "train loss:2.310593236491787\n",
      "train loss:2.296637631112011\n",
      "train loss:2.294954005738627\n",
      "train loss:2.3056892840632406\n",
      "train loss:2.2918934772698107\n",
      "train loss:2.3059131658778\n",
      "train loss:2.2918798509359983\n",
      "train loss:2.2946319372521096\n",
      "train loss:2.306058805626211\n",
      "train loss:2.307144559204697\n",
      "train loss:2.3045194179393445\n",
      "train loss:2.296699610144151\n",
      "train loss:2.299218764732108\n",
      "train loss:2.3077916531598612\n",
      "train loss:2.2997379570278853\n",
      "train loss:2.302271567572311\n",
      "train loss:2.307262636174327\n",
      "train loss:2.307508735190776\n",
      "train loss:2.2939945932676675\n",
      "train loss:2.3049068748706243\n",
      "train loss:2.302445865365342\n",
      "train loss:2.2950736982013535\n",
      "train loss:2.307564536388879\n",
      "train loss:2.303957033357028\n",
      "train loss:2.298139126688676\n",
      "train loss:2.2966431074672653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.306212016892649\n",
      "train loss:2.2936984703777186\n",
      "train loss:2.3059950008563392\n",
      "train loss:2.3024383395287624\n",
      "train loss:2.3026103458349922\n",
      "train loss:2.2949286795578905\n",
      "train loss:2.2963257108250863\n",
      "train loss:2.29623978276754\n",
      "train loss:2.299286227734682\n",
      "train loss:2.303548075061893\n",
      "train loss:2.297676509272565\n",
      "train loss:2.29655322179533\n",
      "train loss:2.3030359782082632\n",
      "train loss:2.3049154238829157\n",
      "train loss:2.3028700832675306\n",
      "train loss:2.284000038227485\n",
      "train loss:2.2964324396558706\n",
      "train loss:2.3027675044174343\n",
      "train loss:2.2974199854527306\n",
      "train loss:2.2979289483746292\n",
      "train loss:2.3022395446934487\n",
      "train loss:2.3085250881125843\n",
      "train loss:2.287899732008169\n",
      "train loss:2.2927464351786675\n",
      "train loss:2.3011098551136766\n",
      "train loss:2.2907292166789395\n",
      "train loss:2.288162377109886\n",
      "train loss:2.30040235196384\n",
      "train loss:2.3010081719743454\n",
      "train loss:2.299441568541339\n",
      "train loss:2.3008991410917234\n",
      "train loss:2.3051080565684114\n",
      "train loss:2.3037964570330987\n",
      "train loss:2.2997079512257974\n",
      "train loss:2.3088338939207547\n",
      "train loss:2.311145344342253\n",
      "train loss:2.297729274301683\n",
      "train loss:2.302570445499809\n",
      "train loss:2.309350756819966\n",
      "train loss:2.306846351239845\n",
      "train loss:2.3023674507449954\n",
      "train loss:2.302840771224986\n",
      "train loss:2.302759593756592\n",
      "train loss:2.305026166225244\n",
      "train loss:2.303687146644562\n",
      "train loss:2.303748311343875\n",
      "train loss:2.2998562308244828\n",
      "train loss:2.3006852847722286\n",
      "train loss:2.296464320300311\n",
      "train loss:2.3047814329190226\n",
      "train loss:2.3018654577604374\n",
      "train loss:2.304175494258579\n",
      "train loss:2.299851404294533\n",
      "train loss:2.299059299058831\n",
      "train loss:2.2960033927302783\n",
      "train loss:2.3023012339038837\n",
      "train loss:2.304495190067354\n",
      "train loss:2.3029069153594577\n",
      "train loss:2.2917760400354035\n",
      "train loss:2.3056787206822627\n",
      "train loss:2.2998375753618046\n",
      "train loss:2.3021727544405164\n",
      "train loss:2.297269494887476\n",
      "train loss:2.2966943237436293\n",
      "train loss:2.3041491760509767\n",
      "train loss:2.3031144967092207\n",
      "train loss:2.2969001718695874\n",
      "train loss:2.295055086141768\n",
      "train loss:2.3062277943604244\n",
      "train loss:2.30656358696853\n",
      "train loss:2.2979386582095414\n",
      "train loss:2.3095057133688996\n",
      "train loss:2.298409352405903\n",
      "train loss:2.2995757570461697\n",
      "train loss:2.2986550624183097\n",
      "train loss:2.3057736071666963\n",
      "train loss:2.295979030487082\n",
      "train loss:2.2966414191441618\n",
      "train loss:2.3032804845261787\n",
      "train loss:2.295640626227662\n",
      "train loss:2.2969246783447184\n",
      "train loss:2.298927303639055\n",
      "train loss:2.2964378761287065\n",
      "train loss:2.2926989388678205\n",
      "train loss:2.2911230119390336\n",
      "train loss:2.296016826720721\n",
      "train loss:2.297220913317719\n",
      "train loss:2.300654361244741\n",
      "train loss:2.298053448582817\n",
      "train loss:2.305739847237158\n",
      "train loss:2.3042996264654554\n",
      "train loss:2.289574276794604\n",
      "train loss:2.303184871492754\n",
      "train loss:2.292474010351063\n",
      "train loss:2.303938288400629\n",
      "train loss:2.3044599786985467\n",
      "train loss:2.2977419287277003\n",
      "train loss:2.30671507604362\n",
      "train loss:2.3052073727615023\n",
      "train loss:2.3063827105545007\n",
      "train loss:2.296485949070428\n",
      "train loss:2.3044680589667625\n",
      "train loss:2.293423289394909\n",
      "train loss:2.285237110628325\n",
      "train loss:2.3010388750500708\n",
      "train loss:2.3053595551632795\n",
      "train loss:2.302184259935604\n",
      "train loss:2.3043609777222023\n",
      "train loss:2.3037934154861155\n",
      "train loss:2.2958553951534943\n",
      "train loss:2.3026241715190934\n",
      "train loss:2.3026401793244378\n",
      "train loss:2.303703051402276\n",
      "train loss:2.2971780347605497\n",
      "train loss:2.3056995635103643\n",
      "train loss:2.2929277191144966\n",
      "train loss:2.303165927533285\n",
      "train loss:2.299930786039342\n",
      "train loss:2.3004159151955736\n",
      "train loss:2.306909389651108\n",
      "train loss:2.301786894213735\n",
      "train loss:2.291680548197779\n",
      "train loss:2.304037989603938\n",
      "train loss:2.296302136458088\n",
      "train loss:2.3002972349659725\n",
      "train loss:2.3007689227544956\n",
      "train loss:2.3059415926788387\n",
      "train loss:2.2990430051262547\n",
      "train loss:2.298986231401549\n",
      "train loss:2.3025750473966906\n",
      "train loss:2.2981827456429684\n",
      "train loss:2.3026839813472764\n",
      "train loss:2.3024160951489945\n",
      "train loss:2.296573370838533\n",
      "train loss:2.3013649012904565\n",
      "train loss:2.288622391522355\n",
      "train loss:2.3018684401168534\n",
      "train loss:2.297908718296711\n",
      "train loss:2.3046871797954074\n",
      "train loss:2.3005494788895997\n",
      "train loss:2.3060238122544092\n",
      "train loss:2.3072106348143295\n",
      "train loss:2.3095252197560243\n",
      "train loss:2.301196809514901\n",
      "train loss:2.308418898670271\n",
      "train loss:2.3059098760434016\n",
      "train loss:2.300589266099577\n",
      "train loss:2.303764232284738\n",
      "train loss:2.303814151172847\n",
      "train loss:2.3148961199491134\n",
      "train loss:2.2990929063230636\n",
      "=== epoch:52, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.300419980639013\n",
      "train loss:2.299289344070389\n",
      "train loss:2.296973300687972\n",
      "train loss:2.3051489840697665\n",
      "train loss:2.3034252654251843\n",
      "train loss:2.3029707354503266\n",
      "train loss:2.2988497604348335\n",
      "train loss:2.3053745401539394\n",
      "train loss:2.295220127778496\n",
      "train loss:2.2932942424124896\n",
      "train loss:2.305025892975843\n",
      "train loss:2.3090673490194322\n",
      "train loss:2.303498138904023\n",
      "train loss:2.3049244770825443\n",
      "train loss:2.3021165526565457\n",
      "train loss:2.2977169290377137\n",
      "train loss:2.3028738008199996\n",
      "train loss:2.2996698384681955\n",
      "train loss:2.306475312910163\n",
      "train loss:2.298991676157936\n",
      "train loss:2.3026473488343373\n",
      "train loss:2.298925779625948\n",
      "train loss:2.306220686594285\n",
      "train loss:2.2960388100081977\n",
      "train loss:2.303136149092397\n",
      "train loss:2.3016732542581035\n",
      "train loss:2.298378852493578\n",
      "train loss:2.3050104406546454\n",
      "train loss:2.3019217665030665\n",
      "train loss:2.302992745438379\n",
      "train loss:2.298091781555848\n",
      "train loss:2.3021299151636674\n",
      "train loss:2.302763976221181\n",
      "train loss:2.3069699692601504\n",
      "train loss:2.3009313024880838\n",
      "train loss:2.3043106901474952\n",
      "train loss:2.3022566332698062\n",
      "train loss:2.3008337570809254\n",
      "train loss:2.303143510497204\n",
      "train loss:2.301773330754085\n",
      "train loss:2.30384565846869\n",
      "train loss:2.304533043640868\n",
      "train loss:2.3002374947720226\n",
      "train loss:2.3005972811028106\n",
      "train loss:2.2949414025164163\n",
      "train loss:2.2984517173311128\n",
      "train loss:2.304159969462189\n",
      "train loss:2.2969395112851463\n",
      "train loss:2.301766576620821\n",
      "train loss:2.30252810274019\n",
      "train loss:2.3062161371245935\n",
      "train loss:2.2999589009933574\n",
      "train loss:2.307720931256043\n",
      "train loss:2.308490574013623\n",
      "train loss:2.297206262365931\n",
      "train loss:2.3047052736266065\n",
      "train loss:2.3004892289377525\n",
      "train loss:2.3000219183685737\n",
      "train loss:2.2968438514397853\n",
      "train loss:2.3058427556033814\n",
      "train loss:2.305545146935117\n",
      "train loss:2.304203939574495\n",
      "train loss:2.301687562100751\n",
      "train loss:2.304629928357339\n",
      "train loss:2.3003917532518674\n",
      "train loss:2.30028599134814\n",
      "train loss:2.305444284103701\n",
      "train loss:2.303165620355874\n",
      "train loss:2.3003078849803225\n",
      "train loss:2.3001227513011195\n",
      "train loss:2.3036766066803502\n",
      "train loss:2.301193333348723\n",
      "train loss:2.3028505314340664\n",
      "train loss:2.306784638083398\n",
      "train loss:2.3019555837469974\n",
      "train loss:2.3046293683014376\n",
      "train loss:2.3037264926750822\n",
      "train loss:2.3058992049747657\n",
      "train loss:2.291719930832403\n",
      "train loss:2.3036466809092913\n",
      "train loss:2.3072162908873635\n",
      "train loss:2.297187837413075\n",
      "train loss:2.2998660365573107\n",
      "train loss:2.296854730533957\n",
      "train loss:2.306873494655547\n",
      "train loss:2.3054418031241055\n",
      "train loss:2.302865924941642\n",
      "train loss:2.3025115502780302\n",
      "train loss:2.301995180842084\n",
      "train loss:2.2968798151063896\n",
      "train loss:2.298825879278132\n",
      "train loss:2.3002109196448877\n",
      "train loss:2.3077178157376332\n",
      "train loss:2.3038950591011544\n",
      "train loss:2.3066080344981486\n",
      "train loss:2.303563644135654\n",
      "train loss:2.304336730153569\n",
      "train loss:2.2944523880502627\n",
      "train loss:2.3002329257104743\n",
      "train loss:2.3033616283248604\n",
      "train loss:2.2937914789449327\n",
      "train loss:2.296679685939876\n",
      "train loss:2.3033266769278398\n",
      "train loss:2.3083137620763594\n",
      "train loss:2.3034291344233737\n",
      "train loss:2.299923981861745\n",
      "train loss:2.3004605223416568\n",
      "train loss:2.30434024250783\n",
      "train loss:2.3050957547418567\n",
      "train loss:2.3048093039699733\n",
      "train loss:2.3067410689002736\n",
      "train loss:2.3084259038673927\n",
      "train loss:2.304418759072584\n",
      "train loss:2.29811685401853\n",
      "train loss:2.2981619714992867\n",
      "train loss:2.296727126436184\n",
      "train loss:2.2953979052053177\n",
      "train loss:2.310327708942205\n",
      "train loss:2.29685550650421\n",
      "train loss:2.2969598152509514\n",
      "train loss:2.2980899789433655\n",
      "train loss:2.287968717223291\n",
      "train loss:2.2995823363551646\n",
      "train loss:2.298400980722349\n",
      "train loss:2.2978216669883205\n",
      "train loss:2.300523839235856\n",
      "train loss:2.3024322141443023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2901694735255003\n",
      "train loss:2.3023379330734364\n",
      "train loss:2.3009528966668413\n",
      "train loss:2.3042928508645484\n",
      "train loss:2.291817735562785\n",
      "train loss:2.3046727333301065\n",
      "train loss:2.298407836020384\n",
      "train loss:2.3009803049668234\n",
      "train loss:2.3000860417590476\n",
      "train loss:2.2929695841534\n",
      "train loss:2.301133683184509\n",
      "train loss:2.3030752376680943\n",
      "train loss:2.3003744523030467\n",
      "train loss:2.3107318142119535\n",
      "train loss:2.2967584148064333\n",
      "train loss:2.298202023723695\n",
      "train loss:2.298415115631889\n",
      "train loss:2.3020761178736295\n",
      "train loss:2.298338162699263\n",
      "train loss:2.30807866193936\n",
      "train loss:2.300247634020935\n",
      "train loss:2.2958476550376634\n",
      "train loss:2.3042822906385325\n",
      "train loss:2.2989589139833893\n",
      "train loss:2.3038050430996795\n",
      "train loss:2.3044127074659597\n",
      "train loss:2.299708818086781\n",
      "train loss:2.288302468941042\n",
      "train loss:2.3069028989002422\n",
      "train loss:2.298407855672995\n",
      "train loss:2.30271392313916\n",
      "train loss:2.2968488484005998\n",
      "train loss:2.3151122957701924\n",
      "train loss:2.2895737276848194\n",
      "train loss:2.3058756730141274\n",
      "train loss:2.294047658246834\n",
      "train loss:2.3093261059039762\n",
      "train loss:2.3049221131556252\n",
      "train loss:2.2975418157951624\n",
      "train loss:2.300960372799321\n",
      "train loss:2.2984528127979935\n",
      "train loss:2.290706575890733\n",
      "train loss:2.3094405969477236\n",
      "train loss:2.3014896220262924\n",
      "train loss:2.3016489683818753\n",
      "train loss:2.3053690574126517\n",
      "train loss:2.3082856532409832\n",
      "train loss:2.2931900878324223\n",
      "train loss:2.307047702180641\n",
      "train loss:2.289563130302524\n",
      "train loss:2.3002950164237173\n",
      "train loss:2.294508785831335\n",
      "train loss:2.295840995951603\n",
      "train loss:2.2988422807471545\n",
      "train loss:2.30142439371781\n",
      "train loss:2.3069301352249436\n",
      "train loss:2.3052081650959764\n",
      "train loss:2.301565887269234\n",
      "train loss:2.296773478172122\n",
      "train loss:2.304469842685493\n",
      "train loss:2.2889226721637486\n",
      "train loss:2.300895483894256\n",
      "train loss:2.3057236083230026\n",
      "train loss:2.299727430374089\n",
      "train loss:2.2927732338613644\n",
      "train loss:2.3046734756946003\n",
      "train loss:2.3024384827517728\n",
      "train loss:2.312222062046842\n",
      "train loss:2.300141543738248\n",
      "train loss:2.3131813899146927\n",
      "train loss:2.2983244590295238\n",
      "train loss:2.307495469173895\n",
      "train loss:2.2981337413528045\n",
      "=== epoch:53, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.304970394697372\n",
      "train loss:2.3027649040520672\n",
      "train loss:2.315817602462816\n",
      "train loss:2.296757461280406\n",
      "train loss:2.2968298422419338\n",
      "train loss:2.305654424130263\n",
      "train loss:2.3031554121445863\n",
      "train loss:2.2976516349138953\n",
      "train loss:2.296200183983431\n",
      "train loss:2.3045279367610743\n",
      "train loss:2.2988245124970303\n",
      "train loss:2.3017968135668365\n",
      "train loss:2.298047820168788\n",
      "train loss:2.295574439914338\n",
      "train loss:2.310750248806745\n",
      "train loss:2.3067422832110878\n",
      "train loss:2.2916988615766285\n",
      "train loss:2.3000891459006247\n",
      "train loss:2.3015515472303347\n",
      "train loss:2.299571608701635\n",
      "train loss:2.302763758205691\n",
      "train loss:2.306768490827104\n",
      "train loss:2.2971612514382254\n",
      "train loss:2.3100960229099097\n",
      "train loss:2.30206904614368\n",
      "train loss:2.296584850253491\n",
      "train loss:2.3020782212226747\n",
      "train loss:2.2973481973230965\n",
      "train loss:2.3005384118468446\n",
      "train loss:2.300169536837485\n",
      "train loss:2.300882318144176\n",
      "train loss:2.3036827003516462\n",
      "train loss:2.3059135386781646\n",
      "train loss:2.303461192036829\n",
      "train loss:2.3025189535624433\n",
      "train loss:2.3030429214421306\n",
      "train loss:2.3043737355290013\n",
      "train loss:2.3024563625877534\n",
      "train loss:2.301485452854144\n",
      "train loss:2.2996123414274945\n",
      "train loss:2.301620883353855\n",
      "train loss:2.306697880586364\n",
      "train loss:2.30519226599081\n",
      "train loss:2.300753004852624\n",
      "train loss:2.297949094230493\n",
      "train loss:2.301265056873877\n",
      "train loss:2.2983162885475283\n",
      "train loss:2.2925117193970532\n",
      "train loss:2.305255583619131\n",
      "train loss:2.2998442388487597\n",
      "train loss:2.306146713349074\n",
      "train loss:2.3002554662606256\n",
      "train loss:2.297250668405925\n",
      "train loss:2.302795805504165\n",
      "train loss:2.3085373152063537\n",
      "train loss:2.3008630234902485\n",
      "train loss:2.298343202916094\n",
      "train loss:2.3049285943715816\n",
      "train loss:2.3009906429779554\n",
      "train loss:2.3009653740072964\n",
      "train loss:2.297875353077605\n",
      "train loss:2.298091487568603\n",
      "train loss:2.2936060439133743\n",
      "train loss:2.3121936842539244\n",
      "train loss:2.303022932724786\n",
      "train loss:2.3042061434035292\n",
      "train loss:2.30289405453107\n",
      "train loss:2.301106765983553\n",
      "train loss:2.2979089234831473\n",
      "train loss:2.299462238831412\n",
      "train loss:2.3056254262376097\n",
      "train loss:2.2978749247794314\n",
      "train loss:2.2981461206212335\n",
      "train loss:2.306349884626289\n",
      "train loss:2.3045708186242435\n",
      "train loss:2.2997625570166487\n",
      "train loss:2.300516851217163\n",
      "train loss:2.308992740711594\n",
      "train loss:2.2978177731321843\n",
      "train loss:2.2976428173912393\n",
      "train loss:2.29711047705047\n",
      "train loss:2.3068722023688566\n",
      "train loss:2.3017256781035824\n",
      "train loss:2.3082071288677724\n",
      "train loss:2.300214225455984\n",
      "train loss:2.3073652313108424\n",
      "train loss:2.3037307226537855\n",
      "train loss:2.3028283095493673\n",
      "train loss:2.3017971858874833\n",
      "train loss:2.3025509385562195\n",
      "train loss:2.293816679794054\n",
      "train loss:2.3094895206350596\n",
      "train loss:2.2986863238009922\n",
      "train loss:2.2951726483011567\n",
      "train loss:2.295958748903261\n",
      "train loss:2.3053318246521903\n",
      "train loss:2.2965387483562436\n",
      "train loss:2.302652890068916\n",
      "train loss:2.3020116453161665\n",
      "train loss:2.2995704969552184\n",
      "train loss:2.3002945813000233\n",
      "train loss:2.305750590527975\n",
      "train loss:2.301561545266811\n",
      "train loss:2.299454643882262\n",
      "train loss:2.300483318511437\n",
      "train loss:2.3043685152915643\n",
      "train loss:2.3036938346157134\n",
      "train loss:2.3086647562726745\n",
      "train loss:2.308694922460496\n",
      "train loss:2.299485973336008\n",
      "train loss:2.301980073983498\n",
      "train loss:2.3025857803900314\n",
      "train loss:2.2949377023246633\n",
      "train loss:2.302363334928161\n",
      "train loss:2.2950193646785544\n",
      "train loss:2.295880649215018\n",
      "train loss:2.30488405560419\n",
      "train loss:2.294571132310739\n",
      "train loss:2.3005337910735673\n",
      "train loss:2.29755978670519\n",
      "train loss:2.3056865446281427\n",
      "train loss:2.296918309600076\n",
      "train loss:2.3003715877058926\n",
      "train loss:2.304108638929088\n",
      "train loss:2.3001374389220848\n",
      "train loss:2.299718916068514\n",
      "train loss:2.3112641730776633\n",
      "train loss:2.300893094000477\n",
      "train loss:2.305154265139053\n",
      "train loss:2.305013353287944\n",
      "train loss:2.301467780341\n",
      "train loss:2.2952100417210555\n",
      "train loss:2.304498972104333\n",
      "train loss:2.3057062473946397\n",
      "train loss:2.304437548703594\n",
      "train loss:2.2964945915609047\n",
      "train loss:2.3026740708325044\n",
      "train loss:2.3045425813586826\n",
      "train loss:2.2980942746568784\n",
      "train loss:2.3050814886692996\n",
      "train loss:2.293750626087619\n",
      "train loss:2.3053961053592467\n",
      "train loss:2.3000513141084826\n",
      "train loss:2.2962872322521846\n",
      "train loss:2.29994464285098\n",
      "train loss:2.309438421411223\n",
      "train loss:2.299078183506119\n",
      "train loss:2.2986150699460923\n",
      "train loss:2.2968761414258334\n",
      "train loss:2.302911328397887\n",
      "train loss:2.297851030213645\n",
      "train loss:2.3037179731438875\n",
      "train loss:2.292134429925742\n",
      "train loss:2.298747228522595\n",
      "train loss:2.307100139368436\n",
      "train loss:2.2980353212356532\n",
      "train loss:2.28759408409108\n",
      "train loss:2.2942877671241435\n",
      "train loss:2.3089785737152817\n",
      "train loss:2.299775129146979\n",
      "train loss:2.301680690215639\n",
      "train loss:2.304661515694622\n",
      "train loss:2.2938083979267723\n",
      "train loss:2.304263500456409\n",
      "train loss:2.3059725802928512\n",
      "train loss:2.3055796020723656\n",
      "train loss:2.2923851771689354\n",
      "train loss:2.295809771596797\n",
      "train loss:2.299778703014176\n",
      "train loss:2.304172089136178\n",
      "train loss:2.301194394136691\n",
      "train loss:2.3064103213042704\n",
      "train loss:2.301433898449893\n",
      "train loss:2.294161031519255\n",
      "train loss:2.300197930377525\n",
      "train loss:2.294100532578596\n",
      "train loss:2.301259733534729\n",
      "train loss:2.3041880125448566\n",
      "train loss:2.2996055429675177\n",
      "train loss:2.302300735981071\n",
      "train loss:2.302640549065613\n",
      "train loss:2.306948557013167\n",
      "train loss:2.298768183502766\n",
      "train loss:2.300123179353071\n",
      "train loss:2.3023386725421586\n",
      "train loss:2.298218131244187\n",
      "train loss:2.30215002134186\n",
      "train loss:2.297906119349299\n",
      "train loss:2.2993837292070034\n",
      "train loss:2.2939998171537375\n",
      "train loss:2.304868897547053\n",
      "train loss:2.30483153633412\n",
      "train loss:2.3032828347048824\n",
      "train loss:2.2969969153735446\n",
      "train loss:2.302571063461566\n",
      "train loss:2.308674529855145\n",
      "train loss:2.308180358587953\n",
      "train loss:2.288489416777096\n",
      "train loss:2.3085169258227096\n",
      "train loss:2.3002464872553983\n",
      "=== epoch:54, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308388936138723\n",
      "train loss:2.2949701112587273\n",
      "train loss:2.3097723016168925\n",
      "train loss:2.3040218390468916\n",
      "train loss:2.299472676626076\n",
      "train loss:2.299407392671611\n",
      "train loss:2.3055447106395874\n",
      "train loss:2.308031034434047\n",
      "train loss:2.298523463887142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.303640654769156\n",
      "train loss:2.303143378277493\n",
      "train loss:2.3031918435269425\n",
      "train loss:2.2966328382495806\n",
      "train loss:2.305531204115561\n",
      "train loss:2.3039358298623984\n",
      "train loss:2.296283846569784\n",
      "train loss:2.3026058542302597\n",
      "train loss:2.301136903519023\n",
      "train loss:2.305608510983841\n",
      "train loss:2.3091811963513824\n",
      "train loss:2.2966446651247443\n",
      "train loss:2.303737062669936\n",
      "train loss:2.3008613022207616\n",
      "train loss:2.300208293413292\n",
      "train loss:2.297738269839232\n",
      "train loss:2.3004915011023304\n",
      "train loss:2.2977661612413387\n",
      "train loss:2.294235830275639\n",
      "train loss:2.301029137661553\n",
      "train loss:2.2947075884706374\n",
      "train loss:2.2943316539372898\n",
      "train loss:2.308168003787143\n",
      "train loss:2.307372181758487\n",
      "train loss:2.302380929659591\n",
      "train loss:2.300304709794001\n",
      "train loss:2.305155364520579\n",
      "train loss:2.3029351237179356\n",
      "train loss:2.306379588864672\n",
      "train loss:2.2997329265798236\n",
      "train loss:2.3005340031881647\n",
      "train loss:2.296580723035662\n",
      "train loss:2.2997054277825733\n",
      "train loss:2.302775987177831\n",
      "train loss:2.3000113383513794\n",
      "train loss:2.296102144029925\n",
      "train loss:2.2966749959422113\n",
      "train loss:2.2971326415755606\n",
      "train loss:2.297098151270243\n",
      "train loss:2.307917862829269\n",
      "train loss:2.2981468742126427\n",
      "train loss:2.297604538076745\n",
      "train loss:2.308257946530475\n",
      "train loss:2.3010516707326842\n",
      "train loss:2.3013630062958255\n",
      "train loss:2.299256946834406\n",
      "train loss:2.287213547433013\n",
      "train loss:2.3034029849688014\n",
      "train loss:2.3060523851789276\n",
      "train loss:2.301980573017385\n",
      "train loss:2.301762996309639\n",
      "train loss:2.2991310048988183\n",
      "train loss:2.2958851975346115\n",
      "train loss:2.2985777318793685\n",
      "train loss:2.304969748733919\n",
      "train loss:2.301648855350847\n",
      "train loss:2.302640219398008\n",
      "train loss:2.311580582150515\n",
      "train loss:2.304610442317431\n",
      "train loss:2.302839133062846\n",
      "train loss:2.2930439439003725\n",
      "train loss:2.2953500972025633\n",
      "train loss:2.2941894194330215\n",
      "train loss:2.2923946577333036\n",
      "train loss:2.2978462455884414\n",
      "train loss:2.2950955471241326\n",
      "train loss:2.299249153936588\n",
      "train loss:2.298818203953884\n",
      "train loss:2.293407125539681\n",
      "train loss:2.298511314853803\n",
      "train loss:2.3084980297590105\n",
      "train loss:2.3033987528863085\n",
      "train loss:2.307930956966721\n",
      "train loss:2.309068678134517\n",
      "train loss:2.3033819699488993\n",
      "train loss:2.306624861231161\n",
      "train loss:2.3034499844735614\n",
      "train loss:2.3014227795671762\n",
      "train loss:2.2961600922707066\n",
      "train loss:2.2971317395581847\n",
      "train loss:2.291768940747239\n",
      "train loss:2.3003890918323626\n",
      "train loss:2.3046589812693514\n",
      "train loss:2.2943984345052324\n",
      "train loss:2.294704026425545\n",
      "train loss:2.3045650400849187\n",
      "train loss:2.30111399963575\n",
      "train loss:2.3024478970820077\n",
      "train loss:2.3048921413633128\n",
      "train loss:2.2979543872639328\n",
      "train loss:2.3064907963597103\n",
      "train loss:2.3134922943693317\n",
      "train loss:2.2951704135432327\n",
      "train loss:2.2999384900445805\n",
      "train loss:2.3024441339165587\n",
      "train loss:2.3076296707064805\n",
      "train loss:2.2985749849408745\n",
      "train loss:2.3040912456699396\n",
      "train loss:2.302509296639005\n",
      "train loss:2.2965225080190406\n",
      "train loss:2.3002029734966727\n",
      "train loss:2.298224151499615\n",
      "train loss:2.298543886534184\n",
      "train loss:2.305494653674137\n",
      "train loss:2.296289583841442\n",
      "train loss:2.303839740307246\n",
      "train loss:2.297475724459997\n",
      "train loss:2.3067979272767833\n",
      "train loss:2.3087846130004728\n",
      "train loss:2.3047591152938716\n",
      "train loss:2.300304445579549\n",
      "train loss:2.298592250223099\n",
      "train loss:2.292956664269422\n",
      "train loss:2.3053809676685275\n",
      "train loss:2.2994705833543674\n",
      "train loss:2.3051701484062987\n",
      "train loss:2.3032926441630446\n",
      "train loss:2.299992525682001\n",
      "train loss:2.285079800949942\n",
      "train loss:2.2960589456146145\n",
      "train loss:2.296862295823515\n",
      "train loss:2.2999519810790447\n",
      "train loss:2.296418600488837\n",
      "train loss:2.3052583120619\n",
      "train loss:2.305263712111719\n",
      "train loss:2.2991107646520534\n",
      "train loss:2.305769897854274\n",
      "train loss:2.3050971599643453\n",
      "train loss:2.300781474305736\n",
      "train loss:2.308724740674831\n",
      "train loss:2.3003782482158077\n",
      "train loss:2.2926733966228827\n",
      "train loss:2.292978943887805\n",
      "train loss:2.3068918676250374\n",
      "train loss:2.302468968293326\n",
      "train loss:2.2950408802857516\n",
      "train loss:2.307647420760036\n",
      "train loss:2.3002215527875505\n",
      "train loss:2.3026160085006024\n",
      "train loss:2.297171494441046\n",
      "train loss:2.3003076298420755\n",
      "train loss:2.2960408544073623\n",
      "train loss:2.3000795746773806\n",
      "train loss:2.310491372124921\n",
      "train loss:2.306340890334148\n",
      "train loss:2.2970180366250337\n",
      "train loss:2.3000504577279575\n",
      "train loss:2.301076614457816\n",
      "train loss:2.295539293880949\n",
      "train loss:2.315148038937888\n",
      "train loss:2.302100252801554\n",
      "train loss:2.305429349060824\n",
      "train loss:2.3003707205058403\n",
      "train loss:2.3059047896889635\n",
      "train loss:2.296832776686128\n",
      "train loss:2.3058472126089096\n",
      "train loss:2.3035269491059425\n",
      "train loss:2.304697495778739\n",
      "train loss:2.302888568271576\n",
      "train loss:2.304127113331966\n",
      "train loss:2.2938413491734546\n",
      "train loss:2.308857064429155\n",
      "train loss:2.3036627058826564\n",
      "train loss:2.2983014998977995\n",
      "train loss:2.301539900384423\n",
      "train loss:2.3033031074320895\n",
      "train loss:2.3046783321449062\n",
      "train loss:2.2986698298047474\n",
      "train loss:2.2982220669024804\n",
      "train loss:2.295080928046252\n",
      "train loss:2.3006211752281036\n",
      "train loss:2.2986636542285064\n",
      "train loss:2.2980239187550757\n",
      "train loss:2.294790544183674\n",
      "train loss:2.308532534429135\n",
      "train loss:2.30480632102038\n",
      "train loss:2.3036138900904377\n",
      "train loss:2.3065456730213514\n",
      "train loss:2.301471631828825\n",
      "train loss:2.3011721708728152\n",
      "train loss:2.3006891695008473\n",
      "train loss:2.2992519835023084\n",
      "train loss:2.304928158784046\n",
      "train loss:2.3015299662768394\n",
      "train loss:2.296544975190184\n",
      "train loss:2.3043420641049073\n",
      "train loss:2.3023158662217624\n",
      "train loss:2.297925496283837\n",
      "train loss:2.3031005501074455\n",
      "train loss:2.302531483078183\n",
      "train loss:2.3049375633265345\n",
      "=== epoch:55, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2974169937524587\n",
      "train loss:2.2988985359965364\n",
      "train loss:2.2967696306405974\n",
      "train loss:2.293063479275728\n",
      "train loss:2.295344587369558\n",
      "train loss:2.3035780263003875\n",
      "train loss:2.3000203456801547\n",
      "train loss:2.3018695715126416\n",
      "train loss:2.3001119337942026\n",
      "train loss:2.2981534950116735\n",
      "train loss:2.303717735160055\n",
      "train loss:2.3058816105907556\n",
      "train loss:2.2993676722736724\n",
      "train loss:2.3017190570120003\n",
      "train loss:2.303602217264897\n",
      "train loss:2.2968159648269557\n",
      "train loss:2.2898606459370097\n",
      "train loss:2.3020546636759978\n",
      "train loss:2.296557153570909\n",
      "train loss:2.2967263627206638\n",
      "train loss:2.3050911068017297\n",
      "train loss:2.2933632473372465\n",
      "train loss:2.300523453529075\n",
      "train loss:2.3002992685468158\n",
      "train loss:2.2961288490801453\n",
      "train loss:2.2951360228619375\n",
      "train loss:2.2998460591536185\n",
      "train loss:2.2968444216058597\n",
      "train loss:2.307395358045332\n",
      "train loss:2.302830806711174\n",
      "train loss:2.3042675387370664\n",
      "train loss:2.2998335042173883\n",
      "train loss:2.3083355830630627\n",
      "train loss:2.3075827972794123\n",
      "train loss:2.3010046660009325\n",
      "train loss:2.294255759228556\n",
      "train loss:2.301990656182251\n",
      "train loss:2.302639782447332\n",
      "train loss:2.3025829377024025\n",
      "train loss:2.3012177245418517\n",
      "train loss:2.2966294603478046\n",
      "train loss:2.2924270116222467\n",
      "train loss:2.292217996793596\n",
      "train loss:2.2982896342081416\n",
      "train loss:2.2882301150508013\n",
      "train loss:2.306477245433905\n",
      "train loss:2.2951925779308398\n",
      "train loss:2.3159467177147692\n",
      "train loss:2.2992172738983654\n",
      "train loss:2.299540762454289\n",
      "train loss:2.304706166435562\n",
      "train loss:2.3034604413651536\n",
      "train loss:2.3054602276003746\n",
      "train loss:2.3024621213011085\n",
      "train loss:2.2986162785906297\n",
      "train loss:2.307502443870153\n",
      "train loss:2.309503480584673\n",
      "train loss:2.303272245979895\n",
      "train loss:2.298703034887144\n",
      "train loss:2.291238257659545\n",
      "train loss:2.3106199730653514\n",
      "train loss:2.2992750777470263\n",
      "train loss:2.2994201057964614\n",
      "train loss:2.3020067288120205\n",
      "train loss:2.30653959627997\n",
      "train loss:2.3090606174216086\n",
      "train loss:2.299741276503779\n",
      "train loss:2.3025384725112357\n",
      "train loss:2.2985957865567013\n",
      "train loss:2.295533440511387\n",
      "train loss:2.2986473226423314\n",
      "train loss:2.30676361779707\n",
      "train loss:2.30305573654694\n",
      "train loss:2.3020270575970265\n",
      "train loss:2.2979058249815374\n",
      "train loss:2.3044737288863604\n",
      "train loss:2.3106681344159723\n",
      "train loss:2.3029857107724645\n",
      "train loss:2.3013267790825456\n",
      "train loss:2.305723133750882\n",
      "train loss:2.2911216169390465\n",
      "train loss:2.307027389689732\n",
      "train loss:2.301940728166076\n",
      "train loss:2.299408587833077\n",
      "train loss:2.3022503666306204\n",
      "train loss:2.3034102963541967\n",
      "train loss:2.298469238653865\n",
      "train loss:2.3039477158596045\n",
      "train loss:2.2959269245928082\n",
      "train loss:2.2974676394766576\n",
      "train loss:2.306628664521722\n",
      "train loss:2.292327422410269\n",
      "train loss:2.3009640402606393\n",
      "train loss:2.294756275605966\n",
      "train loss:2.3029599932047327\n",
      "train loss:2.300475019269068\n",
      "train loss:2.305703450056537\n",
      "train loss:2.3055484123080543\n",
      "train loss:2.3040211391118968\n",
      "train loss:2.29944517813375\n",
      "train loss:2.297746397987517\n",
      "train loss:2.3032931899349096\n",
      "train loss:2.298739674515074\n",
      "train loss:2.3060541473856513\n",
      "train loss:2.30183282886182\n",
      "train loss:2.299902468639384\n",
      "train loss:2.298272495658315\n",
      "train loss:2.2971930718454106\n",
      "train loss:2.3032179465990352\n",
      "train loss:2.2919851798378237\n",
      "train loss:2.304477253187235\n",
      "train loss:2.2945853512580165\n",
      "train loss:2.3093342602248805\n",
      "train loss:2.299596311445928\n",
      "train loss:2.299037055074964\n",
      "train loss:2.310650521735512\n",
      "train loss:2.307584127345874\n",
      "train loss:2.300860451754101\n",
      "train loss:2.297771568461057\n",
      "train loss:2.2974883759063998\n",
      "train loss:2.2935545766602976\n",
      "train loss:2.3003933065763222\n",
      "train loss:2.301251259903381\n",
      "train loss:2.2987827010994284\n",
      "train loss:2.307433050149431\n",
      "train loss:2.3008145890368468\n",
      "train loss:2.2968002975922515\n",
      "train loss:2.296707692017815\n",
      "train loss:2.3004132601863017\n",
      "train loss:2.3009366537976574\n",
      "train loss:2.3074617793165837\n",
      "train loss:2.3035086088749157\n",
      "train loss:2.303771703540051\n",
      "train loss:2.288621600176659\n",
      "train loss:2.3054921217201434\n",
      "train loss:2.304503436555061\n",
      "train loss:2.300674158485516\n",
      "train loss:2.3018755351934663\n",
      "train loss:2.2925010440123246\n",
      "train loss:2.297098640941767\n",
      "train loss:2.2993438792490104\n",
      "train loss:2.303806144408207\n",
      "train loss:2.2997960439041534\n",
      "train loss:2.2932794736629103\n",
      "train loss:2.3109669317817936\n",
      "train loss:2.295416840005898\n",
      "train loss:2.3070658505211994\n",
      "train loss:2.3016782191307685\n",
      "train loss:2.3031861127041426\n",
      "train loss:2.3050888493035426\n",
      "train loss:2.296243394637087\n",
      "train loss:2.298156552786158\n",
      "train loss:2.300817023263535\n",
      "train loss:2.3078957600946124\n",
      "train loss:2.3090442750008267\n",
      "train loss:2.3059356003697893\n",
      "train loss:2.2990174045222447\n",
      "train loss:2.308821919980875\n",
      "train loss:2.3015606189084648\n",
      "train loss:2.3061617918297532\n",
      "train loss:2.304089287412282\n",
      "train loss:2.3046520077724684\n",
      "train loss:2.29653672984576\n",
      "train loss:2.2972914458072786\n",
      "train loss:2.2981292849533768\n",
      "train loss:2.2976165078134065\n",
      "train loss:2.3020187194370387\n",
      "train loss:2.306601210709863\n",
      "train loss:2.303655680480914\n",
      "train loss:2.303578365723333\n",
      "train loss:2.304045976421172\n",
      "train loss:2.300958172694203\n",
      "train loss:2.3001706431974482\n",
      "train loss:2.304978331544145\n",
      "train loss:2.296682949003194\n",
      "train loss:2.3061204874013197\n",
      "train loss:2.295073476744857\n",
      "train loss:2.3011161488093688\n",
      "train loss:2.302439281204828\n",
      "train loss:2.301315359038422\n",
      "train loss:2.299308273399161\n",
      "train loss:2.297495059573896\n",
      "train loss:2.2990874095425684\n",
      "train loss:2.30378756201947\n",
      "train loss:2.309553196125715\n",
      "train loss:2.2960326293044893\n",
      "train loss:2.304630607126618\n",
      "train loss:2.3062447895097953\n",
      "train loss:2.302373938712026\n",
      "train loss:2.3010822504027098\n",
      "train loss:2.3124989694912306\n",
      "train loss:2.3090381011637686\n",
      "train loss:2.305245830754331\n",
      "train loss:2.312925285099695\n",
      "train loss:2.301906851041548\n",
      "train loss:2.30381987890782\n",
      "train loss:2.2996016483548543\n",
      "train loss:2.301626211421521\n",
      "train loss:2.2963657716811317\n",
      "train loss:2.301645004459771\n",
      "=== epoch:56, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308838352210464\n",
      "train loss:2.2924384604643953\n",
      "train loss:2.303944613211797\n",
      "train loss:2.3016040904144526\n",
      "train loss:2.2957460189128107\n",
      "train loss:2.2973380321923482\n",
      "train loss:2.2986035918578325\n",
      "train loss:2.297561844888542\n",
      "train loss:2.3024789479559957\n",
      "train loss:2.308412500148203\n",
      "train loss:2.299552975016887\n",
      "train loss:2.2955098284208715\n",
      "train loss:2.2985557785342956\n",
      "train loss:2.301177006998137\n",
      "train loss:2.3033792068580388\n",
      "train loss:2.2979321298843365\n",
      "train loss:2.3047483128067645\n",
      "train loss:2.3058891490315276\n",
      "train loss:2.2963058645697285\n",
      "train loss:2.3052585204120737\n",
      "train loss:2.301887449146265\n",
      "train loss:2.2962959574132236\n",
      "train loss:2.3097622914674756\n",
      "train loss:2.3023484340850313\n",
      "train loss:2.301223342799024\n",
      "train loss:2.3026097971327144\n",
      "train loss:2.2983729566050237\n",
      "train loss:2.305776630192918\n",
      "train loss:2.2936810050366447\n",
      "train loss:2.3000849643111216\n",
      "train loss:2.2965461019467175\n",
      "train loss:2.3128193635023626\n",
      "train loss:2.3117298277006233\n",
      "train loss:2.2970476917883165\n",
      "train loss:2.2948729250267896\n",
      "train loss:2.303312868990625\n",
      "train loss:2.300013091746964\n",
      "train loss:2.2982375409698172\n",
      "train loss:2.297097170599899\n",
      "train loss:2.2970624063823366\n",
      "train loss:2.3027397809758043\n",
      "train loss:2.3033666391451693\n",
      "train loss:2.3034777995582565\n",
      "train loss:2.306645217154419\n",
      "train loss:2.2979043764873603\n",
      "train loss:2.304567746300089\n",
      "train loss:2.310385811292216\n",
      "train loss:2.298787547261572\n",
      "train loss:2.3005762362148214\n",
      "train loss:2.307064264909472\n",
      "train loss:2.300744274957967\n",
      "train loss:2.3021657221773233\n",
      "train loss:2.3032386458797123\n",
      "train loss:2.3038831397460533\n",
      "train loss:2.301308416110172\n",
      "train loss:2.3049336890618286\n",
      "train loss:2.301076398207715\n",
      "train loss:2.302307341310127\n",
      "train loss:2.308690774353622\n",
      "train loss:2.299200058035406\n",
      "train loss:2.2989561589764467\n",
      "train loss:2.305816152335686\n",
      "train loss:2.2944622808485353\n",
      "train loss:2.3036717401233506\n",
      "train loss:2.295746675807224\n",
      "train loss:2.3064425694619484\n",
      "train loss:2.2978219878476334\n",
      "train loss:2.30275491255862\n",
      "train loss:2.3013357650928516\n",
      "train loss:2.3020703923478467\n",
      "train loss:2.2988900897213678\n",
      "train loss:2.3053029793064357\n",
      "train loss:2.302207959387436\n",
      "train loss:2.3116910163819404\n",
      "train loss:2.3032808104117537\n",
      "train loss:2.3024671371783034\n",
      "train loss:2.295601267142878\n",
      "train loss:2.315761742417486\n",
      "train loss:2.2967991886835657\n",
      "train loss:2.3039377257385665\n",
      "train loss:2.298652835261343\n",
      "train loss:2.3067885754562334\n",
      "train loss:2.3023594624970407\n",
      "train loss:2.297834710377643\n",
      "train loss:2.2961390105188175\n",
      "train loss:2.3071862623016695\n",
      "train loss:2.307415213854428\n",
      "train loss:2.3013388959948498\n",
      "train loss:2.3038256403774526\n",
      "train loss:2.3015740876154793\n",
      "train loss:2.3027119837480496\n",
      "train loss:2.309581857963999\n",
      "train loss:2.3083761984224282\n",
      "train loss:2.297976806247737\n",
      "train loss:2.306112578002376\n",
      "train loss:2.299530361743191\n",
      "train loss:2.2994574284190183\n",
      "train loss:2.297175217688844\n",
      "train loss:2.3050659446459996\n",
      "train loss:2.301760474385933\n",
      "train loss:2.300481913093896\n",
      "train loss:2.303812440722355\n",
      "train loss:2.2978486296526226\n",
      "train loss:2.2995173241655085\n",
      "train loss:2.3077362187872503\n",
      "train loss:2.296972583376636\n",
      "train loss:2.3086143352377535\n",
      "train loss:2.3044800285473976\n",
      "train loss:2.3109203909563614\n",
      "train loss:2.298749559130459\n",
      "train loss:2.303283317418492\n",
      "train loss:2.299105758310638\n",
      "train loss:2.300144880462844\n",
      "train loss:2.300527810496847\n",
      "train loss:2.307323164227028\n",
      "train loss:2.301116403043459\n",
      "train loss:2.294960111938906\n",
      "train loss:2.304615221321379\n",
      "train loss:2.2955910081602933\n",
      "train loss:2.304819367545154\n",
      "train loss:2.3054498679283135\n",
      "train loss:2.2965940518036767\n",
      "train loss:2.3091445700132915\n",
      "train loss:2.3066990489308252\n",
      "train loss:2.3039045474267335\n",
      "train loss:2.2989483061296396\n",
      "train loss:2.2991230440234776\n",
      "train loss:2.3083702706860243\n",
      "train loss:2.2996638766406505\n",
      "train loss:2.3039632487083948\n",
      "train loss:2.300197256389155\n",
      "train loss:2.3018864524357303\n",
      "train loss:2.3023034730303324\n",
      "train loss:2.299683655922937\n",
      "train loss:2.2989769109018856\n",
      "train loss:2.3011003999016575\n",
      "train loss:2.2951115162466036\n",
      "train loss:2.311131951213556\n",
      "train loss:2.299176070510745\n",
      "train loss:2.3002772910904197\n",
      "train loss:2.3097890554561133\n",
      "train loss:2.2950314611872695\n",
      "train loss:2.2959925559234464\n",
      "train loss:2.301372293318698\n",
      "train loss:2.2981511637892216\n",
      "train loss:2.294542865383446\n",
      "train loss:2.3035414117788813\n",
      "train loss:2.2979897890303937\n",
      "train loss:2.299519667917245\n",
      "train loss:2.297626553491872\n",
      "train loss:2.3019687356267893\n",
      "train loss:2.309511236703584\n",
      "train loss:2.3006936460502643\n",
      "train loss:2.304149004177436\n",
      "train loss:2.300063105813514\n",
      "train loss:2.298054106850827\n",
      "train loss:2.2941445510240586\n",
      "train loss:2.3057244414216034\n",
      "train loss:2.2992406628344817\n",
      "train loss:2.3007203628405866\n",
      "train loss:2.3037158402034326\n",
      "train loss:2.2999958972252177\n",
      "train loss:2.3078697956103853\n",
      "train loss:2.2994926938191407\n",
      "train loss:2.303932581292107\n",
      "train loss:2.2906517288254338\n",
      "train loss:2.295778540102118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.297348617203397\n",
      "train loss:2.3036898255157054\n",
      "train loss:2.3053131706392467\n",
      "train loss:2.2938254588245\n",
      "train loss:2.3050144113910953\n",
      "train loss:2.298104009541181\n",
      "train loss:2.303639069185558\n",
      "train loss:2.299003861927994\n",
      "train loss:2.307520315269691\n",
      "train loss:2.296823062660129\n",
      "train loss:2.306516000455613\n",
      "train loss:2.3098900852043776\n",
      "train loss:2.307843222127351\n",
      "train loss:2.2982913364555086\n",
      "train loss:2.3045581153217087\n",
      "train loss:2.304444041184944\n",
      "train loss:2.2973583879915127\n",
      "train loss:2.2948680941779\n",
      "train loss:2.305332966661559\n",
      "train loss:2.3010380094833\n",
      "train loss:2.2975745250921342\n",
      "train loss:2.3047218646272114\n",
      "train loss:2.301447458316829\n",
      "train loss:2.303138839158274\n",
      "train loss:2.3106927308640235\n",
      "train loss:2.3069984237056094\n",
      "train loss:2.29622502355736\n",
      "train loss:2.3000603948497913\n",
      "train loss:2.301996433558089\n",
      "train loss:2.2948588246804347\n",
      "train loss:2.2995444768742805\n",
      "train loss:2.301074821295632\n",
      "train loss:2.2927231896993816\n",
      "=== epoch:57, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.309519980803361\n",
      "train loss:2.3027560883761287\n",
      "train loss:2.294092566462261\n",
      "train loss:2.303262089963122\n",
      "train loss:2.300419383394503\n",
      "train loss:2.3008565909866374\n",
      "train loss:2.3045673800429616\n",
      "train loss:2.3079755429584687\n",
      "train loss:2.2900110358068173\n",
      "train loss:2.299887497362071\n",
      "train loss:2.3063865127693006\n",
      "train loss:2.2984478296784485\n",
      "train loss:2.3013912881645817\n",
      "train loss:2.3051077530487856\n",
      "train loss:2.307892504166976\n",
      "train loss:2.293694677638961\n",
      "train loss:2.296801689158368\n",
      "train loss:2.2950567483020072\n",
      "train loss:2.299532824812282\n",
      "train loss:2.28599832199942\n",
      "train loss:2.301195857320546\n",
      "train loss:2.3042639670430236\n",
      "train loss:2.2921055997870505\n",
      "train loss:2.2995926030054017\n",
      "train loss:2.3020562573805257\n",
      "train loss:2.294672627662835\n",
      "train loss:2.2984537301374597\n",
      "train loss:2.2921803540595977\n",
      "train loss:2.303902039534491\n",
      "train loss:2.3195875814147078\n",
      "train loss:2.312088236796442\n",
      "train loss:2.3010173322923015\n",
      "train loss:2.2979111253039597\n",
      "train loss:2.2961652651106865\n",
      "train loss:2.298437537592287\n",
      "train loss:2.2983162371972754\n",
      "train loss:2.300595590866301\n",
      "train loss:2.3004486396532693\n",
      "train loss:2.3114601306463887\n",
      "train loss:2.3064037093036402\n",
      "train loss:2.3008137719742128\n",
      "train loss:2.2948522488133443\n",
      "train loss:2.300039359561632\n",
      "train loss:2.3033087232670257\n",
      "train loss:2.3024393755680377\n",
      "train loss:2.295510984092386\n",
      "train loss:2.299408342957781\n",
      "train loss:2.302405357791611\n",
      "train loss:2.3089125440833764\n",
      "train loss:2.294489046430549\n",
      "train loss:2.3005241222748674\n",
      "train loss:2.3103153755560077\n",
      "train loss:2.302441334011147\n",
      "train loss:2.30017270773017\n",
      "train loss:2.293863766287209\n",
      "train loss:2.295411913903302\n",
      "train loss:2.298559350761038\n",
      "train loss:2.3024205245956653\n",
      "train loss:2.300091909713398\n",
      "train loss:2.2998231291072497\n",
      "train loss:2.2994345590974294\n",
      "train loss:2.295588625638722\n",
      "train loss:2.3081492237534853\n",
      "train loss:2.3042194747450995\n",
      "train loss:2.3067632197639503\n",
      "train loss:2.301964451837535\n",
      "train loss:2.296454967281261\n",
      "train loss:2.2976927456151914\n",
      "train loss:2.294135460374312\n",
      "train loss:2.306124882094792\n",
      "train loss:2.2941391242762275\n",
      "train loss:2.304671627186089\n",
      "train loss:2.303892048953363\n",
      "train loss:2.3085936335020905\n",
      "train loss:2.301847242561911\n",
      "train loss:2.305972113146677\n",
      "train loss:2.286560238342239\n",
      "train loss:2.302527631477235\n",
      "train loss:2.305502293689682\n",
      "train loss:2.302186704981255\n",
      "train loss:2.2995811330609004\n",
      "train loss:2.298069306157844\n",
      "train loss:2.301479973386827\n",
      "train loss:2.2998459891012146\n",
      "train loss:2.294143048458241\n",
      "train loss:2.3001666457228955\n",
      "train loss:2.293105999718336\n",
      "train loss:2.298910216908885\n",
      "train loss:2.305679089163618\n",
      "train loss:2.3065367163878028\n",
      "train loss:2.300598980078751\n",
      "train loss:2.3081078445411154\n",
      "train loss:2.2938596443719717\n",
      "train loss:2.3033618381675924\n",
      "train loss:2.301912715158209\n",
      "train loss:2.3032521365313676\n",
      "train loss:2.29501678334863\n",
      "train loss:2.3004614764698044\n",
      "train loss:2.2926017615875254\n",
      "train loss:2.305582614531769\n",
      "train loss:2.2996771209492786\n",
      "train loss:2.30020772396222\n",
      "train loss:2.3051242625172725\n",
      "train loss:2.293460475523709\n",
      "train loss:2.298083888024823\n",
      "train loss:2.3055801028290666\n",
      "train loss:2.296161364443366\n",
      "train loss:2.3016486755926633\n",
      "train loss:2.296505862336401\n",
      "train loss:2.3001848489488506\n",
      "train loss:2.29781648447383\n",
      "train loss:2.3007753749777904\n",
      "train loss:2.3035930666588107\n",
      "train loss:2.305286851925498\n",
      "train loss:2.3117844131169183\n",
      "train loss:2.298883199236686\n",
      "train loss:2.3048345689860033\n",
      "train loss:2.307864124847853\n",
      "train loss:2.301819385541549\n",
      "train loss:2.303337430821882\n",
      "train loss:2.3004956517523145\n",
      "train loss:2.3012451709747013\n",
      "train loss:2.3035793778736102\n",
      "train loss:2.293798367740356\n",
      "train loss:2.2994459197138695\n",
      "train loss:2.3095179703039763\n",
      "train loss:2.3033185667975475\n",
      "train loss:2.2977988315608333\n",
      "train loss:2.301624977572289\n",
      "train loss:2.298301473090739\n",
      "train loss:2.308111648952675\n",
      "train loss:2.2975720077214614\n",
      "train loss:2.2957001942886692\n",
      "train loss:2.2991518249326686\n",
      "train loss:2.2921730134830147\n",
      "train loss:2.2998234946036478\n",
      "train loss:2.3090664749971275\n",
      "train loss:2.292295304794331\n",
      "train loss:2.3030000960211505\n",
      "train loss:2.2956684067742024\n",
      "train loss:2.305601109743404\n",
      "train loss:2.301396465833541\n",
      "train loss:2.302964790060494\n",
      "train loss:2.3037983292145388\n",
      "train loss:2.296937795584358\n",
      "train loss:2.299043434324697\n",
      "train loss:2.30062848452728\n",
      "train loss:2.300645814157645\n",
      "train loss:2.3039525674367565\n",
      "train loss:2.3020451096841024\n",
      "train loss:2.297388215172199\n",
      "train loss:2.301788653146064\n",
      "train loss:2.2999888187394535\n",
      "train loss:2.299992482530321\n",
      "train loss:2.3057170873222987\n",
      "train loss:2.3011101941785808\n",
      "train loss:2.289876817835014\n",
      "train loss:2.306659269505645\n",
      "train loss:2.3069777170436434\n",
      "train loss:2.3029197111357314\n",
      "train loss:2.299899443129208\n",
      "train loss:2.297569328164598\n",
      "train loss:2.30161929788671\n",
      "train loss:2.300148744322402\n",
      "train loss:2.3003292975890828\n",
      "train loss:2.299057108487317\n",
      "train loss:2.2852686448962665\n",
      "train loss:2.3040205441707187\n",
      "train loss:2.3056207946667984\n",
      "train loss:2.302542728130724\n",
      "train loss:2.306689328948853\n",
      "train loss:2.3006764689714747\n",
      "train loss:2.299925051906782\n",
      "train loss:2.3003968315642784\n",
      "train loss:2.30422234187336\n",
      "train loss:2.305122591688139\n",
      "train loss:2.301906992876609\n",
      "train loss:2.3068534064521677\n",
      "train loss:2.305679784760039\n",
      "train loss:2.3014185617310274\n",
      "train loss:2.3016032188336673\n",
      "train loss:2.2996768874076734\n",
      "train loss:2.305513656283458\n",
      "train loss:2.3034234579911126\n",
      "train loss:2.2998411536909478\n",
      "train loss:2.3002650605312827\n",
      "train loss:2.3092875506387456\n",
      "train loss:2.296631676998962\n",
      "train loss:2.3042412519272792\n",
      "train loss:2.2968144713878975\n",
      "train loss:2.292594105591006\n",
      "train loss:2.3002275397457836\n",
      "train loss:2.2985941969279606\n",
      "train loss:2.2926366488428456\n",
      "train loss:2.2930629379081062\n",
      "train loss:2.299988376853225\n",
      "train loss:2.307444907271558\n",
      "train loss:2.3035099110768114\n",
      "train loss:2.3035636834047954\n",
      "train loss:2.2967361689669836\n",
      "=== epoch:58, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2949173582321665\n",
      "train loss:2.296454718034386\n",
      "train loss:2.300783554670376\n",
      "train loss:2.3043665356818663\n",
      "train loss:2.2988733495085585\n",
      "train loss:2.3011936272948414\n",
      "train loss:2.308761115544696\n",
      "train loss:2.294634155271263\n",
      "train loss:2.3038037581174278\n",
      "train loss:2.2987809614156416\n",
      "train loss:2.2985576386992985\n",
      "train loss:2.294847164408863\n",
      "train loss:2.3115679261355377\n",
      "train loss:2.2967091919360136\n",
      "train loss:2.298648263867367\n",
      "train loss:2.302878210444652\n",
      "train loss:2.2973873374315033\n",
      "train loss:2.3074300182876017\n",
      "train loss:2.2979643944567276\n",
      "train loss:2.2994103621656783\n",
      "train loss:2.299658286878674\n",
      "train loss:2.3039805209021647\n",
      "train loss:2.3000453988478116\n",
      "train loss:2.3047306177146076\n",
      "train loss:2.304300089660633\n",
      "train loss:2.2968841923290215\n",
      "train loss:2.3040965310596406\n",
      "train loss:2.2951833086823314\n",
      "train loss:2.3015547341593083\n",
      "train loss:2.3002044352025304\n",
      "train loss:2.304639633095456\n",
      "train loss:2.293380238565108\n",
      "train loss:2.2956210611153574\n",
      "train loss:2.3051538389182613\n",
      "train loss:2.3007688746206734\n",
      "train loss:2.3090770947362516\n",
      "train loss:2.296475764294816\n",
      "train loss:2.306134486613279\n",
      "train loss:2.3074801519320585\n",
      "train loss:2.304687260254495\n",
      "train loss:2.2955326727388092\n",
      "train loss:2.304078874582749\n",
      "train loss:2.3031839284172104\n",
      "train loss:2.306326214196461\n",
      "train loss:2.299486310581264\n",
      "train loss:2.288902107891156\n",
      "train loss:2.297943280105209\n",
      "train loss:2.3060700676127626\n",
      "train loss:2.2998885904409114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3031896534517085\n",
      "train loss:2.2962272610593937\n",
      "train loss:2.306577218631187\n",
      "train loss:2.300587998115288\n",
      "train loss:2.301872743873926\n",
      "train loss:2.2967869733706014\n",
      "train loss:2.2994331461249504\n",
      "train loss:2.2998484134140265\n",
      "train loss:2.303192914413409\n",
      "train loss:2.296993788255302\n",
      "train loss:2.3043824738915664\n",
      "train loss:2.28919987672286\n",
      "train loss:2.2987620977254557\n",
      "train loss:2.309167954504158\n",
      "train loss:2.3023922811725464\n",
      "train loss:2.3003363822258662\n",
      "train loss:2.301367677237312\n",
      "train loss:2.3069741699797293\n",
      "train loss:2.308682666584978\n",
      "train loss:2.303788252587558\n",
      "train loss:2.301783521400105\n",
      "train loss:2.30926241999133\n",
      "train loss:2.3009700320914397\n",
      "train loss:2.3006148654230496\n",
      "train loss:2.300549472682523\n",
      "train loss:2.3025280303153957\n",
      "train loss:2.298559673096221\n",
      "train loss:2.3075282950591864\n",
      "train loss:2.3061193664921356\n",
      "train loss:2.303124092640402\n",
      "train loss:2.3074470516324723\n",
      "train loss:2.3057422226940982\n",
      "train loss:2.295704082663736\n",
      "train loss:2.302727715648668\n",
      "train loss:2.3061846321093737\n",
      "train loss:2.303130810391222\n",
      "train loss:2.3009081163386575\n",
      "train loss:2.2989835454874346\n",
      "train loss:2.312452635306269\n",
      "train loss:2.30989469742036\n",
      "train loss:2.2985242618143165\n",
      "train loss:2.3074141058009214\n",
      "train loss:2.3030577431606005\n",
      "train loss:2.3066733425127364\n",
      "train loss:2.292123790103326\n",
      "train loss:2.3075100652679277\n",
      "train loss:2.3028431747381592\n",
      "train loss:2.3077720736261997\n",
      "train loss:2.306152773233568\n",
      "train loss:2.29520395819876\n",
      "train loss:2.301718679823852\n",
      "train loss:2.3001957509574273\n",
      "train loss:2.308016014656423\n",
      "train loss:2.301600768888857\n",
      "train loss:2.2999686087080153\n",
      "train loss:2.3044513219637244\n",
      "train loss:2.3011931098896428\n",
      "train loss:2.2974785673700975\n",
      "train loss:2.3068335540279135\n",
      "train loss:2.3019317260393293\n",
      "train loss:2.3058333559497073\n",
      "train loss:2.3026480836285814\n",
      "train loss:2.298431216900252\n",
      "train loss:2.295195978112271\n",
      "train loss:2.2989362524634385\n",
      "train loss:2.301175719030345\n",
      "train loss:2.2937329617042423\n",
      "train loss:2.3028122429909454\n",
      "train loss:2.308638530208516\n",
      "train loss:2.304332572474889\n",
      "train loss:2.2984163784540876\n",
      "train loss:2.296405190656495\n",
      "train loss:2.3022115876429075\n",
      "train loss:2.304572554745801\n",
      "train loss:2.2936974750328925\n",
      "train loss:2.304568494982324\n",
      "train loss:2.3032573377820347\n",
      "train loss:2.3067449102642965\n",
      "train loss:2.2913900854717046\n",
      "train loss:2.298362726407838\n",
      "train loss:2.3052232708311124\n",
      "train loss:2.302839128201077\n",
      "train loss:2.305098810469894\n",
      "train loss:2.3016782643584053\n",
      "train loss:2.299315622236224\n",
      "train loss:2.3038950207968\n",
      "train loss:2.3064371693823005\n",
      "train loss:2.298505065789661\n",
      "train loss:2.3051786717092417\n",
      "train loss:2.3028196313693896\n",
      "train loss:2.3026572944264267\n",
      "train loss:2.302315728845464\n",
      "train loss:2.2975935585122857\n",
      "train loss:2.301711797843096\n",
      "train loss:2.2978905138764714\n",
      "train loss:2.291251230990784\n",
      "train loss:2.2966276004582373\n",
      "train loss:2.305168551639361\n",
      "train loss:2.3130407520573764\n",
      "train loss:2.2970659308154753\n",
      "train loss:2.3068226014716324\n",
      "train loss:2.3016754618219477\n",
      "train loss:2.2963121247934066\n",
      "train loss:2.305647402647483\n",
      "train loss:2.301346484286721\n",
      "train loss:2.3001835817039957\n",
      "train loss:2.30095715772643\n",
      "train loss:2.3028644729189933\n",
      "train loss:2.298002916767271\n",
      "train loss:2.304667062889172\n",
      "train loss:2.298763744437039\n",
      "train loss:2.3049590515657035\n",
      "train loss:2.30496107887318\n",
      "train loss:2.297796225411811\n",
      "train loss:2.3065880571484927\n",
      "train loss:2.295928332645877\n",
      "train loss:2.3089719736010825\n",
      "train loss:2.301479950538387\n",
      "train loss:2.2965179887186227\n",
      "train loss:2.3030651010569656\n",
      "train loss:2.304403833326296\n",
      "train loss:2.307934829334272\n",
      "train loss:2.297998031102876\n",
      "train loss:2.29772959466936\n",
      "train loss:2.306672434759876\n",
      "train loss:2.3013716026996067\n",
      "train loss:2.3039224865754617\n",
      "train loss:2.299002388403732\n",
      "train loss:2.3012170930895537\n",
      "train loss:2.303938585904564\n",
      "train loss:2.300067249782314\n",
      "train loss:2.3000739114631004\n",
      "train loss:2.2985888544848203\n",
      "train loss:2.299182172527955\n",
      "train loss:2.302889140471938\n",
      "train loss:2.298415725954134\n",
      "train loss:2.304229636155265\n",
      "train loss:2.297030176929462\n",
      "train loss:2.300837527452869\n",
      "train loss:2.3015696969923005\n",
      "train loss:2.310462052776228\n",
      "train loss:2.3125531299534985\n",
      "train loss:2.3018573680362353\n",
      "train loss:2.317376201165511\n",
      "train loss:2.3011519888160703\n",
      "train loss:2.2979526894314115\n",
      "train loss:2.2940686940852095\n",
      "train loss:2.3043563239430163\n",
      "train loss:2.3087091666177275\n",
      "train loss:2.3000722635121438\n",
      "train loss:2.297103009191806\n",
      "=== epoch:59, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.300099021542692\n",
      "train loss:2.3061410454476543\n",
      "train loss:2.2971087919206328\n",
      "train loss:2.2979186355392764\n",
      "train loss:2.309047993771049\n",
      "train loss:2.3076143009496577\n",
      "train loss:2.2970794247619466\n",
      "train loss:2.31081804872514\n",
      "train loss:2.2994619695143417\n",
      "train loss:2.3013571053358826\n",
      "train loss:2.3028352577670423\n",
      "train loss:2.3034404233251626\n",
      "train loss:2.294306726251462\n",
      "train loss:2.303993125925626\n",
      "train loss:2.303956389241059\n",
      "train loss:2.294823654911438\n",
      "train loss:2.2962004397646854\n",
      "train loss:2.3027587631727195\n",
      "train loss:2.302446955764499\n",
      "train loss:2.293757232455473\n",
      "train loss:2.296730654501285\n",
      "train loss:2.3005237381824797\n",
      "train loss:2.2999364181325714\n",
      "train loss:2.304874122737816\n",
      "train loss:2.2988513529903556\n",
      "train loss:2.305239730607043\n",
      "train loss:2.2992585028137302\n",
      "train loss:2.300147506707162\n",
      "train loss:2.303550165463821\n",
      "train loss:2.3095127531743036\n",
      "train loss:2.3035813154523206\n",
      "train loss:2.298575733130809\n",
      "train loss:2.2929054592203575\n",
      "train loss:2.3014812571433505\n",
      "train loss:2.3000806512206924\n",
      "train loss:2.2970079553248026\n",
      "train loss:2.3075072481657553\n",
      "train loss:2.303829422182824\n",
      "train loss:2.301499915505936\n",
      "train loss:2.297660076517824\n",
      "train loss:2.3021782154644863\n",
      "train loss:2.3000928531527896\n",
      "train loss:2.3163339203650173\n",
      "train loss:2.3033658092340707\n",
      "train loss:2.3082896966573485\n",
      "train loss:2.303917562292772\n",
      "train loss:2.3097772752672703\n",
      "train loss:2.301953827341973\n",
      "train loss:2.2994838730653053\n",
      "train loss:2.297230371096645\n",
      "train loss:2.3008946934926073\n",
      "train loss:2.3055588790335912\n",
      "train loss:2.2968396360792696\n",
      "train loss:2.3018993626341007\n",
      "train loss:2.3027990144279014\n",
      "train loss:2.3036693171260847\n",
      "train loss:2.3055861039671575\n",
      "train loss:2.3032003045365723\n",
      "train loss:2.300617621021637\n",
      "train loss:2.29343629626221\n",
      "train loss:2.30076467946121\n",
      "train loss:2.3046666480982485\n",
      "train loss:2.3011795378863638\n",
      "train loss:2.3024436297300523\n",
      "train loss:2.2960605028141763\n",
      "train loss:2.303844871588775\n",
      "train loss:2.3061268036007156\n",
      "train loss:2.296673820131548\n",
      "train loss:2.298308164827962\n",
      "train loss:2.3177940913350796\n",
      "train loss:2.29528419893684\n",
      "train loss:2.2929652050368943\n",
      "train loss:2.2974086935464193\n",
      "train loss:2.300113480761214\n",
      "train loss:2.2962765795600624\n",
      "train loss:2.3062459029008515\n",
      "train loss:2.2989261281876874\n",
      "train loss:2.301331865579494\n",
      "train loss:2.2990482523443245\n",
      "train loss:2.3006911434977764\n",
      "train loss:2.3026364258740126\n",
      "train loss:2.307205865646575\n",
      "train loss:2.3008222645953693\n",
      "train loss:2.3038015418743685\n",
      "train loss:2.2977793593146205\n",
      "train loss:2.298465544624918\n",
      "train loss:2.2936740636628055\n",
      "train loss:2.2985775261356602\n",
      "train loss:2.3094406511477703\n",
      "train loss:2.29910496332223\n",
      "train loss:2.3029596542089235\n",
      "train loss:2.2993887536544575\n",
      "train loss:2.304666955059364\n",
      "train loss:2.2946361514886044\n",
      "train loss:2.298839754347014\n",
      "train loss:2.301408841112682\n",
      "train loss:2.3014651425697252\n",
      "train loss:2.3013925820987895\n",
      "train loss:2.3037975582710715\n",
      "train loss:2.303635199076761\n",
      "train loss:2.2978123772841306\n",
      "train loss:2.304764535450639\n",
      "train loss:2.29632738586247\n",
      "train loss:2.2978233955829825\n",
      "train loss:2.3014857033295657\n",
      "train loss:2.3019259208975775\n",
      "train loss:2.300817085041211\n",
      "train loss:2.2993455469315442\n",
      "train loss:2.3038349286017694\n",
      "train loss:2.3017965431325673\n",
      "train loss:2.2988334462099074\n",
      "train loss:2.3033367900853383\n",
      "train loss:2.3023819990423475\n",
      "train loss:2.301988437149885\n",
      "train loss:2.2954837917191155\n",
      "train loss:2.3005804304797435\n",
      "train loss:2.3000204113897778\n",
      "train loss:2.2973860032655926\n",
      "train loss:2.3033969699695187\n",
      "train loss:2.3096176809424693\n",
      "train loss:2.30711994494194\n",
      "train loss:2.311364186974015\n",
      "train loss:2.296735276949192\n",
      "train loss:2.2927962807280147\n",
      "train loss:2.299220025114378\n",
      "train loss:2.2996480316089105\n",
      "train loss:2.304597101288125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3024149262716604\n",
      "train loss:2.3079954589242058\n",
      "train loss:2.301248417910144\n",
      "train loss:2.2959428625279807\n",
      "train loss:2.2967859156540116\n",
      "train loss:2.2975123626150302\n",
      "train loss:2.3016851553594293\n",
      "train loss:2.3088006261827196\n",
      "train loss:2.3095143298324277\n",
      "train loss:2.299631184589804\n",
      "train loss:2.298502878556317\n",
      "train loss:2.3045745227821266\n",
      "train loss:2.303786363169665\n",
      "train loss:2.298890760182031\n",
      "train loss:2.311981737480502\n",
      "train loss:2.2993390478761784\n",
      "train loss:2.297506328018076\n",
      "train loss:2.305760074596378\n",
      "train loss:2.292578333033962\n",
      "train loss:2.3016466047772672\n",
      "train loss:2.2942172410085093\n",
      "train loss:2.295666931029572\n",
      "train loss:2.306462394389056\n",
      "train loss:2.3015826364049885\n",
      "train loss:2.2994408252060348\n",
      "train loss:2.293393474238187\n",
      "train loss:2.306953393866635\n",
      "train loss:2.3028138476564326\n",
      "train loss:2.2996553717820514\n",
      "train loss:2.302617673400198\n",
      "train loss:2.311299507727208\n",
      "train loss:2.304468466351965\n",
      "train loss:2.3012947942395683\n",
      "train loss:2.3020997706149657\n",
      "train loss:2.2959070306965326\n",
      "train loss:2.3056982070287724\n",
      "train loss:2.3003937608267107\n",
      "train loss:2.300668843107723\n",
      "train loss:2.296856871656804\n",
      "train loss:2.302484441628959\n",
      "train loss:2.2933798038361153\n",
      "train loss:2.3004749007697645\n",
      "train loss:2.303348415476154\n",
      "train loss:2.3028629391888833\n",
      "train loss:2.2962431310929943\n",
      "train loss:2.3012650630706872\n",
      "train loss:2.306192767288897\n",
      "train loss:2.310892454546234\n",
      "train loss:2.2988062643313243\n",
      "train loss:2.2943342929744044\n",
      "train loss:2.2990019826711605\n",
      "train loss:2.2991215643498517\n",
      "train loss:2.292277014125438\n",
      "train loss:2.2961212296209648\n",
      "train loss:2.30111650278448\n",
      "train loss:2.2986748808758177\n",
      "train loss:2.3018085707558895\n",
      "train loss:2.3006199906021503\n",
      "train loss:2.3042192924551967\n",
      "train loss:2.295368743068502\n",
      "train loss:2.296198505465143\n",
      "train loss:2.2952284860831016\n",
      "train loss:2.3021535637723076\n",
      "train loss:2.2968109596982234\n",
      "train loss:2.2973328402991444\n",
      "train loss:2.2967063518793642\n",
      "train loss:2.301433085461717\n",
      "train loss:2.300642744248286\n",
      "train loss:2.2991352696337737\n",
      "train loss:2.3091724599779186\n",
      "train loss:2.305426312745125\n",
      "train loss:2.311715801818106\n",
      "train loss:2.3060801444557426\n",
      "=== epoch:60, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301268116646755\n",
      "train loss:2.2999060306035553\n",
      "train loss:2.295989886656767\n",
      "train loss:2.298109454310937\n",
      "train loss:2.29612045410933\n",
      "train loss:2.2974008783929816\n",
      "train loss:2.303386108793018\n",
      "train loss:2.311112539283839\n",
      "train loss:2.3069254633973237\n",
      "train loss:2.307661649405743\n",
      "train loss:2.293911791598133\n",
      "train loss:2.2880122919969894\n",
      "train loss:2.3001886977649195\n",
      "train loss:2.2993047588867124\n",
      "train loss:2.306588353276337\n",
      "train loss:2.301399247154088\n",
      "train loss:2.307852089377819\n",
      "train loss:2.303307446840787\n",
      "train loss:2.3022112941981194\n",
      "train loss:2.3030873425631135\n",
      "train loss:2.2957246176350243\n",
      "train loss:2.2969202840451737\n",
      "train loss:2.2971929194589698\n",
      "train loss:2.305601566807492\n",
      "train loss:2.2986562696002384\n",
      "train loss:2.29906466975152\n",
      "train loss:2.2979164488958808\n",
      "train loss:2.295639579582931\n",
      "train loss:2.299752939201996\n",
      "train loss:2.3037172025326687\n",
      "train loss:2.299820765620393\n",
      "train loss:2.310064555372622\n",
      "train loss:2.3029875150261705\n",
      "train loss:2.308654281221656\n",
      "train loss:2.2937882847931\n",
      "train loss:2.3079585026223026\n",
      "train loss:2.2997476210509946\n",
      "train loss:2.2993114778092414\n",
      "train loss:2.296578327642808\n",
      "train loss:2.304531912032703\n",
      "train loss:2.2924024183694764\n",
      "train loss:2.294826392666811\n",
      "train loss:2.2966473434693775\n",
      "train loss:2.305166179257449\n",
      "train loss:2.3073820866362555\n",
      "train loss:2.3072250925261684\n",
      "train loss:2.309217239632455\n",
      "train loss:2.2955563905707947\n",
      "train loss:2.3073669721026278\n",
      "train loss:2.303959373180569\n",
      "train loss:2.30172165479303\n",
      "train loss:2.3118598036830296\n",
      "train loss:2.2950575532515427\n",
      "train loss:2.309517314950543\n",
      "train loss:2.301750911836468\n",
      "train loss:2.3038734699017196\n",
      "train loss:2.3010647116055876\n",
      "train loss:2.297361659144223\n",
      "train loss:2.303778979259574\n",
      "train loss:2.2971554168367296\n",
      "train loss:2.2942839892055007\n",
      "train loss:2.302233592708429\n",
      "train loss:2.2968753822609167\n",
      "train loss:2.3061271040871767\n",
      "train loss:2.3004644362201216\n",
      "train loss:2.2976662340599616\n",
      "train loss:2.3033300747100998\n",
      "train loss:2.30220081930038\n",
      "train loss:2.295100538396409\n",
      "train loss:2.3022781372684777\n",
      "train loss:2.2970018124049028\n",
      "train loss:2.3003125100198876\n",
      "train loss:2.302905376445771\n",
      "train loss:2.2994484447318264\n",
      "train loss:2.2953482266861056\n",
      "train loss:2.297411033400852\n",
      "train loss:2.3102629275985387\n",
      "train loss:2.2932570748968795\n",
      "train loss:2.3076811744451\n",
      "train loss:2.304716185553856\n",
      "train loss:2.307678937303416\n",
      "train loss:2.305015221911828\n",
      "train loss:2.3053503082424456\n",
      "train loss:2.305492297139309\n",
      "train loss:2.297796622929434\n",
      "train loss:2.309130461421107\n",
      "train loss:2.3073285903116005\n",
      "train loss:2.3115884212026065\n",
      "train loss:2.3068170398315098\n",
      "train loss:2.305193573819881\n",
      "train loss:2.3013607589346536\n",
      "train loss:2.2983066120000797\n",
      "train loss:2.2926949695744985\n",
      "train loss:2.2922334163196223\n",
      "train loss:2.30085841814121\n",
      "train loss:2.30473976305962\n",
      "train loss:2.2947049842985106\n",
      "train loss:2.302720906171649\n",
      "train loss:2.295642601118272\n",
      "train loss:2.3001486036206407\n",
      "train loss:2.299592417761412\n",
      "train loss:2.2992041977067985\n",
      "train loss:2.304159203978794\n",
      "train loss:2.3014095995076085\n",
      "train loss:2.2984657724110584\n",
      "train loss:2.3057609442077664\n",
      "train loss:2.295318015173549\n",
      "train loss:2.3044996702095553\n",
      "train loss:2.301832168087928\n",
      "train loss:2.3056909954929963\n",
      "train loss:2.2977129729401353\n",
      "train loss:2.299916086960456\n",
      "train loss:2.3025832211393604\n",
      "train loss:2.2988800382777153\n",
      "train loss:2.3011803985902675\n",
      "train loss:2.3019725783002056\n",
      "train loss:2.2997725786943737\n",
      "train loss:2.297533077520313\n",
      "train loss:2.3034268557286297\n",
      "train loss:2.3052157089123906\n",
      "train loss:2.298782565080076\n",
      "train loss:2.2986037427376482\n",
      "train loss:2.3053995392306756\n",
      "train loss:2.301700834317153\n",
      "train loss:2.3017363848583927\n",
      "train loss:2.2976024443025582\n",
      "train loss:2.2955171121947777\n",
      "train loss:2.298672374639202\n",
      "train loss:2.3008055522601376\n",
      "train loss:2.30302621987517\n",
      "train loss:2.298251650079758\n",
      "train loss:2.294047254079805\n",
      "train loss:2.303565870039138\n",
      "train loss:2.292446912612542\n",
      "train loss:2.2991802440459113\n",
      "train loss:2.306402206369015\n",
      "train loss:2.3032187061927907\n",
      "train loss:2.2884949498934275\n",
      "train loss:2.3029870288829075\n",
      "train loss:2.302955503804534\n",
      "train loss:2.2915783025153003\n",
      "train loss:2.300217356666029\n",
      "train loss:2.301342187069178\n",
      "train loss:2.29995742967113\n",
      "train loss:2.307314320305328\n",
      "train loss:2.3040210220737047\n",
      "train loss:2.306857804218765\n",
      "train loss:2.297864904264022\n",
      "train loss:2.308586266915238\n",
      "train loss:2.3008390582394447\n",
      "train loss:2.2992473172670698\n",
      "train loss:2.294582308354216\n",
      "train loss:2.2979957639189146\n",
      "train loss:2.2989484177242145\n",
      "train loss:2.3033261006322454\n",
      "train loss:2.303079056507219\n",
      "train loss:2.297811345344307\n",
      "train loss:2.3014675560965108\n",
      "train loss:2.309810600883406\n",
      "train loss:2.3001109897691285\n",
      "train loss:2.3052464304306075\n",
      "train loss:2.3045094134764335\n",
      "train loss:2.301933624807933\n",
      "train loss:2.3101372532276927\n",
      "train loss:2.3036638997689445\n",
      "train loss:2.311095443429846\n",
      "train loss:2.2981195658271454\n",
      "train loss:2.309465084937125\n",
      "train loss:2.2979226902173147\n",
      "train loss:2.3113723231899868\n",
      "train loss:2.3041170203643166\n",
      "train loss:2.3046294069034756\n",
      "train loss:2.3078087146977486\n",
      "train loss:2.2969066628437442\n",
      "train loss:2.311701157289711\n",
      "train loss:2.2944558178309427\n",
      "train loss:2.296066078695702\n",
      "train loss:2.300295823265683\n",
      "train loss:2.2998356817003645\n",
      "train loss:2.3055469386326735\n",
      "train loss:2.2888722490828424\n",
      "train loss:2.3040441411806927\n",
      "train loss:2.2984766210023504\n",
      "train loss:2.2982564823977754\n",
      "train loss:2.2992192406742697\n",
      "train loss:2.3059300869046275\n",
      "train loss:2.2962331204272544\n",
      "train loss:2.298587666864552\n",
      "train loss:2.3018468130290604\n",
      "train loss:2.3003754567618553\n",
      "train loss:2.3033949239966742\n",
      "train loss:2.302723118792699\n",
      "train loss:2.2986593680863177\n",
      "train loss:2.305124537143354\n",
      "train loss:2.3059629533531876\n",
      "train loss:2.3104643876077966\n",
      "train loss:2.3157275877400534\n",
      "train loss:2.3125314634147127\n",
      "train loss:2.3027410648760296\n",
      "train loss:2.2968588008178523\n",
      "=== epoch:61, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2999010826908277\n",
      "train loss:2.307660498147022\n",
      "train loss:2.295368557936881\n",
      "train loss:2.3007862822687515\n",
      "train loss:2.3005118698264537\n",
      "train loss:2.2938553083836464\n",
      "train loss:2.306794912998032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.294577506822344\n",
      "train loss:2.307679920007545\n",
      "train loss:2.2948122540737215\n",
      "train loss:2.2995752485556573\n",
      "train loss:2.3067431055338075\n",
      "train loss:2.299893249857054\n",
      "train loss:2.3079017014770047\n",
      "train loss:2.303270210937213\n",
      "train loss:2.296260299556261\n",
      "train loss:2.292203350876569\n",
      "train loss:2.304331826594898\n",
      "train loss:2.3046176031432477\n",
      "train loss:2.3002602216203503\n",
      "train loss:2.3065339337864144\n",
      "train loss:2.3026599285190463\n",
      "train loss:2.299168575387404\n",
      "train loss:2.3013424864455163\n",
      "train loss:2.310458221890035\n",
      "train loss:2.303122562613759\n",
      "train loss:2.3068443914363\n",
      "train loss:2.297764208372556\n",
      "train loss:2.301217204220834\n",
      "train loss:2.3006907656370834\n",
      "train loss:2.2965684453010913\n",
      "train loss:2.295581297072536\n",
      "train loss:2.2986789300112735\n",
      "train loss:2.2988005661262485\n",
      "train loss:2.3071693610358723\n",
      "train loss:2.310112546673296\n",
      "train loss:2.2956180339470595\n",
      "train loss:2.296603088113011\n",
      "train loss:2.304580213964092\n",
      "train loss:2.298932938482271\n",
      "train loss:2.310975952522181\n",
      "train loss:2.3008312157333997\n",
      "train loss:2.306778357977777\n",
      "train loss:2.309140996843081\n",
      "train loss:2.298430276259806\n",
      "train loss:2.303098080050083\n",
      "train loss:2.2983681692100992\n",
      "train loss:2.2976569776282907\n",
      "train loss:2.30413807986553\n",
      "train loss:2.30666207864576\n",
      "train loss:2.3006793517016746\n",
      "train loss:2.3046323361213776\n",
      "train loss:2.3012288410796296\n",
      "train loss:2.3162864093639977\n",
      "train loss:2.2891999170175312\n",
      "train loss:2.3076112480493585\n",
      "train loss:2.2995767246860903\n",
      "train loss:2.294644487656207\n",
      "train loss:2.3044812865497843\n",
      "train loss:2.3042353049038025\n",
      "train loss:2.3019281142448746\n",
      "train loss:2.3069830485020164\n",
      "train loss:2.3000413118662113\n",
      "train loss:2.2993672460619683\n",
      "train loss:2.2942336293482133\n",
      "train loss:2.304589467421422\n",
      "train loss:2.3019311653344583\n",
      "train loss:2.3033643364978644\n",
      "train loss:2.302452290137247\n",
      "train loss:2.3010245664344193\n",
      "train loss:2.291905080528747\n",
      "train loss:2.300039785141165\n",
      "train loss:2.3057208715341613\n",
      "train loss:2.306538716031956\n",
      "train loss:2.301790235016029\n",
      "train loss:2.292392029261693\n",
      "train loss:2.2980210543724313\n",
      "train loss:2.299304371889113\n",
      "train loss:2.299973862630056\n",
      "train loss:2.3009906117118075\n",
      "train loss:2.2921431460252886\n",
      "train loss:2.3073226009311\n",
      "train loss:2.305665942235595\n",
      "train loss:2.3001571123345994\n",
      "train loss:2.305262442825846\n",
      "train loss:2.295937517573517\n",
      "train loss:2.29599996977814\n",
      "train loss:2.30514116288088\n",
      "train loss:2.304088832776777\n",
      "train loss:2.3035112622917326\n",
      "train loss:2.3040927680736494\n",
      "train loss:2.3020289815614157\n",
      "train loss:2.2979764246037466\n",
      "train loss:2.302545058260294\n",
      "train loss:2.303669794497831\n",
      "train loss:2.3059681321901313\n",
      "train loss:2.302000839170566\n",
      "train loss:2.2965121438866833\n",
      "train loss:2.311504445291479\n",
      "train loss:2.2983459560439012\n",
      "train loss:2.3034919819936084\n",
      "train loss:2.2953252774333746\n",
      "train loss:2.3063257967723128\n",
      "train loss:2.307842905720661\n",
      "train loss:2.3086915164077193\n",
      "train loss:2.3033815781128517\n",
      "train loss:2.2995764998347576\n",
      "train loss:2.302961862553697\n",
      "train loss:2.3019198189518955\n",
      "train loss:2.2892028638496957\n",
      "train loss:2.2959169201639837\n",
      "train loss:2.3021423511481864\n",
      "train loss:2.3000266023622915\n",
      "train loss:2.307074132048974\n",
      "train loss:2.299018544642941\n",
      "train loss:2.3006094052949653\n",
      "train loss:2.3005017396010903\n",
      "train loss:2.306357931321489\n",
      "train loss:2.300786213676728\n",
      "train loss:2.300263489893793\n",
      "train loss:2.302084387155999\n",
      "train loss:2.3032939189407364\n",
      "train loss:2.2994450781963636\n",
      "train loss:2.306854557780719\n",
      "train loss:2.302239781747213\n",
      "train loss:2.2919897878095004\n",
      "train loss:2.301490000543165\n",
      "train loss:2.3022492242447474\n",
      "train loss:2.303242558394607\n",
      "train loss:2.3025726331649405\n",
      "train loss:2.299886408173579\n",
      "train loss:2.305340685223028\n",
      "train loss:2.3004176752174077\n",
      "train loss:2.2981969057999803\n",
      "train loss:2.2910266187794424\n",
      "train loss:2.304684103026794\n",
      "train loss:2.3064801742795265\n",
      "train loss:2.293500526819322\n",
      "train loss:2.3040651826777667\n",
      "train loss:2.303896887364142\n",
      "train loss:2.294389198189482\n",
      "train loss:2.296922469100891\n",
      "train loss:2.3030289250959255\n",
      "train loss:2.3082246751331845\n",
      "train loss:2.3039440403884743\n",
      "train loss:2.3029224410826212\n",
      "train loss:2.2959182759664354\n",
      "train loss:2.2980713780402438\n",
      "train loss:2.3068562127288015\n",
      "train loss:2.303996669910802\n",
      "train loss:2.3077571298259145\n",
      "train loss:2.3139211616651947\n",
      "train loss:2.2971718367101612\n",
      "train loss:2.2965704782532574\n",
      "train loss:2.3054748423268565\n",
      "train loss:2.2968580454451404\n",
      "train loss:2.302616059813372\n",
      "train loss:2.299153653065604\n",
      "train loss:2.309388333790823\n",
      "train loss:2.2992135768041675\n",
      "train loss:2.3026872659261075\n",
      "train loss:2.294800773698598\n",
      "train loss:2.2986652384067607\n",
      "train loss:2.2985699580270715\n",
      "train loss:2.2986881034151057\n",
      "train loss:2.29946593316737\n",
      "train loss:2.296261586201637\n",
      "train loss:2.2972671038584918\n",
      "train loss:2.2983182514953784\n",
      "train loss:2.302367326211968\n",
      "train loss:2.300530057369412\n",
      "train loss:2.3051358225097913\n",
      "train loss:2.3052264016240227\n",
      "train loss:2.2961046272194374\n",
      "train loss:2.3003888145769125\n",
      "train loss:2.297194830358303\n",
      "train loss:2.2973057619759847\n",
      "train loss:2.3085145412509385\n",
      "train loss:2.2976905343711955\n",
      "train loss:2.2966855437274956\n",
      "train loss:2.3012698178489246\n",
      "train loss:2.2923968103521686\n",
      "train loss:2.2981769930930867\n",
      "train loss:2.308513961077496\n",
      "train loss:2.309542143417052\n",
      "train loss:2.3119379247287366\n",
      "train loss:2.2989157128077626\n",
      "train loss:2.3030408826881694\n",
      "train loss:2.2982801212158144\n",
      "train loss:2.30175685998626\n",
      "train loss:2.3099998053693214\n",
      "train loss:2.310604975280682\n",
      "train loss:2.3030425150670455\n",
      "train loss:2.2953295413315553\n",
      "train loss:2.299429143843896\n",
      "train loss:2.295183470711042\n",
      "train loss:2.2999710295304214\n",
      "train loss:2.3109962751463287\n",
      "train loss:2.290093218138962\n",
      "train loss:2.294280399866712\n",
      "=== epoch:62, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3038353143718053\n",
      "train loss:2.298132424710691\n",
      "train loss:2.2988059631879953\n",
      "train loss:2.3042570206353585\n",
      "train loss:2.30268722273168\n",
      "train loss:2.3025497906826566\n",
      "train loss:2.3016383182910367\n",
      "train loss:2.3066516017643695\n",
      "train loss:2.30861956934514\n",
      "train loss:2.3070111020108226\n",
      "train loss:2.292706762712507\n",
      "train loss:2.3042314076626558\n",
      "train loss:2.3029900205997604\n",
      "train loss:2.299526507802152\n",
      "train loss:2.294275589820791\n",
      "train loss:2.306001546178043\n",
      "train loss:2.2991649874125715\n",
      "train loss:2.3104262357002714\n",
      "train loss:2.295162632505591\n",
      "train loss:2.2986750704715844\n",
      "train loss:2.300267994358072\n",
      "train loss:2.3102101222190368\n",
      "train loss:2.2948309346682767\n",
      "train loss:2.305520551689066\n",
      "train loss:2.303841517813138\n",
      "train loss:2.3089914520145696\n",
      "train loss:2.30727145099629\n",
      "train loss:2.305644600107772\n",
      "train loss:2.301467915035968\n",
      "train loss:2.3107510834139\n",
      "train loss:2.2986034010927487\n",
      "train loss:2.306171508317893\n",
      "train loss:2.3099391977914396\n",
      "train loss:2.295673939532027\n",
      "train loss:2.3015818162807067\n",
      "train loss:2.304263967326256\n",
      "train loss:2.299537631558296\n",
      "train loss:2.2997154568248743\n",
      "train loss:2.296907143654814\n",
      "train loss:2.303347100529141\n",
      "train loss:2.298772461816815\n",
      "train loss:2.3098917244133252\n",
      "train loss:2.2990824441601405\n",
      "train loss:2.3070406178317575\n",
      "train loss:2.30373155100038\n",
      "train loss:2.304381531400403\n",
      "train loss:2.2972165756221044\n",
      "train loss:2.301667401878272\n",
      "train loss:2.2999172809336472\n",
      "train loss:2.300312512657859\n",
      "train loss:2.2977614911902493\n",
      "train loss:2.2989470638069256\n",
      "train loss:2.3011397353264664\n",
      "train loss:2.3001067428313084\n",
      "train loss:2.2987313163401466\n",
      "train loss:2.3001314494383815\n",
      "train loss:2.3015524831387846\n",
      "train loss:2.3049502768676797\n",
      "train loss:2.296600838376937\n",
      "train loss:2.2950633391901074\n",
      "train loss:2.3058161550541922\n",
      "train loss:2.307175842261278\n",
      "train loss:2.2972009908278443\n",
      "train loss:2.299332330443769\n",
      "train loss:2.30311282133435\n",
      "train loss:2.3012200577343047\n",
      "train loss:2.298790325432062\n",
      "train loss:2.3102585529986244\n",
      "train loss:2.3092633164836225\n",
      "train loss:2.302171936201964\n",
      "train loss:2.300348880700164\n",
      "train loss:2.3005685794666486\n",
      "train loss:2.2945628580778155\n",
      "train loss:2.2974827086489387\n",
      "train loss:2.29763081317049\n",
      "train loss:2.296195540776788\n",
      "train loss:2.2985800749435694\n",
      "train loss:2.298235908033457\n",
      "train loss:2.3069327667167103\n",
      "train loss:2.2954550380923706\n",
      "train loss:2.3020803761567485\n",
      "train loss:2.3005584436294555\n",
      "train loss:2.293739156404194\n",
      "train loss:2.300124089548265\n",
      "train loss:2.3026901521849044\n",
      "train loss:2.3003776625802237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.292992892038033\n",
      "train loss:2.3042529567362715\n",
      "train loss:2.3030798499609544\n",
      "train loss:2.293866092683307\n",
      "train loss:2.3021845251106194\n",
      "train loss:2.3014334765080777\n",
      "train loss:2.2925446222519565\n",
      "train loss:2.3056654437085373\n",
      "train loss:2.292809901189717\n",
      "train loss:2.2975677304511515\n",
      "train loss:2.2919529665413974\n",
      "train loss:2.299362186192223\n",
      "train loss:2.3070365327367712\n",
      "train loss:2.306679613460318\n",
      "train loss:2.305762159857124\n",
      "train loss:2.298233648510192\n",
      "train loss:2.293863727449225\n",
      "train loss:2.304976677304168\n",
      "train loss:2.2962602155628824\n",
      "train loss:2.301615092162127\n",
      "train loss:2.3016784074968673\n",
      "train loss:2.306532732138641\n",
      "train loss:2.297368694252133\n",
      "train loss:2.2984951211760216\n",
      "train loss:2.3000416702325475\n",
      "train loss:2.303540518479233\n",
      "train loss:2.302436995128\n",
      "train loss:2.3084377889496683\n",
      "train loss:2.303790691984947\n",
      "train loss:2.2947848861733386\n",
      "train loss:2.300234302172462\n",
      "train loss:2.309212396570562\n",
      "train loss:2.3013365279324804\n",
      "train loss:2.3069702816209268\n",
      "train loss:2.301019030603993\n",
      "train loss:2.298621530780053\n",
      "train loss:2.3000388661022653\n",
      "train loss:2.3027908396952563\n",
      "train loss:2.305261450211607\n",
      "train loss:2.3032232052895814\n",
      "train loss:2.2949524396137524\n",
      "train loss:2.3136439974119885\n",
      "train loss:2.2976870488034455\n",
      "train loss:2.3006144619554103\n",
      "train loss:2.2952769365458745\n",
      "train loss:2.29349757911438\n",
      "train loss:2.296534556656082\n",
      "train loss:2.301767638872129\n",
      "train loss:2.3079798897850576\n",
      "train loss:2.2973703067229354\n",
      "train loss:2.2958848353160826\n",
      "train loss:2.2992237569909744\n",
      "train loss:2.296001744856318\n",
      "train loss:2.299341916995581\n",
      "train loss:2.2972481529944533\n",
      "train loss:2.2998300904137707\n",
      "train loss:2.2943534898430364\n",
      "train loss:2.300046926745089\n",
      "train loss:2.3023174148038015\n",
      "train loss:2.2953529210274635\n",
      "train loss:2.300313513462677\n",
      "train loss:2.3012154246948704\n",
      "train loss:2.302074865485239\n",
      "train loss:2.3067469491031996\n",
      "train loss:2.2957901566712344\n",
      "train loss:2.296912675713276\n",
      "train loss:2.299884893163456\n",
      "train loss:2.306937938028128\n",
      "train loss:2.300814754077222\n",
      "train loss:2.29924283741143\n",
      "train loss:2.309506827264551\n",
      "train loss:2.3114795919159437\n",
      "train loss:2.2998902708556273\n",
      "train loss:2.298125740192244\n",
      "train loss:2.3021240600930324\n",
      "train loss:2.2964477108888564\n",
      "train loss:2.3010611850290985\n",
      "train loss:2.3132523629748083\n",
      "train loss:2.3030429672287998\n",
      "train loss:2.301522089837889\n",
      "train loss:2.3034298307382888\n",
      "train loss:2.3002207285134153\n",
      "train loss:2.3035035534641533\n",
      "train loss:2.302794595380342\n",
      "train loss:2.292828149557204\n",
      "train loss:2.299030347923098\n",
      "train loss:2.3034177656508796\n",
      "train loss:2.3065441316607256\n",
      "train loss:2.2989313971953784\n",
      "train loss:2.293064545545485\n",
      "train loss:2.306090653637095\n",
      "train loss:2.3130638060682367\n",
      "train loss:2.302311943430331\n",
      "train loss:2.303242228829191\n",
      "train loss:2.308033987520211\n",
      "train loss:2.299134760965259\n",
      "train loss:2.300070692795124\n",
      "train loss:2.302491868451353\n",
      "train loss:2.3006721345486802\n",
      "train loss:2.30768422072042\n",
      "train loss:2.2902759545881324\n",
      "train loss:2.307274277234901\n",
      "train loss:2.297050037422096\n",
      "train loss:2.2974263608908836\n",
      "train loss:2.296817838802768\n",
      "train loss:2.2970901713207468\n",
      "train loss:2.295082157756364\n",
      "train loss:2.301560018724863\n",
      "train loss:2.302700895486613\n",
      "train loss:2.3000744783068563\n",
      "train loss:2.295287455002163\n",
      "train loss:2.3068579757016665\n",
      "train loss:2.302934522238911\n",
      "train loss:2.300333749870014\n",
      "=== epoch:63, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3083486651255125\n",
      "train loss:2.2976792376629427\n",
      "train loss:2.308041270067276\n",
      "train loss:2.3026551186104447\n",
      "train loss:2.3032019218108872\n",
      "train loss:2.2990061026746167\n",
      "train loss:2.306444143227825\n",
      "train loss:2.3032497372591965\n",
      "train loss:2.291859573948203\n",
      "train loss:2.299119591207171\n",
      "train loss:2.303872667670683\n",
      "train loss:2.2981571127880165\n",
      "train loss:2.3057423937330173\n",
      "train loss:2.307148282308768\n",
      "train loss:2.302236015251255\n",
      "train loss:2.3051489764887347\n",
      "train loss:2.3016983313908983\n",
      "train loss:2.3052949906719595\n",
      "train loss:2.300218401293746\n",
      "train loss:2.3018972477801674\n",
      "train loss:2.3054450373213613\n",
      "train loss:2.2913745614730656\n",
      "train loss:2.303532808564017\n",
      "train loss:2.2984933639623257\n",
      "train loss:2.3048460370861497\n",
      "train loss:2.3042373571142307\n",
      "train loss:2.302532495911557\n",
      "train loss:2.3010848868427014\n",
      "train loss:2.3012617393869603\n",
      "train loss:2.3036541576483285\n",
      "train loss:2.2968476033899425\n",
      "train loss:2.3087361930396257\n",
      "train loss:2.305022220012032\n",
      "train loss:2.2903990398382748\n",
      "train loss:2.302863494538862\n",
      "train loss:2.3012524877579974\n",
      "train loss:2.3047071001703148\n",
      "train loss:2.3015246633857176\n",
      "train loss:2.2982053638945295\n",
      "train loss:2.3058622903849897\n",
      "train loss:2.299047640962574\n",
      "train loss:2.3030418804171893\n",
      "train loss:2.3041925389092976\n",
      "train loss:2.297976511186313\n",
      "train loss:2.3066735630072115\n",
      "train loss:2.2958912428674783\n",
      "train loss:2.3058960140626836\n",
      "train loss:2.297088239081251\n",
      "train loss:2.3038960162080073\n",
      "train loss:2.297111098943531\n",
      "train loss:2.3045905159577735\n",
      "train loss:2.303021253677176\n",
      "train loss:2.3035870900586373\n",
      "train loss:2.301172260136753\n",
      "train loss:2.2986418456988615\n",
      "train loss:2.2971542082841276\n",
      "train loss:2.2991648601613814\n",
      "train loss:2.3133295546412893\n",
      "train loss:2.3016735442612015\n",
      "train loss:2.295473926480996\n",
      "train loss:2.301935256385983\n",
      "train loss:2.307103700107493\n",
      "train loss:2.2902222239891916\n",
      "train loss:2.3037601034632473\n",
      "train loss:2.3009615866782673\n",
      "train loss:2.3031499912903506\n",
      "train loss:2.302688245615231\n",
      "train loss:2.3029562659044998\n",
      "train loss:2.304302952758887\n",
      "train loss:2.3016028555201156\n",
      "train loss:2.296331374507849\n",
      "train loss:2.2957671904935193\n",
      "train loss:2.307356107189925\n",
      "train loss:2.297559548788521\n",
      "train loss:2.2998456969186045\n",
      "train loss:2.291195501174884\n",
      "train loss:2.3010559925929033\n",
      "train loss:2.2999926430834563\n",
      "train loss:2.3059940850872627\n",
      "train loss:2.3076860327604125\n",
      "train loss:2.301110336750507\n",
      "train loss:2.3054951836377287\n",
      "train loss:2.300005312587103\n",
      "train loss:2.2996933116688547\n",
      "train loss:2.3036283170216203\n",
      "train loss:2.296886731415304\n",
      "train loss:2.2973847478159266\n",
      "train loss:2.3066762733946953\n",
      "train loss:2.3011608306821416\n",
      "train loss:2.2975155060898067\n",
      "train loss:2.298200191697504\n",
      "train loss:2.3095993027851502\n",
      "train loss:2.292095398962949\n",
      "train loss:2.297170866841932\n",
      "train loss:2.299029388209268\n",
      "train loss:2.3016094490963104\n",
      "train loss:2.3031503172818106\n",
      "train loss:2.3107385641620817\n",
      "train loss:2.294037880128033\n",
      "train loss:2.2934285672540877\n",
      "train loss:2.292430642582443\n",
      "train loss:2.2920467784116627\n",
      "train loss:2.29793919025928\n",
      "train loss:2.3020800163316473\n",
      "train loss:2.3016702503911\n",
      "train loss:2.3020308895340142\n",
      "train loss:2.3080406429692095\n",
      "train loss:2.308413752485308\n",
      "train loss:2.298609984763089\n",
      "train loss:2.299499392537987\n",
      "train loss:2.30731208011906\n",
      "train loss:2.304242876981654\n",
      "train loss:2.3083067530492545\n",
      "train loss:2.2942425924473935\n",
      "train loss:2.300822141014135\n",
      "train loss:2.3070122524579735\n",
      "train loss:2.3022898740771107\n",
      "train loss:2.312620731260793\n",
      "train loss:2.3084760506573634\n",
      "train loss:2.3060759854092816\n",
      "train loss:2.289456549277149\n",
      "train loss:2.303777632348781\n",
      "train loss:2.297069308408579\n",
      "train loss:2.3010222942662693\n",
      "train loss:2.3055656774799176\n",
      "train loss:2.311076429195418\n",
      "train loss:2.3062251869533\n",
      "train loss:2.2911140172196798\n",
      "train loss:2.3048935507505766\n",
      "train loss:2.299719352527983\n",
      "train loss:2.301947335829182\n",
      "train loss:2.292356485575509\n",
      "train loss:2.3032704820593093\n",
      "train loss:2.2996068995710286\n",
      "train loss:2.2979239728339462\n",
      "train loss:2.3013915943007457\n",
      "train loss:2.3030893297436146\n",
      "train loss:2.303069934278098\n",
      "train loss:2.3051064619618336\n",
      "train loss:2.304688696507719\n",
      "train loss:2.2952409847207798\n",
      "train loss:2.2991768891920406\n",
      "train loss:2.3080641734868466\n",
      "train loss:2.311553701889969\n",
      "train loss:2.290231384619384\n",
      "train loss:2.3068708965112026\n",
      "train loss:2.298875298538194\n",
      "train loss:2.303751418147195\n",
      "train loss:2.307824067033369\n",
      "train loss:2.290031779395889\n",
      "train loss:2.3063887798248284\n",
      "train loss:2.306571298583633\n",
      "train loss:2.3042630241693187\n",
      "train loss:2.30371162157987\n",
      "train loss:2.3054048898720993\n",
      "train loss:2.300999269242858\n",
      "train loss:2.300421231138535\n",
      "train loss:2.297102847742993\n",
      "train loss:2.3055535469742896\n",
      "train loss:2.2990815403530123\n",
      "train loss:2.2973270016619827\n",
      "train loss:2.299432040928286\n",
      "train loss:2.299305875822291\n",
      "train loss:2.2988644719649747\n",
      "train loss:2.3019278533052434\n",
      "train loss:2.304410120467648\n",
      "train loss:2.3009950827128396\n",
      "train loss:2.2994003074950187\n",
      "train loss:2.302130329286942\n",
      "train loss:2.29971672263431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301945844780803\n",
      "train loss:2.2964828083223017\n",
      "train loss:2.302151618518129\n",
      "train loss:2.3049129299304996\n",
      "train loss:2.299545489645734\n",
      "train loss:2.3043459644181628\n",
      "train loss:2.2988557242250502\n",
      "train loss:2.310052130450596\n",
      "train loss:2.287988395212634\n",
      "train loss:2.298796825059645\n",
      "train loss:2.3018390487846307\n",
      "train loss:2.2998578911369503\n",
      "train loss:2.297036279690466\n",
      "train loss:2.30932908169185\n",
      "train loss:2.3009479494842977\n",
      "train loss:2.2999931234207587\n",
      "train loss:2.29987627886691\n",
      "train loss:2.3017516835785456\n",
      "train loss:2.297438031498912\n",
      "train loss:2.3038506330156885\n",
      "train loss:2.2987131142322896\n",
      "train loss:2.2931125986400205\n",
      "train loss:2.3106634915762694\n",
      "train loss:2.300851450525767\n",
      "train loss:2.307053530177071\n",
      "train loss:2.3002325692352015\n",
      "train loss:2.305581047376277\n",
      "train loss:2.2970216651222706\n",
      "train loss:2.3023260554900764\n",
      "train loss:2.3103574329928116\n",
      "=== epoch:64, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3112025787719777\n",
      "train loss:2.3063097139382545\n",
      "train loss:2.3059914942634077\n",
      "train loss:2.3041161243462014\n",
      "train loss:2.3038395165975833\n",
      "train loss:2.2988538048110385\n",
      "train loss:2.3037341202302186\n",
      "train loss:2.3056548692401924\n",
      "train loss:2.2936732806594615\n",
      "train loss:2.3104497501666392\n",
      "train loss:2.306869409388161\n",
      "train loss:2.305755766993712\n",
      "train loss:2.306287603570732\n",
      "train loss:2.304246045713511\n",
      "train loss:2.3095555920101614\n",
      "train loss:2.3046718093955474\n",
      "train loss:2.297371633359286\n",
      "train loss:2.309725457371317\n",
      "train loss:2.3003193263523807\n",
      "train loss:2.3057828139731584\n",
      "train loss:2.2985174657106215\n",
      "train loss:2.304473981648863\n",
      "train loss:2.2965617197472605\n",
      "train loss:2.301268689659119\n",
      "train loss:2.307645668659461\n",
      "train loss:2.3052764382281894\n",
      "train loss:2.306836549666573\n",
      "train loss:2.3031808061256065\n",
      "train loss:2.301850328485429\n",
      "train loss:2.2991015697390838\n",
      "train loss:2.2990189418878253\n",
      "train loss:2.3059608920250283\n",
      "train loss:2.2987839435917805\n",
      "train loss:2.302333330814635\n",
      "train loss:2.3009614457675647\n",
      "train loss:2.303281327407653\n",
      "train loss:2.3063147686965477\n",
      "train loss:2.301065281240422\n",
      "train loss:2.3064328147937823\n",
      "train loss:2.301786285945232\n",
      "train loss:2.2975552686936\n",
      "train loss:2.3029244932866106\n",
      "train loss:2.303227312751279\n",
      "train loss:2.296066738278581\n",
      "train loss:2.3007538520780346\n",
      "train loss:2.302976280562353\n",
      "train loss:2.3026384289456368\n",
      "train loss:2.3094141048680177\n",
      "train loss:2.3022582858643896\n",
      "train loss:2.301041654363188\n",
      "train loss:2.302374490227501\n",
      "train loss:2.3106964733532935\n",
      "train loss:2.2941348157421944\n",
      "train loss:2.2985252583010594\n",
      "train loss:2.3068059480239977\n",
      "train loss:2.2956675363876067\n",
      "train loss:2.2981471583354667\n",
      "train loss:2.298585655802011\n",
      "train loss:2.300840807294299\n",
      "train loss:2.29741438450478\n",
      "train loss:2.2982060281466863\n",
      "train loss:2.3021788539030275\n",
      "train loss:2.308429190411486\n",
      "train loss:2.2964530495483864\n",
      "train loss:2.295179475257599\n",
      "train loss:2.3033257251109136\n",
      "train loss:2.303775705861977\n",
      "train loss:2.3042003605153685\n",
      "train loss:2.306131364111453\n",
      "train loss:2.3033574889530293\n",
      "train loss:2.2970899521144097\n",
      "train loss:2.3064904159699324\n",
      "train loss:2.3029786226543276\n",
      "train loss:2.302653320624796\n",
      "train loss:2.3074053452494696\n",
      "train loss:2.299279732597748\n",
      "train loss:2.3019833375452547\n",
      "train loss:2.29458010236424\n",
      "train loss:2.307452658450807\n",
      "train loss:2.3073094263718774\n",
      "train loss:2.3006250234613046\n",
      "train loss:2.2995128623998755\n",
      "train loss:2.3025544409496237\n",
      "train loss:2.3057977089145183\n",
      "train loss:2.3009127531565574\n",
      "train loss:2.3077997425774206\n",
      "train loss:2.300441483338722\n",
      "train loss:2.301439956626621\n",
      "train loss:2.299518228137808\n",
      "train loss:2.306048684722744\n",
      "train loss:2.300124837370219\n",
      "train loss:2.306551077433521\n",
      "train loss:2.300373297729233\n",
      "train loss:2.3009571525191674\n",
      "train loss:2.298384077366705\n",
      "train loss:2.311655643871746\n",
      "train loss:2.2981731696766636\n",
      "train loss:2.3079668374533115\n",
      "train loss:2.29209682115274\n",
      "train loss:2.3044653384168665\n",
      "train loss:2.298716854621359\n",
      "train loss:2.3024427136306835\n",
      "train loss:2.299227039118121\n",
      "train loss:2.2965655820841575\n",
      "train loss:2.299833679010171\n",
      "train loss:2.3093175903169603\n",
      "train loss:2.309140998211634\n",
      "train loss:2.3017756916328467\n",
      "train loss:2.3031952495747587\n",
      "train loss:2.3031316257223486\n",
      "train loss:2.3043586295920293\n",
      "train loss:2.295664439030668\n",
      "train loss:2.2928991497455455\n",
      "train loss:2.2967400253975447\n",
      "train loss:2.3077283255604364\n",
      "train loss:2.305362768202051\n",
      "train loss:2.298533005022351\n",
      "train loss:2.298473390553184\n",
      "train loss:2.299522550464899\n",
      "train loss:2.3102425044423116\n",
      "train loss:2.2979504685620173\n",
      "train loss:2.304359975402997\n",
      "train loss:2.2998222995351387\n",
      "train loss:2.3072162354700936\n",
      "train loss:2.294234204738342\n",
      "train loss:2.3035988709531177\n",
      "train loss:2.304632974894759\n",
      "train loss:2.301109122340611\n",
      "train loss:2.3016623266665213\n",
      "train loss:2.295490300553904\n",
      "train loss:2.3143487485935355\n",
      "train loss:2.302488710182687\n",
      "train loss:2.307240014632657\n",
      "train loss:2.2979135960215875\n",
      "train loss:2.3046605239641225\n",
      "train loss:2.295282593935894\n",
      "train loss:2.305246660882521\n",
      "train loss:2.287365896554489\n",
      "train loss:2.298336118795028\n",
      "train loss:2.293694425402783\n",
      "train loss:2.295727523132799\n",
      "train loss:2.300600523102428\n",
      "train loss:2.2964256742821045\n",
      "train loss:2.304152793433516\n",
      "train loss:2.3036079691569236\n",
      "train loss:2.304968515837727\n",
      "train loss:2.300414991473343\n",
      "train loss:2.2982217211055658\n",
      "train loss:2.291665408336045\n",
      "train loss:2.3065979762616124\n",
      "train loss:2.305237251654345\n",
      "train loss:2.2950558394650735\n",
      "train loss:2.2973088136249027\n",
      "train loss:2.3041090334532557\n",
      "train loss:2.304116224728927\n",
      "train loss:2.310225063442266\n",
      "train loss:2.298989972674868\n",
      "train loss:2.3095964210857587\n",
      "train loss:2.2955093153825548\n",
      "train loss:2.29621594560727\n",
      "train loss:2.3008602684936665\n",
      "train loss:2.303928399766642\n",
      "train loss:2.3036149509573356\n",
      "train loss:2.3012264411882444\n",
      "train loss:2.302173351214247\n",
      "train loss:2.2946435261891716\n",
      "train loss:2.3020596064937906\n",
      "train loss:2.2912135276717924\n",
      "train loss:2.295491246979228\n",
      "train loss:2.2988072461035873\n",
      "train loss:2.315296516026205\n",
      "train loss:2.2962302977131186\n",
      "train loss:2.2983628154621436\n",
      "train loss:2.3021873052293333\n",
      "train loss:2.2971162488618995\n",
      "train loss:2.297727484554614\n",
      "train loss:2.3044287721259393\n",
      "train loss:2.301409062190303\n",
      "train loss:2.2944019331352763\n",
      "train loss:2.30191772194696\n",
      "train loss:2.2954503689524794\n",
      "train loss:2.2934145023379995\n",
      "train loss:2.303156606283225\n",
      "train loss:2.3063162726487114\n",
      "train loss:2.2983911331198184\n",
      "train loss:2.3022650278036236\n",
      "train loss:2.292820711047169\n",
      "train loss:2.30407526606806\n",
      "train loss:2.299482938727633\n",
      "train loss:2.306225553149336\n",
      "train loss:2.3040075954360155\n",
      "train loss:2.29824293656564\n",
      "train loss:2.3036497372851037\n",
      "train loss:2.306824799402617\n",
      "train loss:2.3011203047362736\n",
      "train loss:2.308589309487721\n",
      "train loss:2.3024565061664646\n",
      "train loss:2.297905590377923\n",
      "train loss:2.3009307050445504\n",
      "train loss:2.3039639566049237\n",
      "=== epoch:65, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3079472521343565\n",
      "train loss:2.3017666029180117\n",
      "train loss:2.2921065721401996\n",
      "train loss:2.3013658288493266\n",
      "train loss:2.29962757888664\n",
      "train loss:2.293620970313907\n",
      "train loss:2.3080812257362537\n",
      "train loss:2.2979448610100355\n",
      "train loss:2.3030390842674873\n",
      "train loss:2.3023010283192193\n",
      "train loss:2.3007491478538924\n",
      "train loss:2.304058385912208\n",
      "train loss:2.3041671713345386\n",
      "train loss:2.3049576116197352\n",
      "train loss:2.2897398276280563\n",
      "train loss:2.309543067465332\n",
      "train loss:2.3032335014121235\n",
      "train loss:2.2970172388768195\n",
      "train loss:2.303933755145278\n",
      "train loss:2.3062983328783306\n",
      "train loss:2.296773298362223\n",
      "train loss:2.2935884406677034\n",
      "train loss:2.2992394287328466\n",
      "train loss:2.3092697146876366\n",
      "train loss:2.301712609597631\n",
      "train loss:2.3094657493377806\n",
      "train loss:2.306869810660393\n",
      "train loss:2.2960320355641346\n",
      "train loss:2.3025780371743525\n",
      "train loss:2.296266297796134\n",
      "train loss:2.304810208216557\n",
      "train loss:2.296670528932382\n",
      "train loss:2.300752734481279\n",
      "train loss:2.2987643359960375\n",
      "train loss:2.294220608930773\n",
      "train loss:2.301361820248295\n",
      "train loss:2.2978990376924506\n",
      "train loss:2.2961980717450112\n",
      "train loss:2.3034563224151166\n",
      "train loss:2.301676497271499\n",
      "train loss:2.30398797556408\n",
      "train loss:2.298090493057572\n",
      "train loss:2.304802562695416\n",
      "train loss:2.2947216314006047\n",
      "train loss:2.3003760159656914\n",
      "train loss:2.2999065820312317\n",
      "train loss:2.3076048021735938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3036454335323557\n",
      "train loss:2.2968389103522653\n",
      "train loss:2.305916273482886\n",
      "train loss:2.299633047731714\n",
      "train loss:2.2953914678364846\n",
      "train loss:2.2995926096107073\n",
      "train loss:2.3048013593741676\n",
      "train loss:2.3070537100145403\n",
      "train loss:2.3080306597017386\n",
      "train loss:2.2991335169294964\n",
      "train loss:2.3029312939951176\n",
      "train loss:2.293501291983688\n",
      "train loss:2.303481941226477\n",
      "train loss:2.299601826843364\n",
      "train loss:2.3036909560996306\n",
      "train loss:2.294172230349617\n",
      "train loss:2.3065703133043995\n",
      "train loss:2.3023592814801304\n",
      "train loss:2.2989041917371447\n",
      "train loss:2.3054890961842984\n",
      "train loss:2.2986336470443436\n",
      "train loss:2.306215032640647\n",
      "train loss:2.3026409756842208\n",
      "train loss:2.298409101759581\n",
      "train loss:2.2933498010772544\n",
      "train loss:2.303992606910978\n",
      "train loss:2.308927666436707\n",
      "train loss:2.2952292846320934\n",
      "train loss:2.3046648691462512\n",
      "train loss:2.306753724965529\n",
      "train loss:2.3007892305557336\n",
      "train loss:2.3090002496230353\n",
      "train loss:2.299058240036695\n",
      "train loss:2.3047878263022903\n",
      "train loss:2.2971122580074814\n",
      "train loss:2.2968747622585273\n",
      "train loss:2.3081352517602354\n",
      "train loss:2.2937702961215662\n",
      "train loss:2.312513114424507\n",
      "train loss:2.3061116626313356\n",
      "train loss:2.306053181631634\n",
      "train loss:2.302931227352495\n",
      "train loss:2.2992592515737553\n",
      "train loss:2.2964389510145264\n",
      "train loss:2.3039396796011014\n",
      "train loss:2.3008211065064494\n",
      "train loss:2.301495825123567\n",
      "train loss:2.3083664625306923\n",
      "train loss:2.303446568127898\n",
      "train loss:2.298647324996746\n",
      "train loss:2.30486602783414\n",
      "train loss:2.304060145645061\n",
      "train loss:2.2967808771991076\n",
      "train loss:2.3093236723602586\n",
      "train loss:2.2894884994670277\n",
      "train loss:2.3068427524234063\n",
      "train loss:2.304273123596057\n",
      "train loss:2.298175013756628\n",
      "train loss:2.3076043729035374\n",
      "train loss:2.3053273955796016\n",
      "train loss:2.30741144899303\n",
      "train loss:2.304907319134408\n",
      "train loss:2.2987928493829775\n",
      "train loss:2.302970475457445\n",
      "train loss:2.3040215993215694\n",
      "train loss:2.2988992008742812\n",
      "train loss:2.306230280076287\n",
      "train loss:2.301805691430379\n",
      "train loss:2.303901053636493\n",
      "train loss:2.306992458444542\n",
      "train loss:2.305905403825978\n",
      "train loss:2.3041589492438033\n",
      "train loss:2.302357608277245\n",
      "train loss:2.306659986949486\n",
      "train loss:2.3004310722547636\n",
      "train loss:2.2925663416212214\n",
      "train loss:2.3049498559921253\n",
      "train loss:2.3008673673819695\n",
      "train loss:2.2990026997588133\n",
      "train loss:2.300143461852854\n",
      "train loss:2.305156939907296\n",
      "train loss:2.291176655884548\n",
      "train loss:2.3023885926114134\n",
      "train loss:2.298179873449328\n",
      "train loss:2.2968509815771165\n",
      "train loss:2.3025554033967706\n",
      "train loss:2.2980151692999327\n",
      "train loss:2.3080601083958334\n",
      "train loss:2.3003325010246853\n",
      "train loss:2.3008157986600057\n",
      "train loss:2.2993133936748675\n",
      "train loss:2.3043368889022595\n",
      "train loss:2.306492047004917\n",
      "train loss:2.2994690138385216\n",
      "train loss:2.306617788282756\n",
      "train loss:2.3054096268470454\n",
      "train loss:2.30677324634863\n",
      "train loss:2.3087613922183476\n",
      "train loss:2.2998445057369836\n",
      "train loss:2.299229295775095\n",
      "train loss:2.301629752617922\n",
      "train loss:2.2999593535076137\n",
      "train loss:2.2906332407630043\n",
      "train loss:2.29951533961027\n",
      "train loss:2.2961448760519154\n",
      "train loss:2.294811487395297\n",
      "train loss:2.304632182386744\n",
      "train loss:2.295654069260379\n",
      "train loss:2.299298279968324\n",
      "train loss:2.294003452528814\n",
      "train loss:2.3077860652799154\n",
      "train loss:2.3036134908873764\n",
      "train loss:2.3062128888817672\n",
      "train loss:2.302613351676303\n",
      "train loss:2.3046037816352256\n",
      "train loss:2.3009600658081246\n",
      "train loss:2.3060392202605104\n",
      "train loss:2.303304248694951\n",
      "train loss:2.3000568299388156\n",
      "train loss:2.3001090886779\n",
      "train loss:2.305614157431688\n",
      "train loss:2.3014892022503455\n",
      "train loss:2.302746494533853\n",
      "train loss:2.3109239623693076\n",
      "train loss:2.30161495694471\n",
      "train loss:2.2987834689923\n",
      "train loss:2.2929000019301946\n",
      "train loss:2.311995311502243\n",
      "train loss:2.3010156416972283\n",
      "train loss:2.3010240482995874\n",
      "train loss:2.303531730584272\n",
      "train loss:2.303822104878638\n",
      "train loss:2.3067274974620533\n",
      "train loss:2.303745852576241\n",
      "train loss:2.2942629694863657\n",
      "train loss:2.303876825435106\n",
      "train loss:2.300849828228371\n",
      "train loss:2.3018015392481233\n",
      "train loss:2.304784422088026\n",
      "train loss:2.3005145554493365\n",
      "train loss:2.2962141422956917\n",
      "train loss:2.2997512622560894\n",
      "train loss:2.306069690804074\n",
      "train loss:2.300328599417248\n",
      "train loss:2.292974681096265\n",
      "train loss:2.296726763480392\n",
      "train loss:2.293561162071196\n",
      "train loss:2.29986788185492\n",
      "train loss:2.3066210891609567\n",
      "train loss:2.2902140057119547\n",
      "train loss:2.3067313218684373\n",
      "train loss:2.3018631962761193\n",
      "train loss:2.3045827891024966\n",
      "=== epoch:66, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301166202211273\n",
      "train loss:2.296689862342718\n",
      "train loss:2.3009293871879777\n",
      "train loss:2.3026848257464394\n",
      "train loss:2.301219834257366\n",
      "train loss:2.3024651086960706\n",
      "train loss:2.30306157547359\n",
      "train loss:2.301561743135667\n",
      "train loss:2.3014031043791183\n",
      "train loss:2.2995868682854357\n",
      "train loss:2.3014338924258912\n",
      "train loss:2.29225842552503\n",
      "train loss:2.3061367466940377\n",
      "train loss:2.298206398872472\n",
      "train loss:2.3075792043202155\n",
      "train loss:2.3027057769295234\n",
      "train loss:2.298530164629518\n",
      "train loss:2.2949500200896624\n",
      "train loss:2.2919422362428326\n",
      "train loss:2.299752755096232\n",
      "train loss:2.2932899421262114\n",
      "train loss:2.2960166017129855\n",
      "train loss:2.2900811275872828\n",
      "train loss:2.3042794991874045\n",
      "train loss:2.3087148228627976\n",
      "train loss:2.2985191385307693\n",
      "train loss:2.3010764568029334\n",
      "train loss:2.3052141828589043\n",
      "train loss:2.2982966354435144\n",
      "train loss:2.3029999826696623\n",
      "train loss:2.3046624881509326\n",
      "train loss:2.302349555843357\n",
      "train loss:2.2937171479397533\n",
      "train loss:2.2994704074425663\n",
      "train loss:2.3042071493339176\n",
      "train loss:2.306940905043941\n",
      "train loss:2.3030137346656034\n",
      "train loss:2.3063819186153136\n",
      "train loss:2.2985914334376476\n",
      "train loss:2.2967446056213903\n",
      "train loss:2.3047055922801314\n",
      "train loss:2.3104452831506945\n",
      "train loss:2.2979818761303603\n",
      "train loss:2.303387149106586\n",
      "train loss:2.304155756947124\n",
      "train loss:2.3029490846209324\n",
      "train loss:2.300579594321282\n",
      "train loss:2.3003350417239012\n",
      "train loss:2.3094834672447573\n",
      "train loss:2.2924705754343226\n",
      "train loss:2.3034882775944925\n",
      "train loss:2.302166982663557\n",
      "train loss:2.301987954025607\n",
      "train loss:2.297394033661046\n",
      "train loss:2.3042318107272934\n",
      "train loss:2.297262917857436\n",
      "train loss:2.3041768608847124\n",
      "train loss:2.300506816941204\n",
      "train loss:2.2989672854009746\n",
      "train loss:2.299576680321546\n",
      "train loss:2.297002135679734\n",
      "train loss:2.3048158666660172\n",
      "train loss:2.300041691912669\n",
      "train loss:2.2975856875683767\n",
      "train loss:2.292978720373138\n",
      "train loss:2.3082180238277914\n",
      "train loss:2.2996907381353355\n",
      "train loss:2.2936614285034183\n",
      "train loss:2.294142834349073\n",
      "train loss:2.312026821467994\n",
      "train loss:2.2986472815174204\n",
      "train loss:2.297692816067948\n",
      "train loss:2.298188913595756\n",
      "train loss:2.3085808723728722\n",
      "train loss:2.3043072650543026\n",
      "train loss:2.3022545990190384\n",
      "train loss:2.3001128648749893\n",
      "train loss:2.3008712260219255\n",
      "train loss:2.3029264890318304\n",
      "train loss:2.3069676692660104\n",
      "train loss:2.2945683796286866\n",
      "train loss:2.293787492055571\n",
      "train loss:2.305302890995312\n",
      "train loss:2.2958144422083806\n",
      "train loss:2.2962723415564312\n",
      "train loss:2.3064846536097345\n",
      "train loss:2.306072408700188\n",
      "train loss:2.306149900453998\n",
      "train loss:2.298626424827665\n",
      "train loss:2.3031859222439452\n",
      "train loss:2.297007640806471\n",
      "train loss:2.306384247722173\n",
      "train loss:2.3044864781806864\n",
      "train loss:2.3065406045917705\n",
      "train loss:2.2959320355407047\n",
      "train loss:2.303001040121011\n",
      "train loss:2.3017427662914822\n",
      "train loss:2.302124559438769\n",
      "train loss:2.2931188087275607\n",
      "train loss:2.3054356066503003\n",
      "train loss:2.3071570816614506\n",
      "train loss:2.308622475797421\n",
      "train loss:2.3010446885705234\n",
      "train loss:2.299576833288852\n",
      "train loss:2.305681681905324\n",
      "train loss:2.306087149143787\n",
      "train loss:2.297667030087726\n",
      "train loss:2.298994321627993\n",
      "train loss:2.3045266025919044\n",
      "train loss:2.3028903538191203\n",
      "train loss:2.305205930317119\n",
      "train loss:2.2920879921841357\n",
      "train loss:2.294964485387114\n",
      "train loss:2.3047864949624692\n",
      "train loss:2.3010543073251415\n",
      "train loss:2.2971299385668082\n",
      "train loss:2.3037563292317413\n",
      "train loss:2.2955988484360565\n",
      "train loss:2.2952763565129626\n",
      "train loss:2.3078299495659493\n",
      "train loss:2.288120517281821\n",
      "train loss:2.307713513137378\n",
      "train loss:2.2984051151644573\n",
      "train loss:2.2992125651899205\n",
      "train loss:2.3059486885059743\n",
      "train loss:2.2971017733673476\n",
      "train loss:2.304352062878513\n",
      "train loss:2.3017190832888548\n",
      "train loss:2.29679173051095\n",
      "train loss:2.3110745514790203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302927160668981\n",
      "train loss:2.2909725188321968\n",
      "train loss:2.30667262301238\n",
      "train loss:2.29713913117979\n",
      "train loss:2.305478208360534\n",
      "train loss:2.3071050181940507\n",
      "train loss:2.3065298628176287\n",
      "train loss:2.299348809515851\n",
      "train loss:2.301240281411487\n",
      "train loss:2.289682847907644\n",
      "train loss:2.2943068826428608\n",
      "train loss:2.302196069818052\n",
      "train loss:2.300262377855943\n",
      "train loss:2.3120142469319878\n",
      "train loss:2.3067333933238316\n",
      "train loss:2.298578018629323\n",
      "train loss:2.293464316585502\n",
      "train loss:2.298836815680211\n",
      "train loss:2.307820430328093\n",
      "train loss:2.3001691096458607\n",
      "train loss:2.3071914199571455\n",
      "train loss:2.308352455943537\n",
      "train loss:2.3008316946987732\n",
      "train loss:2.3034039366282175\n",
      "train loss:2.2995570283439837\n",
      "train loss:2.3031598863356995\n",
      "train loss:2.291202259069497\n",
      "train loss:2.2954461968136406\n",
      "train loss:2.3059831791316188\n",
      "train loss:2.309631008630271\n",
      "train loss:2.3002532037540546\n",
      "train loss:2.3077787044166476\n",
      "train loss:2.30154759375641\n",
      "train loss:2.3029325254803177\n",
      "train loss:2.305116839250658\n",
      "train loss:2.2961708706961774\n",
      "train loss:2.3023660851973435\n",
      "train loss:2.2973251325383357\n",
      "train loss:2.308945788986162\n",
      "train loss:2.295992461179775\n",
      "train loss:2.300576928562284\n",
      "train loss:2.3003322388302254\n",
      "train loss:2.3078997330019666\n",
      "train loss:2.3029932483989577\n",
      "train loss:2.303408199455855\n",
      "train loss:2.296138976521514\n",
      "train loss:2.296614467736525\n",
      "train loss:2.314157274084723\n",
      "train loss:2.292453530676333\n",
      "train loss:2.309596656616765\n",
      "train loss:2.302522723999772\n",
      "train loss:2.2996853318166446\n",
      "train loss:2.2985238417880023\n",
      "train loss:2.307895941680179\n",
      "train loss:2.3042101611993857\n",
      "train loss:2.3077019309788978\n",
      "train loss:2.3068466958038036\n",
      "train loss:2.3045228337166246\n",
      "train loss:2.3063875154297624\n",
      "train loss:2.301275035379305\n",
      "train loss:2.299747001887994\n",
      "train loss:2.307796443722669\n",
      "train loss:2.3064313849729334\n",
      "train loss:2.3132556674099765\n",
      "train loss:2.3046011606509462\n",
      "train loss:2.2977237249315365\n",
      "train loss:2.300075086008436\n",
      "train loss:2.307751977773787\n",
      "train loss:2.299535077474627\n",
      "train loss:2.3013027270385598\n",
      "=== epoch:67, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.29748702099811\n",
      "train loss:2.307954919305922\n",
      "train loss:2.2944124860667077\n",
      "train loss:2.3011591887089153\n",
      "train loss:2.299743227251529\n",
      "train loss:2.297346975989265\n",
      "train loss:2.2990953537626937\n",
      "train loss:2.2985083859353943\n",
      "train loss:2.2998539101975313\n",
      "train loss:2.304894524212035\n",
      "train loss:2.296768826411453\n",
      "train loss:2.2978558000112446\n",
      "train loss:2.3102123414730222\n",
      "train loss:2.301933282046473\n",
      "train loss:2.305872865110482\n",
      "train loss:2.3057822951102\n",
      "train loss:2.290183577391679\n",
      "train loss:2.308200856475654\n",
      "train loss:2.299871189956748\n",
      "train loss:2.2999946527028636\n",
      "train loss:2.3090147429935888\n",
      "train loss:2.302918732185106\n",
      "train loss:2.3090415975405665\n",
      "train loss:2.301722609025331\n",
      "train loss:2.3020766444524865\n",
      "train loss:2.2983952554745213\n",
      "train loss:2.297183036668798\n",
      "train loss:2.3091957425056675\n",
      "train loss:2.3028188194015837\n",
      "train loss:2.296834920175296\n",
      "train loss:2.3079019817161384\n",
      "train loss:2.3073681605220946\n",
      "train loss:2.296019849267142\n",
      "train loss:2.297624911465406\n",
      "train loss:2.302378902325179\n",
      "train loss:2.3032395494707876\n",
      "train loss:2.300074603147726\n",
      "train loss:2.3067977898507728\n",
      "train loss:2.3004924655718795\n",
      "train loss:2.2995301667016195\n",
      "train loss:2.298658773552991\n",
      "train loss:2.2985758956521547\n",
      "train loss:2.3068539324580972\n",
      "train loss:2.3022754234893297\n",
      "train loss:2.304803890718478\n",
      "train loss:2.302328341395491\n",
      "train loss:2.298887207803041\n",
      "train loss:2.2988233125782083\n",
      "train loss:2.3027047298045753\n",
      "train loss:2.2925657017131686\n",
      "train loss:2.2956613805102157\n",
      "train loss:2.30199428061679\n",
      "train loss:2.2962332309219833\n",
      "train loss:2.2963965842386513\n",
      "train loss:2.3019906331220295\n",
      "train loss:2.305202614660507\n",
      "train loss:2.3050702787000716\n",
      "train loss:2.302798570595905\n",
      "train loss:2.2997885793334603\n",
      "train loss:2.3050805767210116\n",
      "train loss:2.295109474033422\n",
      "train loss:2.3058802359986927\n",
      "train loss:2.302111183762983\n",
      "train loss:2.2970942144342508\n",
      "train loss:2.2975372551951296\n",
      "train loss:2.3014270736191325\n",
      "train loss:2.3010318998553076\n",
      "train loss:2.296074477628095\n",
      "train loss:2.306600838303419\n",
      "train loss:2.2970200175522204\n",
      "train loss:2.304929845387721\n",
      "train loss:2.2989486962943158\n",
      "train loss:2.2988313891960312\n",
      "train loss:2.29747066093087\n",
      "train loss:2.2987462593118626\n",
      "train loss:2.3072410778184644\n",
      "train loss:2.2997115531963694\n",
      "train loss:2.3054541177665873\n",
      "train loss:2.309549386791626\n",
      "train loss:2.30462291144369\n",
      "train loss:2.3042392890356873\n",
      "train loss:2.3013096992252824\n",
      "train loss:2.3015308759083717\n",
      "train loss:2.3023463491196923\n",
      "train loss:2.2969407633307863\n",
      "train loss:2.3079146369287917\n",
      "train loss:2.300794098871366\n",
      "train loss:2.297780849997956\n",
      "train loss:2.3026617292319433\n",
      "train loss:2.308719910303422\n",
      "train loss:2.3016618886408056\n",
      "train loss:2.2996281479858616\n",
      "train loss:2.297994063919882\n",
      "train loss:2.298782251409173\n",
      "train loss:2.3085650337580397\n",
      "train loss:2.303473947593905\n",
      "train loss:2.2961517707012233\n",
      "train loss:2.2923040359478652\n",
      "train loss:2.29811382751629\n",
      "train loss:2.2998018133353773\n",
      "train loss:2.3036806308420643\n",
      "train loss:2.3079803683971383\n",
      "train loss:2.306202960421387\n",
      "train loss:2.2925404308814112\n",
      "train loss:2.302824255298808\n",
      "train loss:2.299282494262382\n",
      "train loss:2.2991818655981544\n",
      "train loss:2.295874914276612\n",
      "train loss:2.3022655483173926\n",
      "train loss:2.30019252412814\n",
      "train loss:2.2985450356811397\n",
      "train loss:2.2986089784017083\n",
      "train loss:2.3013012961715393\n",
      "train loss:2.3077923930221096\n",
      "train loss:2.300017480252971\n",
      "train loss:2.297225154306176\n",
      "train loss:2.299528849844306\n",
      "train loss:2.2936125406259356\n",
      "train loss:2.2975036308138894\n",
      "train loss:2.292878241574878\n",
      "train loss:2.30224680915913\n",
      "train loss:2.300519145779563\n",
      "train loss:2.2986969795217402\n",
      "train loss:2.3001149878867104\n",
      "train loss:2.303151116068782\n",
      "train loss:2.3022830095501154\n",
      "train loss:2.2924298363433553\n",
      "train loss:2.30421915247097\n",
      "train loss:2.2889001039086945\n",
      "train loss:2.2981527687947034\n",
      "train loss:2.3045065428635336\n",
      "train loss:2.300350016219757\n",
      "train loss:2.29864543356894\n",
      "train loss:2.301518049277565\n",
      "train loss:2.3018823536277897\n",
      "train loss:2.307008168947897\n",
      "train loss:2.302417300194244\n",
      "train loss:2.304630676642862\n",
      "train loss:2.296605034181608\n",
      "train loss:2.29839714458662\n",
      "train loss:2.3082947618547536\n",
      "train loss:2.3043507242039367\n",
      "train loss:2.296203622751064\n",
      "train loss:2.2969901822547705\n",
      "train loss:2.308830900910853\n",
      "train loss:2.3037114223392394\n",
      "train loss:2.299496165303368\n",
      "train loss:2.3049912606968452\n",
      "train loss:2.3023779865797436\n",
      "train loss:2.299960573301804\n",
      "train loss:2.3021395987944717\n",
      "train loss:2.3056425954132242\n",
      "train loss:2.300949659942747\n",
      "train loss:2.306344123800389\n",
      "train loss:2.3012623921106283\n",
      "train loss:2.3022652113714046\n",
      "train loss:2.2980332498350697\n",
      "train loss:2.298299852248102\n",
      "train loss:2.3032366583464605\n",
      "train loss:2.295519850457076\n",
      "train loss:2.2984537143564254\n",
      "train loss:2.29673196260693\n",
      "train loss:2.3060686888719393\n",
      "train loss:2.295973397555787\n",
      "train loss:2.3009083880164782\n",
      "train loss:2.3070071193090174\n",
      "train loss:2.3003921386853587\n",
      "train loss:2.293391213346668\n",
      "train loss:2.313331451926542\n",
      "train loss:2.299256263167324\n",
      "train loss:2.3039297457421006\n",
      "train loss:2.2963962403156306\n",
      "train loss:2.297038219176876\n",
      "train loss:2.3099419090364015\n",
      "train loss:2.3030986276084775\n",
      "train loss:2.3023400812068586\n",
      "train loss:2.294816172861027\n",
      "train loss:2.2959848708329007\n",
      "train loss:2.2983302000462844\n",
      "train loss:2.293928637559903\n",
      "train loss:2.300518053010711\n",
      "train loss:2.3012372291147165\n",
      "train loss:2.301950884141563\n",
      "train loss:2.3036932918900734\n",
      "train loss:2.3050972982377647\n",
      "train loss:2.3091085482602702\n",
      "train loss:2.30050791112959\n",
      "train loss:2.298591324463486\n",
      "train loss:2.2956422845254907\n",
      "train loss:2.301413991019651\n",
      "train loss:2.30362442305138\n",
      "train loss:2.3009638476579006\n",
      "train loss:2.3065051660163802\n",
      "train loss:2.305067984848832\n",
      "train loss:2.297770489339117\n",
      "train loss:2.301014788948288\n",
      "train loss:2.3075157207389405\n",
      "train loss:2.2953578131432515\n",
      "train loss:2.306385801944344\n",
      "train loss:2.301370676313207\n",
      "=== epoch:68, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.299367570123485\n",
      "train loss:2.2899134514552952\n",
      "train loss:2.304822537479148\n",
      "train loss:2.297484463866023\n",
      "train loss:2.2965059707291577\n",
      "train loss:2.301843117853067\n",
      "train loss:2.2988017730348287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3036853615262727\n",
      "train loss:2.3026431280608315\n",
      "train loss:2.301024310604834\n",
      "train loss:2.3030084120541607\n",
      "train loss:2.2992022422791694\n",
      "train loss:2.3033843942696706\n",
      "train loss:2.296622746802627\n",
      "train loss:2.297929965835575\n",
      "train loss:2.2942952725062744\n",
      "train loss:2.2964237813323805\n",
      "train loss:2.302240896876399\n",
      "train loss:2.297648975952024\n",
      "train loss:2.3015101624818772\n",
      "train loss:2.2896394770737882\n",
      "train loss:2.2921397389501554\n",
      "train loss:2.305601022509335\n",
      "train loss:2.30089820802623\n",
      "train loss:2.2960272014559826\n",
      "train loss:2.296738606378961\n",
      "train loss:2.30416503091768\n",
      "train loss:2.30717029155846\n",
      "train loss:2.298569971098071\n",
      "train loss:2.304727727980542\n",
      "train loss:2.3012067011731188\n",
      "train loss:2.3021245838386357\n",
      "train loss:2.306231193573088\n",
      "train loss:2.3108544087531366\n",
      "train loss:2.306219303770851\n",
      "train loss:2.296984659382079\n",
      "train loss:2.303743698986022\n",
      "train loss:2.2923680408780314\n",
      "train loss:2.304689774177786\n",
      "train loss:2.2996610549700627\n",
      "train loss:2.306320275343507\n",
      "train loss:2.3015851284459927\n",
      "train loss:2.2910801363722966\n",
      "train loss:2.302798181988189\n",
      "train loss:2.2991421375912986\n",
      "train loss:2.305647434127258\n",
      "train loss:2.295340806083388\n",
      "train loss:2.308776082966137\n",
      "train loss:2.2976071446543114\n",
      "train loss:2.3017762582507917\n",
      "train loss:2.303321174356861\n",
      "train loss:2.306270788737909\n",
      "train loss:2.301049759496815\n",
      "train loss:2.302083708462453\n",
      "train loss:2.307377259103437\n",
      "train loss:2.2977387890405954\n",
      "train loss:2.2977490458650256\n",
      "train loss:2.3065942363807603\n",
      "train loss:2.3044232920045893\n",
      "train loss:2.2946475064480762\n",
      "train loss:2.300528417470504\n",
      "train loss:2.3022001373324645\n",
      "train loss:2.306656709241457\n",
      "train loss:2.302014668889425\n",
      "train loss:2.299020796948797\n",
      "train loss:2.2888473216787633\n",
      "train loss:2.2993148692696734\n",
      "train loss:2.306441534060724\n",
      "train loss:2.2950796402195768\n",
      "train loss:2.3001416471515976\n",
      "train loss:2.306323682649393\n",
      "train loss:2.3010450576302137\n",
      "train loss:2.2967687880996\n",
      "train loss:2.3075723260185126\n",
      "train loss:2.3045595729257204\n",
      "train loss:2.2931090059854693\n",
      "train loss:2.2975717652705585\n",
      "train loss:2.3113682988584023\n",
      "train loss:2.298694428888713\n",
      "train loss:2.303492403824793\n",
      "train loss:2.3066748757905216\n",
      "train loss:2.310568443216197\n",
      "train loss:2.293539728895721\n",
      "train loss:2.302818461354588\n",
      "train loss:2.3090044124263525\n",
      "train loss:2.3115885319493317\n",
      "train loss:2.2932517856194496\n",
      "train loss:2.3057621446399805\n",
      "train loss:2.300550626636161\n",
      "train loss:2.3010031432530402\n",
      "train loss:2.301445331182384\n",
      "train loss:2.303136735242325\n",
      "train loss:2.299452122494568\n",
      "train loss:2.304765552595503\n",
      "train loss:2.30045323233085\n",
      "train loss:2.298296308522542\n",
      "train loss:2.2953243776546195\n",
      "train loss:2.3078157611574004\n",
      "train loss:2.3006207465840545\n",
      "train loss:2.299959850625393\n",
      "train loss:2.2965784959257407\n",
      "train loss:2.3001751878514716\n",
      "train loss:2.3045666419039454\n",
      "train loss:2.2954030636142835\n",
      "train loss:2.299148137123628\n",
      "train loss:2.300621327904856\n",
      "train loss:2.3002078199157\n",
      "train loss:2.305557443017856\n",
      "train loss:2.2982888279787828\n",
      "train loss:2.2934079379747017\n",
      "train loss:2.2986861598399133\n",
      "train loss:2.3032737123406273\n",
      "train loss:2.2968462905230944\n",
      "train loss:2.2938784762849824\n",
      "train loss:2.2914073049060293\n",
      "train loss:2.30056663263054\n",
      "train loss:2.3030752873771037\n",
      "train loss:2.2974655285461965\n",
      "train loss:2.3013879981509087\n",
      "train loss:2.301706073378845\n",
      "train loss:2.3013102318745817\n",
      "train loss:2.300615782209171\n",
      "train loss:2.3032061820543848\n",
      "train loss:2.304422846348973\n",
      "train loss:2.3019300586165925\n",
      "train loss:2.2945979847658418\n",
      "train loss:2.3007168064449877\n",
      "train loss:2.304339003045123\n",
      "train loss:2.301382676259773\n",
      "train loss:2.3066630816718643\n",
      "train loss:2.299640865889105\n",
      "train loss:2.3060868340005847\n",
      "train loss:2.295567656616839\n",
      "train loss:2.3019163295949907\n",
      "train loss:2.303531253886744\n",
      "train loss:2.2990808126232394\n",
      "train loss:2.310506134425428\n",
      "train loss:2.2983985824731326\n",
      "train loss:2.3004975780089225\n",
      "train loss:2.304417422768024\n",
      "train loss:2.311975027785461\n",
      "train loss:2.3006560859029315\n",
      "train loss:2.300094021109959\n",
      "train loss:2.3007902345282654\n",
      "train loss:2.2958425353620995\n",
      "train loss:2.3002026952445074\n",
      "train loss:2.3064951579186905\n",
      "train loss:2.3002144394919575\n",
      "train loss:2.301645581091616\n",
      "train loss:2.311040321939804\n",
      "train loss:2.2955366073041525\n",
      "train loss:2.3025108120318523\n",
      "train loss:2.3018655085462902\n",
      "train loss:2.2996061862462867\n",
      "train loss:2.3075075644372873\n",
      "train loss:2.3002771529794597\n",
      "train loss:2.301671999521068\n",
      "train loss:2.3024359517776025\n",
      "train loss:2.2988282977722\n",
      "train loss:2.300913753611062\n",
      "train loss:2.2998304759797974\n",
      "train loss:2.292670791229408\n",
      "train loss:2.305340178210189\n",
      "train loss:2.3073844357303632\n",
      "train loss:2.297148616885325\n",
      "train loss:2.3067729742319765\n",
      "train loss:2.308043638869489\n",
      "train loss:2.302215731062213\n",
      "train loss:2.2903622630165037\n",
      "train loss:2.306679004920876\n",
      "train loss:2.3014601669335155\n",
      "train loss:2.3060014017845796\n",
      "train loss:2.2963652410654833\n",
      "train loss:2.3095841947934495\n",
      "train loss:2.2942626231059227\n",
      "train loss:2.29652977328139\n",
      "train loss:2.3012837955456877\n",
      "train loss:2.298084335672831\n",
      "train loss:2.3015650344879957\n",
      "train loss:2.306109320840657\n",
      "train loss:2.3024417015894136\n",
      "train loss:2.3084547516417593\n",
      "train loss:2.3033518640214776\n",
      "train loss:2.300173985350102\n",
      "train loss:2.310208795136152\n",
      "train loss:2.3009750491144754\n",
      "train loss:2.2985774429413084\n",
      "train loss:2.3012185277455957\n",
      "train loss:2.2949409448025135\n",
      "train loss:2.301778758962313\n",
      "train loss:2.3033671433358434\n",
      "train loss:2.306269781351751\n",
      "train loss:2.30261977130825\n",
      "train loss:2.2939924384910126\n",
      "train loss:2.308782186994182\n",
      "train loss:2.3033049895269473\n",
      "train loss:2.3058245146727874\n",
      "train loss:2.305620888934565\n",
      "train loss:2.30896474820371\n",
      "train loss:2.3052320568666738\n",
      "=== epoch:69, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.299153185134884\n",
      "train loss:2.2989951308443914\n",
      "train loss:2.295050022208857\n",
      "train loss:2.298280971641184\n",
      "train loss:2.312772457678309\n",
      "train loss:2.303130197206729\n",
      "train loss:2.303563375437486\n",
      "train loss:2.286059664671268\n",
      "train loss:2.301578814064504\n",
      "train loss:2.296142873461963\n",
      "train loss:2.295122333095837\n",
      "train loss:2.307317478434212\n",
      "train loss:2.299285717688412\n",
      "train loss:2.308244718685792\n",
      "train loss:2.3032889723243204\n",
      "train loss:2.3056053114737374\n",
      "train loss:2.302991624270994\n",
      "train loss:2.302625485536474\n",
      "train loss:2.3060622756227676\n",
      "train loss:2.3045184731299466\n",
      "train loss:2.3029309045289317\n",
      "train loss:2.3041568792908227\n",
      "train loss:2.300622422868625\n",
      "train loss:2.2981037233828867\n",
      "train loss:2.300682692883941\n",
      "train loss:2.3049360409569815\n",
      "train loss:2.3063450690185046\n",
      "train loss:2.294435028056162\n",
      "train loss:2.2943018023471207\n",
      "train loss:2.3040650864847922\n",
      "train loss:2.3003563971785\n",
      "train loss:2.2943066336581976\n",
      "train loss:2.3020858200547285\n",
      "train loss:2.296825439412391\n",
      "train loss:2.2990368554727185\n",
      "train loss:2.3043515014509395\n",
      "train loss:2.3018191561976167\n",
      "train loss:2.2997047000386748\n",
      "train loss:2.286612502099239\n",
      "train loss:2.3073705357634924\n",
      "train loss:2.3014538091093706\n",
      "train loss:2.300429602739326\n",
      "train loss:2.296061294688453\n",
      "train loss:2.3052612335734084\n",
      "train loss:2.3026679022466614\n",
      "train loss:2.3025602905178415\n",
      "train loss:2.3018285287978544\n",
      "train loss:2.299276779099605\n",
      "train loss:2.3039136764638903\n",
      "train loss:2.2921435069552074\n",
      "train loss:2.3000489194627605\n",
      "train loss:2.2986294763750115\n",
      "train loss:2.295709299674852\n",
      "train loss:2.29940086144261\n",
      "train loss:2.2986764501401673\n",
      "train loss:2.3056383862911654\n",
      "train loss:2.3031259271468616\n",
      "train loss:2.2958161480063857\n",
      "train loss:2.295459291844541\n",
      "train loss:2.3028498201782535\n",
      "train loss:2.293599879626042\n",
      "train loss:2.294668770709449\n",
      "train loss:2.30535143278985\n",
      "train loss:2.298066664861606\n",
      "train loss:2.2987362213927995\n",
      "train loss:2.3028648407272847\n",
      "train loss:2.298042011581209\n",
      "train loss:2.30056649525858\n",
      "train loss:2.305124665404583\n",
      "train loss:2.3052907146597197\n",
      "train loss:2.3000695605714365\n",
      "train loss:2.3124973816435266\n",
      "train loss:2.299671122363773\n",
      "train loss:2.2996237164832736\n",
      "train loss:2.302397001540243\n",
      "train loss:2.303739016962489\n",
      "train loss:2.292986125278516\n",
      "train loss:2.301618338539952\n",
      "train loss:2.307682488649437\n",
      "train loss:2.2951888210785807\n",
      "train loss:2.307376992051623\n",
      "train loss:2.3013079841317112\n",
      "train loss:2.308442484632731\n",
      "train loss:2.3039333262465065\n",
      "train loss:2.3061504435468967\n",
      "train loss:2.3049141292865736\n",
      "train loss:2.2994664671085294\n",
      "train loss:2.3082077840183786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3075899566716815\n",
      "train loss:2.3046359181286893\n",
      "train loss:2.303018283491063\n",
      "train loss:2.3048542914741863\n",
      "train loss:2.2912624870275256\n",
      "train loss:2.304675339656027\n",
      "train loss:2.29269562537339\n",
      "train loss:2.3066804328073256\n",
      "train loss:2.292528925824783\n",
      "train loss:2.301532726100796\n",
      "train loss:2.295031197976294\n",
      "train loss:2.305982093399493\n",
      "train loss:2.298899907339733\n",
      "train loss:2.3040796952216738\n",
      "train loss:2.3120496433044018\n",
      "train loss:2.294920851134614\n",
      "train loss:2.2981294186365253\n",
      "train loss:2.294492234692571\n",
      "train loss:2.3025719859216305\n",
      "train loss:2.303804921534001\n",
      "train loss:2.301941660588015\n",
      "train loss:2.300250353676288\n",
      "train loss:2.297460927641152\n",
      "train loss:2.30302143561411\n",
      "train loss:2.3024599765583127\n",
      "train loss:2.2945961493497533\n",
      "train loss:2.3025194241674707\n",
      "train loss:2.297684704670457\n",
      "train loss:2.3000278191003978\n",
      "train loss:2.305094089940777\n",
      "train loss:2.302514209928701\n",
      "train loss:2.2963536798880035\n",
      "train loss:2.3095082180888697\n",
      "train loss:2.3085721446840446\n",
      "train loss:2.3012805200885276\n",
      "train loss:2.295979328530866\n",
      "train loss:2.314621442925573\n",
      "train loss:2.2871884481339837\n",
      "train loss:2.2966942203943876\n",
      "train loss:2.3082164271895573\n",
      "train loss:2.3059853974488145\n",
      "train loss:2.2960469846244114\n",
      "train loss:2.299141754433312\n",
      "train loss:2.3058795842174566\n",
      "train loss:2.2992637832714986\n",
      "train loss:2.3032316186528874\n",
      "train loss:2.300199677511731\n",
      "train loss:2.307951283203306\n",
      "train loss:2.303808637110419\n",
      "train loss:2.3055582196652344\n",
      "train loss:2.3007707870381595\n",
      "train loss:2.3018528102229383\n",
      "train loss:2.301270313446109\n",
      "train loss:2.307341256726611\n",
      "train loss:2.2982692878257396\n",
      "train loss:2.3054002408304783\n",
      "train loss:2.3048265633641276\n",
      "train loss:2.299377810729092\n",
      "train loss:2.306777844922933\n",
      "train loss:2.304593856473448\n",
      "train loss:2.3022890025065488\n",
      "train loss:2.296326054848612\n",
      "train loss:2.3007288017960166\n",
      "train loss:2.30908999110357\n",
      "train loss:2.299186095318171\n",
      "train loss:2.3006500442928988\n",
      "train loss:2.303070043355915\n",
      "train loss:2.3090233946920216\n",
      "train loss:2.3020885168623844\n",
      "train loss:2.307011746617004\n",
      "train loss:2.3083544997823386\n",
      "train loss:2.3013135096710715\n",
      "train loss:2.302356155816701\n",
      "train loss:2.3015814055604715\n",
      "train loss:2.2974423387537732\n",
      "train loss:2.305623032550048\n",
      "train loss:2.304991595191486\n",
      "train loss:2.2974171192535127\n",
      "train loss:2.2955792435062414\n",
      "train loss:2.301627477939829\n",
      "train loss:2.3069328621858127\n",
      "train loss:2.2912853282118397\n",
      "train loss:2.307534088524728\n",
      "train loss:2.306128053801492\n",
      "train loss:2.30603673773702\n",
      "train loss:2.309504863523065\n",
      "train loss:2.297348275054346\n",
      "train loss:2.2978270866646042\n",
      "train loss:2.3019554133111586\n",
      "train loss:2.2968703008578726\n",
      "train loss:2.3041729416708834\n",
      "train loss:2.305180198023897\n",
      "train loss:2.3054997586547783\n",
      "train loss:2.299049489760792\n",
      "train loss:2.3105569761898006\n",
      "train loss:2.293933182808546\n",
      "train loss:2.3081912606410318\n",
      "train loss:2.3001560548874993\n",
      "train loss:2.2945951831650593\n",
      "train loss:2.2999806465613943\n",
      "train loss:2.2926592907866823\n",
      "train loss:2.3043949764383758\n",
      "train loss:2.2969340868565395\n",
      "train loss:2.3070377170119327\n",
      "train loss:2.2972181824310756\n",
      "train loss:2.3050566616940085\n",
      "train loss:2.308895632021126\n",
      "train loss:2.2944628716343414\n",
      "train loss:2.3059301045946166\n",
      "train loss:2.3006980300397264\n",
      "train loss:2.302526941612092\n",
      "train loss:2.302617345594105\n",
      "=== epoch:70, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2906447350168113\n",
      "train loss:2.3047244054374776\n",
      "train loss:2.3073098876048275\n",
      "train loss:2.299554561081056\n",
      "train loss:2.3088666776036337\n",
      "train loss:2.3004308129608972\n",
      "train loss:2.3017894270097545\n",
      "train loss:2.288907692819734\n",
      "train loss:2.2949178737468525\n",
      "train loss:2.2952384724002104\n",
      "train loss:2.3022368692334583\n",
      "train loss:2.31020578801185\n",
      "train loss:2.3022217862063785\n",
      "train loss:2.302191407901929\n",
      "train loss:2.297772731307965\n",
      "train loss:2.299727544854584\n",
      "train loss:2.306988990548353\n",
      "train loss:2.3043475193035072\n",
      "train loss:2.2972138881228816\n",
      "train loss:2.2945837381696355\n",
      "train loss:2.3001478614801747\n",
      "train loss:2.3066212316997214\n",
      "train loss:2.3012578273107978\n",
      "train loss:2.302462101379097\n",
      "train loss:2.296293905856373\n",
      "train loss:2.3150561821319253\n",
      "train loss:2.3030949312713487\n",
      "train loss:2.3008994793916906\n",
      "train loss:2.3096254483006375\n",
      "train loss:2.2982914998169397\n",
      "train loss:2.30048909855419\n",
      "train loss:2.3085756407789226\n",
      "train loss:2.297118764079521\n",
      "train loss:2.2985255678884515\n",
      "train loss:2.3019639833561754\n",
      "train loss:2.2997326260547974\n",
      "train loss:2.3058903454633897\n",
      "train loss:2.294539845549655\n",
      "train loss:2.3011652196021015\n",
      "train loss:2.3034244428891677\n",
      "train loss:2.3070874882859367\n",
      "train loss:2.3058138440397404\n",
      "train loss:2.3015470313654425\n",
      "train loss:2.296051703043818\n",
      "train loss:2.306382177267523\n",
      "train loss:2.302857302693882\n",
      "train loss:2.306671397650446\n",
      "train loss:2.3099742913123444\n",
      "train loss:2.302096203329275\n",
      "train loss:2.309240550049456\n",
      "train loss:2.301682082847471\n",
      "train loss:2.295276049308651\n",
      "train loss:2.303520429626392\n",
      "train loss:2.3009912791590876\n",
      "train loss:2.2981453041356965\n",
      "train loss:2.3053680930131244\n",
      "train loss:2.3026642269409314\n",
      "train loss:2.3021712491756996\n",
      "train loss:2.3003209907222266\n",
      "train loss:2.3013472038855927\n",
      "train loss:2.3022716497558644\n",
      "train loss:2.3003214230120728\n",
      "train loss:2.2944516945309767\n",
      "train loss:2.3074270246645527\n",
      "train loss:2.3106029140358317\n",
      "train loss:2.293806462191611\n",
      "train loss:2.300895601676999\n",
      "train loss:2.3020571378754715\n",
      "train loss:2.3109619885730988\n",
      "train loss:2.306657923099803\n",
      "train loss:2.305609568014144\n",
      "train loss:2.3026045808783078\n",
      "train loss:2.3015084752516373\n",
      "train loss:2.305264552788518\n",
      "train loss:2.2958520888827243\n",
      "train loss:2.29681142211645\n",
      "train loss:2.310458840352229\n",
      "train loss:2.3072708434210556\n",
      "train loss:2.301526377065724\n",
      "train loss:2.298750706665414\n",
      "train loss:2.303173356078239\n",
      "train loss:2.296891552433848\n",
      "train loss:2.303807990600668\n",
      "train loss:2.304375585337415\n",
      "train loss:2.2906039306235657\n",
      "train loss:2.2958644862905255\n",
      "train loss:2.298491029554209\n",
      "train loss:2.292043660842261\n",
      "train loss:2.3066462048232523\n",
      "train loss:2.2986699727240114\n",
      "train loss:2.3021503089758633\n",
      "train loss:2.302063208694112\n",
      "train loss:2.2967463744969887\n",
      "train loss:2.29792320711769\n",
      "train loss:2.301599052940618\n",
      "train loss:2.3017332034415294\n",
      "train loss:2.298496423972207\n",
      "train loss:2.299850051217757\n",
      "train loss:2.296920496362255\n",
      "train loss:2.3069110069362937\n",
      "train loss:2.300824359487872\n",
      "train loss:2.2988306297089816\n",
      "train loss:2.3010549331781065\n",
      "train loss:2.3003833195577355\n",
      "train loss:2.300577430493247\n",
      "train loss:2.3027135898608404\n",
      "train loss:2.3008812274271935\n",
      "train loss:2.307137067108596\n",
      "train loss:2.299410404970405\n",
      "train loss:2.2969482431167036\n",
      "train loss:2.3050768620230926\n",
      "train loss:2.3091114619453075\n",
      "train loss:2.3038447841803587\n",
      "train loss:2.3070536972883726\n",
      "train loss:2.303295489319055\n",
      "train loss:2.2964882913235676\n",
      "train loss:2.3078866135008456\n",
      "train loss:2.293891442018887\n",
      "train loss:2.2972947868812854\n",
      "train loss:2.3003027457617957\n",
      "train loss:2.3054517978467093\n",
      "train loss:2.305376459445147\n",
      "train loss:2.3052445106249415\n",
      "train loss:2.2997952457060964\n",
      "train loss:2.292338531603397\n",
      "train loss:2.3016118321140246\n",
      "train loss:2.2969507415627155\n",
      "train loss:2.305401053351582\n",
      "train loss:2.304043671674883\n",
      "train loss:2.302759199708843\n",
      "train loss:2.3038082075071036\n",
      "train loss:2.2964191919918777\n",
      "train loss:2.2942845459129115\n",
      "train loss:2.3000473478342314\n",
      "train loss:2.3051518027104603\n",
      "train loss:2.304112146711312\n",
      "train loss:2.30484799183618\n",
      "train loss:2.3023630571484177\n",
      "train loss:2.3034020812967158\n",
      "train loss:2.3031173319580347\n",
      "train loss:2.305451056447661\n",
      "train loss:2.304482198878576\n",
      "train loss:2.2981809731598966\n",
      "train loss:2.296244280972122\n",
      "train loss:2.3072362154616437\n",
      "train loss:2.3005665926418315\n",
      "train loss:2.2961885495011565\n",
      "train loss:2.3071558402451715\n",
      "train loss:2.3042582255867514\n",
      "train loss:2.3063004707610983\n",
      "train loss:2.303026359233853\n",
      "train loss:2.2978240679695747\n",
      "train loss:2.304688401799135\n",
      "train loss:2.299557625120929\n",
      "train loss:2.307813650781152\n",
      "train loss:2.301118554653063\n",
      "train loss:2.297205364965553\n",
      "train loss:2.29982243327889\n",
      "train loss:2.3003489853568153\n",
      "train loss:2.29489643822452\n",
      "train loss:2.296282009742232\n",
      "train loss:2.308486609255329\n",
      "train loss:2.2989339258214354\n",
      "train loss:2.2998637426766027\n",
      "train loss:2.3041201472013477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.305875854249483\n",
      "train loss:2.3018058172734075\n",
      "train loss:2.29858237197642\n",
      "train loss:2.3039226309909813\n",
      "train loss:2.3055670748209565\n",
      "train loss:2.2976273662177453\n",
      "train loss:2.296301316380885\n",
      "train loss:2.301136600077607\n",
      "train loss:2.302825826667188\n",
      "train loss:2.2954587426668396\n",
      "train loss:2.3074241490101035\n",
      "train loss:2.3026532233659784\n",
      "train loss:2.3052225113642426\n",
      "train loss:2.29909505974399\n",
      "train loss:2.30325733315142\n",
      "train loss:2.3011150868970405\n",
      "train loss:2.297755308373956\n",
      "train loss:2.297353876924716\n",
      "train loss:2.301279227255229\n",
      "train loss:2.3029212919449176\n",
      "train loss:2.300200597539507\n",
      "train loss:2.30649945446436\n",
      "train loss:2.2916116340020998\n",
      "train loss:2.301457818469187\n",
      "train loss:2.3027402899869927\n",
      "train loss:2.2939937917155637\n",
      "train loss:2.3052197570176176\n",
      "train loss:2.296920425304327\n",
      "train loss:2.30314256348668\n",
      "train loss:2.3038597306578756\n",
      "train loss:2.3014491102850303\n",
      "train loss:2.297010189672092\n",
      "train loss:2.2975687931453366\n",
      "train loss:2.3126972359979834\n",
      "train loss:2.3007114327484666\n",
      "=== epoch:71, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.304765051260366\n",
      "train loss:2.302185199726636\n",
      "train loss:2.2985887933167324\n",
      "train loss:2.301896101348959\n",
      "train loss:2.2957116918750153\n",
      "train loss:2.30228732242728\n",
      "train loss:2.301430331609095\n",
      "train loss:2.2974424301569227\n",
      "train loss:2.2995179946786246\n",
      "train loss:2.295110785544711\n",
      "train loss:2.299470681300217\n",
      "train loss:2.303559686757212\n",
      "train loss:2.301900168244311\n",
      "train loss:2.2986386438323914\n",
      "train loss:2.3004619628127587\n",
      "train loss:2.3045730625583727\n",
      "train loss:2.295350501080542\n",
      "train loss:2.302089772831492\n",
      "train loss:2.3052308587796797\n",
      "train loss:2.3019484365016005\n",
      "train loss:2.2964614616987014\n",
      "train loss:2.2938508966901736\n",
      "train loss:2.299146821336947\n",
      "train loss:2.3005445169727166\n",
      "train loss:2.302958166876612\n",
      "train loss:2.3016630372669247\n",
      "train loss:2.3017860671062578\n",
      "train loss:2.3075139448349833\n",
      "train loss:2.29889409628992\n",
      "train loss:2.3029659084239595\n",
      "train loss:2.3026056513270667\n",
      "train loss:2.2997836332393295\n",
      "train loss:2.3030364093634184\n",
      "train loss:2.300530500735097\n",
      "train loss:2.3007347637503965\n",
      "train loss:2.2958210467566023\n",
      "train loss:2.294066883863733\n",
      "train loss:2.300832338974178\n",
      "train loss:2.307197956820501\n",
      "train loss:2.3076490213163403\n",
      "train loss:2.2975152442285016\n",
      "train loss:2.3033414648965675\n",
      "train loss:2.3066849209423794\n",
      "train loss:2.3030258627076523\n",
      "train loss:2.303597706995739\n",
      "train loss:2.295645673374809\n",
      "train loss:2.3009854438125914\n",
      "train loss:2.2950525666220285\n",
      "train loss:2.3001297366671767\n",
      "train loss:2.29385766204765\n",
      "train loss:2.3001634106060975\n",
      "train loss:2.29323986039779\n",
      "train loss:2.2970789890088885\n",
      "train loss:2.3012685811270557\n",
      "train loss:2.3032856814387817\n",
      "train loss:2.295856633410785\n",
      "train loss:2.2940078091621783\n",
      "train loss:2.2972254855305647\n",
      "train loss:2.30095779346534\n",
      "train loss:2.3092260964732665\n",
      "train loss:2.3026590895731944\n",
      "train loss:2.2979722014667665\n",
      "train loss:2.290376269759433\n",
      "train loss:2.300863014777048\n",
      "train loss:2.305696086023169\n",
      "train loss:2.292576826077821\n",
      "train loss:2.2986966011728813\n",
      "train loss:2.3019866131788844\n",
      "train loss:2.3007255650760015\n",
      "train loss:2.29417265532196\n",
      "train loss:2.2943449555306348\n",
      "train loss:2.303906848787239\n",
      "train loss:2.2956950437866515\n",
      "train loss:2.3013300523747504\n",
      "train loss:2.3022280306404967\n",
      "train loss:2.302089736184733\n",
      "train loss:2.3016595605451973\n",
      "train loss:2.2959213734281074\n",
      "train loss:2.3049191944077143\n",
      "train loss:2.304308977485281\n",
      "train loss:2.3007646007553872\n",
      "train loss:2.302623530230625\n",
      "train loss:2.301336255424126\n",
      "train loss:2.3044470630757576\n",
      "train loss:2.2999916315099194\n",
      "train loss:2.3024786996993765\n",
      "train loss:2.3061890607493076\n",
      "train loss:2.297956735909529\n",
      "train loss:2.305112357353957\n",
      "train loss:2.301833671719524\n",
      "train loss:2.2965855854628994\n",
      "train loss:2.2976962442486886\n",
      "train loss:2.297246571613202\n",
      "train loss:2.303121183602932\n",
      "train loss:2.301340498466227\n",
      "train loss:2.299753087503212\n",
      "train loss:2.3022240330460693\n",
      "train loss:2.3035842610282056\n",
      "train loss:2.3045640887434105\n",
      "train loss:2.299911296488545\n",
      "train loss:2.303530207721616\n",
      "train loss:2.298025537782935\n",
      "train loss:2.300168594206924\n",
      "train loss:2.303724442948844\n",
      "train loss:2.303542398410721\n",
      "train loss:2.3053410834379577\n",
      "train loss:2.2916739255468417\n",
      "train loss:2.2996892553224666\n",
      "train loss:2.304606777545636\n",
      "train loss:2.300188130324097\n",
      "train loss:2.3050247114474702\n",
      "train loss:2.30590438887523\n",
      "train loss:2.303825161249045\n",
      "train loss:2.305565779756286\n",
      "train loss:2.298317857930653\n",
      "train loss:2.303079192313476\n",
      "train loss:2.309719390580124\n",
      "train loss:2.3011472185592363\n",
      "train loss:2.2950804348194938\n",
      "train loss:2.2905297937725155\n",
      "train loss:2.3052961243454795\n",
      "train loss:2.3025433387272405\n",
      "train loss:2.302849272008978\n",
      "train loss:2.2998902116321336\n",
      "train loss:2.3002361477286932\n",
      "train loss:2.2954322803020055\n",
      "train loss:2.302841358173853\n",
      "train loss:2.299136237334947\n",
      "train loss:2.306125557291052\n",
      "train loss:2.3040660859374307\n",
      "train loss:2.3054128260291007\n",
      "train loss:2.3002271385518127\n",
      "train loss:2.2964149817458366\n",
      "train loss:2.3053871356723947\n",
      "train loss:2.3008284981361706\n",
      "train loss:2.30674838086303\n",
      "train loss:2.300774740719193\n",
      "train loss:2.3007732424534524\n",
      "train loss:2.297275869497658\n",
      "train loss:2.3088387110740265\n",
      "train loss:2.301154427569569\n",
      "train loss:2.302560640327422\n",
      "train loss:2.3039623045361255\n",
      "train loss:2.3027722057356383\n",
      "train loss:2.3039291626009777\n",
      "train loss:2.303964816172344\n",
      "train loss:2.2956384676953228\n",
      "train loss:2.2932133848059837\n",
      "train loss:2.3087459752586676\n",
      "train loss:2.304765910652124\n",
      "train loss:2.3044346187646503\n",
      "train loss:2.2975338969521473\n",
      "train loss:2.3002438753151115\n",
      "train loss:2.304912041826279\n",
      "train loss:2.3008668886493977\n",
      "train loss:2.3035947859345405\n",
      "train loss:2.297335123527617\n",
      "train loss:2.3048357216833995\n",
      "train loss:2.297043757891455\n",
      "train loss:2.2963503086373\n",
      "train loss:2.2986424461690174\n",
      "train loss:2.3038369789228255\n",
      "train loss:2.303488691321034\n",
      "train loss:2.3032266477323504\n",
      "train loss:2.2956037941861664\n",
      "train loss:2.2928275201288155\n",
      "train loss:2.2956518632492013\n",
      "train loss:2.308864116729078\n",
      "train loss:2.3006328984217674\n",
      "train loss:2.302167980983673\n",
      "train loss:2.296497133269071\n",
      "train loss:2.3039300434782968\n",
      "train loss:2.305476888772908\n",
      "train loss:2.3072147420695677\n",
      "train loss:2.3050588901678672\n",
      "train loss:2.290251478010064\n",
      "train loss:2.313586459547033\n",
      "train loss:2.296206398272138\n",
      "train loss:2.2964036377356485\n",
      "train loss:2.302129706167526\n",
      "train loss:2.297925237336172\n",
      "train loss:2.3019930113401768\n",
      "train loss:2.3069190169735596\n",
      "train loss:2.2999118022589617\n",
      "train loss:2.301685991237885\n",
      "train loss:2.3087295621481156\n",
      "train loss:2.3009828851522136\n",
      "train loss:2.305493022369756\n",
      "train loss:2.3107749264727504\n",
      "train loss:2.2998938603917325\n",
      "train loss:2.30721046053368\n",
      "train loss:2.3018192639596604\n",
      "train loss:2.3043743233591254\n",
      "train loss:2.2981529446019398\n",
      "train loss:2.306403601698646\n",
      "train loss:2.309914018715185\n",
      "train loss:2.3013039608537826\n",
      "train loss:2.297717432598254\n",
      "train loss:2.304340690179508\n",
      "train loss:2.2999937976625278\n",
      "=== epoch:72, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.29801251169351\n",
      "train loss:2.2964615930706898\n",
      "train loss:2.3019121921727543\n",
      "train loss:2.302672414603903\n",
      "train loss:2.293665510096276\n",
      "train loss:2.2961368923639576\n",
      "train loss:2.3043575892228216\n",
      "train loss:2.297856922707471\n",
      "train loss:2.3010082996507144\n",
      "train loss:2.306096488992222\n",
      "train loss:2.3042336124737663\n",
      "train loss:2.3012693325214633\n",
      "train loss:2.2961994151765888\n",
      "train loss:2.302709029296229\n",
      "train loss:2.300115924335285\n",
      "train loss:2.30475478791117\n",
      "train loss:2.2896902972986055\n",
      "train loss:2.303791045098886\n",
      "train loss:2.308218163306774\n",
      "train loss:2.3047677919732767\n",
      "train loss:2.302860556646832\n",
      "train loss:2.3046027799474778\n",
      "train loss:2.3034503228722727\n",
      "train loss:2.301617971460458\n",
      "train loss:2.3070275805323845\n",
      "train loss:2.3019862129184596\n",
      "train loss:2.303510427126274\n",
      "train loss:2.3099011613060516\n",
      "train loss:2.3027582197200136\n",
      "train loss:2.3058223353015825\n",
      "train loss:2.29040186121413\n",
      "train loss:2.2946816917501045\n",
      "train loss:2.30195072474486\n",
      "train loss:2.2980070086683493\n",
      "train loss:2.3051396249556446\n",
      "train loss:2.301847204995342\n",
      "train loss:2.3062005724421746\n",
      "train loss:2.312600194788425\n",
      "train loss:2.298737064368489\n",
      "train loss:2.29519719982309\n",
      "train loss:2.30299409046552\n",
      "train loss:2.299665660036718\n",
      "train loss:2.30131772765148\n",
      "train loss:2.301232988049325\n",
      "train loss:2.2993689746746098\n",
      "train loss:2.299477227229052\n",
      "train loss:2.302513767633471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.305154143033717\n",
      "train loss:2.302816482964857\n",
      "train loss:2.297059655013634\n",
      "train loss:2.3028738338379275\n",
      "train loss:2.301797587888\n",
      "train loss:2.300936072159146\n",
      "train loss:2.2971395245316337\n",
      "train loss:2.299324172920369\n",
      "train loss:2.3046776236332325\n",
      "train loss:2.2954184250218965\n",
      "train loss:2.3063130746798524\n",
      "train loss:2.3020477210332504\n",
      "train loss:2.3091813919635347\n",
      "train loss:2.296870002893472\n",
      "train loss:2.308184951949232\n",
      "train loss:2.298265657909163\n",
      "train loss:2.302736489235841\n",
      "train loss:2.3054713486510474\n",
      "train loss:2.302051881168553\n",
      "train loss:2.3019277502464326\n",
      "train loss:2.3043607211184036\n",
      "train loss:2.3048178470499625\n",
      "train loss:2.296191030838856\n",
      "train loss:2.2966562578479675\n",
      "train loss:2.3004141391762953\n",
      "train loss:2.3033718399788614\n",
      "train loss:2.3078733041798594\n",
      "train loss:2.3009925543238596\n",
      "train loss:2.3003505449544175\n",
      "train loss:2.3011385107347584\n",
      "train loss:2.3045589613043904\n",
      "train loss:2.3053123106031514\n",
      "train loss:2.30496360545419\n",
      "train loss:2.3056031395888223\n",
      "train loss:2.29814060838551\n",
      "train loss:2.3031169772617512\n",
      "train loss:2.2959233056443726\n",
      "train loss:2.3019508263561277\n",
      "train loss:2.3012785137137515\n",
      "train loss:2.294669185822769\n",
      "train loss:2.306777765550251\n",
      "train loss:2.3005497234821064\n",
      "train loss:2.2943728285034877\n",
      "train loss:2.3044305550449837\n",
      "train loss:2.2978183686026625\n",
      "train loss:2.313566491383561\n",
      "train loss:2.304111941248049\n",
      "train loss:2.3027161793603343\n",
      "train loss:2.2996246577691086\n",
      "train loss:2.296607121246369\n",
      "train loss:2.3140344787159566\n",
      "train loss:2.3046802883429316\n",
      "train loss:2.296008265402979\n",
      "train loss:2.3042647527747047\n",
      "train loss:2.300633666911823\n",
      "train loss:2.305616864894722\n",
      "train loss:2.302866843522041\n",
      "train loss:2.29592544661229\n",
      "train loss:2.296176577268495\n",
      "train loss:2.295447926153287\n",
      "train loss:2.30760435962704\n",
      "train loss:2.3029440766783873\n",
      "train loss:2.3021926034252234\n",
      "train loss:2.3008459092634848\n",
      "train loss:2.3038135301653395\n",
      "train loss:2.3005487881937743\n",
      "train loss:2.2958723396433274\n",
      "train loss:2.296936945760115\n",
      "train loss:2.303554903847222\n",
      "train loss:2.294731776468628\n",
      "train loss:2.2969059659128703\n",
      "train loss:2.3050305845528474\n",
      "train loss:2.298737168704042\n",
      "train loss:2.299261908297652\n",
      "train loss:2.304069266758677\n",
      "train loss:2.3059559217348413\n",
      "train loss:2.2997968796095227\n",
      "train loss:2.307507665544112\n",
      "train loss:2.2933119294546156\n",
      "train loss:2.29936265696666\n",
      "train loss:2.300561773473514\n",
      "train loss:2.301516923816947\n",
      "train loss:2.305894160677311\n",
      "train loss:2.2994416249588423\n",
      "train loss:2.298407508111957\n",
      "train loss:2.297120070043067\n",
      "train loss:2.3122614186337422\n",
      "train loss:2.3053996810007704\n",
      "train loss:2.3066393546069146\n",
      "train loss:2.3023745639205684\n",
      "train loss:2.298034269996333\n",
      "train loss:2.307839478902093\n",
      "train loss:2.310636743266494\n",
      "train loss:2.300199217992919\n",
      "train loss:2.30253300890654\n",
      "train loss:2.3032556203512606\n",
      "train loss:2.3046530500703\n",
      "train loss:2.3033720227702768\n",
      "train loss:2.301472651559778\n",
      "train loss:2.306653314865566\n",
      "train loss:2.2955886387767235\n",
      "train loss:2.3019792479943426\n",
      "train loss:2.30137817379773\n",
      "train loss:2.306399450391732\n",
      "train loss:2.299169476392948\n",
      "train loss:2.3002897274132854\n",
      "train loss:2.2950759030097627\n",
      "train loss:2.297180953841717\n",
      "train loss:2.302485806214125\n",
      "train loss:2.3025362016806117\n",
      "train loss:2.3073323682693623\n",
      "train loss:2.3051117751685153\n",
      "train loss:2.30800380576128\n",
      "train loss:2.2979524214825897\n",
      "train loss:2.2978851273199172\n",
      "train loss:2.2939067900340486\n",
      "train loss:2.302771294975034\n",
      "train loss:2.300642751879722\n",
      "train loss:2.292482413768587\n",
      "train loss:2.3024898739097077\n",
      "train loss:2.3022057529073376\n",
      "train loss:2.3042003005041445\n",
      "train loss:2.2947926566723718\n",
      "train loss:2.3057534852679598\n",
      "train loss:2.287013875427773\n",
      "train loss:2.302912256797702\n",
      "train loss:2.3019886907145497\n",
      "train loss:2.3015283606356345\n",
      "train loss:2.2965917569714227\n",
      "train loss:2.304537662337568\n",
      "train loss:2.2992076001868837\n",
      "train loss:2.2977404037276936\n",
      "train loss:2.3016789710524606\n",
      "train loss:2.299845617937989\n",
      "train loss:2.2998992126502653\n",
      "train loss:2.307855907861512\n",
      "train loss:2.3064118035071135\n",
      "train loss:2.301371807724703\n",
      "train loss:2.298947048847694\n",
      "train loss:2.297631537609503\n",
      "train loss:2.294552446259323\n",
      "train loss:2.3047255972273444\n",
      "train loss:2.3078408690700787\n",
      "train loss:2.31151024828867\n",
      "train loss:2.3069673751337487\n",
      "train loss:2.3046183712313675\n",
      "train loss:2.297916550807245\n",
      "train loss:2.301669407119237\n",
      "train loss:2.300844792214126\n",
      "train loss:2.306516381172275\n",
      "train loss:2.29462560428702\n",
      "train loss:2.301995880198356\n",
      "train loss:2.2944766506511196\n",
      "=== epoch:73, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3034379461607952\n",
      "train loss:2.301214711728498\n",
      "train loss:2.3041021669011776\n",
      "train loss:2.2998192169291563\n",
      "train loss:2.2976392416892346\n",
      "train loss:2.300591053520931\n",
      "train loss:2.301709625627435\n",
      "train loss:2.2980889431299554\n",
      "train loss:2.2899854835709306\n",
      "train loss:2.2988878329065803\n",
      "train loss:2.2984294710914965\n",
      "train loss:2.3045380708700374\n",
      "train loss:2.2928412925028376\n",
      "train loss:2.2911609195239957\n",
      "train loss:2.302118394271363\n",
      "train loss:2.306436988806425\n",
      "train loss:2.301038474350218\n",
      "train loss:2.297415825202134\n",
      "train loss:2.2962402651526164\n",
      "train loss:2.2989578744708523\n",
      "train loss:2.3040769263606604\n",
      "train loss:2.306058559824226\n",
      "train loss:2.301391168352285\n",
      "train loss:2.29798258764192\n",
      "train loss:2.3005766064636752\n",
      "train loss:2.3011150636205184\n",
      "train loss:2.2964718035746112\n",
      "train loss:2.3000676280518153\n",
      "train loss:2.3029060562664116\n",
      "train loss:2.3000923688631447\n",
      "train loss:2.3022090975524176\n",
      "train loss:2.2963138028987107\n",
      "train loss:2.297994242795199\n",
      "train loss:2.302755525864869\n",
      "train loss:2.304876620740842\n",
      "train loss:2.298310519739793\n",
      "train loss:2.2935164752726\n",
      "train loss:2.303388631904614\n",
      "train loss:2.3018219498327586\n",
      "train loss:2.3106356583165915\n",
      "train loss:2.2975566146765236\n",
      "train loss:2.295464961863204\n",
      "train loss:2.300593733883006\n",
      "train loss:2.2985877277567393\n",
      "train loss:2.308398040612653\n",
      "train loss:2.295101306875339\n",
      "train loss:2.3034389326539144\n",
      "train loss:2.301439677488401\n",
      "train loss:2.3027749348491637\n",
      "train loss:2.2966209031394245\n",
      "train loss:2.294118944855814\n",
      "train loss:2.299710855900692\n",
      "train loss:2.312998889447274\n",
      "train loss:2.2961441671422587\n",
      "train loss:2.300039331666886\n",
      "train loss:2.294022240252629\n",
      "train loss:2.295555040059076\n",
      "train loss:2.3115672301528645\n",
      "train loss:2.3014234746375464\n",
      "train loss:2.297840132530137\n",
      "train loss:2.30420438098504\n",
      "train loss:2.2997826849704586\n",
      "train loss:2.299220762350195\n",
      "train loss:2.3034952851435353\n",
      "train loss:2.298408092767214\n",
      "train loss:2.303669172006041\n",
      "train loss:2.308217496578147\n",
      "train loss:2.301748033970524\n",
      "train loss:2.3006766639181735\n",
      "train loss:2.3018338776706764\n",
      "train loss:2.3001748912543105\n",
      "train loss:2.2995583247572795\n",
      "train loss:2.296434272068221\n",
      "train loss:2.298462673892164\n",
      "train loss:2.297615285553882\n",
      "train loss:2.2983724552727707\n",
      "train loss:2.3003643056165877\n",
      "train loss:2.306445370581159\n",
      "train loss:2.3100884529518138\n",
      "train loss:2.306778351700691\n",
      "train loss:2.2993939929516243\n",
      "train loss:2.2851459360393322\n",
      "train loss:2.3081264030315127\n",
      "train loss:2.3002248732671013\n",
      "train loss:2.3009311070921994\n",
      "train loss:2.3002484977796294\n",
      "train loss:2.30623730884169\n",
      "train loss:2.301947724800652\n",
      "train loss:2.3038556085316095\n",
      "train loss:2.298742376928478\n",
      "train loss:2.288863524762911\n",
      "train loss:2.2904410329049867\n",
      "train loss:2.301020560268939\n",
      "train loss:2.3071447859974596\n",
      "train loss:2.303541097224854\n",
      "train loss:2.298203263824434\n",
      "train loss:2.3074752071432747\n",
      "train loss:2.306260118000417\n",
      "train loss:2.300064798501273\n",
      "train loss:2.302948489773927\n",
      "train loss:2.302014773776153\n",
      "train loss:2.3067023759257475\n",
      "train loss:2.3063341125804246\n",
      "train loss:2.301350122207279\n",
      "train loss:2.3022979484780404\n",
      "train loss:2.2980421904706008\n",
      "train loss:2.303326887697966\n",
      "train loss:2.3160245087957043\n",
      "train loss:2.2991875855076844\n",
      "train loss:2.299637236518536\n",
      "train loss:2.296015966490549\n",
      "train loss:2.305464443386054\n",
      "train loss:2.294790209121937\n",
      "train loss:2.2993283489733627\n",
      "train loss:2.3044675597796456\n",
      "train loss:2.295501275492356\n",
      "train loss:2.302747016548117\n",
      "train loss:2.3046879131005915\n",
      "train loss:2.300809075486185\n",
      "train loss:2.300566284108695\n",
      "train loss:2.3014777744534394\n",
      "train loss:2.306170173145623\n",
      "train loss:2.298230290993225\n",
      "train loss:2.3033755260310222\n",
      "train loss:2.304746318280543\n",
      "train loss:2.304313009853321\n",
      "train loss:2.3003005167035666\n",
      "train loss:2.30560890198184\n",
      "train loss:2.3031673884128034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3080723654443904\n",
      "train loss:2.30545903967009\n",
      "train loss:2.300842471352507\n",
      "train loss:2.3067714155714376\n",
      "train loss:2.3014593047517566\n",
      "train loss:2.297934654891904\n",
      "train loss:2.2974751483975004\n",
      "train loss:2.306664549148115\n",
      "train loss:2.302381116355398\n",
      "train loss:2.3018678084112505\n",
      "train loss:2.299689529941602\n",
      "train loss:2.3055322616763347\n",
      "train loss:2.2942005394294944\n",
      "train loss:2.303703985761518\n",
      "train loss:2.307177563152441\n",
      "train loss:2.306664583235516\n",
      "train loss:2.309800129156022\n",
      "train loss:2.305927167449871\n",
      "train loss:2.3060999280398007\n",
      "train loss:2.297898802319717\n",
      "train loss:2.3000804098264864\n",
      "train loss:2.296915396876188\n",
      "train loss:2.2997475512300003\n",
      "train loss:2.3018323343327545\n",
      "train loss:2.300645641102274\n",
      "train loss:2.301948622764068\n",
      "train loss:2.3005887482960645\n",
      "train loss:2.2964860570405974\n",
      "train loss:2.3051719338013847\n",
      "train loss:2.296780874766204\n",
      "train loss:2.297133359072178\n",
      "train loss:2.302746457227121\n",
      "train loss:2.2995272046366617\n",
      "train loss:2.3034483652067594\n",
      "train loss:2.297156802575361\n",
      "train loss:2.296442209502303\n",
      "train loss:2.3036365047923204\n",
      "train loss:2.304408093692825\n",
      "train loss:2.2994503311252013\n",
      "train loss:2.295440095500719\n",
      "train loss:2.2975626600114283\n",
      "train loss:2.3035737176633906\n",
      "train loss:2.2994144569706556\n",
      "train loss:2.3005857004624457\n",
      "train loss:2.289937664086731\n",
      "train loss:2.3008122620209686\n",
      "train loss:2.3061778053920774\n",
      "train loss:2.3040494768376716\n",
      "train loss:2.3046487710399135\n",
      "train loss:2.3111702771885496\n",
      "train loss:2.30171229138418\n",
      "train loss:2.307104907606246\n",
      "train loss:2.3059517874511455\n",
      "train loss:2.305407254618047\n",
      "train loss:2.294388948810736\n",
      "train loss:2.307985730774955\n",
      "train loss:2.3027826051423688\n",
      "train loss:2.302099110065694\n",
      "train loss:2.3084280036557976\n",
      "train loss:2.29517012254268\n",
      "train loss:2.2988065337432255\n",
      "train loss:2.299974736001793\n",
      "train loss:2.301754146792178\n",
      "train loss:2.2994984231026825\n",
      "train loss:2.297828551785151\n",
      "train loss:2.30733944744962\n",
      "train loss:2.306312110984067\n",
      "train loss:2.3021539101417248\n",
      "train loss:2.29833470288512\n",
      "train loss:2.2996694250145957\n",
      "train loss:2.304135854613027\n",
      "=== epoch:74, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3031174895578315\n",
      "train loss:2.300641076779655\n",
      "train loss:2.298649661326854\n",
      "train loss:2.3045014187683686\n",
      "train loss:2.2923933524780264\n",
      "train loss:2.301822540261562\n",
      "train loss:2.300512166834598\n",
      "train loss:2.299108446632302\n",
      "train loss:2.3042971082831603\n",
      "train loss:2.299438104594514\n",
      "train loss:2.3014207300295166\n",
      "train loss:2.299597622666106\n",
      "train loss:2.306812445446958\n",
      "train loss:2.3012888461789416\n",
      "train loss:2.3053341454363956\n",
      "train loss:2.2985514886998577\n",
      "train loss:2.3034724439425993\n",
      "train loss:2.2993731280199095\n",
      "train loss:2.301482180377701\n",
      "train loss:2.3066449173663073\n",
      "train loss:2.295263513343905\n",
      "train loss:2.297248997024882\n",
      "train loss:2.2990836011505276\n",
      "train loss:2.3021765837911743\n",
      "train loss:2.3071764311477323\n",
      "train loss:2.3083309672802113\n",
      "train loss:2.2986390890158166\n",
      "train loss:2.3014014421470392\n",
      "train loss:2.3015309597151625\n",
      "train loss:2.306817302988134\n",
      "train loss:2.300955927120735\n",
      "train loss:2.2992601988207193\n",
      "train loss:2.3047630237397025\n",
      "train loss:2.305568735755821\n",
      "train loss:2.3090784761200345\n",
      "train loss:2.299116058725511\n",
      "train loss:2.3037286179089143\n",
      "train loss:2.2909933822870743\n",
      "train loss:2.3063078714978325\n",
      "train loss:2.300373789039121\n",
      "train loss:2.294652178151731\n",
      "train loss:2.3006630381253133\n",
      "train loss:2.3034273238631977\n",
      "train loss:2.2971699720262726\n",
      "train loss:2.2951842243405394\n",
      "train loss:2.3021945180208654\n",
      "train loss:2.296971580517348\n",
      "train loss:2.3000362365406715\n",
      "train loss:2.303676995246596\n",
      "train loss:2.2928283403462744\n",
      "train loss:2.30499777372862\n",
      "train loss:2.2955714069381106\n",
      "train loss:2.292356688195787\n",
      "train loss:2.309310279640083\n",
      "train loss:2.3067526609987947\n",
      "train loss:2.304759951019399\n",
      "train loss:2.3036498382885298\n",
      "train loss:2.302723299076951\n",
      "train loss:2.301714937859479\n",
      "train loss:2.3065869057562898\n",
      "train loss:2.3030126628145253\n",
      "train loss:2.3110858300232837\n",
      "train loss:2.299392111762352\n",
      "train loss:2.3026222503766314\n",
      "train loss:2.2997725565687435\n",
      "train loss:2.305272255916816\n",
      "train loss:2.3018954979230832\n",
      "train loss:2.290950473238744\n",
      "train loss:2.2982827638089622\n",
      "train loss:2.2995203024727964\n",
      "train loss:2.299037768803173\n",
      "train loss:2.3033873183655023\n",
      "train loss:2.294837403092562\n",
      "train loss:2.3042697575161197\n",
      "train loss:2.302930488149968\n",
      "train loss:2.3017526289077943\n",
      "train loss:2.300163935919056\n",
      "train loss:2.3015512149233373\n",
      "train loss:2.300685432405791\n",
      "train loss:2.3022806610265243\n",
      "train loss:2.3018296590121476\n",
      "train loss:2.298823817429659\n",
      "train loss:2.3026615223893945\n",
      "train loss:2.303307972681023\n",
      "train loss:2.3027080842625796\n",
      "train loss:2.2971826689142665\n",
      "train loss:2.300896290677342\n",
      "train loss:2.301243780436474\n",
      "train loss:2.301523579849614\n",
      "train loss:2.2986278902322796\n",
      "train loss:2.308881931535789\n",
      "train loss:2.2961182131896316\n",
      "train loss:2.303719147209444\n",
      "train loss:2.307277501488753\n",
      "train loss:2.3074072181039127\n",
      "train loss:2.3015559292848784\n",
      "train loss:2.3001137631720727\n",
      "train loss:2.309465111410057\n",
      "train loss:2.303400417248547\n",
      "train loss:2.300381554381873\n",
      "train loss:2.298462593362978\n",
      "train loss:2.296695316566817\n",
      "train loss:2.3002281921203633\n",
      "train loss:2.3057587418959846\n",
      "train loss:2.305724251541225\n",
      "train loss:2.2961560946873973\n",
      "train loss:2.2970290661079944\n",
      "train loss:2.2950550414591944\n",
      "train loss:2.305574933536655\n",
      "train loss:2.303049242620008\n",
      "train loss:2.306610034600915\n",
      "train loss:2.3015828980392232\n",
      "train loss:2.30401457028864\n",
      "train loss:2.306434591705995\n",
      "train loss:2.3035056160976404\n",
      "train loss:2.2961166472430192\n",
      "train loss:2.294239269698909\n",
      "train loss:2.3057183342959062\n",
      "train loss:2.3075113372963014\n",
      "train loss:2.292479453983971\n",
      "train loss:2.3101883240278847\n",
      "train loss:2.302089907596208\n",
      "train loss:2.3042352848228664\n",
      "train loss:2.3095104049252475\n",
      "train loss:2.2996293767243805\n",
      "train loss:2.311100824546392\n",
      "train loss:2.3038594580720644\n",
      "train loss:2.2964010695356123\n",
      "train loss:2.2984412254102042\n",
      "train loss:2.3143027674322885\n",
      "train loss:2.3043845599667994\n",
      "train loss:2.303594621881289\n",
      "train loss:2.2950107165892164\n",
      "train loss:2.304624373550001\n",
      "train loss:2.2995172909567847\n",
      "train loss:2.3035730695836225\n",
      "train loss:2.3046020877328965\n",
      "train loss:2.3095318350364873\n",
      "train loss:2.3046079223091627\n",
      "train loss:2.3008013565360588\n",
      "train loss:2.3021128445171617\n",
      "train loss:2.301623370445703\n",
      "train loss:2.297339631319693\n",
      "train loss:2.300004764842122\n",
      "train loss:2.2979334603477564\n",
      "train loss:2.3103471007405822\n",
      "train loss:2.2964523417737848\n",
      "train loss:2.2991618082823453\n",
      "train loss:2.3038678323420316\n",
      "train loss:2.3050013181392552\n",
      "train loss:2.3115450463814007\n",
      "train loss:2.297970496119242\n",
      "train loss:2.3032864987971107\n",
      "train loss:2.3088964571299173\n",
      "train loss:2.2992149107415094\n",
      "train loss:2.3007640777305713\n",
      "train loss:2.299957864207199\n",
      "train loss:2.3040812153973023\n",
      "train loss:2.3054357363373623\n",
      "train loss:2.2892277154256804\n",
      "train loss:2.294121924688606\n",
      "train loss:2.30572484253393\n",
      "train loss:2.3025531223779123\n",
      "train loss:2.3050297296527416\n",
      "train loss:2.3018366454181094\n",
      "train loss:2.3006960370152516\n",
      "train loss:2.304914221113064\n",
      "train loss:2.296560756520147\n",
      "train loss:2.307269382699079\n",
      "train loss:2.296750380128689\n",
      "train loss:2.2997690405040103\n",
      "train loss:2.3047803265165427\n",
      "train loss:2.2960946376007385\n",
      "train loss:2.2984793462647484\n",
      "train loss:2.2926382508731096\n",
      "train loss:2.306649473680042\n",
      "train loss:2.3084560633671223\n",
      "train loss:2.3028885608678378\n",
      "train loss:2.295307500638787\n",
      "train loss:2.303643649513999\n",
      "train loss:2.307912184474219\n",
      "train loss:2.2950147119622115\n",
      "train loss:2.3058751759294793\n",
      "train loss:2.3089906754354623\n",
      "train loss:2.301019132208184\n",
      "train loss:2.289896618404394\n",
      "train loss:2.2904068479140967\n",
      "train loss:2.301147968238601\n",
      "train loss:2.2918018674478304\n",
      "train loss:2.304362782466498\n",
      "train loss:2.3023369727239977\n",
      "train loss:2.3075549204965276\n",
      "train loss:2.295442687455549\n",
      "train loss:2.300088718165143\n",
      "train loss:2.2966454580699103\n",
      "train loss:2.3053265252805297\n",
      "train loss:2.2943450788521074\n",
      "train loss:2.304144816487215\n",
      "train loss:2.296642214613902\n",
      "train loss:2.3008934568174646\n",
      "=== epoch:75, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3031793911912977\n",
      "train loss:2.2987644769292115\n",
      "train loss:2.3003640733258512\n",
      "train loss:2.2883505651934306\n",
      "train loss:2.303261078030023\n",
      "train loss:2.310392948507905\n",
      "train loss:2.3045481502472462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301898438570874\n",
      "train loss:2.2952201797200895\n",
      "train loss:2.3064970856374667\n",
      "train loss:2.3068526590594565\n",
      "train loss:2.3018623640171425\n",
      "train loss:2.2960333492659735\n",
      "train loss:2.304681773690292\n",
      "train loss:2.2868567436003686\n",
      "train loss:2.305877713515672\n",
      "train loss:2.302330594666073\n",
      "train loss:2.299294313635123\n",
      "train loss:2.3022981090234436\n",
      "train loss:2.2911553323834015\n",
      "train loss:2.299323761156497\n",
      "train loss:2.295452504343462\n",
      "train loss:2.308373220263121\n",
      "train loss:2.2992261443464597\n",
      "train loss:2.3021269477909874\n",
      "train loss:2.2992034204906586\n",
      "train loss:2.30090899364833\n",
      "train loss:2.2995694088327037\n",
      "train loss:2.2968924090964964\n",
      "train loss:2.302916257596118\n",
      "train loss:2.300640781089736\n",
      "train loss:2.298339917444535\n",
      "train loss:2.2924338712071473\n",
      "train loss:2.299338539779806\n",
      "train loss:2.3031797304434303\n",
      "train loss:2.3030781329971455\n",
      "train loss:2.2924624746484694\n",
      "train loss:2.2907811320009905\n",
      "train loss:2.299280558424919\n",
      "train loss:2.301596406751832\n",
      "train loss:2.3040288437941197\n",
      "train loss:2.303414438288692\n",
      "train loss:2.300972939264092\n",
      "train loss:2.2955889186400342\n",
      "train loss:2.3078442715975864\n",
      "train loss:2.3004383111331346\n",
      "train loss:2.304208548707654\n",
      "train loss:2.2993216794761726\n",
      "train loss:2.3032279656860997\n",
      "train loss:2.3039561521280487\n",
      "train loss:2.2918907967840934\n",
      "train loss:2.2996472597073603\n",
      "train loss:2.2971374990582936\n",
      "train loss:2.295145782790111\n",
      "train loss:2.3033053207211798\n",
      "train loss:2.3055571244891935\n",
      "train loss:2.305475963392777\n",
      "train loss:2.302410017272498\n",
      "train loss:2.302850261113514\n",
      "train loss:2.3074210721915644\n",
      "train loss:2.300640007548885\n",
      "train loss:2.304247454939653\n",
      "train loss:2.3097966676989716\n",
      "train loss:2.3058940803573056\n",
      "train loss:2.2927169807213437\n",
      "train loss:2.3082378056980715\n",
      "train loss:2.293970846050396\n",
      "train loss:2.2981211709813705\n",
      "train loss:2.298963803732021\n",
      "train loss:2.3027146334939603\n",
      "train loss:2.3004259110573253\n",
      "train loss:2.3121142939679347\n",
      "train loss:2.306125659046063\n",
      "train loss:2.3015876423800683\n",
      "train loss:2.2912838110795755\n",
      "train loss:2.3051203573999817\n",
      "train loss:2.2941136040685914\n",
      "train loss:2.298135815491197\n",
      "train loss:2.3036970779827812\n",
      "train loss:2.295023803832612\n",
      "train loss:2.2997988818027406\n",
      "train loss:2.298598459255223\n",
      "train loss:2.304390991852723\n",
      "train loss:2.3105559417081345\n",
      "train loss:2.291229666604619\n",
      "train loss:2.3027257983178786\n",
      "train loss:2.3031542835229177\n",
      "train loss:2.296969236474887\n",
      "train loss:2.297371913461138\n",
      "train loss:2.291229554566863\n",
      "train loss:2.302230370471273\n",
      "train loss:2.2993165387247587\n",
      "train loss:2.2965428267290062\n",
      "train loss:2.301108811876784\n",
      "train loss:2.3042252191294152\n",
      "train loss:2.310357217694681\n",
      "train loss:2.304988436326149\n",
      "train loss:2.2996180179877426\n",
      "train loss:2.2998466111166267\n",
      "train loss:2.30899927356779\n",
      "train loss:2.309106793093739\n",
      "train loss:2.3041351069254885\n",
      "train loss:2.29674981843935\n",
      "train loss:2.3027025525718425\n",
      "train loss:2.310919723199714\n",
      "train loss:2.308399570205561\n",
      "train loss:2.301507160621529\n",
      "train loss:2.2935858056201863\n",
      "train loss:2.306077107161215\n",
      "train loss:2.2939474191793123\n",
      "train loss:2.298285302110912\n",
      "train loss:2.303924676322349\n",
      "train loss:2.3025685945616625\n",
      "train loss:2.2906625936257523\n",
      "train loss:2.300656108176354\n",
      "train loss:2.298371486156689\n",
      "train loss:2.301711981282343\n",
      "train loss:2.305134450170419\n",
      "train loss:2.2983162600333995\n",
      "train loss:2.2985861207725784\n",
      "train loss:2.3007419995690674\n",
      "train loss:2.304654030976373\n",
      "train loss:2.305145278362458\n",
      "train loss:2.3031630456304373\n",
      "train loss:2.2969111841680725\n",
      "train loss:2.3008174116072113\n",
      "train loss:2.302656755894068\n",
      "train loss:2.2971362066582817\n",
      "train loss:2.2992216794617786\n",
      "train loss:2.303389926072538\n",
      "train loss:2.3080917157800696\n",
      "train loss:2.3054355580737513\n",
      "train loss:2.3061665659946313\n",
      "train loss:2.28973489562567\n",
      "train loss:2.2965562471919303\n",
      "train loss:2.3075292817916266\n",
      "train loss:2.303256036157265\n",
      "train loss:2.3092824540774113\n",
      "train loss:2.300123364455472\n",
      "train loss:2.3004572754586157\n",
      "train loss:2.3080986671948738\n",
      "train loss:2.2951032203414905\n",
      "train loss:2.303796945069099\n",
      "train loss:2.300662958598211\n",
      "train loss:2.30875968038579\n",
      "train loss:2.3001663991084427\n",
      "train loss:2.3040114728342327\n",
      "train loss:2.3028807410240617\n",
      "train loss:2.312060374956804\n",
      "train loss:2.2945213968156004\n",
      "train loss:2.299373170056931\n",
      "train loss:2.2907650714348105\n",
      "train loss:2.2940494569217065\n",
      "train loss:2.3023907342611976\n",
      "train loss:2.3044493240872677\n",
      "train loss:2.303178635567186\n",
      "train loss:2.3099037925404557\n",
      "train loss:2.3066386003988057\n",
      "train loss:2.300829466875891\n",
      "train loss:2.3010785371459623\n",
      "train loss:2.3005444054051187\n",
      "train loss:2.300385127878062\n",
      "train loss:2.3098176802294392\n",
      "train loss:2.30548989454767\n",
      "train loss:2.300482888278187\n",
      "train loss:2.3013460575772977\n",
      "train loss:2.304937745941105\n",
      "train loss:2.297977912774812\n",
      "train loss:2.309290351246224\n",
      "train loss:2.3001919583735684\n",
      "train loss:2.306090581342015\n",
      "train loss:2.3011977991213763\n",
      "train loss:2.3009602942340246\n",
      "train loss:2.3005031086494916\n",
      "train loss:2.299560760816856\n",
      "train loss:2.304189350920577\n",
      "train loss:2.2982512051643496\n",
      "train loss:2.3053374162332427\n",
      "train loss:2.2970235186080457\n",
      "train loss:2.3028887967699254\n",
      "train loss:2.29642245864089\n",
      "train loss:2.3063691329511453\n",
      "train loss:2.3083868713989375\n",
      "train loss:2.3003960977537656\n",
      "train loss:2.294291692516246\n",
      "train loss:2.3041800263817493\n",
      "train loss:2.297929657247002\n",
      "train loss:2.2908657677192714\n",
      "train loss:2.3097789049974455\n",
      "train loss:2.299405958678414\n",
      "train loss:2.3018210584466594\n",
      "train loss:2.308408383640573\n",
      "train loss:2.2950921936752398\n",
      "train loss:2.297550222624215\n",
      "train loss:2.2990421266533767\n",
      "train loss:2.301250344610606\n",
      "train loss:2.3147427258801336\n",
      "train loss:2.302867894514782\n",
      "train loss:2.298124476843517\n",
      "train loss:2.310155079884457\n",
      "=== epoch:76, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3057014884115348\n",
      "train loss:2.2998743726179414\n",
      "train loss:2.298039465079383\n",
      "train loss:2.30496089019939\n",
      "train loss:2.299846317568285\n",
      "train loss:2.3043964940497785\n",
      "train loss:2.298860247923451\n",
      "train loss:2.300932705440225\n",
      "train loss:2.3090632830268354\n",
      "train loss:2.2982691121259755\n",
      "train loss:2.2968382842339743\n",
      "train loss:2.298350902491792\n",
      "train loss:2.298661859631089\n",
      "train loss:2.3011127014945627\n",
      "train loss:2.3014531686977313\n",
      "train loss:2.3109332947439234\n",
      "train loss:2.2995838499717585\n",
      "train loss:2.3010831567526115\n",
      "train loss:2.303987512394337\n",
      "train loss:2.3031878097196032\n",
      "train loss:2.3026656003635204\n",
      "train loss:2.3031583658943124\n",
      "train loss:2.301882639133521\n",
      "train loss:2.3045164471162125\n",
      "train loss:2.2966078801982768\n",
      "train loss:2.295013228270733\n",
      "train loss:2.3031161218163736\n",
      "train loss:2.3075449114165374\n",
      "train loss:2.2965438131527063\n",
      "train loss:2.3023547881055766\n",
      "train loss:2.305201152105465\n",
      "train loss:2.297060735557227\n",
      "train loss:2.30499132695561\n",
      "train loss:2.2987879280424064\n",
      "train loss:2.2964467322824196\n",
      "train loss:2.3079388855719283\n",
      "train loss:2.2990386458582663\n",
      "train loss:2.302772232199588\n",
      "train loss:2.2950763104458947\n",
      "train loss:2.301797173878178\n",
      "train loss:2.3028159804330737\n",
      "train loss:2.310516055734153\n",
      "train loss:2.3051108571488377\n",
      "train loss:2.3044226839874318\n",
      "train loss:2.3065026455837643\n",
      "train loss:2.309325008278436\n",
      "train loss:2.3057119519437066\n",
      "train loss:2.29816337463954\n",
      "train loss:2.3049169408840666\n",
      "train loss:2.3103900062137503\n",
      "train loss:2.3034011080889027\n",
      "train loss:2.3016677550615046\n",
      "train loss:2.3078094924760806\n",
      "train loss:2.2957818054951384\n",
      "train loss:2.2980200809674582\n",
      "train loss:2.3016530162988063\n",
      "train loss:2.295560038024205\n",
      "train loss:2.298383512935009\n",
      "train loss:2.2980251441938195\n",
      "train loss:2.294523843405558\n",
      "train loss:2.305109262480649\n",
      "train loss:2.30125297402567\n",
      "train loss:2.305397345995929\n",
      "train loss:2.297623975465661\n",
      "train loss:2.301355294143962\n",
      "train loss:2.304034668539316\n",
      "train loss:2.3054201162130528\n",
      "train loss:2.2980612006641072\n",
      "train loss:2.2983911533319388\n",
      "train loss:2.2907836692705743\n",
      "train loss:2.30232896615033\n",
      "train loss:2.3072825006150213\n",
      "train loss:2.308664170380776\n",
      "train loss:2.296858487140703\n",
      "train loss:2.3037057327764567\n",
      "train loss:2.303306793929357\n",
      "train loss:2.3026356205842426\n",
      "train loss:2.2980019487423324\n",
      "train loss:2.3034995623963894\n",
      "train loss:2.290931861718219\n",
      "train loss:2.295200809521406\n",
      "train loss:2.305402009307401\n",
      "train loss:2.2931665904158045\n",
      "train loss:2.2952340741923503\n",
      "train loss:2.295044978184101\n",
      "train loss:2.294961680928166\n",
      "train loss:2.3035421708783117\n",
      "train loss:2.3026921981051682\n",
      "train loss:2.3011158361128894\n",
      "train loss:2.30525896496007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3008632309403825\n",
      "train loss:2.2965687381959956\n",
      "train loss:2.2929538583310065\n",
      "train loss:2.30505700397221\n",
      "train loss:2.2918882482488616\n",
      "train loss:2.293140824013031\n",
      "train loss:2.3020505896289163\n",
      "train loss:2.300841946220154\n",
      "train loss:2.3005876293434966\n",
      "train loss:2.3004533555741573\n",
      "train loss:2.2960255668571024\n",
      "train loss:2.3034690498875268\n",
      "train loss:2.3026959784064966\n",
      "train loss:2.2974167876308655\n",
      "train loss:2.305764775187861\n",
      "train loss:2.2983932628622536\n",
      "train loss:2.3003968750909634\n",
      "train loss:2.2954366220954063\n",
      "train loss:2.3098640657223144\n",
      "train loss:2.2953753946728477\n",
      "train loss:2.2990011462301845\n",
      "train loss:2.3071312817492284\n",
      "train loss:2.304299339769876\n",
      "train loss:2.302637303475747\n",
      "train loss:2.3021911467775196\n",
      "train loss:2.2948113475911756\n",
      "train loss:2.3017337462125544\n",
      "train loss:2.303205819077576\n",
      "train loss:2.3063462649995436\n",
      "train loss:2.304059821111093\n",
      "train loss:2.302882673431496\n",
      "train loss:2.296195603735607\n",
      "train loss:2.300322755437676\n",
      "train loss:2.3043345321224655\n",
      "train loss:2.290978146002555\n",
      "train loss:2.3031319587781693\n",
      "train loss:2.3009564833233873\n",
      "train loss:2.300608703387864\n",
      "train loss:2.293955611841775\n",
      "train loss:2.2977625850536567\n",
      "train loss:2.3061227208579957\n",
      "train loss:2.29575838243821\n",
      "train loss:2.3015039574760836\n",
      "train loss:2.302612119503048\n",
      "train loss:2.304773286028328\n",
      "train loss:2.3037073501232417\n",
      "train loss:2.301237341214385\n",
      "train loss:2.3026811775645286\n",
      "train loss:2.299299905274751\n",
      "train loss:2.305403020979192\n",
      "train loss:2.3064398053112942\n",
      "train loss:2.29422240810893\n",
      "train loss:2.297900064765122\n",
      "train loss:2.303248355859939\n",
      "train loss:2.3112527852797418\n",
      "train loss:2.2968448763268547\n",
      "train loss:2.306646836669966\n",
      "train loss:2.3010148526067677\n",
      "train loss:2.302715340781992\n",
      "train loss:2.30482201913358\n",
      "train loss:2.2951873808369796\n",
      "train loss:2.301539130983641\n",
      "train loss:2.2977747815418876\n",
      "train loss:2.2951335262296384\n",
      "train loss:2.3063236031599303\n",
      "train loss:2.2958079866071803\n",
      "train loss:2.3016674869435647\n",
      "train loss:2.3051594010274843\n",
      "train loss:2.302647011214943\n",
      "train loss:2.296512694934934\n",
      "train loss:2.296386110384185\n",
      "train loss:2.3010336139321654\n",
      "train loss:2.2998912881400786\n",
      "train loss:2.3015643669046835\n",
      "train loss:2.307636277316482\n",
      "train loss:2.304765900706507\n",
      "train loss:2.2941741953952413\n",
      "train loss:2.3046670477330093\n",
      "train loss:2.3000259947089225\n",
      "train loss:2.2964921644382024\n",
      "train loss:2.297780742623303\n",
      "train loss:2.3056535561881155\n",
      "train loss:2.296335049020625\n",
      "train loss:2.2996448420232642\n",
      "train loss:2.3071244041117707\n",
      "train loss:2.297299825488834\n",
      "train loss:2.291743403514346\n",
      "train loss:2.306613874621012\n",
      "train loss:2.2949361274048288\n",
      "train loss:2.306702941260618\n",
      "train loss:2.297591276939497\n",
      "train loss:2.2972030241838692\n",
      "train loss:2.2965041197036715\n",
      "train loss:2.3026328016145423\n",
      "train loss:2.305003055784079\n",
      "train loss:2.301028161556095\n",
      "train loss:2.2943512516487736\n",
      "train loss:2.304878768408834\n",
      "train loss:2.296748982581966\n",
      "train loss:2.2967987215169448\n",
      "train loss:2.3043731232490754\n",
      "train loss:2.297358474578856\n",
      "train loss:2.303281851047841\n",
      "train loss:2.2927990547679014\n",
      "train loss:2.2991696686707668\n",
      "train loss:2.2991336566925695\n",
      "train loss:2.2963544458919523\n",
      "train loss:2.29386591981796\n",
      "train loss:2.300928236711254\n",
      "train loss:2.306942000508173\n",
      "=== epoch:77, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2942161970601314\n",
      "train loss:2.3015552847157386\n",
      "train loss:2.2984513918726646\n",
      "train loss:2.301180960268386\n",
      "train loss:2.2913900320259493\n",
      "train loss:2.296208616071051\n",
      "train loss:2.307151339310758\n",
      "train loss:2.299343771752123\n",
      "train loss:2.308842663868722\n",
      "train loss:2.300560222293093\n",
      "train loss:2.3048060642441284\n",
      "train loss:2.2991231633889417\n",
      "train loss:2.3037803241727848\n",
      "train loss:2.2992106081342203\n",
      "train loss:2.3019819098918126\n",
      "train loss:2.2984760299724623\n",
      "train loss:2.307896501223134\n",
      "train loss:2.304675214398683\n",
      "train loss:2.3070012919068974\n",
      "train loss:2.304971521638543\n",
      "train loss:2.2983002549335305\n",
      "train loss:2.3091355864036625\n",
      "train loss:2.299550657773297\n",
      "train loss:2.299631779511009\n",
      "train loss:2.292236657002196\n",
      "train loss:2.294341619598918\n",
      "train loss:2.299502403250521\n",
      "train loss:2.2979046077188428\n",
      "train loss:2.2988727962568523\n",
      "train loss:2.2983833492442702\n",
      "train loss:2.296882281614178\n",
      "train loss:2.303946313384135\n",
      "train loss:2.3047422354430016\n",
      "train loss:2.304092164413007\n",
      "train loss:2.2922183430905188\n",
      "train loss:2.3007322730957203\n",
      "train loss:2.2961671945638003\n",
      "train loss:2.293972005511942\n",
      "train loss:2.2933136281791158\n",
      "train loss:2.3003845803957925\n",
      "train loss:2.304440645455732\n",
      "train loss:2.304248589921734\n",
      "train loss:2.3021052770370587\n",
      "train loss:2.299806828702484\n",
      "train loss:2.303917203078857\n",
      "train loss:2.3010096387158523\n",
      "train loss:2.309619758260191\n",
      "train loss:2.307106150299706\n",
      "train loss:2.2932682650662644\n",
      "train loss:2.3011225104527324\n",
      "train loss:2.299992448186751\n",
      "train loss:2.294370344005791\n",
      "train loss:2.299566597899271\n",
      "train loss:2.301342198591439\n",
      "train loss:2.3024045169957583\n",
      "train loss:2.3006160221540877\n",
      "train loss:2.3017435072226533\n",
      "train loss:2.3013942158619134\n",
      "train loss:2.2972834233267614\n",
      "train loss:2.298397663854498\n",
      "train loss:2.3031940715069203\n",
      "train loss:2.296236944647529\n",
      "train loss:2.3004417859867683\n",
      "train loss:2.2946023474134103\n",
      "train loss:2.299614750314745\n",
      "train loss:2.3145841624051786\n",
      "train loss:2.29535033071668\n",
      "train loss:2.2990168272375966\n",
      "train loss:2.311726357149076\n",
      "train loss:2.2998633543719533\n",
      "train loss:2.308205836158534\n",
      "train loss:2.3007647976444554\n",
      "train loss:2.2972210194827025\n",
      "train loss:2.296007433459065\n",
      "train loss:2.3050005053651677\n",
      "train loss:2.292024863777035\n",
      "train loss:2.299304607283361\n",
      "train loss:2.3083096860038235\n",
      "train loss:2.2923542462740483\n",
      "train loss:2.3043170234773216\n",
      "train loss:2.2957993512210297\n",
      "train loss:2.2993700486948425\n",
      "train loss:2.2991600358197393\n",
      "train loss:2.3068889461276587\n",
      "train loss:2.2970405353607726\n",
      "train loss:2.307381999563798\n",
      "train loss:2.301405326110539\n",
      "train loss:2.3061043047974454\n",
      "train loss:2.304421724435041\n",
      "train loss:2.301779290678986\n",
      "train loss:2.298382908531598\n",
      "train loss:2.2983230829020713\n",
      "train loss:2.305507519839298\n",
      "train loss:2.3083069348674754\n",
      "train loss:2.2996008937043673\n",
      "train loss:2.299011873065064\n",
      "train loss:2.303745519990047\n",
      "train loss:2.3085587907618423\n",
      "train loss:2.298935669465739\n",
      "train loss:2.2935507602722227\n",
      "train loss:2.289820559133768\n",
      "train loss:2.2977062335796283\n",
      "train loss:2.29781442686125\n",
      "train loss:2.3025487643367715\n",
      "train loss:2.293192743501349\n",
      "train loss:2.2999767266007076\n",
      "train loss:2.3043907352099806\n",
      "train loss:2.301379848276604\n",
      "train loss:2.304098151832557\n",
      "train loss:2.307224382985313\n",
      "train loss:2.304243854897927\n",
      "train loss:2.308145786741514\n",
      "train loss:2.3036388687942626\n",
      "train loss:2.304098792439755\n",
      "train loss:2.304966162710819\n",
      "train loss:2.3035192130717697\n",
      "train loss:2.2953895003524423\n",
      "train loss:2.305901979360732\n",
      "train loss:2.296819384988404\n",
      "train loss:2.295724645786057\n",
      "train loss:2.2921247435692034\n",
      "train loss:2.3002940074702387\n",
      "train loss:2.3009304644280633\n",
      "train loss:2.296177637876705\n",
      "train loss:2.297515052197639\n",
      "train loss:2.298362609289657\n",
      "train loss:2.2966375753353088\n",
      "train loss:2.3138098151908357\n",
      "train loss:2.300335834423854\n",
      "train loss:2.303042600257219\n",
      "train loss:2.3117574268600047\n",
      "train loss:2.2927932263810855\n",
      "train loss:2.3068989334351957\n",
      "train loss:2.2996841025181056\n",
      "train loss:2.2998372301720176\n",
      "train loss:2.300654833079328\n",
      "train loss:2.2976549558099375\n",
      "train loss:2.3089242637350225\n",
      "train loss:2.2957910136259563\n",
      "train loss:2.301586248821351\n",
      "train loss:2.2958290647905404\n",
      "train loss:2.3023574724396227\n",
      "train loss:2.2993246876866675\n",
      "train loss:2.3050542101227114\n",
      "train loss:2.3004110813618346\n",
      "train loss:2.3018374589702333\n",
      "train loss:2.289902024025561\n",
      "train loss:2.300432445651291\n",
      "train loss:2.3123447864345046\n",
      "train loss:2.2881530506431638\n",
      "train loss:2.2993472220114652\n",
      "train loss:2.308690625716233\n",
      "train loss:2.2951718645493635\n",
      "train loss:2.299947826086473\n",
      "train loss:2.304022035514441\n",
      "train loss:2.3020971976589557\n",
      "train loss:2.3020321048681853\n",
      "train loss:2.3128065960326776\n",
      "train loss:2.302317298180856\n",
      "train loss:2.303928567128158\n",
      "train loss:2.297842191148768\n",
      "train loss:2.2989327956978527\n",
      "train loss:2.294402085980948\n",
      "train loss:2.2964041084778697\n",
      "train loss:2.305688000131744\n",
      "train loss:2.292977479949515\n",
      "train loss:2.3068693442258628\n",
      "train loss:2.292347803613045\n",
      "train loss:2.301793386481691\n",
      "train loss:2.300699111138889\n",
      "train loss:2.2909631864524345\n",
      "train loss:2.3010469327295104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.309382743822969\n",
      "train loss:2.2940591250192415\n",
      "train loss:2.2963374176052693\n",
      "train loss:2.3026610983845153\n",
      "train loss:2.2978856076673506\n",
      "train loss:2.306992308011809\n",
      "train loss:2.3060031970407397\n",
      "train loss:2.289223539668031\n",
      "train loss:2.297395111486584\n",
      "train loss:2.3014573763703186\n",
      "train loss:2.300708699406167\n",
      "train loss:2.3041077225066364\n",
      "train loss:2.300564580644695\n",
      "train loss:2.301511992621806\n",
      "train loss:2.300707723666774\n",
      "train loss:2.300748302930176\n",
      "train loss:2.3042108081232198\n",
      "train loss:2.3078250653508894\n",
      "train loss:2.3013645980176336\n",
      "train loss:2.3085478366257477\n",
      "train loss:2.301738561890924\n",
      "train loss:2.3018495568027824\n",
      "train loss:2.3033197474755\n",
      "train loss:2.2981334128331508\n",
      "train loss:2.3019417402743283\n",
      "train loss:2.303317223226865\n",
      "train loss:2.3065769900019952\n",
      "train loss:2.2979430204006426\n",
      "=== epoch:78, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2957892560471014\n",
      "train loss:2.30272364878978\n",
      "train loss:2.2957524135822998\n",
      "train loss:2.2987834718792146\n",
      "train loss:2.2967514882034394\n",
      "train loss:2.2981392370447007\n",
      "train loss:2.300503296304008\n",
      "train loss:2.301035845167353\n",
      "train loss:2.2978384988153313\n",
      "train loss:2.2931604073605096\n",
      "train loss:2.2961755671769764\n",
      "train loss:2.299653648565669\n",
      "train loss:2.299878420902577\n",
      "train loss:2.315925320150403\n",
      "train loss:2.3053971194781893\n",
      "train loss:2.300580905273905\n",
      "train loss:2.302925458449311\n",
      "train loss:2.3056891156948573\n",
      "train loss:2.3031879859254514\n",
      "train loss:2.3092650090325555\n",
      "train loss:2.3005384127510995\n",
      "train loss:2.296885768029963\n",
      "train loss:2.3023510839698402\n",
      "train loss:2.3059558088352183\n",
      "train loss:2.3074223335224864\n",
      "train loss:2.3059506019249296\n",
      "train loss:2.301242136885349\n",
      "train loss:2.3050018650170943\n",
      "train loss:2.293163597345964\n",
      "train loss:2.3085487045467357\n",
      "train loss:2.3103299579654606\n",
      "train loss:2.3014575152645658\n",
      "train loss:2.298636545179553\n",
      "train loss:2.3084026828839934\n",
      "train loss:2.30620093798073\n",
      "train loss:2.296876598571992\n",
      "train loss:2.2989971764252437\n",
      "train loss:2.293864710953205\n",
      "train loss:2.3019579005746125\n",
      "train loss:2.299442062838913\n",
      "train loss:2.3041186662548983\n",
      "train loss:2.3004216932030825\n",
      "train loss:2.299626845369407\n",
      "train loss:2.2967307013511826\n",
      "train loss:2.305623173197083\n",
      "train loss:2.2979506285297178\n",
      "train loss:2.3000392953896514\n",
      "train loss:2.3030093263588007\n",
      "train loss:2.3015878598963893\n",
      "train loss:2.3072444717594878\n",
      "train loss:2.291649032919112\n",
      "train loss:2.303358322616788\n",
      "train loss:2.296683855955856\n",
      "train loss:2.296871605581821\n",
      "train loss:2.3033669416298936\n",
      "train loss:2.3071701700444853\n",
      "train loss:2.301723428316108\n",
      "train loss:2.305163808201021\n",
      "train loss:2.296907964967826\n",
      "train loss:2.3072654857908397\n",
      "train loss:2.302724920600878\n",
      "train loss:2.3017120515100045\n",
      "train loss:2.3077750375580357\n",
      "train loss:2.301223368310537\n",
      "train loss:2.292629498758004\n",
      "train loss:2.2997998475410566\n",
      "train loss:2.3029812559994056\n",
      "train loss:2.2992748653494384\n",
      "train loss:2.300503870374682\n",
      "train loss:2.2989086378618877\n",
      "train loss:2.310524382330334\n",
      "train loss:2.2961092910071783\n",
      "train loss:2.3005414090294343\n",
      "train loss:2.2999148755173398\n",
      "train loss:2.3058471226709663\n",
      "train loss:2.3012650039262956\n",
      "train loss:2.3079208716658868\n",
      "train loss:2.306846342236864\n",
      "train loss:2.300138483804098\n",
      "train loss:2.294512338526359\n",
      "train loss:2.3060785476534016\n",
      "train loss:2.299901622533117\n",
      "train loss:2.3022162561669846\n",
      "train loss:2.306766364501391\n",
      "train loss:2.31068603528646\n",
      "train loss:2.30175999267784\n",
      "train loss:2.293842346232427\n",
      "train loss:2.3062709046972905\n",
      "train loss:2.300501992095726\n",
      "train loss:2.302334123465411\n",
      "train loss:2.2951116750017455\n",
      "train loss:2.290592646187809\n",
      "train loss:2.298459737754562\n",
      "train loss:2.29952708112069\n",
      "train loss:2.294993452588297\n",
      "train loss:2.3096891089226625\n",
      "train loss:2.301744018117999\n",
      "train loss:2.307643285060792\n",
      "train loss:2.2977840922621375\n",
      "train loss:2.3024306199820814\n",
      "train loss:2.3079952701403923\n",
      "train loss:2.294175031486084\n",
      "train loss:2.2903914266818264\n",
      "train loss:2.309311378937913\n",
      "train loss:2.3045655850990685\n",
      "train loss:2.302701272480018\n",
      "train loss:2.3012087906902634\n",
      "train loss:2.305742342579968\n",
      "train loss:2.299971790114037\n",
      "train loss:2.300682474010509\n",
      "train loss:2.3009747559401657\n",
      "train loss:2.3005525758837644\n",
      "train loss:2.303644896854453\n",
      "train loss:2.296981075070638\n",
      "train loss:2.291916266396028\n",
      "train loss:2.2945191318062452\n",
      "train loss:2.304394018965645\n",
      "train loss:2.297659537161694\n",
      "train loss:2.3036690661200288\n",
      "train loss:2.301909191912942\n",
      "train loss:2.3066510102364695\n",
      "train loss:2.2980021000276896\n",
      "train loss:2.2991011612718775\n",
      "train loss:2.2939156789216044\n",
      "train loss:2.3089126193974567\n",
      "train loss:2.3016675663780317\n",
      "train loss:2.2967664787930366\n",
      "train loss:2.299989229731628\n",
      "train loss:2.288014376398487\n",
      "train loss:2.294706535290412\n",
      "train loss:2.2992626377173027\n",
      "train loss:2.2973963610396724\n",
      "train loss:2.2866394992644263\n",
      "train loss:2.3072335757366904\n",
      "train loss:2.3008335509227242\n",
      "train loss:2.3043252719175693\n",
      "train loss:2.294972325594707\n",
      "train loss:2.29227262962673\n",
      "train loss:2.295058453393664\n",
      "train loss:2.293777686004892\n",
      "train loss:2.3031374648823895\n",
      "train loss:2.296004769385293\n",
      "train loss:2.289021797487693\n",
      "train loss:2.296773673858546\n",
      "train loss:2.3037537528680208\n",
      "train loss:2.2987804437648243\n",
      "train loss:2.30018563237717\n",
      "train loss:2.2951784438986156\n",
      "train loss:2.296079449373272\n",
      "train loss:2.300597027597864\n",
      "train loss:2.2979913704681287\n",
      "train loss:2.2994511017549453\n",
      "train loss:2.299542520500083\n",
      "train loss:2.2924840916134133\n",
      "train loss:2.3003234616724093\n",
      "train loss:2.298568719373943\n",
      "train loss:2.3007023227132235\n",
      "train loss:2.2974156165884776\n",
      "train loss:2.2970469827257136\n",
      "train loss:2.309417122606175\n",
      "train loss:2.295743248627348\n",
      "train loss:2.2995212347075507\n",
      "train loss:2.3030804785031864\n",
      "train loss:2.296306861592445\n",
      "train loss:2.306961343391399\n",
      "train loss:2.2833351607861303\n",
      "train loss:2.2944989765831556\n",
      "train loss:2.301616710171309\n",
      "train loss:2.3009701042407666\n",
      "train loss:2.3002062780157955\n",
      "train loss:2.305038324079379\n",
      "train loss:2.294118494625589\n",
      "train loss:2.3005165769169578\n",
      "train loss:2.296121365341011\n",
      "train loss:2.2958091624430623\n",
      "train loss:2.3032156804693606\n",
      "train loss:2.2964661923927787\n",
      "train loss:2.2892045555821627\n",
      "train loss:2.293634752720484\n",
      "train loss:2.298427031168869\n",
      "train loss:2.302820677565544\n",
      "train loss:2.3014954325531525\n",
      "train loss:2.302597076899667\n",
      "train loss:2.299492122984389\n",
      "train loss:2.3151061650085505\n",
      "train loss:2.295925390577722\n",
      "train loss:2.3044207349148818\n",
      "train loss:2.3046105761551927\n",
      "train loss:2.308354756667938\n",
      "train loss:2.3090563984424906\n",
      "train loss:2.3038233207894505\n",
      "train loss:2.30672986250339\n",
      "train loss:2.291932174405291\n",
      "train loss:2.295825139079442\n",
      "train loss:2.29957114949106\n",
      "train loss:2.3015058213389183\n",
      "train loss:2.3023090099158403\n",
      "train loss:2.3139795567321\n",
      "train loss:2.3084520073765846\n",
      "train loss:2.297503542487581\n",
      "=== epoch:79, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3066916998127285\n",
      "train loss:2.3005071734082128\n",
      "train loss:2.293698162200128\n",
      "train loss:2.302196112645854\n",
      "train loss:2.3077007134097536\n",
      "train loss:2.299494662842564\n",
      "train loss:2.302634421200652\n",
      "train loss:2.2933213779073167\n",
      "train loss:2.303735736520809\n",
      "train loss:2.2958641533705655\n",
      "train loss:2.29453480067358\n",
      "train loss:2.2888153461470475\n",
      "train loss:2.3133474677558326\n",
      "train loss:2.307841847922453\n",
      "train loss:2.3007470154064316\n",
      "train loss:2.311677716491814\n",
      "train loss:2.302110381922243\n",
      "train loss:2.2927604297731805\n",
      "train loss:2.2953681674069495\n",
      "train loss:2.2992653542692336\n",
      "train loss:2.296530928389481\n",
      "train loss:2.29805958918789\n",
      "train loss:2.3005362562176663\n",
      "train loss:2.296049124551061\n",
      "train loss:2.2981611390296184\n",
      "train loss:2.3059684656571706\n",
      "train loss:2.303548404581007\n",
      "train loss:2.300222715068102\n",
      "train loss:2.3022700642599974\n",
      "train loss:2.30184100928945\n",
      "train loss:2.2968797206042906\n",
      "train loss:2.3003970795517565\n",
      "train loss:2.296637008021115\n",
      "train loss:2.3090845442306867\n",
      "train loss:2.307102213399757\n",
      "train loss:2.297139050661453\n",
      "train loss:2.310738135551834\n",
      "train loss:2.2994070038801504\n",
      "train loss:2.3007487106425404\n",
      "train loss:2.295645849587814\n",
      "train loss:2.3019757956211757\n",
      "train loss:2.298363646432006\n",
      "train loss:2.299268977737099\n",
      "train loss:2.299397852989819\n",
      "train loss:2.307947406465567\n",
      "train loss:2.297120062713807\n",
      "train loss:2.29281086347098\n",
      "train loss:2.301918614961733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2993197967205288\n",
      "train loss:2.30369947851502\n",
      "train loss:2.303339132070918\n",
      "train loss:2.305717321365198\n",
      "train loss:2.301224737355788\n",
      "train loss:2.302098001044095\n",
      "train loss:2.299372746576302\n",
      "train loss:2.2987659138880785\n",
      "train loss:2.2949636511491693\n",
      "train loss:2.3024775318338713\n",
      "train loss:2.295752521572118\n",
      "train loss:2.296942301335942\n",
      "train loss:2.300983709127329\n",
      "train loss:2.3019385947591786\n",
      "train loss:2.2925850306895956\n",
      "train loss:2.2978491812405313\n",
      "train loss:2.304105694263658\n",
      "train loss:2.301953418718824\n",
      "train loss:2.2987574314722266\n",
      "train loss:2.3027381077762836\n",
      "train loss:2.301305146425989\n",
      "train loss:2.297124638443944\n",
      "train loss:2.301616133226454\n",
      "train loss:2.2988379443557263\n",
      "train loss:2.301147977456588\n",
      "train loss:2.301041646213648\n",
      "train loss:2.3062313011519335\n",
      "train loss:2.301467273527332\n",
      "train loss:2.298199542225943\n",
      "train loss:2.3019196150553234\n",
      "train loss:2.306499840700128\n",
      "train loss:2.294444353139428\n",
      "train loss:2.3009361282455356\n",
      "train loss:2.2961068407374117\n",
      "train loss:2.3000967054268244\n",
      "train loss:2.2986589049102806\n",
      "train loss:2.3027388424813022\n",
      "train loss:2.304566449525904\n",
      "train loss:2.2952494590299737\n",
      "train loss:2.3069061539624536\n",
      "train loss:2.298460704540299\n",
      "train loss:2.295801388877106\n",
      "train loss:2.3117844358476205\n",
      "train loss:2.3001759840167733\n",
      "train loss:2.3057735582754164\n",
      "train loss:2.3037521166048642\n",
      "train loss:2.301833174992143\n",
      "train loss:2.3093345691567633\n",
      "train loss:2.2995484076718444\n",
      "train loss:2.2884663208724776\n",
      "train loss:2.3010675973945562\n",
      "train loss:2.3010176427304976\n",
      "train loss:2.3022925859803918\n",
      "train loss:2.3025253381085773\n",
      "train loss:2.297580394598873\n",
      "train loss:2.292054716567178\n",
      "train loss:2.299969331712539\n",
      "train loss:2.3031710616466663\n",
      "train loss:2.2965862679206333\n",
      "train loss:2.2988609100111006\n",
      "train loss:2.2940381639463965\n",
      "train loss:2.296179470944885\n",
      "train loss:2.3020126308689997\n",
      "train loss:2.298289117617684\n",
      "train loss:2.3020273656144297\n",
      "train loss:2.3012985730433475\n",
      "train loss:2.295062614695266\n",
      "train loss:2.30513634560229\n",
      "train loss:2.3075905614303673\n",
      "train loss:2.307433419732199\n",
      "train loss:2.3090831573530792\n",
      "train loss:2.301821825415818\n",
      "train loss:2.302850763623433\n",
      "train loss:2.2991302351461016\n",
      "train loss:2.295948237085241\n",
      "train loss:2.296824254745194\n",
      "train loss:2.2999116981040006\n",
      "train loss:2.3035834552582743\n",
      "train loss:2.301577182365378\n",
      "train loss:2.2905256189924654\n",
      "train loss:2.2874649243676277\n",
      "train loss:2.308699804573824\n",
      "train loss:2.2998601827785903\n",
      "train loss:2.3045580028096593\n",
      "train loss:2.3014276612681606\n",
      "train loss:2.2958449069248417\n",
      "train loss:2.3033088792023553\n",
      "train loss:2.2994063440398116\n",
      "train loss:2.2998698447647867\n",
      "train loss:2.3065812922548274\n",
      "train loss:2.294688177829519\n",
      "train loss:2.2966410322492408\n",
      "train loss:2.303592093828844\n",
      "train loss:2.3031819786715007\n",
      "train loss:2.3040145138157846\n",
      "train loss:2.305651485033304\n",
      "train loss:2.2996197747722804\n",
      "train loss:2.293218015779031\n",
      "train loss:2.2993113959318223\n",
      "train loss:2.3122430425208336\n",
      "train loss:2.304876110649079\n",
      "train loss:2.294260152854719\n",
      "train loss:2.2987012619765137\n",
      "train loss:2.309279021651031\n",
      "train loss:2.3083923278287144\n",
      "train loss:2.2969143080904693\n",
      "train loss:2.3119969067567694\n",
      "train loss:2.292756341003215\n",
      "train loss:2.298980458291442\n",
      "train loss:2.301368175671828\n",
      "train loss:2.3079724288908863\n",
      "train loss:2.308395266536786\n",
      "train loss:2.305296666408026\n",
      "train loss:2.305044645744571\n",
      "train loss:2.2987413519737614\n",
      "train loss:2.2966952682441533\n",
      "train loss:2.301179240013963\n",
      "train loss:2.3009587716705795\n",
      "train loss:2.3077328052907866\n",
      "train loss:2.3045351679392905\n",
      "train loss:2.303821546215082\n",
      "train loss:2.3046562147485052\n",
      "train loss:2.30664551057594\n",
      "train loss:2.307906405890541\n",
      "train loss:2.303508075610574\n",
      "train loss:2.308795620716197\n",
      "train loss:2.2906720103252063\n",
      "train loss:2.2991595340312747\n",
      "train loss:2.302198158647076\n",
      "train loss:2.3050014403547774\n",
      "train loss:2.3053155105944523\n",
      "train loss:2.308832685493477\n",
      "train loss:2.296459726134535\n",
      "train loss:2.301937846328899\n",
      "train loss:2.3036367934653\n",
      "train loss:2.291214872245433\n",
      "train loss:2.2997351291766797\n",
      "train loss:2.299398638866309\n",
      "train loss:2.2929988746458014\n",
      "train loss:2.299778435244127\n",
      "train loss:2.2978666368372664\n",
      "train loss:2.3044006462684417\n",
      "train loss:2.312933469622112\n",
      "train loss:2.299418026233881\n",
      "train loss:2.3024876292277265\n",
      "train loss:2.303792946304516\n",
      "train loss:2.29758003233226\n",
      "train loss:2.2975102507834664\n",
      "train loss:2.3010491537647684\n",
      "train loss:2.302735904636475\n",
      "train loss:2.3042130437578536\n",
      "train loss:2.289591034081775\n",
      "=== epoch:80, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.302030056745159\n",
      "train loss:2.303068324962366\n",
      "train loss:2.2955368030433454\n",
      "train loss:2.3087971473052553\n",
      "train loss:2.286784876965298\n",
      "train loss:2.2925557830609677\n",
      "train loss:2.3023636331806867\n",
      "train loss:2.2969739923326533\n",
      "train loss:2.2986242609670104\n",
      "train loss:2.2953654090830327\n",
      "train loss:2.305768826967729\n",
      "train loss:2.299877837520859\n",
      "train loss:2.3023904434306712\n",
      "train loss:2.306536107885274\n",
      "train loss:2.296877348610404\n",
      "train loss:2.2953959978416454\n",
      "train loss:2.3004229336823765\n",
      "train loss:2.3092639632728567\n",
      "train loss:2.302660132552958\n",
      "train loss:2.2988939925580283\n",
      "train loss:2.2979433589275535\n",
      "train loss:2.3040142298980735\n",
      "train loss:2.302696824816541\n",
      "train loss:2.298092923808388\n",
      "train loss:2.2988943302141442\n",
      "train loss:2.2916410371758746\n",
      "train loss:2.306222809287128\n",
      "train loss:2.303103107074626\n",
      "train loss:2.3043034379619347\n",
      "train loss:2.2994452798924003\n",
      "train loss:2.3043276034036726\n",
      "train loss:2.311393100201739\n",
      "train loss:2.305970342189339\n",
      "train loss:2.303427409317573\n",
      "train loss:2.299415020794229\n",
      "train loss:2.289080133168833\n",
      "train loss:2.3054468892405366\n",
      "train loss:2.3072520212103598\n",
      "train loss:2.304115955217199\n",
      "train loss:2.2998399102243843\n",
      "train loss:2.309711914706144\n",
      "train loss:2.300104943309175\n",
      "train loss:2.2999250811389693\n",
      "train loss:2.3071647534409494\n",
      "train loss:2.2953796719599358\n",
      "train loss:2.2918763415573435\n",
      "train loss:2.3012631024021832\n",
      "train loss:2.301821054407738\n",
      "train loss:2.3008480254076136\n",
      "train loss:2.2912845436487292\n",
      "train loss:2.2947792860605936\n",
      "train loss:2.2947595044759335\n",
      "train loss:2.2964726684321133\n",
      "train loss:2.2975181501899002\n",
      "train loss:2.308038679348908\n",
      "train loss:2.307707687458458\n",
      "train loss:2.2948869174666418\n",
      "train loss:2.296708401357159\n",
      "train loss:2.290558064023076\n",
      "train loss:2.3058044208533057\n",
      "train loss:2.296006873480198\n",
      "train loss:2.3050537136874545\n",
      "train loss:2.2976203407196905\n",
      "train loss:2.3061744147369136\n",
      "train loss:2.3068168412056145\n",
      "train loss:2.3027515029566867\n",
      "train loss:2.308167671950261\n",
      "train loss:2.312674707499925\n",
      "train loss:2.297990231839268\n",
      "train loss:2.296045622582053\n",
      "train loss:2.304885730867667\n",
      "train loss:2.2942936490441763\n",
      "train loss:2.299978510584906\n",
      "train loss:2.302679804188693\n",
      "train loss:2.2973500570840004\n",
      "train loss:2.3006027000294824\n",
      "train loss:2.3072473759763126\n",
      "train loss:2.3051333707296355\n",
      "train loss:2.302970018238085\n",
      "train loss:2.297836470538317\n",
      "train loss:2.2998848962289857\n",
      "train loss:2.302921327791871\n",
      "train loss:2.3048993282259116\n",
      "train loss:2.30126124999303\n",
      "train loss:2.3130606595793535\n",
      "train loss:2.299854368428085\n",
      "train loss:2.2894425190397683\n",
      "train loss:2.300874942412133\n",
      "train loss:2.305695589349769\n",
      "train loss:2.3039120038869165\n",
      "train loss:2.299485892573653\n",
      "train loss:2.3043399960235087\n",
      "train loss:2.2969139145129804\n",
      "train loss:2.293498792327337\n",
      "train loss:2.3038418591652503\n",
      "train loss:2.299503363828337\n",
      "train loss:2.3141674729165356\n",
      "train loss:2.3014409424074875\n",
      "train loss:2.2936553775489936\n",
      "train loss:2.3043092053463883\n",
      "train loss:2.3013774827752966\n",
      "train loss:2.3102551360900834\n",
      "train loss:2.296345920946635\n",
      "train loss:2.3066271962547455\n",
      "train loss:2.2985533140640566\n",
      "train loss:2.3062897963661335\n",
      "train loss:2.2989114146221694\n",
      "train loss:2.2948029634011964\n",
      "train loss:2.309525647613964\n",
      "train loss:2.30743619309039\n",
      "train loss:2.290299298660988\n",
      "train loss:2.3065454507488723\n",
      "train loss:2.306267219140861\n",
      "train loss:2.29636130291267\n",
      "train loss:2.306657967946413\n",
      "train loss:2.2989369251606506\n",
      "train loss:2.29610155706977\n",
      "train loss:2.30857855644671\n",
      "train loss:2.300291187692492\n",
      "train loss:2.305378063428091\n",
      "train loss:2.299976434767281\n",
      "train loss:2.297955654650945\n",
      "train loss:2.3084256143752198\n",
      "train loss:2.3054750496013257\n",
      "train loss:2.296891562562225\n",
      "train loss:2.2966143718029923\n",
      "train loss:2.2953877367953983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.31114191897347\n",
      "train loss:2.3055444887091543\n",
      "train loss:2.301797591794005\n",
      "train loss:2.3042228452779008\n",
      "train loss:2.2953845319458117\n",
      "train loss:2.2986473828589498\n",
      "train loss:2.3002630194771645\n",
      "train loss:2.294851022335829\n",
      "train loss:2.3041658769906994\n",
      "train loss:2.294062282035899\n",
      "train loss:2.3041131417997467\n",
      "train loss:2.295442185153417\n",
      "train loss:2.2964773240945875\n",
      "train loss:2.297277558947173\n",
      "train loss:2.304098175723246\n",
      "train loss:2.300116777727248\n",
      "train loss:2.306276235606479\n",
      "train loss:2.3053371420754476\n",
      "train loss:2.3129500648196646\n",
      "train loss:2.2984946273096813\n",
      "train loss:2.295945908362709\n",
      "train loss:2.2986121802511432\n",
      "train loss:2.306097318264342\n",
      "train loss:2.3034648979295405\n",
      "train loss:2.2918677802756107\n",
      "train loss:2.303668631015107\n",
      "train loss:2.30980960537784\n",
      "train loss:2.2986340226669846\n",
      "train loss:2.2928052008023307\n",
      "train loss:2.29788333644311\n",
      "train loss:2.3035655279653606\n",
      "train loss:2.2996942614688005\n",
      "train loss:2.3032422269260966\n",
      "train loss:2.3032359434270755\n",
      "train loss:2.2953519592809597\n",
      "train loss:2.2907309774369597\n",
      "train loss:2.2991714160407306\n",
      "train loss:2.3026361046792014\n",
      "train loss:2.3013326074904503\n",
      "train loss:2.3118633506506114\n",
      "train loss:2.30346172081112\n",
      "train loss:2.304321277333096\n",
      "train loss:2.3117116591370386\n",
      "train loss:2.3064274008900534\n",
      "train loss:2.288670736345645\n",
      "train loss:2.3023777409604382\n",
      "train loss:2.3071666717204877\n",
      "train loss:2.3058956966082045\n",
      "train loss:2.292708723135272\n",
      "train loss:2.304205128559906\n",
      "train loss:2.301649123225345\n",
      "train loss:2.296530052442895\n",
      "train loss:2.3006639018613075\n",
      "train loss:2.287901323169322\n",
      "train loss:2.302965968976406\n",
      "train loss:2.3043181288499794\n",
      "train loss:2.299455138481826\n",
      "train loss:2.310872672101506\n",
      "train loss:2.2899974776289596\n",
      "train loss:2.3046281657009544\n",
      "train loss:2.297927796631346\n",
      "train loss:2.294530518638129\n",
      "train loss:2.30385838103583\n",
      "train loss:2.298595548125204\n",
      "train loss:2.2970540175874703\n",
      "train loss:2.3067479152237857\n",
      "train loss:2.300550021543588\n",
      "train loss:2.2954393383784417\n",
      "train loss:2.2955173293050044\n",
      "train loss:2.29703267949003\n",
      "train loss:2.3060835984676364\n",
      "train loss:2.310865450553015\n",
      "train loss:2.3022709607588103\n",
      "=== epoch:81, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.308057832958469\n",
      "train loss:2.29052020087585\n",
      "train loss:2.306022285422784\n",
      "train loss:2.307146183137959\n",
      "train loss:2.3052060268854313\n",
      "train loss:2.310743368158053\n",
      "train loss:2.29836065657931\n",
      "train loss:2.2988764172297373\n",
      "train loss:2.307344222477682\n",
      "train loss:2.2928900581174085\n",
      "train loss:2.303153687488996\n",
      "train loss:2.305483323331753\n",
      "train loss:2.2916485188228166\n",
      "train loss:2.309532805710265\n",
      "train loss:2.300414989257531\n",
      "train loss:2.2996377749003183\n",
      "train loss:2.3006861198627697\n",
      "train loss:2.2967633501524642\n",
      "train loss:2.3010791134821766\n",
      "train loss:2.295798716611339\n",
      "train loss:2.3044370795397806\n",
      "train loss:2.300935907709377\n",
      "train loss:2.2911887856442816\n",
      "train loss:2.2983814333351034\n",
      "train loss:2.3018108913240094\n",
      "train loss:2.293860136965631\n",
      "train loss:2.297882212652273\n",
      "train loss:2.311743676901896\n",
      "train loss:2.302028944921911\n",
      "train loss:2.300944957412436\n",
      "train loss:2.296843937169565\n",
      "train loss:2.3078544865516837\n",
      "train loss:2.3116164702859936\n",
      "train loss:2.2972166023602916\n",
      "train loss:2.2995304310491367\n",
      "train loss:2.3039310298684117\n",
      "train loss:2.301100906236109\n",
      "train loss:2.3034510541504023\n",
      "train loss:2.2947328061494336\n",
      "train loss:2.2936569045580546\n",
      "train loss:2.3002510264622478\n",
      "train loss:2.2971854394533047\n",
      "train loss:2.299218982546674\n",
      "train loss:2.304504711388245\n",
      "train loss:2.304728624712081\n",
      "train loss:2.305540770645892\n",
      "train loss:2.3033827155890623\n",
      "train loss:2.300981692429232\n",
      "train loss:2.2997412719313357\n",
      "train loss:2.295208601250115\n",
      "train loss:2.28848153888971\n",
      "train loss:2.3015672421847277\n",
      "train loss:2.3059215153346946\n",
      "train loss:2.3054735184660884\n",
      "train loss:2.2942795935775204\n",
      "train loss:2.293987362083588\n",
      "train loss:2.311566216755121\n",
      "train loss:2.299305428461948\n",
      "train loss:2.3060972046958774\n",
      "train loss:2.3027184658853437\n",
      "train loss:2.3058091860452974\n",
      "train loss:2.306491656326089\n",
      "train loss:2.2997220345215608\n",
      "train loss:2.2957948449936985\n",
      "train loss:2.2978624453680054\n",
      "train loss:2.2916725564039444\n",
      "train loss:2.302619596633958\n",
      "train loss:2.300912301612674\n",
      "train loss:2.298884161650691\n",
      "train loss:2.305515239079991\n",
      "train loss:2.2970629357596364\n",
      "train loss:2.289804086698038\n",
      "train loss:2.304775370805352\n",
      "train loss:2.297912417088915\n",
      "train loss:2.305230662184936\n",
      "train loss:2.3082388013894892\n",
      "train loss:2.295413333649432\n",
      "train loss:2.296910967615309\n",
      "train loss:2.3056639928829914\n",
      "train loss:2.30809330167144\n",
      "train loss:2.3023276262523567\n",
      "train loss:2.2958135147934002\n",
      "train loss:2.308378064722178\n",
      "train loss:2.301545490887839\n",
      "train loss:2.3074199658053147\n",
      "train loss:2.3039315963740616\n",
      "train loss:2.2955020357882767\n",
      "train loss:2.301157239583895\n",
      "train loss:2.301793790446504\n",
      "train loss:2.2976390229349923\n",
      "train loss:2.306675180099337\n",
      "train loss:2.301847155289052\n",
      "train loss:2.303224679796955\n",
      "train loss:2.3009748107769337\n",
      "train loss:2.302304323342063\n",
      "train loss:2.2972229036371825\n",
      "train loss:2.2939714809802236\n",
      "train loss:2.2948559707351253\n",
      "train loss:2.2978377913214842\n",
      "train loss:2.28852607466632\n",
      "train loss:2.3020395476552435\n",
      "train loss:2.2988130292320363\n",
      "train loss:2.3021312229400372\n",
      "train loss:2.2971472895943426\n",
      "train loss:2.307866529353362\n",
      "train loss:2.2985699613540462\n",
      "train loss:2.306030171225586\n",
      "train loss:2.299868936376837\n",
      "train loss:2.3077675442558254\n",
      "train loss:2.2960553518878917\n",
      "train loss:2.3040439632719907\n",
      "train loss:2.301591294656264\n",
      "train loss:2.297719147114968\n",
      "train loss:2.288116077479644\n",
      "train loss:2.306332414227824\n",
      "train loss:2.286942730770356\n",
      "train loss:2.314151339901099\n",
      "train loss:2.310737692279357\n",
      "train loss:2.2915931301298937\n",
      "train loss:2.2987475677873315\n",
      "train loss:2.2998879392669864\n",
      "train loss:2.306249142738122\n",
      "train loss:2.300321068563492\n",
      "train loss:2.303959190755294\n",
      "train loss:2.290767600996741\n",
      "train loss:2.299536886326604\n",
      "train loss:2.301030757166694\n",
      "train loss:2.3060559535434044\n",
      "train loss:2.309629561226103\n",
      "train loss:2.2994600794326523\n",
      "train loss:2.3048261278704785\n",
      "train loss:2.3031317971132217\n",
      "train loss:2.309246047100516\n",
      "train loss:2.305731736070109\n",
      "train loss:2.2998983648776226\n",
      "train loss:2.2986267348974296\n",
      "train loss:2.2907672468339944\n",
      "train loss:2.304737347317286\n",
      "train loss:2.302034697462321\n",
      "train loss:2.3052372438184254\n",
      "train loss:2.2944572497487514\n",
      "train loss:2.29765535311483\n",
      "train loss:2.306434755103812\n",
      "train loss:2.294871096328082\n",
      "train loss:2.3065551750734015\n",
      "train loss:2.2935386362420145\n",
      "train loss:2.304014991738308\n",
      "train loss:2.2958127814283977\n",
      "train loss:2.2958099185217127\n",
      "train loss:2.296988448643919\n",
      "train loss:2.295720324439838\n",
      "train loss:2.2999482469890027\n",
      "train loss:2.293736914067955\n",
      "train loss:2.3070054618939615\n",
      "train loss:2.3075711662481138\n",
      "train loss:2.297508089093173\n",
      "train loss:2.302489605819719\n",
      "train loss:2.302275112831458\n",
      "train loss:2.2986500566018004\n",
      "train loss:2.3098504810723743\n",
      "train loss:2.29908082439993\n",
      "train loss:2.2934947743316614\n",
      "train loss:2.3047924371049695\n",
      "train loss:2.300818452060972\n",
      "train loss:2.310323708596528\n",
      "train loss:2.311153984481268\n",
      "train loss:2.300601467386945\n",
      "train loss:2.2988150679465784\n",
      "train loss:2.3006212702371567\n",
      "train loss:2.2958229014969245\n",
      "train loss:2.2977560842247953\n",
      "train loss:2.2991105099714706\n",
      "train loss:2.2984372553187913\n",
      "train loss:2.3053909571229765\n",
      "train loss:2.304823560179334\n",
      "train loss:2.2984007470287633\n",
      "train loss:2.3036630888342073\n",
      "train loss:2.3015990368246166\n",
      "train loss:2.29533374129069\n",
      "train loss:2.299832516959976\n",
      "train loss:2.298047295708595\n",
      "train loss:2.292866428981095\n",
      "train loss:2.3013556967158397\n",
      "train loss:2.3033084927271052\n",
      "train loss:2.2979028818574077\n",
      "train loss:2.300400512902436\n",
      "train loss:2.311092488009365\n",
      "train loss:2.309961833448339\n",
      "train loss:2.290791679120082\n",
      "train loss:2.300031522243218\n",
      "train loss:2.3000857355213187\n",
      "train loss:2.312511112534396\n",
      "train loss:2.3069750516365817\n",
      "train loss:2.295843715183334\n",
      "train loss:2.2991860776661666\n",
      "train loss:2.289086743076237\n",
      "train loss:2.303585869510819\n",
      "train loss:2.303733407391694\n",
      "train loss:2.301996092497945\n",
      "train loss:2.30000988837528\n",
      "=== epoch:82, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.305034680391714\n",
      "train loss:2.305666719598811\n",
      "train loss:2.3052496239820663\n",
      "train loss:2.3011755592818615\n",
      "train loss:2.2995041089592902\n",
      "train loss:2.3095863501995963\n",
      "train loss:2.3054230153177526\n",
      "train loss:2.3024651382862498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3006868086009984\n",
      "train loss:2.30497998829787\n",
      "train loss:2.2993696827184924\n",
      "train loss:2.296965503620319\n",
      "train loss:2.306994979312917\n",
      "train loss:2.2938407533015437\n",
      "train loss:2.291366217472313\n",
      "train loss:2.311203828881898\n",
      "train loss:2.300875216733342\n",
      "train loss:2.3077462609724657\n",
      "train loss:2.3062385371514655\n",
      "train loss:2.2930202896168437\n",
      "train loss:2.3072667254830477\n",
      "train loss:2.3028365194523484\n",
      "train loss:2.295850787732938\n",
      "train loss:2.304781527974229\n",
      "train loss:2.2945426221755896\n",
      "train loss:2.3013126746634027\n",
      "train loss:2.3016708421574843\n",
      "train loss:2.3040544199100257\n",
      "train loss:2.313803009663804\n",
      "train loss:2.29689775979819\n",
      "train loss:2.3050282846393015\n",
      "train loss:2.300886236370622\n",
      "train loss:2.3015132360409356\n",
      "train loss:2.2966410264392265\n",
      "train loss:2.3043551100602624\n",
      "train loss:2.297218310623796\n",
      "train loss:2.294372023190144\n",
      "train loss:2.3016281221687303\n",
      "train loss:2.3040969146503634\n",
      "train loss:2.2989950860808817\n",
      "train loss:2.2900315815838663\n",
      "train loss:2.3009079743651983\n",
      "train loss:2.300632190812934\n",
      "train loss:2.3034181667539086\n",
      "train loss:2.314305575785969\n",
      "train loss:2.3036431180675567\n",
      "train loss:2.301452973435864\n",
      "train loss:2.300609251568091\n",
      "train loss:2.29906458412891\n",
      "train loss:2.3076748720809266\n",
      "train loss:2.298486622027681\n",
      "train loss:2.293142040275458\n",
      "train loss:2.3024757895392582\n",
      "train loss:2.302036196121457\n",
      "train loss:2.306417948993366\n",
      "train loss:2.299458249997186\n",
      "train loss:2.29687290420257\n",
      "train loss:2.309442983851572\n",
      "train loss:2.30472826197414\n",
      "train loss:2.3153851713735425\n",
      "train loss:2.3037128590133142\n",
      "train loss:2.3054635793867635\n",
      "train loss:2.295779357787563\n",
      "train loss:2.298728436720948\n",
      "train loss:2.300052242941353\n",
      "train loss:2.303692845947347\n",
      "train loss:2.3133329756914116\n",
      "train loss:2.298952284411877\n",
      "train loss:2.3016950509279845\n",
      "train loss:2.302980596384135\n",
      "train loss:2.3061488892855095\n",
      "train loss:2.30778187360911\n",
      "train loss:2.3017754358776683\n",
      "train loss:2.2966074927574165\n",
      "train loss:2.3019294012158817\n",
      "train loss:2.3052951960821595\n",
      "train loss:2.2982542826935655\n",
      "train loss:2.3078413030116143\n",
      "train loss:2.2997253313071924\n",
      "train loss:2.301366283358186\n",
      "train loss:2.304070318124924\n",
      "train loss:2.2992096122494563\n",
      "train loss:2.30222568313008\n",
      "train loss:2.296896153052623\n",
      "train loss:2.3023165404239787\n",
      "train loss:2.296651237145959\n",
      "train loss:2.292988110799428\n",
      "train loss:2.298581262915157\n",
      "train loss:2.307505040953606\n",
      "train loss:2.304767902883733\n",
      "train loss:2.2963920455076092\n",
      "train loss:2.2989239715633927\n",
      "train loss:2.3060310009387983\n",
      "train loss:2.29622443848875\n",
      "train loss:2.2971024218967306\n",
      "train loss:2.3010825677130744\n",
      "train loss:2.303399369239581\n",
      "train loss:2.307645840240288\n",
      "train loss:2.31108066997176\n",
      "train loss:2.2968598772360798\n",
      "train loss:2.3028876657358794\n",
      "train loss:2.2956511215402777\n",
      "train loss:2.288467399381461\n",
      "train loss:2.3079273163171337\n",
      "train loss:2.310840954728212\n",
      "train loss:2.30655470870588\n",
      "train loss:2.306559979957254\n",
      "train loss:2.301591684307623\n",
      "train loss:2.296337690557304\n",
      "train loss:2.3070367358519133\n",
      "train loss:2.2949573416848477\n",
      "train loss:2.307968067319607\n",
      "train loss:2.2996849312617065\n",
      "train loss:2.3005098387675957\n",
      "train loss:2.299040701383249\n",
      "train loss:2.316289907639034\n",
      "train loss:2.3051479024968513\n",
      "train loss:2.3036156783316093\n",
      "train loss:2.3071401961304474\n",
      "train loss:2.3002475328299137\n",
      "train loss:2.291500757335844\n",
      "train loss:2.303487977623973\n",
      "train loss:2.3046006240064227\n",
      "train loss:2.3047920448155947\n",
      "train loss:2.298165541100519\n",
      "train loss:2.310927357777575\n",
      "train loss:2.30374445170723\n",
      "train loss:2.30091648379636\n",
      "train loss:2.303721730013053\n",
      "train loss:2.3040411916729395\n",
      "train loss:2.306203041207448\n",
      "train loss:2.2982481241149824\n",
      "train loss:2.3078961775136184\n",
      "train loss:2.3002728936846055\n",
      "train loss:2.310972485062556\n",
      "train loss:2.3022896096466554\n",
      "train loss:2.3090723415960626\n",
      "train loss:2.303256668797829\n",
      "train loss:2.3081347938859076\n",
      "train loss:2.300154333879786\n",
      "train loss:2.302257800813072\n",
      "train loss:2.300138510396414\n",
      "train loss:2.3076183606936467\n",
      "train loss:2.298227218451671\n",
      "train loss:2.2980080985992775\n",
      "train loss:2.3085649089004896\n",
      "train loss:2.309352374376392\n",
      "train loss:2.3042388107283793\n",
      "train loss:2.2996654789490196\n",
      "train loss:2.29847336938402\n",
      "train loss:2.303398915730978\n",
      "train loss:2.300118487092439\n",
      "train loss:2.296902256615256\n",
      "train loss:2.3108629878834717\n",
      "train loss:2.289105830294268\n",
      "train loss:2.307922142852514\n",
      "train loss:2.3020318692270685\n",
      "train loss:2.2880539242940054\n",
      "train loss:2.303425032425269\n",
      "train loss:2.29649738404193\n",
      "train loss:2.3020416195392364\n",
      "train loss:2.3030074354915873\n",
      "train loss:2.3005701444759015\n",
      "train loss:2.3033547061107322\n",
      "train loss:2.3019921254146327\n",
      "train loss:2.307390339179218\n",
      "train loss:2.30977013156943\n",
      "train loss:2.309184174678437\n",
      "train loss:2.3011300778052983\n",
      "train loss:2.299587935999184\n",
      "train loss:2.2966822461222893\n",
      "train loss:2.3003691212538655\n",
      "train loss:2.2980163609303577\n",
      "train loss:2.30256613681171\n",
      "train loss:2.3048716112729974\n",
      "train loss:2.307343847847192\n",
      "train loss:2.293355186417335\n",
      "train loss:2.300295611609772\n",
      "train loss:2.3049063897047213\n",
      "train loss:2.297922069936201\n",
      "train loss:2.3021315993600235\n",
      "train loss:2.305703392316479\n",
      "train loss:2.2992850123509063\n",
      "train loss:2.2973399323969645\n",
      "train loss:2.296377133091979\n",
      "train loss:2.299049164300716\n",
      "train loss:2.3015853723116106\n",
      "train loss:2.2998070210442147\n",
      "train loss:2.3039630777048914\n",
      "train loss:2.2932066431327685\n",
      "train loss:2.310305926763195\n",
      "train loss:2.3026868999964143\n",
      "train loss:2.302058174407644\n",
      "train loss:2.30651975511156\n",
      "train loss:2.300211248067853\n",
      "train loss:2.2981377988373137\n",
      "train loss:2.2978357900447977\n",
      "train loss:2.296855226398246\n",
      "train loss:2.309261801939362\n",
      "train loss:2.3016753703719526\n",
      "=== epoch:83, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3113017510367158\n",
      "train loss:2.302340726888644\n",
      "train loss:2.3014473550394463\n",
      "train loss:2.2991219425064067\n",
      "train loss:2.301285112829042\n",
      "train loss:2.304227723191208\n",
      "train loss:2.2998293384525703\n",
      "train loss:2.2903589665124273\n",
      "train loss:2.304057979695849\n",
      "train loss:2.300015751236412\n",
      "train loss:2.2989538111729972\n",
      "train loss:2.316771466321067\n",
      "train loss:2.299540042966244\n",
      "train loss:2.3003784086611048\n",
      "train loss:2.295549801690813\n",
      "train loss:2.294367872701245\n",
      "train loss:2.308312884389887\n",
      "train loss:2.297311447141841\n",
      "train loss:2.3045554736509555\n",
      "train loss:2.296614085772053\n",
      "train loss:2.3003638449920705\n",
      "train loss:2.2990250095496125\n",
      "train loss:2.3000291522964726\n",
      "train loss:2.2972994382989853\n",
      "train loss:2.29994335459414\n",
      "train loss:2.3028936021672277\n",
      "train loss:2.2941106956527113\n",
      "train loss:2.296908130430504\n",
      "train loss:2.2972888234967668\n",
      "train loss:2.2999212329679155\n",
      "train loss:2.291617331554554\n",
      "train loss:2.3013748824973437\n",
      "train loss:2.29927683155179\n",
      "train loss:2.295743984727725\n",
      "train loss:2.3021606219651396\n",
      "train loss:2.309589165476923\n",
      "train loss:2.2927388112738294\n",
      "train loss:2.305995555050695\n",
      "train loss:2.301020140144666\n",
      "train loss:2.305056039047084\n",
      "train loss:2.3119902542612074\n",
      "train loss:2.3006685038358006\n",
      "train loss:2.301357636770895\n",
      "train loss:2.3013255544595252\n",
      "train loss:2.3052064681620217\n",
      "train loss:2.3060711033911683\n",
      "train loss:2.3077974043279803\n",
      "train loss:2.3085984419629\n",
      "train loss:2.3071341676850388\n",
      "train loss:2.2971812186804805\n",
      "train loss:2.309384696023708\n",
      "train loss:2.3015880665065236\n",
      "train loss:2.3072974346193456\n",
      "train loss:2.296279388547054\n",
      "train loss:2.293351283516184\n",
      "train loss:2.3008485294241448\n",
      "train loss:2.3041220606728414\n",
      "train loss:2.301020592912604\n",
      "train loss:2.301420899171398\n",
      "train loss:2.3071723350077344\n",
      "train loss:2.2932922005483314\n",
      "train loss:2.294727774453327\n",
      "train loss:2.3007060860935185\n",
      "train loss:2.306458132666419\n",
      "train loss:2.294559596916382\n",
      "train loss:2.3083920517387195\n",
      "train loss:2.302932695939418\n",
      "train loss:2.2995552754117545\n",
      "train loss:2.304590517955908\n",
      "train loss:2.3108134041267117\n",
      "train loss:2.3039891233001892\n",
      "train loss:2.301772385814668\n",
      "train loss:2.3009413564859926\n",
      "train loss:2.3111868803085898\n",
      "train loss:2.300638820520891\n",
      "train loss:2.3028876409725942\n",
      "train loss:2.2948432973699546\n",
      "train loss:2.314616816911022\n",
      "train loss:2.305472050697668\n",
      "train loss:2.3018334017531297\n",
      "train loss:2.2977853237195065\n",
      "train loss:2.2971103037822265\n",
      "train loss:2.296697714330051\n",
      "train loss:2.3016403739812694\n",
      "train loss:2.2955841092440243\n",
      "train loss:2.2978133219397474\n",
      "train loss:2.304891448482121\n",
      "train loss:2.3096306687160255\n",
      "train loss:2.3042386969462534\n",
      "train loss:2.3094885415675783\n",
      "train loss:2.294485316959281\n",
      "train loss:2.3005646369772275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2956683083092257\n",
      "train loss:2.2994789392286545\n",
      "train loss:2.30140907597759\n",
      "train loss:2.301928035665289\n",
      "train loss:2.309623803591773\n",
      "train loss:2.3002152892460472\n",
      "train loss:2.2997257611297677\n",
      "train loss:2.2961701095211464\n",
      "train loss:2.2994680366408757\n",
      "train loss:2.297006404800116\n",
      "train loss:2.3096572374517117\n",
      "train loss:2.2986289953833037\n",
      "train loss:2.2903325030754482\n",
      "train loss:2.2948261479335996\n",
      "train loss:2.28988427299771\n",
      "train loss:2.2981971178916853\n",
      "train loss:2.310349096549051\n",
      "train loss:2.301430112950765\n",
      "train loss:2.305522300087703\n",
      "train loss:2.2980213914928314\n",
      "train loss:2.2946472469363335\n",
      "train loss:2.301658876936943\n",
      "train loss:2.2971008505407977\n",
      "train loss:2.300489507136094\n",
      "train loss:2.30342140065034\n",
      "train loss:2.3028764656564906\n",
      "train loss:2.3046353770922012\n",
      "train loss:2.2902588860646085\n",
      "train loss:2.3031922158963343\n",
      "train loss:2.305424774097354\n",
      "train loss:2.3040076918941694\n",
      "train loss:2.3045334736254\n",
      "train loss:2.3045060127745485\n",
      "train loss:2.2986393131463214\n",
      "train loss:2.3025093262594343\n",
      "train loss:2.2959032685513123\n",
      "train loss:2.2985512246417605\n",
      "train loss:2.2965778495979756\n",
      "train loss:2.30000885680974\n",
      "train loss:2.2957618023455293\n",
      "train loss:2.29553700288627\n",
      "train loss:2.3127930939924335\n",
      "train loss:2.295029755417108\n",
      "train loss:2.2997931614155056\n",
      "train loss:2.297781523303167\n",
      "train loss:2.3071790032007233\n",
      "train loss:2.305370228882275\n",
      "train loss:2.3016455973290872\n",
      "train loss:2.3061745585260613\n",
      "train loss:2.306843712350611\n",
      "train loss:2.2949434627868173\n",
      "train loss:2.301537441686792\n",
      "train loss:2.3064938970378415\n",
      "train loss:2.297005420833782\n",
      "train loss:2.296181763709747\n",
      "train loss:2.305737143744114\n",
      "train loss:2.3015766821112935\n",
      "train loss:2.306103068923425\n",
      "train loss:2.301099378602111\n",
      "train loss:2.301387757437033\n",
      "train loss:2.3059288350725717\n",
      "train loss:2.2960955889138797\n",
      "train loss:2.303747234196792\n",
      "train loss:2.297677112823162\n",
      "train loss:2.2992446260803834\n",
      "train loss:2.300676365841859\n",
      "train loss:2.304252947929917\n",
      "train loss:2.314296245415874\n",
      "train loss:2.30464859653308\n",
      "train loss:2.314396549872977\n",
      "train loss:2.3005633403224865\n",
      "train loss:2.3050673612311603\n",
      "train loss:2.3062901666202413\n",
      "train loss:2.2848820621701984\n",
      "train loss:2.2933200371018763\n",
      "train loss:2.305509678544644\n",
      "train loss:2.306496084134521\n",
      "train loss:2.3032320704981073\n",
      "train loss:2.3011706151411446\n",
      "train loss:2.2965652793714635\n",
      "train loss:2.2972459108201484\n",
      "train loss:2.305404445502717\n",
      "train loss:2.298030782197171\n",
      "train loss:2.3104369049463105\n",
      "train loss:2.3009118495725294\n",
      "train loss:2.2936736983469093\n",
      "train loss:2.300680408453124\n",
      "train loss:2.3018388582850866\n",
      "train loss:2.303138007288497\n",
      "train loss:2.3000928286380247\n",
      "train loss:2.302959621461139\n",
      "train loss:2.2902158059849005\n",
      "train loss:2.2925691621951128\n",
      "train loss:2.302537310110855\n",
      "train loss:2.301549454434122\n",
      "train loss:2.2966145908956275\n",
      "train loss:2.3001832262907986\n",
      "train loss:2.2996299048915017\n",
      "train loss:2.2970490608639897\n",
      "train loss:2.296609044977026\n",
      "train loss:2.300216366417363\n",
      "train loss:2.309573193770145\n",
      "train loss:2.2949827793565754\n",
      "train loss:2.2942665293245588\n",
      "train loss:2.3044833307139316\n",
      "train loss:2.298337764700009\n",
      "train loss:2.3041086598925964\n",
      "train loss:2.3033053405903345\n",
      "=== epoch:84, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3043501250314464\n",
      "train loss:2.2969567338036185\n",
      "train loss:2.300303850676099\n",
      "train loss:2.292585456710422\n",
      "train loss:2.2987404780717102\n",
      "train loss:2.2975769695136288\n",
      "train loss:2.2953596477850273\n",
      "train loss:2.2961464712221296\n",
      "train loss:2.300666310199131\n",
      "train loss:2.3019634097349826\n",
      "train loss:2.303398394126522\n",
      "train loss:2.2992077812787395\n",
      "train loss:2.30155561764111\n",
      "train loss:2.2995749229395073\n",
      "train loss:2.3034792664231865\n",
      "train loss:2.2980661158660785\n",
      "train loss:2.3053635690419503\n",
      "train loss:2.3076656576852788\n",
      "train loss:2.3129613888382234\n",
      "train loss:2.304169327945214\n",
      "train loss:2.299498576155086\n",
      "train loss:2.3047158504373546\n",
      "train loss:2.3055499495047873\n",
      "train loss:2.302839769414615\n",
      "train loss:2.2993380922335946\n",
      "train loss:2.2984511895579796\n",
      "train loss:2.3011435014926853\n",
      "train loss:2.3055726021250917\n",
      "train loss:2.297446211372696\n",
      "train loss:2.301960622530037\n",
      "train loss:2.3086366629468524\n",
      "train loss:2.305247957604178\n",
      "train loss:2.3035670484578867\n",
      "train loss:2.302461164866685\n",
      "train loss:2.3019714872494683\n",
      "train loss:2.2944315766756853\n",
      "train loss:2.2958118315742526\n",
      "train loss:2.299418930779063\n",
      "train loss:2.303640222149488\n",
      "train loss:2.3017571994442596\n",
      "train loss:2.30471796562259\n",
      "train loss:2.2980306057267077\n",
      "train loss:2.3035702384501\n",
      "train loss:2.298420312675729\n",
      "train loss:2.3008934378838233\n",
      "train loss:2.2999389640107286\n",
      "train loss:2.30545277615882\n",
      "train loss:2.293268148329194\n",
      "train loss:2.2864485280579356\n",
      "train loss:2.297844165518564\n",
      "train loss:2.2973402927956483\n",
      "train loss:2.2972590108759676\n",
      "train loss:2.2969006622727504\n",
      "train loss:2.2995390931874695\n",
      "train loss:2.29050492805748\n",
      "train loss:2.293694458287555\n",
      "train loss:2.304693890514912\n",
      "train loss:2.2963510627965156\n",
      "train loss:2.29597827820348\n",
      "train loss:2.305491186281892\n",
      "train loss:2.303932835700747\n",
      "train loss:2.3081014739403285\n",
      "train loss:2.297570194922006\n",
      "train loss:2.2947743196508816\n",
      "train loss:2.2961323128851037\n",
      "train loss:2.294132627958495\n",
      "train loss:2.2969477236800233\n",
      "train loss:2.302579945855179\n",
      "train loss:2.300229409371791\n",
      "train loss:2.3113386116652115\n",
      "train loss:2.310801576209093\n",
      "train loss:2.3009838140646655\n",
      "train loss:2.3106131568127064\n",
      "train loss:2.301423344559643\n",
      "train loss:2.3000827185570074\n",
      "train loss:2.304016029459187\n",
      "train loss:2.304869507773405\n",
      "train loss:2.300748474939452\n",
      "train loss:2.308211202768354\n",
      "train loss:2.302423055471327\n",
      "train loss:2.300099963584564\n",
      "train loss:2.3124967205047824\n",
      "train loss:2.299421502210209\n",
      "train loss:2.2925430477077073\n",
      "train loss:2.2979834212318666\n",
      "train loss:2.2969247705364233\n",
      "train loss:2.29359271552824\n",
      "train loss:2.304340427575239\n",
      "train loss:2.2947497210255796\n",
      "train loss:2.2973649181287823\n",
      "train loss:2.2999365825166302\n",
      "train loss:2.2942491576395754\n",
      "train loss:2.3065906034020913\n",
      "train loss:2.307654062561416\n",
      "train loss:2.3097596693141096\n",
      "train loss:2.3075494670575973\n",
      "train loss:2.2926715055450106\n",
      "train loss:2.2943557539923374\n",
      "train loss:2.2969504623300665\n",
      "train loss:2.303374553313669\n",
      "train loss:2.298581168796428\n",
      "train loss:2.296924381974531\n",
      "train loss:2.3016452247458083\n",
      "train loss:2.2895047524036247\n",
      "train loss:2.305373173665675\n",
      "train loss:2.294408616285684\n",
      "train loss:2.306982954135781\n",
      "train loss:2.3021953938313082\n",
      "train loss:2.3093134787542766\n",
      "train loss:2.3050288171109123\n",
      "train loss:2.2988984662542187\n",
      "train loss:2.301772667070457\n",
      "train loss:2.306605414751368\n",
      "train loss:2.3118502439717235\n",
      "train loss:2.2954211048281814\n",
      "train loss:2.3057476349058397\n",
      "train loss:2.297232287893498\n",
      "train loss:2.2990474480769407\n",
      "train loss:2.3048540258609123\n",
      "train loss:2.296750137710614\n",
      "train loss:2.3059051697204156\n",
      "train loss:2.294816754994796\n",
      "train loss:2.292280395043198\n",
      "train loss:2.2971370368036292\n",
      "train loss:2.304644739663457\n",
      "train loss:2.2973015257911182\n",
      "train loss:2.305443361616579\n",
      "train loss:2.2946349444917478\n",
      "train loss:2.304638199685917\n",
      "train loss:2.301001876243232\n",
      "train loss:2.3042301078459206\n",
      "train loss:2.2979189844505408\n",
      "train loss:2.2979490029886542\n",
      "train loss:2.299857933178149\n",
      "train loss:2.294300635334998\n",
      "train loss:2.3049065127749198\n",
      "train loss:2.3006935954266163\n",
      "train loss:2.305114786519265\n",
      "train loss:2.3005968316093925\n",
      "train loss:2.3057083770699633\n",
      "train loss:2.2959050929494\n",
      "train loss:2.3013044279511057\n",
      "train loss:2.2996062730095588\n",
      "train loss:2.3049527692363614\n",
      "train loss:2.288209230730689\n",
      "train loss:2.2982276371107058\n",
      "train loss:2.297157497344838\n",
      "train loss:2.3009004202580017\n",
      "train loss:2.292104797523313\n",
      "train loss:2.29573895454971\n",
      "train loss:2.299726454872813\n",
      "train loss:2.305258286125762\n",
      "train loss:2.293436728904517\n",
      "train loss:2.303006098203329\n",
      "train loss:2.301551204457502\n",
      "train loss:2.307050145620518\n",
      "train loss:2.2985340447053333\n",
      "train loss:2.3051730486868607\n",
      "train loss:2.309319956561208\n",
      "train loss:2.3001126093242235\n",
      "train loss:2.288850413889434\n",
      "train loss:2.3030260040307122\n",
      "train loss:2.306938395058248\n",
      "train loss:2.3074458205102335\n",
      "train loss:2.294403574715715\n",
      "train loss:2.3048237784455545\n",
      "train loss:2.2998296832417315\n",
      "train loss:2.300209336414871\n",
      "train loss:2.2910310900399207\n",
      "train loss:2.305156256283865\n",
      "train loss:2.3064152886614933\n",
      "train loss:2.3047157001927734\n",
      "train loss:2.307310587064676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2873341623449934\n",
      "train loss:2.3000041307563284\n",
      "train loss:2.2983804613368513\n",
      "train loss:2.2995196786141494\n",
      "train loss:2.2969240303157883\n",
      "train loss:2.3050132425836978\n",
      "train loss:2.30274559066191\n",
      "train loss:2.298013772694565\n",
      "train loss:2.2985277830951567\n",
      "train loss:2.3068928827169604\n",
      "train loss:2.30385557056067\n",
      "train loss:2.2983912242886313\n",
      "train loss:2.3043031369750664\n",
      "train loss:2.3048816466436413\n",
      "train loss:2.310918053432859\n",
      "train loss:2.3019144116313557\n",
      "train loss:2.3061582892104298\n",
      "train loss:2.306846462515984\n",
      "train loss:2.304492004975228\n",
      "train loss:2.30897522074298\n",
      "train loss:2.3023854710152354\n",
      "train loss:2.3036392451534655\n",
      "train loss:2.3076303662923756\n",
      "train loss:2.301712565389952\n",
      "train loss:2.2980341482040623\n",
      "train loss:2.293946309255276\n",
      "train loss:2.299449285382636\n",
      "=== epoch:85, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2963131572887243\n",
      "train loss:2.295109691201168\n",
      "train loss:2.2993427513449283\n",
      "train loss:2.300694352237161\n",
      "train loss:2.3065734777044438\n",
      "train loss:2.2963397203691454\n",
      "train loss:2.2944163000209628\n",
      "train loss:2.3001499392145424\n",
      "train loss:2.302501506823212\n",
      "train loss:2.3009892074787484\n",
      "train loss:2.3051343885051736\n",
      "train loss:2.291266061136506\n",
      "train loss:2.2919312699003718\n",
      "train loss:2.310402225622717\n",
      "train loss:2.3072680834024286\n",
      "train loss:2.311151262630154\n",
      "train loss:2.311246107493893\n",
      "train loss:2.3064651247709245\n",
      "train loss:2.29897362676066\n",
      "train loss:2.2950090607527733\n",
      "train loss:2.307925930110027\n",
      "train loss:2.302102083003333\n",
      "train loss:2.2944816205574465\n",
      "train loss:2.310980414623261\n",
      "train loss:2.312826190405738\n",
      "train loss:2.3025944294077605\n",
      "train loss:2.3027333641781804\n",
      "train loss:2.315138439721978\n",
      "train loss:2.295085399563495\n",
      "train loss:2.3027763007443345\n",
      "train loss:2.2986161167038444\n",
      "train loss:2.3016447962256996\n",
      "train loss:2.302324211483296\n",
      "train loss:2.301665926745873\n",
      "train loss:2.3104098537858855\n",
      "train loss:2.2982062800830643\n",
      "train loss:2.298823915355486\n",
      "train loss:2.3015204762065613\n",
      "train loss:2.3151744306159623\n",
      "train loss:2.298995820661534\n",
      "train loss:2.296824269342578\n",
      "train loss:2.2974450896963567\n",
      "train loss:2.3065312406192024\n",
      "train loss:2.302878475063261\n",
      "train loss:2.299767925269815\n",
      "train loss:2.3010263312389436\n",
      "train loss:2.3036147548516017\n",
      "train loss:2.3100915222271925\n",
      "train loss:2.3088645911159205\n",
      "train loss:2.2972221339516916\n",
      "train loss:2.3001068661957147\n",
      "train loss:2.296786690756227\n",
      "train loss:2.3040120487524893\n",
      "train loss:2.2937328237705095\n",
      "train loss:2.296434861945396\n",
      "train loss:2.3000491109524313\n",
      "train loss:2.3015405160618183\n",
      "train loss:2.2922917058443693\n",
      "train loss:2.301195186694932\n",
      "train loss:2.2919134144016615\n",
      "train loss:2.293856843012941\n",
      "train loss:2.304031809979179\n",
      "train loss:2.3025578235189514\n",
      "train loss:2.290357953192256\n",
      "train loss:2.3073409111138785\n",
      "train loss:2.2995562131530294\n",
      "train loss:2.3058597237598226\n",
      "train loss:2.303695300845757\n",
      "train loss:2.2947364657330818\n",
      "train loss:2.302319024407265\n",
      "train loss:2.3014130230069076\n",
      "train loss:2.3099487485371055\n",
      "train loss:2.2954097488771352\n",
      "train loss:2.3080998463757396\n",
      "train loss:2.3113562221648327\n",
      "train loss:2.3021239978173043\n",
      "train loss:2.2941584670212274\n",
      "train loss:2.298351775655452\n",
      "train loss:2.2972794147483913\n",
      "train loss:2.3024386109204906\n",
      "train loss:2.3024464262747575\n",
      "train loss:2.305760345186831\n",
      "train loss:2.2970413753462475\n",
      "train loss:2.30849450510881\n",
      "train loss:2.3052030295059596\n",
      "train loss:2.305842262887637\n",
      "train loss:2.302137705895665\n",
      "train loss:2.300399830586158\n",
      "train loss:2.3010405171359984\n",
      "train loss:2.3070979588083746\n",
      "train loss:2.3010700326856472\n",
      "train loss:2.307911940006815\n",
      "train loss:2.3018448498255273\n",
      "train loss:2.2943700607236694\n",
      "train loss:2.302795448095422\n",
      "train loss:2.310907924416144\n",
      "train loss:2.300855154545085\n",
      "train loss:2.302769809983636\n",
      "train loss:2.306350504884216\n",
      "train loss:2.310492577355548\n",
      "train loss:2.3011799302854428\n",
      "train loss:2.3037790280307635\n",
      "train loss:2.3095284052137335\n",
      "train loss:2.3047410716950596\n",
      "train loss:2.2973329735570642\n",
      "train loss:2.3039948067139093\n",
      "train loss:2.3021652131137746\n",
      "train loss:2.3062145803118943\n",
      "train loss:2.2932006878699576\n",
      "train loss:2.292929707127322\n",
      "train loss:2.3025586584427575\n",
      "train loss:2.3109664635015816\n",
      "train loss:2.3065782291795047\n",
      "train loss:2.300912596528826\n",
      "train loss:2.2983988829720254\n",
      "train loss:2.3055685433654576\n",
      "train loss:2.301391152717573\n",
      "train loss:2.3055681223968807\n",
      "train loss:2.299118049603007\n",
      "train loss:2.2951781076260005\n",
      "train loss:2.3022808565567816\n",
      "train loss:2.2953149184829247\n",
      "train loss:2.3054935776670233\n",
      "train loss:2.29441108448544\n",
      "train loss:2.2960125928876027\n",
      "train loss:2.2936942282073116\n",
      "train loss:2.3067840662736603\n",
      "train loss:2.3022941571839284\n",
      "train loss:2.310812938844613\n",
      "train loss:2.3004517020989312\n",
      "train loss:2.302876145247845\n",
      "train loss:2.299251230207918\n",
      "train loss:2.303201945564087\n",
      "train loss:2.304676217637399\n",
      "train loss:2.3070249891385624\n",
      "train loss:2.304326855126668\n",
      "train loss:2.3060159224077106\n",
      "train loss:2.3033496041290786\n",
      "train loss:2.304509260644921\n",
      "train loss:2.285295918364003\n",
      "train loss:2.300223791402439\n",
      "train loss:2.302621115759437\n",
      "train loss:2.2969202798584782\n",
      "train loss:2.2954102499410136\n",
      "train loss:2.2948016487737406\n",
      "train loss:2.3064318623687474\n",
      "train loss:2.298821539320347\n",
      "train loss:2.2901264901876903\n",
      "train loss:2.2986632947458285\n",
      "train loss:2.3001987221861784\n",
      "train loss:2.306497482874634\n",
      "train loss:2.302998217300061\n",
      "train loss:2.2994980313129583\n",
      "train loss:2.2959941665979025\n",
      "train loss:2.289851383119401\n",
      "train loss:2.3022983989778676\n",
      "train loss:2.3051137201232685\n",
      "train loss:2.2975450945662144\n",
      "train loss:2.2934214615915076\n",
      "train loss:2.2998865872826513\n",
      "train loss:2.307635766361654\n",
      "train loss:2.310526416322334\n",
      "train loss:2.303566431775856\n",
      "train loss:2.308301881492114\n",
      "train loss:2.3034722773181975\n",
      "train loss:2.3058307806532268\n",
      "train loss:2.303406928268536\n",
      "train loss:2.297293099782085\n",
      "train loss:2.2992679455711977\n",
      "train loss:2.2900649746011386\n",
      "train loss:2.3038986577660063\n",
      "train loss:2.3071087656561677\n",
      "train loss:2.2942274973897887\n",
      "train loss:2.3020180634265905\n",
      "train loss:2.3029220103499504\n",
      "train loss:2.3085025002323505\n",
      "train loss:2.2880013954469174\n",
      "train loss:2.2964181612830923\n",
      "train loss:2.2981620962353455\n",
      "train loss:2.2987288344696744\n",
      "train loss:2.2932537001690543\n",
      "train loss:2.307213620532455\n",
      "train loss:2.3077367227958727\n",
      "train loss:2.300280522197368\n",
      "train loss:2.3001486199752437\n",
      "train loss:2.3061174426010598\n",
      "train loss:2.2957889880631748\n",
      "train loss:2.3044640048370284\n",
      "train loss:2.295338744051883\n",
      "train loss:2.3037367275778147\n",
      "train loss:2.3120651739219396\n",
      "train loss:2.30090907519653\n",
      "train loss:2.291208312538279\n",
      "train loss:2.2996444089563925\n",
      "train loss:2.2995158560584747\n",
      "train loss:2.302682300316805\n",
      "train loss:2.304904322033721\n",
      "train loss:2.307201924745006\n",
      "train loss:2.299487713337515\n",
      "train loss:2.297575620241161\n",
      "=== epoch:86, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3093554224974424\n",
      "train loss:2.2926311591462305\n",
      "train loss:2.303656389907782\n",
      "train loss:2.300552450175201\n",
      "train loss:2.3003412044301452\n",
      "train loss:2.3056245771604393\n",
      "train loss:2.298493764605468\n",
      "train loss:2.3024519447373986\n",
      "train loss:2.3061034631922057\n",
      "train loss:2.2895519371328374\n",
      "train loss:2.3091685362647474\n",
      "train loss:2.298093820591988\n",
      "train loss:2.3030317783332657\n",
      "train loss:2.3033426541385844\n",
      "train loss:2.287852594776192\n",
      "train loss:2.3062501351873923\n",
      "train loss:2.3015094798139093\n",
      "train loss:2.3071980525190776\n",
      "train loss:2.302704946838027\n",
      "train loss:2.3016025052054325\n",
      "train loss:2.3050850714648883\n",
      "train loss:2.293472928046188\n",
      "train loss:2.3044165930206044\n",
      "train loss:2.297185745652956\n",
      "train loss:2.3044303783195517\n",
      "train loss:2.300973933169287\n",
      "train loss:2.30682837882469\n",
      "train loss:2.2952911741652704\n",
      "train loss:2.305326505801203\n",
      "train loss:2.300064772014924\n",
      "train loss:2.3047043505076332\n",
      "train loss:2.2991250616391667\n",
      "train loss:2.2925512678130344\n",
      "train loss:2.302223147504897\n",
      "train loss:2.3080521948841466\n",
      "train loss:2.2952268790896753\n",
      "train loss:2.297977522516094\n",
      "train loss:2.3035377662867544\n",
      "train loss:2.2991158119870048\n",
      "train loss:2.3006426930977972\n",
      "train loss:2.3019556600464073\n",
      "train loss:2.3057050290161123\n",
      "train loss:2.3094181029942513\n",
      "train loss:2.3039874952858965\n",
      "train loss:2.29893774387613\n",
      "train loss:2.303956122683611\n",
      "train loss:2.299163092493932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301741879499846\n",
      "train loss:2.3077769111686868\n",
      "train loss:2.290563280652015\n",
      "train loss:2.303951756986754\n",
      "train loss:2.2926067613968892\n",
      "train loss:2.3023818299653707\n",
      "train loss:2.307959547231974\n",
      "train loss:2.3050745550135376\n",
      "train loss:2.299864672590815\n",
      "train loss:2.298133267455366\n",
      "train loss:2.298232707494151\n",
      "train loss:2.2977653019105397\n",
      "train loss:2.2974755604572605\n",
      "train loss:2.2963656962027827\n",
      "train loss:2.3039944595463835\n",
      "train loss:2.301031550351004\n",
      "train loss:2.3046188356473514\n",
      "train loss:2.304440150359421\n",
      "train loss:2.307623253824441\n",
      "train loss:2.301408194496304\n",
      "train loss:2.3098412527706698\n",
      "train loss:2.3005331965241265\n",
      "train loss:2.298877581347032\n",
      "train loss:2.2938894409600845\n",
      "train loss:2.2990426203676257\n",
      "train loss:2.2940574779331766\n",
      "train loss:2.2970051438689856\n",
      "train loss:2.3111080168372937\n",
      "train loss:2.290386883047948\n",
      "train loss:2.30470299878162\n",
      "train loss:2.300255779027686\n",
      "train loss:2.3014490095932807\n",
      "train loss:2.3018145994556525\n",
      "train loss:2.2979607920562555\n",
      "train loss:2.2899949207754347\n",
      "train loss:2.304590866243021\n",
      "train loss:2.2987963751758627\n",
      "train loss:2.2977796093457634\n",
      "train loss:2.296499471164502\n",
      "train loss:2.3061937257080576\n",
      "train loss:2.2991617684723655\n",
      "train loss:2.3004187423992333\n",
      "train loss:2.3042061530110924\n",
      "train loss:2.3054235151493883\n",
      "train loss:2.300958344371627\n",
      "train loss:2.2963620564789373\n",
      "train loss:2.295702859570851\n",
      "train loss:2.30381271796057\n",
      "train loss:2.2878640100315075\n",
      "train loss:2.3013250721418625\n",
      "train loss:2.3033929717065305\n",
      "train loss:2.2994829884184798\n",
      "train loss:2.3016254388538866\n",
      "train loss:2.3030153609990367\n",
      "train loss:2.304028576865855\n",
      "train loss:2.2995792616578754\n",
      "train loss:2.2910050736962613\n",
      "train loss:2.302050012426522\n",
      "train loss:2.305852905455848\n",
      "train loss:2.2994347659700636\n",
      "train loss:2.3081843370385395\n",
      "train loss:2.290427682729709\n",
      "train loss:2.3056714301390038\n",
      "train loss:2.2997554486853504\n",
      "train loss:2.3018503043642107\n",
      "train loss:2.300727929651747\n",
      "train loss:2.300927156654714\n",
      "train loss:2.2969544193605795\n",
      "train loss:2.3014635681899698\n",
      "train loss:2.2998128312581776\n",
      "train loss:2.3051948992745825\n",
      "train loss:2.2949209017690047\n",
      "train loss:2.2968683992359087\n",
      "train loss:2.2946386930531917\n",
      "train loss:2.3051233778793208\n",
      "train loss:2.299407553594493\n",
      "train loss:2.3023562727665294\n",
      "train loss:2.3025441081086715\n",
      "train loss:2.295503535517194\n",
      "train loss:2.3011242476276665\n",
      "train loss:2.300891951231329\n",
      "train loss:2.2997708239272083\n",
      "train loss:2.3049711696806154\n",
      "train loss:2.302268382762525\n",
      "train loss:2.30885519505796\n",
      "train loss:2.2971227169484236\n",
      "train loss:2.2929139046596734\n",
      "train loss:2.304935920178947\n",
      "train loss:2.2989097303309634\n",
      "train loss:2.296623223309571\n",
      "train loss:2.3009438413642904\n",
      "train loss:2.299716259489321\n",
      "train loss:2.3085219546773157\n",
      "train loss:2.30974612729419\n",
      "train loss:2.293482108350947\n",
      "train loss:2.3073883688594616\n",
      "train loss:2.2979943815828947\n",
      "train loss:2.300840083019545\n",
      "train loss:2.2904727635032436\n",
      "train loss:2.3020928898910826\n",
      "train loss:2.311262631463686\n",
      "train loss:2.301016410998469\n",
      "train loss:2.296689512151729\n",
      "train loss:2.2988843484985746\n",
      "train loss:2.308597828514938\n",
      "train loss:2.3056362328879256\n",
      "train loss:2.300794705264166\n",
      "train loss:2.3060779223021757\n",
      "train loss:2.2931750193728373\n",
      "train loss:2.2975605468207854\n",
      "train loss:2.300208815503299\n",
      "train loss:2.3034419228552023\n",
      "train loss:2.303963040014509\n",
      "train loss:2.3000789999886693\n",
      "train loss:2.30149240786354\n",
      "train loss:2.301311852810448\n",
      "train loss:2.300856543091292\n",
      "train loss:2.303571847456339\n",
      "train loss:2.297831404202147\n",
      "train loss:2.3090098877519964\n",
      "train loss:2.3038848562325525\n",
      "train loss:2.3006391006352387\n",
      "train loss:2.307480659588579\n",
      "train loss:2.3016571616645476\n",
      "train loss:2.3055635963958667\n",
      "train loss:2.288057590952025\n",
      "train loss:2.308120140686458\n",
      "train loss:2.3034607149401682\n",
      "train loss:2.3015073223464477\n",
      "train loss:2.307865318915326\n",
      "train loss:2.3035664057805154\n",
      "train loss:2.2917122710485693\n",
      "train loss:2.298716056845737\n",
      "train loss:2.3053631667258614\n",
      "train loss:2.3031195432485014\n",
      "train loss:2.29773185005171\n",
      "train loss:2.2919051220687616\n",
      "train loss:2.3041834616949326\n",
      "train loss:2.3021374456508235\n",
      "train loss:2.303529847224441\n",
      "train loss:2.3000068827580145\n",
      "train loss:2.2934833156136083\n",
      "train loss:2.299255804581768\n",
      "train loss:2.2974243236434457\n",
      "train loss:2.303830755524591\n",
      "train loss:2.3001102725524554\n",
      "train loss:2.294965422882244\n",
      "train loss:2.288664555690104\n",
      "train loss:2.2992885186306147\n",
      "train loss:2.303745761685068\n",
      "train loss:2.305464726025943\n",
      "train loss:2.292770486223004\n",
      "train loss:2.300247423449896\n",
      "=== epoch:87, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.306772310991144\n",
      "train loss:2.29368156728423\n",
      "train loss:2.297184604422361\n",
      "train loss:2.3019061015237208\n",
      "train loss:2.3077321838980716\n",
      "train loss:2.300365335052754\n",
      "train loss:2.290546441032163\n",
      "train loss:2.2990366393038815\n",
      "train loss:2.3002123514689794\n",
      "train loss:2.3055621197200207\n",
      "train loss:2.295845369162723\n",
      "train loss:2.310285482148327\n",
      "train loss:2.2936492148145304\n",
      "train loss:2.304098109661633\n",
      "train loss:2.306939907077866\n",
      "train loss:2.30189085140519\n",
      "train loss:2.3039348176335013\n",
      "train loss:2.287638631116944\n",
      "train loss:2.303127734860605\n",
      "train loss:2.308566220873514\n",
      "train loss:2.292098576485707\n",
      "train loss:2.3010954869856395\n",
      "train loss:2.294032796263011\n",
      "train loss:2.299030410513674\n",
      "train loss:2.30174950548061\n",
      "train loss:2.3022101174030944\n",
      "train loss:2.29912158906799\n",
      "train loss:2.305738190492018\n",
      "train loss:2.302961463742611\n",
      "train loss:2.30377668913805\n",
      "train loss:2.3082888732104596\n",
      "train loss:2.2958336458635777\n",
      "train loss:2.2897408651633726\n",
      "train loss:2.303668825176911\n",
      "train loss:2.3021022543314573\n",
      "train loss:2.308505836390086\n",
      "train loss:2.306908637269102\n",
      "train loss:2.3005086560797294\n",
      "train loss:2.304011124040867\n",
      "train loss:2.2923573128271926\n",
      "train loss:2.291317400125567\n",
      "train loss:2.2965409592213515\n",
      "train loss:2.295882811501816\n",
      "train loss:2.3022498424491653\n",
      "train loss:2.3015427943549276\n",
      "train loss:2.2984692915338303\n",
      "train loss:2.3061795711266138\n",
      "train loss:2.2948349273098305\n",
      "train loss:2.3059855996658736\n",
      "train loss:2.2992625646358786\n",
      "train loss:2.3046466199286604\n",
      "train loss:2.2869148574362206\n",
      "train loss:2.3111486347089585\n",
      "train loss:2.292261703916833\n",
      "train loss:2.3017136100917357\n",
      "train loss:2.311255700437863\n",
      "train loss:2.304596114778712\n",
      "train loss:2.292560333198822\n",
      "train loss:2.3119305138894646\n",
      "train loss:2.315412051106282\n",
      "train loss:2.300226623710385\n",
      "train loss:2.2934477232254173\n",
      "train loss:2.3060264178604037\n",
      "train loss:2.298798463390252\n",
      "train loss:2.2970356280858\n",
      "train loss:2.300093928606053\n",
      "train loss:2.3037641652840697\n",
      "train loss:2.3018562050307625\n",
      "train loss:2.2985253472731273\n",
      "train loss:2.3006460570640157\n",
      "train loss:2.3078005993563586\n",
      "train loss:2.300975971244537\n",
      "train loss:2.301801066308653\n",
      "train loss:2.303420181126927\n",
      "train loss:2.302747744741171\n",
      "train loss:2.2974162259082584\n",
      "train loss:2.2902720708925206\n",
      "train loss:2.2972449822380048\n",
      "train loss:2.30609086957961\n",
      "train loss:2.298401671604757\n",
      "train loss:2.29799313284704\n",
      "train loss:2.303375308660383\n",
      "train loss:2.3041679324724966\n",
      "train loss:2.3009453530756545\n",
      "train loss:2.2826374304870565\n",
      "train loss:2.3100573519986027\n",
      "train loss:2.309803503745861\n",
      "train loss:2.2969545879550424\n",
      "train loss:2.314412377042604\n",
      "train loss:2.303404038617681\n",
      "train loss:2.311551330821486\n",
      "train loss:2.307437821881623\n",
      "train loss:2.299262332950847\n",
      "train loss:2.298770014619816\n",
      "train loss:2.2954980746359452\n",
      "train loss:2.297109234169565\n",
      "train loss:2.306776246450247\n",
      "train loss:2.3016152948001145\n",
      "train loss:2.3044104226424613\n",
      "train loss:2.310614822941269\n",
      "train loss:2.3092776909246537\n",
      "train loss:2.299857209621581\n",
      "train loss:2.3046286868387638\n",
      "train loss:2.2986295352289075\n",
      "train loss:2.2918555650807977\n",
      "train loss:2.299244295247784\n",
      "train loss:2.3074309249689655\n",
      "train loss:2.306951609351779\n",
      "train loss:2.2993787271291994\n",
      "train loss:2.300885301394198\n",
      "train loss:2.309530846081666\n",
      "train loss:2.288232249428383\n",
      "train loss:2.295510185399282\n",
      "train loss:2.2947496369109275\n",
      "train loss:2.2974414759259933\n",
      "train loss:2.2994231246486\n",
      "train loss:2.303618342423051\n",
      "train loss:2.3027644759295423\n",
      "train loss:2.3097485989011717\n",
      "train loss:2.3030576036846817\n",
      "train loss:2.2953387715848126\n",
      "train loss:2.295788264818638\n",
      "train loss:2.3023799634457367\n",
      "train loss:2.2923808513998325\n",
      "train loss:2.3089691141635877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3037204741636885\n",
      "train loss:2.2999547413916743\n",
      "train loss:2.3006180014428854\n",
      "train loss:2.3043358760330785\n",
      "train loss:2.2992855263618073\n",
      "train loss:2.3002340504009706\n",
      "train loss:2.303958558956449\n",
      "train loss:2.304498861611942\n",
      "train loss:2.300807719260588\n",
      "train loss:2.3022277217761014\n",
      "train loss:2.302168888901474\n",
      "train loss:2.302648539530081\n",
      "train loss:2.2995031725499038\n",
      "train loss:2.307797667009471\n",
      "train loss:2.310258528549747\n",
      "train loss:2.302797787185405\n",
      "train loss:2.2932021643842564\n",
      "train loss:2.3047195480736584\n",
      "train loss:2.3056795396038603\n",
      "train loss:2.2987784569466223\n",
      "train loss:2.300778252238483\n",
      "train loss:2.298392165233347\n",
      "train loss:2.2963323853367683\n",
      "train loss:2.29725595386917\n",
      "train loss:2.3015121670887986\n",
      "train loss:2.3019044522483547\n",
      "train loss:2.3003376435598315\n",
      "train loss:2.307039226491023\n",
      "train loss:2.2983925466704003\n",
      "train loss:2.3027862394145115\n",
      "train loss:2.3014448294347183\n",
      "train loss:2.298073104135545\n",
      "train loss:2.3022901982252186\n",
      "train loss:2.3067325132210397\n",
      "train loss:2.303589674471929\n",
      "train loss:2.30409076030504\n",
      "train loss:2.2930484658917467\n",
      "train loss:2.2929491132830813\n",
      "train loss:2.300428948236763\n",
      "train loss:2.291819786360264\n",
      "train loss:2.303355631579745\n",
      "train loss:2.309525218449254\n",
      "train loss:2.307053144713989\n",
      "train loss:2.2916823325737066\n",
      "train loss:2.3060992357877037\n",
      "train loss:2.2973578040408973\n",
      "train loss:2.302914086978379\n",
      "train loss:2.3039681609136236\n",
      "train loss:2.302341133128503\n",
      "train loss:2.291861848053185\n",
      "train loss:2.301652137777699\n",
      "train loss:2.301730445622043\n",
      "train loss:2.2958425581489337\n",
      "train loss:2.288084142708676\n",
      "train loss:2.3013399147732176\n",
      "train loss:2.3045597698936997\n",
      "train loss:2.291363529812959\n",
      "train loss:2.298866467137316\n",
      "train loss:2.2985126400737492\n",
      "train loss:2.298140705489201\n",
      "train loss:2.297799612754735\n",
      "train loss:2.307188126358417\n",
      "train loss:2.3058668721060838\n",
      "train loss:2.297219844756779\n",
      "train loss:2.3045487706035845\n",
      "train loss:2.296135064441099\n",
      "train loss:2.304960824803841\n",
      "train loss:2.3048470933988936\n",
      "train loss:2.309039511601272\n",
      "train loss:2.303174177579925\n",
      "train loss:2.2998437240246075\n",
      "train loss:2.295616345520092\n",
      "train loss:2.3030872232167994\n",
      "train loss:2.294728182809482\n",
      "train loss:2.3088026553415957\n",
      "=== epoch:88, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2968536991815593\n",
      "train loss:2.293793268021678\n",
      "train loss:2.3044728569388786\n",
      "train loss:2.306657610827553\n",
      "train loss:2.3024200065198053\n",
      "train loss:2.296586230626185\n",
      "train loss:2.2979535114816567\n",
      "train loss:2.2962999736640852\n",
      "train loss:2.2954231445038196\n",
      "train loss:2.3022021665541663\n",
      "train loss:2.30331255523715\n",
      "train loss:2.3008246501081273\n",
      "train loss:2.310247902274704\n",
      "train loss:2.309748362340569\n",
      "train loss:2.292395961554955\n",
      "train loss:2.3001174535671507\n",
      "train loss:2.3035664289168607\n",
      "train loss:2.3005649125545284\n",
      "train loss:2.3039478052550764\n",
      "train loss:2.2935602880729395\n",
      "train loss:2.3107359879864426\n",
      "train loss:2.301544375579372\n",
      "train loss:2.3068304883733517\n",
      "train loss:2.3024763968997664\n",
      "train loss:2.2977515635784633\n",
      "train loss:2.29954413135932\n",
      "train loss:2.295104507534982\n",
      "train loss:2.3004556732307413\n",
      "train loss:2.3000162029901903\n",
      "train loss:2.299341616810465\n",
      "train loss:2.291950927952463\n",
      "train loss:2.3078354535899366\n",
      "train loss:2.297250041225804\n",
      "train loss:2.301350850645494\n",
      "train loss:2.299142964968841\n",
      "train loss:2.299671876422889\n",
      "train loss:2.3043082434875894\n",
      "train loss:2.3057153413780798\n",
      "train loss:2.3033751362629338\n",
      "train loss:2.304108616967217\n",
      "train loss:2.287616818525102\n",
      "train loss:2.306322018199391\n",
      "train loss:2.302257310547429\n",
      "train loss:2.294974264060913\n",
      "train loss:2.3031274604701424\n",
      "train loss:2.306576764859358\n",
      "train loss:2.30205297267629\n",
      "train loss:2.303433199601826\n",
      "train loss:2.2980539379962712\n",
      "train loss:2.309182404503534\n",
      "train loss:2.300615275419611\n",
      "train loss:2.302600991288401\n",
      "train loss:2.2963702490774063\n",
      "train loss:2.3018323275882677\n",
      "train loss:2.2927896498139164\n",
      "train loss:2.2893164934938968\n",
      "train loss:2.2982476418402036\n",
      "train loss:2.2977253915280955\n",
      "train loss:2.291004842561039\n",
      "train loss:2.3000840094338937\n",
      "train loss:2.3063433935398048\n",
      "train loss:2.304131686426765\n",
      "train loss:2.3012219310712485\n",
      "train loss:2.3170214880932263\n",
      "train loss:2.297321712937544\n",
      "train loss:2.3001636226106568\n",
      "train loss:2.3126298000657908\n",
      "train loss:2.2985567858076768\n",
      "train loss:2.2967940661203254\n",
      "train loss:2.301344081126839\n",
      "train loss:2.3065164274898047\n",
      "train loss:2.30162193955941\n",
      "train loss:2.292637349075482\n",
      "train loss:2.3051427532525546\n",
      "train loss:2.30614935188848\n",
      "train loss:2.2910415684121532\n",
      "train loss:2.2988504550598443\n",
      "train loss:2.3013549760938283\n",
      "train loss:2.3077545462427684\n",
      "train loss:2.3055772402205723\n",
      "train loss:2.296290890924966\n",
      "train loss:2.306923566965483\n",
      "train loss:2.2896628239360237\n",
      "train loss:2.294837583172676\n",
      "train loss:2.3018586620782098\n",
      "train loss:2.289994569441848\n",
      "train loss:2.2980133674785357\n",
      "train loss:2.293721222106225\n",
      "train loss:2.3016485087238987\n",
      "train loss:2.3049852353935067\n",
      "train loss:2.2996184293644113\n",
      "train loss:2.3037268506794772\n",
      "train loss:2.2905619241750426\n",
      "train loss:2.308935099744816\n",
      "train loss:2.3035347780082547\n",
      "train loss:2.3002122173048476\n",
      "train loss:2.2992019158368118\n",
      "train loss:2.3000888191659796\n",
      "train loss:2.299441413881895\n",
      "train loss:2.2973523374507234\n",
      "train loss:2.289943769612088\n",
      "train loss:2.306909216660973\n",
      "train loss:2.291318787723284\n",
      "train loss:2.29999368470589\n",
      "train loss:2.297471968012088\n",
      "train loss:2.300144029773013\n",
      "train loss:2.2978525238232934\n",
      "train loss:2.297155833094981\n",
      "train loss:2.3004320755080925\n",
      "train loss:2.300903784014383\n",
      "train loss:2.30016095433827\n",
      "train loss:2.2998893230747988\n",
      "train loss:2.2938698633761136\n",
      "train loss:2.295306258316075\n",
      "train loss:2.303931104753373\n",
      "train loss:2.306858239213194\n",
      "train loss:2.294625254859051\n",
      "train loss:2.302017125334482\n",
      "train loss:2.2958306605859633\n",
      "train loss:2.3031856181994366\n",
      "train loss:2.2985783625383394\n",
      "train loss:2.298611088676934\n",
      "train loss:2.3013488183271518\n",
      "train loss:2.3034445264347245\n",
      "train loss:2.3010549377967244\n",
      "train loss:2.2924920143029026\n",
      "train loss:2.303799268035408\n",
      "train loss:2.3086330405328006\n",
      "train loss:2.293204836382306\n",
      "train loss:2.2977885021892206\n",
      "train loss:2.3050573223422672\n",
      "train loss:2.3058187805920856\n",
      "train loss:2.3025664505060948\n",
      "train loss:2.3018479419117335\n",
      "train loss:2.303608786135234\n",
      "train loss:2.3044035940470016\n",
      "train loss:2.301776154971484\n",
      "train loss:2.287946668656263\n",
      "train loss:2.296507861803385\n",
      "train loss:2.3034803041892533\n",
      "train loss:2.3117154642081625\n",
      "train loss:2.3000191772761145\n",
      "train loss:2.290408007740603\n",
      "train loss:2.303362029233045\n",
      "train loss:2.302074914026302\n",
      "train loss:2.296189768560663\n",
      "train loss:2.3045330680154557\n",
      "train loss:2.2945074971065047\n",
      "train loss:2.2995572654549603\n",
      "train loss:2.295745047514769\n",
      "train loss:2.293011491831991\n",
      "train loss:2.3059301205055025\n",
      "train loss:2.301193854766951\n",
      "train loss:2.288970391856982\n",
      "train loss:2.298592892569204\n",
      "train loss:2.295658520153701\n",
      "train loss:2.3018058438474545\n",
      "train loss:2.2991492407710035\n",
      "train loss:2.298523156013864\n",
      "train loss:2.2980075951214576\n",
      "train loss:2.30167659853024\n",
      "train loss:2.300330776107022\n",
      "train loss:2.3054135564540426\n",
      "train loss:2.3121409923703964\n",
      "train loss:2.299801908244689\n",
      "train loss:2.30253316524417\n",
      "train loss:2.299364543805737\n",
      "train loss:2.2965089372475775\n",
      "train loss:2.305313954866451\n",
      "train loss:2.2955095389158364\n",
      "train loss:2.3148528975769107\n",
      "train loss:2.3034439243813374\n",
      "train loss:2.2962401212479486\n",
      "train loss:2.303719970745305\n",
      "train loss:2.303728364991116\n",
      "train loss:2.3008888015210984\n",
      "train loss:2.3045007785001608\n",
      "train loss:2.2987620478248103\n",
      "train loss:2.2966435146545088\n",
      "train loss:2.303211180789121\n",
      "train loss:2.306346945489473\n",
      "train loss:2.3038365517816737\n",
      "train loss:2.2950242979270192\n",
      "train loss:2.3021589487774947\n",
      "train loss:2.2990298208108806\n",
      "train loss:2.3044180172280413\n",
      "train loss:2.2951686771850497\n",
      "train loss:2.309811284518584\n",
      "train loss:2.2997171252495643\n",
      "train loss:2.292785597319605\n",
      "train loss:2.297226093732675\n",
      "train loss:2.3043749525148627\n",
      "train loss:2.3029287584012477\n",
      "train loss:2.2976650371792635\n",
      "train loss:2.294564938994358\n",
      "train loss:2.302834501892556\n",
      "train loss:2.2951778592698067\n",
      "train loss:2.303793127744059\n",
      "train loss:2.2994600166837222\n",
      "train loss:2.298909044626254\n",
      "=== epoch:89, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.305556125751912\n",
      "train loss:2.2959212792052224\n",
      "train loss:2.3057456892254407\n",
      "train loss:2.310726063447799\n",
      "train loss:2.310390580444398\n",
      "train loss:2.2978443447495636\n",
      "train loss:2.303744695505905\n",
      "train loss:2.2872517422860397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2897592195425935\n",
      "train loss:2.302862764854524\n",
      "train loss:2.3045010720405514\n",
      "train loss:2.30011920365282\n",
      "train loss:2.292444114160888\n",
      "train loss:2.293389508250759\n",
      "train loss:2.3061490053919877\n",
      "train loss:2.310596177785396\n",
      "train loss:2.2956979084480134\n",
      "train loss:2.307465303114074\n",
      "train loss:2.2990730978549503\n",
      "train loss:2.296736678556115\n",
      "train loss:2.293314026737331\n",
      "train loss:2.3046827373799683\n",
      "train loss:2.291485946756104\n",
      "train loss:2.3015484703928872\n",
      "train loss:2.2867198377299025\n",
      "train loss:2.292751436036869\n",
      "train loss:2.3050088887635543\n",
      "train loss:2.2907671553948914\n",
      "train loss:2.287651846103886\n",
      "train loss:2.3007432925121245\n",
      "train loss:2.2987555935300508\n",
      "train loss:2.3058050615583143\n",
      "train loss:2.303400029301421\n",
      "train loss:2.3051300219944877\n",
      "train loss:2.2958310681608216\n",
      "train loss:2.3032567517176266\n",
      "train loss:2.2950372236202004\n",
      "train loss:2.297329583371439\n",
      "train loss:2.3017746398054477\n",
      "train loss:2.3045278445292796\n",
      "train loss:2.303501672669198\n",
      "train loss:2.299603182084001\n",
      "train loss:2.3029941698492395\n",
      "train loss:2.298202585727082\n",
      "train loss:2.313836756174433\n",
      "train loss:2.3117448163335714\n",
      "train loss:2.3064006534640744\n",
      "train loss:2.298988427142832\n",
      "train loss:2.311585563286114\n",
      "train loss:2.295292793237182\n",
      "train loss:2.3021483664983755\n",
      "train loss:2.301968907571922\n",
      "train loss:2.3068501147343476\n",
      "train loss:2.3047386524822326\n",
      "train loss:2.3013289663354346\n",
      "train loss:2.3025747301479003\n",
      "train loss:2.3018489002301594\n",
      "train loss:2.2972998854396343\n",
      "train loss:2.3035347838810227\n",
      "train loss:2.307689409145824\n",
      "train loss:2.3027133700165625\n",
      "train loss:2.3018821889260646\n",
      "train loss:2.2911056628466775\n",
      "train loss:2.3051594995881453\n",
      "train loss:2.310199607423916\n",
      "train loss:2.301744284389031\n",
      "train loss:2.2994818756926545\n",
      "train loss:2.2894192796805357\n",
      "train loss:2.299492906370737\n",
      "train loss:2.3070743995118383\n",
      "train loss:2.298064328966557\n",
      "train loss:2.3020291775895463\n",
      "train loss:2.2907792720249502\n",
      "train loss:2.306991879211374\n",
      "train loss:2.30158108545824\n",
      "train loss:2.301629091781788\n",
      "train loss:2.3091122153959383\n",
      "train loss:2.2992717711998076\n",
      "train loss:2.2946066794503066\n",
      "train loss:2.299588735241584\n",
      "train loss:2.3050719055822184\n",
      "train loss:2.2990274816069594\n",
      "train loss:2.2990194354974656\n",
      "train loss:2.298406383239241\n",
      "train loss:2.3011411279136262\n",
      "train loss:2.2987562008687488\n",
      "train loss:2.2997787581460645\n",
      "train loss:2.290207983766257\n",
      "train loss:2.2974529727288258\n",
      "train loss:2.294827853709156\n",
      "train loss:2.304073098357657\n",
      "train loss:2.2923911592296524\n",
      "train loss:2.3027468944248364\n",
      "train loss:2.30021513817151\n",
      "train loss:2.3038620620623966\n",
      "train loss:2.3032888405175616\n",
      "train loss:2.3032941738045416\n",
      "train loss:2.2848902624408107\n",
      "train loss:2.2906468837130247\n",
      "train loss:2.293785422677475\n",
      "train loss:2.311298552845941\n",
      "train loss:2.29895059056806\n",
      "train loss:2.308622501851454\n",
      "train loss:2.3071276062142383\n",
      "train loss:2.304570375620341\n",
      "train loss:2.298748647359889\n",
      "train loss:2.2962129781356184\n",
      "train loss:2.295988968340767\n",
      "train loss:2.2969770857568155\n",
      "train loss:2.3035221515808297\n",
      "train loss:2.307527821471183\n",
      "train loss:2.2967074250335653\n",
      "train loss:2.3085660209746206\n",
      "train loss:2.305179454284907\n",
      "train loss:2.3014949476506414\n",
      "train loss:2.3100689490108155\n",
      "train loss:2.3017435250495133\n",
      "train loss:2.2912065411743328\n",
      "train loss:2.2945834995613423\n",
      "train loss:2.3021286983612095\n",
      "train loss:2.299739944177273\n",
      "train loss:2.3030524757118878\n",
      "train loss:2.2971406336427727\n",
      "train loss:2.2963243214416176\n",
      "train loss:2.3041753000151375\n",
      "train loss:2.3055389366277206\n",
      "train loss:2.2995711944347623\n",
      "train loss:2.303330319371208\n",
      "train loss:2.309208967737568\n",
      "train loss:2.3113640271131657\n",
      "train loss:2.302481108881882\n",
      "train loss:2.3005949753316286\n",
      "train loss:2.304890609041927\n",
      "train loss:2.299889487299311\n",
      "train loss:2.3013105779227825\n",
      "train loss:2.30594448881661\n",
      "train loss:2.29812932230734\n",
      "train loss:2.3097424743885346\n",
      "train loss:2.2970310973848775\n",
      "train loss:2.3058468884205543\n",
      "train loss:2.2960697376742045\n",
      "train loss:2.3151799483762754\n",
      "train loss:2.2941263325188674\n",
      "train loss:2.2913391195324433\n",
      "train loss:2.3062524336126873\n",
      "train loss:2.3046916831575053\n",
      "train loss:2.304013100069907\n",
      "train loss:2.294843324938353\n",
      "train loss:2.295433250553593\n",
      "train loss:2.292147246180873\n",
      "train loss:2.3013396946501876\n",
      "train loss:2.3073322246512427\n",
      "train loss:2.29951257674089\n",
      "train loss:2.2956255528774054\n",
      "train loss:2.304035302953432\n",
      "train loss:2.3067346196753085\n",
      "train loss:2.2959112889310314\n",
      "train loss:2.3036387449356526\n",
      "train loss:2.2967880515654313\n",
      "train loss:2.2928704094445775\n",
      "train loss:2.300278694147814\n",
      "train loss:2.2972514068550143\n",
      "train loss:2.2959274364468643\n",
      "train loss:2.3116232706016424\n",
      "train loss:2.297003523148304\n",
      "train loss:2.302134217267158\n",
      "train loss:2.3031949703850905\n",
      "train loss:2.298460259534635\n",
      "train loss:2.2969675664430675\n",
      "train loss:2.2977978388079303\n",
      "train loss:2.30297701466312\n",
      "train loss:2.30097527488153\n",
      "train loss:2.3033460301937203\n",
      "train loss:2.2907161556851983\n",
      "train loss:2.302224202676339\n",
      "train loss:2.300199774017274\n",
      "train loss:2.2929740063591706\n",
      "train loss:2.2948278139006577\n",
      "train loss:2.3008905869082623\n",
      "train loss:2.3027641896675863\n",
      "train loss:2.2936535970024043\n",
      "train loss:2.302978498265772\n",
      "train loss:2.3069718373022328\n",
      "train loss:2.3034206480394146\n",
      "train loss:2.30880793288633\n",
      "train loss:2.3056879727196757\n",
      "train loss:2.292377939948124\n",
      "train loss:2.311689926738782\n",
      "train loss:2.307976691972796\n",
      "train loss:2.3051135830759173\n",
      "train loss:2.299221647208188\n",
      "train loss:2.295195152612086\n",
      "train loss:2.2969432475297866\n",
      "train loss:2.303637894761402\n",
      "train loss:2.2960031917640116\n",
      "train loss:2.2950947003191677\n",
      "train loss:2.3057232451509884\n",
      "train loss:2.3045595250988744\n",
      "train loss:2.3050381064586767\n",
      "train loss:2.290454145632267\n",
      "=== epoch:90, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.298074326506172\n",
      "train loss:2.3053274603360046\n",
      "train loss:2.29927570393364\n",
      "train loss:2.315223844148908\n",
      "train loss:2.3051635102263743\n",
      "train loss:2.3025455199567624\n",
      "train loss:2.2997358875081346\n",
      "train loss:2.2961028107019286\n",
      "train loss:2.2850121344518963\n",
      "train loss:2.2975519845238845\n",
      "train loss:2.3050608084464175\n",
      "train loss:2.300795024725298\n",
      "train loss:2.3112187937955655\n",
      "train loss:2.3001478230476495\n",
      "train loss:2.301906489345718\n",
      "train loss:2.293666751522239\n",
      "train loss:2.304090823138208\n",
      "train loss:2.2957334286250677\n",
      "train loss:2.3077282275296196\n",
      "train loss:2.3043686614527195\n",
      "train loss:2.3113493432953\n",
      "train loss:2.3023825232005897\n",
      "train loss:2.296235666669496\n",
      "train loss:2.3126271484365577\n",
      "train loss:2.293155708564302\n",
      "train loss:2.2985935752776347\n",
      "train loss:2.3055478010383923\n",
      "train loss:2.301780278615032\n",
      "train loss:2.301142727443712\n",
      "train loss:2.303944162253161\n",
      "train loss:2.2917754232003107\n",
      "train loss:2.3034085309435213\n",
      "train loss:2.300550762069555\n",
      "train loss:2.2983135724372734\n",
      "train loss:2.3009620009651695\n",
      "train loss:2.303091921971131\n",
      "train loss:2.315160708233485\n",
      "train loss:2.3072426981034635\n",
      "train loss:2.295891003659536\n",
      "train loss:2.298177193386538\n",
      "train loss:2.309856902987699\n",
      "train loss:2.303715800216397\n",
      "train loss:2.3057183585881926\n",
      "train loss:2.310120685668698\n",
      "train loss:2.3029246200646822\n",
      "train loss:2.3013098698534873\n",
      "train loss:2.3016242293873055\n",
      "train loss:2.2965304309518135\n",
      "train loss:2.2973823790649464\n",
      "train loss:2.3034769253381997\n",
      "train loss:2.3001005172233424\n",
      "train loss:2.3006292986750148\n",
      "train loss:2.301016380109405\n",
      "train loss:2.297820819242284\n",
      "train loss:2.291359520797873\n",
      "train loss:2.2962225124929954\n",
      "train loss:2.2987796864339125\n",
      "train loss:2.3026624554940587\n",
      "train loss:2.3017905600088233\n",
      "train loss:2.3061374247691613\n",
      "train loss:2.297609264045168\n",
      "train loss:2.2985658493774475\n",
      "train loss:2.3020208031744467\n",
      "train loss:2.3049129129837618\n",
      "train loss:2.3048947773615547\n",
      "train loss:2.301709204339645\n",
      "train loss:2.303434271773026\n",
      "train loss:2.3042444433249543\n",
      "train loss:2.3054273442407003\n",
      "train loss:2.300608983911879\n",
      "train loss:2.3019737185185405\n",
      "train loss:2.3020653406974807\n",
      "train loss:2.2941275245396158\n",
      "train loss:2.296598843250217\n",
      "train loss:2.3038231217040415\n",
      "train loss:2.3011213488730116\n",
      "train loss:2.301299508171785\n",
      "train loss:2.2997281639317664\n",
      "train loss:2.2992092135553537\n",
      "train loss:2.3144882852462207\n",
      "train loss:2.2910772144803815\n",
      "train loss:2.3044515889749584\n",
      "train loss:2.293201934838311\n",
      "train loss:2.2998473830135837\n",
      "train loss:2.295338542307729\n",
      "train loss:2.298282033612152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3002879639679064\n",
      "train loss:2.303561361239211\n",
      "train loss:2.3015416464933587\n",
      "train loss:2.2962791246351903\n",
      "train loss:2.3042600671222817\n",
      "train loss:2.293755919105777\n",
      "train loss:2.3035951662887193\n",
      "train loss:2.2944923183478716\n",
      "train loss:2.301866086614633\n",
      "train loss:2.3003655330560426\n",
      "train loss:2.3061233563652674\n",
      "train loss:2.2991026554326526\n",
      "train loss:2.2966711847988357\n",
      "train loss:2.3023003274337746\n",
      "train loss:2.2943633791483182\n",
      "train loss:2.2988353406311837\n",
      "train loss:2.3010209068318477\n",
      "train loss:2.289350850935461\n",
      "train loss:2.3076630197003007\n",
      "train loss:2.31131390379144\n",
      "train loss:2.2943269870966922\n",
      "train loss:2.3070481292171063\n",
      "train loss:2.3044304631696217\n",
      "train loss:2.3031036726899994\n",
      "train loss:2.299220436527998\n",
      "train loss:2.298948172731241\n",
      "train loss:2.2949379801872802\n",
      "train loss:2.2888055684043853\n",
      "train loss:2.2967322475883094\n",
      "train loss:2.2950153118960523\n",
      "train loss:2.3028505636306176\n",
      "train loss:2.3103321116996742\n",
      "train loss:2.293265564375097\n",
      "train loss:2.301460858915236\n",
      "train loss:2.300891505232866\n",
      "train loss:2.321073380755609\n",
      "train loss:2.3000997264667213\n",
      "train loss:2.298588105218703\n",
      "train loss:2.2997522491388165\n",
      "train loss:2.308585760922957\n",
      "train loss:2.290706285236668\n",
      "train loss:2.303744256359156\n",
      "train loss:2.3076025572196524\n",
      "train loss:2.298590587702834\n",
      "train loss:2.30781584772932\n",
      "train loss:2.3007938245805724\n",
      "train loss:2.301213848298394\n",
      "train loss:2.2940585777343867\n",
      "train loss:2.2996152191851142\n",
      "train loss:2.2916841553764504\n",
      "train loss:2.3098396316229683\n",
      "train loss:2.309844502652252\n",
      "train loss:2.3018845741723175\n",
      "train loss:2.295731491690301\n",
      "train loss:2.2990186712953626\n",
      "train loss:2.3055125573117805\n",
      "train loss:2.291129809228593\n",
      "train loss:2.293894304683841\n",
      "train loss:2.307125814010128\n",
      "train loss:2.2935906182506907\n",
      "train loss:2.3094846269538962\n",
      "train loss:2.30390884990717\n",
      "train loss:2.294761297554531\n",
      "train loss:2.2932698146339976\n",
      "train loss:2.30080840047936\n",
      "train loss:2.2953627243832417\n",
      "train loss:2.2990936680164915\n",
      "train loss:2.2970515904333846\n",
      "train loss:2.3070454359686288\n",
      "train loss:2.298226721918824\n",
      "train loss:2.2995543746457785\n",
      "train loss:2.304338770691093\n",
      "train loss:2.295309159195663\n",
      "train loss:2.298089758378397\n",
      "train loss:2.3126414039797414\n",
      "train loss:2.3023999950617275\n",
      "train loss:2.2969987318232064\n",
      "train loss:2.3006479408165297\n",
      "train loss:2.2989580797093776\n",
      "train loss:2.288868695744884\n",
      "train loss:2.315018797930348\n",
      "train loss:2.3016485028872173\n",
      "train loss:2.310490126372888\n",
      "train loss:2.3049868733317553\n",
      "train loss:2.2969605690497135\n",
      "train loss:2.3066979936174716\n",
      "train loss:2.299711566225755\n",
      "train loss:2.290969038177337\n",
      "train loss:2.290812743455233\n",
      "train loss:2.308709500808324\n",
      "train loss:2.294753748604455\n",
      "train loss:2.3060654225328645\n",
      "train loss:2.2911949670176432\n",
      "train loss:2.300389128189318\n",
      "train loss:2.2995834494688614\n",
      "train loss:2.2988319565785287\n",
      "train loss:2.3044252006184927\n",
      "train loss:2.3015581323340175\n",
      "train loss:2.3035692532744942\n",
      "train loss:2.306679752255239\n",
      "train loss:2.305144114285115\n",
      "train loss:2.308597937782012\n",
      "train loss:2.2985628708587966\n",
      "train loss:2.3007000090829077\n",
      "train loss:2.3008814445003702\n",
      "train loss:2.292128898704998\n",
      "train loss:2.3092546494702146\n",
      "train loss:2.298485270056268\n",
      "train loss:2.2971290155821467\n",
      "train loss:2.299856603154589\n",
      "train loss:2.3014033060274732\n",
      "train loss:2.3049792026680636\n",
      "train loss:2.3036863861068877\n",
      "train loss:2.3072888840235453\n",
      "=== epoch:91, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.299244264668074\n",
      "train loss:2.3027746720147917\n",
      "train loss:2.3016579905926964\n",
      "train loss:2.3008954830609127\n",
      "train loss:2.302019392068218\n",
      "train loss:2.302737054161191\n",
      "train loss:2.3054435497411863\n",
      "train loss:2.296512642312735\n",
      "train loss:2.3112811578908548\n",
      "train loss:2.3053635315773056\n",
      "train loss:2.292509271772678\n",
      "train loss:2.3006874454216493\n",
      "train loss:2.2971818083895843\n",
      "train loss:2.3043965792624554\n",
      "train loss:2.295938644369934\n",
      "train loss:2.2975340259348704\n",
      "train loss:2.3034630540273264\n",
      "train loss:2.30112292834786\n",
      "train loss:2.30599724881967\n",
      "train loss:2.3001699059473597\n",
      "train loss:2.309945960786523\n",
      "train loss:2.3015049142810984\n",
      "train loss:2.2930992727180572\n",
      "train loss:2.3036028697601414\n",
      "train loss:2.2980599374582598\n",
      "train loss:2.2973124641303095\n",
      "train loss:2.304104878750502\n",
      "train loss:2.302293356324765\n",
      "train loss:2.2982834739723867\n",
      "train loss:2.2937135819815135\n",
      "train loss:2.301343979446018\n",
      "train loss:2.2956929134484025\n",
      "train loss:2.2924511282286835\n",
      "train loss:2.3027302371280682\n",
      "train loss:2.3005477875355465\n",
      "train loss:2.312238124026425\n",
      "train loss:2.3038031372163568\n",
      "train loss:2.3033302828922038\n",
      "train loss:2.2973706864137355\n",
      "train loss:2.304363852164454\n",
      "train loss:2.305424886900132\n",
      "train loss:2.301265493193972\n",
      "train loss:2.2964185953430323\n",
      "train loss:2.308086682127257\n",
      "train loss:2.315202917460761\n",
      "train loss:2.2944678216786634\n",
      "train loss:2.3045875584035094\n",
      "train loss:2.299102214838243\n",
      "train loss:2.3122831976044043\n",
      "train loss:2.2993961585614873\n",
      "train loss:2.30409749393599\n",
      "train loss:2.305300158083897\n",
      "train loss:2.3049748092423292\n",
      "train loss:2.3049715021688013\n",
      "train loss:2.3052330529005673\n",
      "train loss:2.3022929257311384\n",
      "train loss:2.293857830461034\n",
      "train loss:2.2914081112476743\n",
      "train loss:2.2991998927979314\n",
      "train loss:2.3025864826231954\n",
      "train loss:2.298011929872398\n",
      "train loss:2.296801827477171\n",
      "train loss:2.29811765079399\n",
      "train loss:2.2879206057021344\n",
      "train loss:2.312362788422049\n",
      "train loss:2.306751181641503\n",
      "train loss:2.3069612957083865\n",
      "train loss:2.3061054594008144\n",
      "train loss:2.3184009473626914\n",
      "train loss:2.31042502703961\n",
      "train loss:2.3059518461728397\n",
      "train loss:2.3024704101359585\n",
      "train loss:2.2907722362197034\n",
      "train loss:2.300345016514114\n",
      "train loss:2.3049616861854307\n",
      "train loss:2.3049332793386523\n",
      "train loss:2.311795244026853\n",
      "train loss:2.3101000061352233\n",
      "train loss:2.297426626992499\n",
      "train loss:2.3054522766270726\n",
      "train loss:2.298764263968205\n",
      "train loss:2.301168066745331\n",
      "train loss:2.3079244175401636\n",
      "train loss:2.3007901335952083\n",
      "train loss:2.2965382803906706\n",
      "train loss:2.303532375174192\n",
      "train loss:2.303639157362545\n",
      "train loss:2.307902997458772\n",
      "train loss:2.3057184802668598\n",
      "train loss:2.307502268082492\n",
      "train loss:2.2958984272697514\n",
      "train loss:2.304038798818639\n",
      "train loss:2.2936706539965765\n",
      "train loss:2.302437144576684\n",
      "train loss:2.2888712612254603\n",
      "train loss:2.3101772522067656\n",
      "train loss:2.3073216763414144\n",
      "train loss:2.2989755651928707\n",
      "train loss:2.3078400310954152\n",
      "train loss:2.3044893914987807\n",
      "train loss:2.298262285272803\n",
      "train loss:2.3014409690376363\n",
      "train loss:2.2948108714423565\n",
      "train loss:2.303689631728044\n",
      "train loss:2.2939791975702817\n",
      "train loss:2.300018847657323\n",
      "train loss:2.3033996456961594\n",
      "train loss:2.2923166471458227\n",
      "train loss:2.3052940438191873\n",
      "train loss:2.296099295228406\n",
      "train loss:2.3048052966476775\n",
      "train loss:2.300674580625206\n",
      "train loss:2.3017623697012883\n",
      "train loss:2.294534930593079\n",
      "train loss:2.30436272673964\n",
      "train loss:2.3048850847286797\n",
      "train loss:2.3039381864519863\n",
      "train loss:2.3012788453582838\n",
      "train loss:2.2978677082355934\n",
      "train loss:2.306305581976483\n",
      "train loss:2.2957788434570547\n",
      "train loss:2.3000692660059263\n",
      "train loss:2.3063236312567437\n",
      "train loss:2.300711584005102\n",
      "train loss:2.301075516863504\n",
      "train loss:2.2902203673497223\n",
      "train loss:2.2980146031605817\n",
      "train loss:2.3080296741472086\n",
      "train loss:2.2927557318513907\n",
      "train loss:2.302506768071506\n",
      "train loss:2.289464664867102\n",
      "train loss:2.302291792096974\n",
      "train loss:2.3027668828328878\n",
      "train loss:2.3030141837815585\n",
      "train loss:2.2961099133421543\n",
      "train loss:2.299191639087612\n",
      "train loss:2.3052668705910633\n",
      "train loss:2.301982146321975\n",
      "train loss:2.2972978823844743\n",
      "train loss:2.3004478613282697\n",
      "train loss:2.2962154927681095\n",
      "train loss:2.310972080131024\n",
      "train loss:2.3015834620122133\n",
      "train loss:2.3028034974907117\n",
      "train loss:2.289156012769005\n",
      "train loss:2.3014199117710454\n",
      "train loss:2.3086138136727605\n",
      "train loss:2.302857206589429\n",
      "train loss:2.30134591610692\n",
      "train loss:2.303871854308824\n",
      "train loss:2.292179322405173\n",
      "train loss:2.2888260259997617\n",
      "train loss:2.2937179564982655\n",
      "train loss:2.3057838412700353\n",
      "train loss:2.303645283026692\n",
      "train loss:2.3070463987894927\n",
      "train loss:2.302346644711926\n",
      "train loss:2.30276128141104\n",
      "train loss:2.299212242140522\n",
      "train loss:2.3010663720137114\n",
      "train loss:2.302942457107464\n",
      "train loss:2.300791211181439\n",
      "train loss:2.293390424446094\n",
      "train loss:2.297327590548916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3023273740635832\n",
      "train loss:2.2992087849913343\n",
      "train loss:2.293894793197783\n",
      "train loss:2.2941756824558204\n",
      "train loss:2.2990781130550633\n",
      "train loss:2.304881994276099\n",
      "train loss:2.3033667078297233\n",
      "train loss:2.305158766069451\n",
      "train loss:2.3048664469324454\n",
      "train loss:2.29987488853139\n",
      "train loss:2.3032131596863197\n",
      "train loss:2.2947642399503825\n",
      "train loss:2.2978710986646798\n",
      "train loss:2.3022475277876455\n",
      "train loss:2.3008647567475795\n",
      "train loss:2.306040865435579\n",
      "train loss:2.2912639424626335\n",
      "train loss:2.296328144707091\n",
      "train loss:2.301743071506021\n",
      "train loss:2.300273675681858\n",
      "train loss:2.2997146875248173\n",
      "train loss:2.298469180891454\n",
      "train loss:2.299835026492681\n",
      "train loss:2.2930033745425655\n",
      "train loss:2.2944798570403653\n",
      "train loss:2.2965657978511307\n",
      "train loss:2.294453072084192\n",
      "train loss:2.302288389826213\n",
      "train loss:2.304888115972883\n",
      "train loss:2.291179813993554\n",
      "train loss:2.300872380761529\n",
      "train loss:2.302247975272722\n",
      "train loss:2.3104553529678897\n",
      "train loss:2.306955657368702\n",
      "train loss:2.3063674123331728\n",
      "train loss:2.2942436546960403\n",
      "=== epoch:92, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.291840810803963\n",
      "train loss:2.299598605133007\n",
      "train loss:2.3050091274740594\n",
      "train loss:2.3058688518591537\n",
      "train loss:2.302566698856162\n",
      "train loss:2.2976252131248978\n",
      "train loss:2.2976224898076656\n",
      "train loss:2.3010359119290116\n",
      "train loss:2.2966804272853545\n",
      "train loss:2.3086080857797\n",
      "train loss:2.3054982691558012\n",
      "train loss:2.307076394127894\n",
      "train loss:2.3057528778833096\n",
      "train loss:2.3064032159758354\n",
      "train loss:2.2947047028603893\n",
      "train loss:2.2984786869118063\n",
      "train loss:2.311065619479038\n",
      "train loss:2.2981048608681363\n",
      "train loss:2.300191095981128\n",
      "train loss:2.29354222567583\n",
      "train loss:2.2995472746463372\n",
      "train loss:2.2932673744472014\n",
      "train loss:2.296161842608714\n",
      "train loss:2.299453699853336\n",
      "train loss:2.29177969296209\n",
      "train loss:2.3004302639415575\n",
      "train loss:2.297151016191564\n",
      "train loss:2.2917662280169346\n",
      "train loss:2.3093488450197985\n",
      "train loss:2.2959564394461047\n",
      "train loss:2.3006317542600416\n",
      "train loss:2.302459063201449\n",
      "train loss:2.2962250648863054\n",
      "train loss:2.305160584272366\n",
      "train loss:2.302275905680617\n",
      "train loss:2.3082470200506378\n",
      "train loss:2.2996144594389634\n",
      "train loss:2.2947738589566056\n",
      "train loss:2.3028550945433643\n",
      "train loss:2.2954633799622055\n",
      "train loss:2.3025675347781616\n",
      "train loss:2.300332539814962\n",
      "train loss:2.295870343908415\n",
      "train loss:2.3064656068327376\n",
      "train loss:2.296419715506814\n",
      "train loss:2.2972994645353886\n",
      "train loss:2.292245966755539\n",
      "train loss:2.3055874061582995\n",
      "train loss:2.3001960298356194\n",
      "train loss:2.3066345815266422\n",
      "train loss:2.2978061822231943\n",
      "train loss:2.302332267025679\n",
      "train loss:2.294317830429361\n",
      "train loss:2.2990059235610203\n",
      "train loss:2.304806848423349\n",
      "train loss:2.304179907964704\n",
      "train loss:2.305822165359042\n",
      "train loss:2.3078031656899585\n",
      "train loss:2.3009225531702526\n",
      "train loss:2.2928409120327222\n",
      "train loss:2.296189697226498\n",
      "train loss:2.313044076557242\n",
      "train loss:2.3066792473841176\n",
      "train loss:2.299030206998723\n",
      "train loss:2.2996269311204216\n",
      "train loss:2.3013387738621076\n",
      "train loss:2.2935389118159217\n",
      "train loss:2.301767414283794\n",
      "train loss:2.3004089902847986\n",
      "train loss:2.306501726393969\n",
      "train loss:2.3029762624082015\n",
      "train loss:2.2899929969494\n",
      "train loss:2.306562052194044\n",
      "train loss:2.302508645260456\n",
      "train loss:2.3105260655426583\n",
      "train loss:2.312276399768271\n",
      "train loss:2.2952072078139585\n",
      "train loss:2.3040204224339536\n",
      "train loss:2.3047382314798\n",
      "train loss:2.2899893174082746\n",
      "train loss:2.2990014989758816\n",
      "train loss:2.3012249361043837\n",
      "train loss:2.3069041448621577\n",
      "train loss:2.294762895403752\n",
      "train loss:2.2939460593014274\n",
      "train loss:2.3034032710079266\n",
      "train loss:2.2987283835076644\n",
      "train loss:2.310844979204388\n",
      "train loss:2.3003821637520017\n",
      "train loss:2.3024952344699288\n",
      "train loss:2.306287249216531\n",
      "train loss:2.303728056171758\n",
      "train loss:2.3046848880052235\n",
      "train loss:2.3029060122899483\n",
      "train loss:2.2948436286472442\n",
      "train loss:2.29435401250895\n",
      "train loss:2.304815267227699\n",
      "train loss:2.3034842561607496\n",
      "train loss:2.30310861187237\n",
      "train loss:2.3013802483934294\n",
      "train loss:2.3018215254349563\n",
      "train loss:2.305103147596031\n",
      "train loss:2.2971239497493583\n",
      "train loss:2.3157187708209457\n",
      "train loss:2.30061293573254\n",
      "train loss:2.3068809025137966\n",
      "train loss:2.295131283477214\n",
      "train loss:2.298973837289319\n",
      "train loss:2.299411026213756\n",
      "train loss:2.301596820506\n",
      "train loss:2.3016153243582695\n",
      "train loss:2.2982691808837683\n",
      "train loss:2.2986273246869224\n",
      "train loss:2.294764974242783\n",
      "train loss:2.303267703463595\n",
      "train loss:2.30739015946949\n",
      "train loss:2.292194069570796\n",
      "train loss:2.3001763048035824\n",
      "train loss:2.2985654899382117\n",
      "train loss:2.30682954029553\n",
      "train loss:2.3029646019014196\n",
      "train loss:2.303980808752221\n",
      "train loss:2.3018813987589053\n",
      "train loss:2.297353838266456\n",
      "train loss:2.308142205838215\n",
      "train loss:2.300453804383537\n",
      "train loss:2.3093868378939235\n",
      "train loss:2.3001638637119983\n",
      "train loss:2.3103412392328075\n",
      "train loss:2.3012335887697843\n",
      "train loss:2.293235781390619\n",
      "train loss:2.294596345748344\n",
      "train loss:2.3087627046846793\n",
      "train loss:2.3042017536302346\n",
      "train loss:2.303880493490989\n",
      "train loss:2.2991875396091954\n",
      "train loss:2.301730020946032\n",
      "train loss:2.3048407606361665\n",
      "train loss:2.312373206086225\n",
      "train loss:2.304920347606011\n",
      "train loss:2.300320606911569\n",
      "train loss:2.295412736979106\n",
      "train loss:2.3053431712220234\n",
      "train loss:2.3015981480505276\n",
      "train loss:2.310138296273335\n",
      "train loss:2.3021565011296548\n",
      "train loss:2.299317001320605\n",
      "train loss:2.3044855208440436\n",
      "train loss:2.3041476538490215\n",
      "train loss:2.2989751971665657\n",
      "train loss:2.3013293771110934\n",
      "train loss:2.305833692244466\n",
      "train loss:2.2952216763960376\n",
      "train loss:2.2991739098418957\n",
      "train loss:2.2978416377403614\n",
      "train loss:2.2929044803336676\n",
      "train loss:2.2972201938707952\n",
      "train loss:2.2959740428577566\n",
      "train loss:2.3047065228519483\n",
      "train loss:2.297687834420868\n",
      "train loss:2.3044924774656113\n",
      "train loss:2.2942426017253186\n",
      "train loss:2.3055132000734058\n",
      "train loss:2.2939342617482654\n",
      "train loss:2.292637377931429\n",
      "train loss:2.2914521171562083\n",
      "train loss:2.3076060583711895\n",
      "train loss:2.305186816427575\n",
      "train loss:2.310522110639671\n",
      "train loss:2.300965345000422\n",
      "train loss:2.3076590398694794\n",
      "train loss:2.2995899456420172\n",
      "train loss:2.3030512338350135\n",
      "train loss:2.309262827202827\n",
      "train loss:2.299656001249889\n",
      "train loss:2.2925611100754266\n",
      "train loss:2.304671034778139\n",
      "train loss:2.3122924548510855\n",
      "train loss:2.2977418164432772\n",
      "train loss:2.313983785258828\n",
      "train loss:2.304173699805601\n",
      "train loss:2.2993103244665507\n",
      "train loss:2.3059883836177812\n",
      "train loss:2.299735490728601\n",
      "train loss:2.303758710784895\n",
      "train loss:2.3080757437743626\n",
      "train loss:2.3026315107944333\n",
      "train loss:2.3083067120932945\n",
      "train loss:2.3124303452378117\n",
      "train loss:2.2970796412256105\n",
      "train loss:2.2921486323688556\n",
      "train loss:2.302545460027577\n",
      "train loss:2.3064803032004417\n",
      "train loss:2.3025150596665185\n",
      "train loss:2.309035323660718\n",
      "train loss:2.3008301151266433\n",
      "train loss:2.3054091129351355\n",
      "train loss:2.300727930693987\n",
      "train loss:2.302106981146584\n",
      "train loss:2.304610530027695\n",
      "=== epoch:93, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.297178527237385\n",
      "train loss:2.3037464998079358\n",
      "train loss:2.3003104542844137\n",
      "train loss:2.299946793055513\n",
      "train loss:2.301959376600079\n",
      "train loss:2.2998281337531714\n",
      "train loss:2.3127755826308927\n",
      "train loss:2.2982231302767775\n",
      "train loss:2.2979565966078472\n",
      "train loss:2.29607225316734\n",
      "train loss:2.315278415347753\n",
      "train loss:2.296776204402217\n",
      "train loss:2.3037197460189067\n",
      "train loss:2.305140846345237\n",
      "train loss:2.304065561285885\n",
      "train loss:2.29594009732782\n",
      "train loss:2.2921670405151944\n",
      "train loss:2.3054437726046673\n",
      "train loss:2.3025136111669013\n",
      "train loss:2.3048494039300014\n",
      "train loss:2.297086255939005\n",
      "train loss:2.3029447858295335\n",
      "train loss:2.3040466419482986\n",
      "train loss:2.2964734081747302\n",
      "train loss:2.2958826086539523\n",
      "train loss:2.300728019430368\n",
      "train loss:2.3000707831593545\n",
      "train loss:2.2969388610286012\n",
      "train loss:2.298093305398565\n",
      "train loss:2.2958793997162434\n",
      "train loss:2.3054292867003414\n",
      "train loss:2.300450252262372\n",
      "train loss:2.312398630874398\n",
      "train loss:2.3049313730405427\n",
      "train loss:2.301065296739138\n",
      "train loss:2.303926645263461\n",
      "train loss:2.3013522937828332\n",
      "train loss:2.305582230205734\n",
      "train loss:2.300938366223527\n",
      "train loss:2.304890042306841\n",
      "train loss:2.303006285413553\n",
      "train loss:2.309552517956005\n",
      "train loss:2.303051578653023\n",
      "train loss:2.3019764252690327\n",
      "train loss:2.3029614572128403\n",
      "train loss:2.2978803706026403\n",
      "train loss:2.299518532310839\n",
      "train loss:2.3044668040692464\n",
      "train loss:2.303529986210046\n",
      "train loss:2.300511385403877\n",
      "train loss:2.30790395766148\n",
      "train loss:2.3014001701468665\n",
      "train loss:2.296125676673744\n",
      "train loss:2.2990745335460576\n",
      "train loss:2.3143042407689296\n",
      "train loss:2.303559037395648\n",
      "train loss:2.3024379378563826\n",
      "train loss:2.3046564510669287\n",
      "train loss:2.3080747658246223\n",
      "train loss:2.304255282633007\n",
      "train loss:2.3050767186154544\n",
      "train loss:2.3089214587844697\n",
      "train loss:2.3057124067537274\n",
      "train loss:2.298605911539846\n",
      "train loss:2.311966600285918\n",
      "train loss:2.30428269218052\n",
      "train loss:2.302960080767057\n",
      "train loss:2.299554020560256\n",
      "train loss:2.2951933306418084\n",
      "train loss:2.3056905686151254\n",
      "train loss:2.2899946915059846\n",
      "train loss:2.289801625310774\n",
      "train loss:2.3011730083169626\n",
      "train loss:2.304196215956791\n",
      "train loss:2.3027084253995067\n",
      "train loss:2.3046224936767254\n",
      "train loss:2.3029121568075976\n",
      "train loss:2.307644897619313\n",
      "train loss:2.3020630499903434\n",
      "train loss:2.302285290846907\n",
      "train loss:2.3021018072916\n",
      "train loss:2.3051641333735926\n",
      "train loss:2.30694392187794\n",
      "train loss:2.3025029431212665\n",
      "train loss:2.2978700488459087\n",
      "train loss:2.305751927844064\n",
      "train loss:2.3019131847854943\n",
      "train loss:2.3091671277816834\n",
      "train loss:2.303271116225199\n",
      "train loss:2.3026912395759704\n",
      "train loss:2.3012184396694026\n",
      "train loss:2.293585762311343\n",
      "train loss:2.303725410130771\n",
      "train loss:2.298523797330558\n",
      "train loss:2.3059665322889034\n",
      "train loss:2.2992285824807293\n",
      "train loss:2.2906293771741146\n",
      "train loss:2.3034561958660045\n",
      "train loss:2.297971442509573\n",
      "train loss:2.298283929898994\n",
      "train loss:2.2995565168402594\n",
      "train loss:2.2966200683323432\n",
      "train loss:2.2982191824478684\n",
      "train loss:2.298782685411329\n",
      "train loss:2.3003091111633585\n",
      "train loss:2.311035019947331\n",
      "train loss:2.3009748749423453\n",
      "train loss:2.299973932700295\n",
      "train loss:2.2988535822522334\n",
      "train loss:2.301738101884405\n",
      "train loss:2.304852660512952\n",
      "train loss:2.307486158763498\n",
      "train loss:2.2982978975843085\n",
      "train loss:2.301212035868743\n",
      "train loss:2.301421905484664\n",
      "train loss:2.3072011883549104\n",
      "train loss:2.3081553793216467\n",
      "train loss:2.296091021475833\n",
      "train loss:2.2976011346348257\n",
      "train loss:2.2998877079519175\n",
      "train loss:2.304612629621238\n",
      "train loss:2.3032074567020215\n",
      "train loss:2.3047224208389303\n",
      "train loss:2.3058371137571343\n",
      "train loss:2.304532102823514\n",
      "train loss:2.30786065566455\n",
      "train loss:2.307633808464325\n",
      "train loss:2.299167575758674\n",
      "train loss:2.300277316124158\n",
      "train loss:2.3024649241581256\n",
      "train loss:2.304567075478647\n",
      "train loss:2.3068299148498483\n",
      "train loss:2.2987915378432917\n",
      "train loss:2.3074499470258902\n",
      "train loss:2.305881282823327\n",
      "train loss:2.3013386465891115\n",
      "train loss:2.294156413922207\n",
      "train loss:2.295384324312565\n",
      "train loss:2.3028501099872316\n",
      "train loss:2.3112251712423477\n",
      "train loss:2.298629459813376\n",
      "train loss:2.2983556915830725\n",
      "train loss:2.2980906911091763\n",
      "train loss:2.3031318533993486\n",
      "train loss:2.29638788056692\n",
      "train loss:2.3098926092013468\n",
      "train loss:2.303466822048136\n",
      "train loss:2.310128609255576\n",
      "train loss:2.299029981632418\n",
      "train loss:2.2957161744027967\n",
      "train loss:2.303130537130023\n",
      "train loss:2.300369824358461\n",
      "train loss:2.306130705438853\n",
      "train loss:2.297587013102072\n",
      "train loss:2.295233551661948\n",
      "train loss:2.305863672136512\n",
      "train loss:2.3017466303351877\n",
      "train loss:2.305834006342284\n",
      "train loss:2.2997817696063265\n",
      "train loss:2.3067541511852228\n",
      "train loss:2.3033520094061743\n",
      "train loss:2.307631036042392\n",
      "train loss:2.2967729858252626\n",
      "train loss:2.3052446967856586\n",
      "train loss:2.2985554776974344\n",
      "train loss:2.3107283308480544\n",
      "train loss:2.296115868930136\n",
      "train loss:2.311217479980322\n",
      "train loss:2.3083857725945998\n",
      "train loss:2.3009016387962475\n",
      "train loss:2.3039785375729176\n",
      "train loss:2.2977245538038593\n",
      "train loss:2.3046294269241954\n",
      "train loss:2.3025949679648443\n",
      "train loss:2.3013033165861905\n",
      "train loss:2.300909042829545\n",
      "train loss:2.2951880987075937\n",
      "train loss:2.3036528672676226\n",
      "train loss:2.310245794232977\n",
      "train loss:2.3056252441239784\n",
      "train loss:2.292862637512865\n",
      "train loss:2.3009260364200284\n",
      "train loss:2.3037509150703617\n",
      "train loss:2.2978598530459844\n",
      "train loss:2.300985541103191\n",
      "train loss:2.303207296237461\n",
      "train loss:2.2995484104873074\n",
      "train loss:2.3000707873774333\n",
      "train loss:2.292330004940238\n",
      "train loss:2.3041377607831453\n",
      "train loss:2.3024562922428053\n",
      "train loss:2.2970823807980825\n",
      "train loss:2.3020442984644442\n",
      "train loss:2.3018045563446083\n",
      "train loss:2.297726457057175\n",
      "train loss:2.3002693478476575\n",
      "train loss:2.300649122555234\n",
      "train loss:2.305955459766721\n",
      "train loss:2.29938102578507\n",
      "train loss:2.293545921080542\n",
      "=== epoch:94, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3031357330271836\n",
      "train loss:2.298559492138995\n",
      "train loss:2.2964632620397367\n",
      "train loss:2.2974396651143647\n",
      "train loss:2.306638771725609\n",
      "train loss:2.304409870261863\n",
      "train loss:2.309391348990325\n",
      "train loss:2.3065767826929284\n",
      "train loss:2.2977422612216003\n",
      "train loss:2.2974435288191315\n",
      "train loss:2.302495257707185\n",
      "train loss:2.3018995542098106\n",
      "train loss:2.3055196814094674\n",
      "train loss:2.2980625866484305\n",
      "train loss:2.307883546979061\n",
      "train loss:2.298538152491842\n",
      "train loss:2.2940279852811916\n",
      "train loss:2.3044754524811535\n",
      "train loss:2.3049458433943726\n",
      "train loss:2.2990699792673914\n",
      "train loss:2.2968477879286704\n",
      "train loss:2.2998502054311265\n",
      "train loss:2.299841918497895\n",
      "train loss:2.299712324413923\n",
      "train loss:2.2982356181877592\n",
      "train loss:2.2997503185367165\n",
      "train loss:2.2978684711122814\n",
      "train loss:2.2990891352487095\n",
      "train loss:2.312304871171006\n",
      "train loss:2.30691577259621\n",
      "train loss:2.2951478640206595\n",
      "train loss:2.299744562166656\n",
      "train loss:2.305981450015046\n",
      "train loss:2.3016655892395086\n",
      "train loss:2.301164658675636\n",
      "train loss:2.300278739719237\n",
      "train loss:2.3055366307784873\n",
      "train loss:2.3033948796613077\n",
      "train loss:2.307491521190281\n",
      "train loss:2.2988677667783137\n",
      "train loss:2.3066493158681163\n",
      "train loss:2.289477623900782\n",
      "train loss:2.2969565685045374\n",
      "train loss:2.3065753099336854\n",
      "train loss:2.3060176418937868\n",
      "train loss:2.2968076331726373\n",
      "train loss:2.3040442322936934\n",
      "train loss:2.3148544875751558\n",
      "train loss:2.2977986659822536\n",
      "train loss:2.3049518016757586\n",
      "train loss:2.3096672741662947\n",
      "train loss:2.3075551543527926\n",
      "train loss:2.2993500656459185\n",
      "train loss:2.298811082742195\n",
      "train loss:2.3062389870299347\n",
      "train loss:2.3002303072322823\n",
      "train loss:2.3040990050860892\n",
      "train loss:2.3090522823574178\n",
      "train loss:2.3028150961975236\n",
      "train loss:2.3000054114622084\n",
      "train loss:2.3104717424690056\n",
      "train loss:2.2978659406950435\n",
      "train loss:2.297861543724655\n",
      "train loss:2.298860253883261\n",
      "train loss:2.2990713295116416\n",
      "train loss:2.30060254126142\n",
      "train loss:2.308976891532491\n",
      "train loss:2.2995437932626412\n",
      "train loss:2.298059609898873\n",
      "train loss:2.297763851632302\n",
      "train loss:2.307357464626882\n",
      "train loss:2.297526941428499\n",
      "train loss:2.2975849640763477\n",
      "train loss:2.297563485011284\n",
      "train loss:2.298660690699946\n",
      "train loss:2.2969269616935053\n",
      "train loss:2.3040346437560357\n",
      "train loss:2.2998021974421707\n",
      "train loss:2.301969693958712\n",
      "train loss:2.2979403471858797\n",
      "train loss:2.300013498153988\n",
      "train loss:2.298588192676407\n",
      "train loss:2.29605090951854\n",
      "train loss:2.301443662075189\n",
      "train loss:2.2921569791648477\n",
      "train loss:2.3015887165952416\n",
      "train loss:2.2974555057630424\n",
      "train loss:2.2927897763312926\n",
      "train loss:2.3085819168566615\n",
      "train loss:2.3035484736193634\n",
      "train loss:2.297999884072605\n",
      "train loss:2.3029802912192046\n",
      "train loss:2.2986844237619084\n",
      "train loss:2.297897613823144\n",
      "train loss:2.299159482124371\n",
      "train loss:2.301000284247678\n",
      "train loss:2.3029118350183313\n",
      "train loss:2.2975791801372893\n",
      "train loss:2.307160505570145\n",
      "train loss:2.300027601774823\n",
      "train loss:2.3065310148425255\n",
      "train loss:2.302884825984865\n",
      "train loss:2.2973909254180596\n",
      "train loss:2.2974641299314666\n",
      "train loss:2.2954220167138484\n",
      "train loss:2.29972951217913\n",
      "train loss:2.287960939388493\n",
      "train loss:2.2956035039942027\n",
      "train loss:2.3045241746652585\n",
      "train loss:2.2976795720128673\n",
      "train loss:2.306497880558142\n",
      "train loss:2.300622657169317\n",
      "train loss:2.2942264098642116\n",
      "train loss:2.3098252623108984\n",
      "train loss:2.302391017643893\n",
      "train loss:2.300829682511484\n",
      "train loss:2.308394459057431\n",
      "train loss:2.304978039360371\n",
      "train loss:2.296282289679499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.304945113837897\n",
      "train loss:2.301684188887629\n",
      "train loss:2.2973559348527455\n",
      "train loss:2.2980176907785617\n",
      "train loss:2.302255087915615\n",
      "train loss:2.30725208651283\n",
      "train loss:2.3051545696975144\n",
      "train loss:2.3067951463621412\n",
      "train loss:2.301467078077689\n",
      "train loss:2.3035208792762525\n",
      "train loss:2.3018819734639546\n",
      "train loss:2.299827769529236\n",
      "train loss:2.303187479957555\n",
      "train loss:2.3058479865800274\n",
      "train loss:2.3036338676001886\n",
      "train loss:2.297424689742583\n",
      "train loss:2.3124996184880264\n",
      "train loss:2.2972008207645085\n",
      "train loss:2.2944477411325894\n",
      "train loss:2.311288914613709\n",
      "train loss:2.305909968096346\n",
      "train loss:2.3022934903548116\n",
      "train loss:2.2995977546716033\n",
      "train loss:2.298676810392007\n",
      "train loss:2.300731062393279\n",
      "train loss:2.301575543917787\n",
      "train loss:2.30518513996881\n",
      "train loss:2.2936579585638706\n",
      "train loss:2.299690853188212\n",
      "train loss:2.308241452227024\n",
      "train loss:2.3018285989858396\n",
      "train loss:2.304279441707408\n",
      "train loss:2.3075537571141527\n",
      "train loss:2.3005960531395546\n",
      "train loss:2.3079600795153827\n",
      "train loss:2.3015141595194004\n",
      "train loss:2.3002907308449716\n",
      "train loss:2.298293115989638\n",
      "train loss:2.294399000945608\n",
      "train loss:2.3023312725756586\n",
      "train loss:2.2987409555993326\n",
      "train loss:2.303450535170593\n",
      "train loss:2.3052274340980574\n",
      "train loss:2.2996100283443304\n",
      "train loss:2.3094888899690926\n",
      "train loss:2.305989090054647\n",
      "train loss:2.3046545929049325\n",
      "train loss:2.3008647146720134\n",
      "train loss:2.303376710451419\n",
      "train loss:2.3010819285451976\n",
      "train loss:2.312671579204724\n",
      "train loss:2.3035101602969488\n",
      "train loss:2.2957706193577905\n",
      "train loss:2.295706436281551\n",
      "train loss:2.3051663710169894\n",
      "train loss:2.307863224139615\n",
      "train loss:2.3057029002768736\n",
      "train loss:2.3087530456049703\n",
      "train loss:2.307659259058045\n",
      "train loss:2.303859658409238\n",
      "train loss:2.2986975564417076\n",
      "train loss:2.3101518654736437\n",
      "train loss:2.3026423028765595\n",
      "train loss:2.300065742242898\n",
      "train loss:2.295317359662864\n",
      "train loss:2.309568680535125\n",
      "train loss:2.296803674423731\n",
      "train loss:2.2992571344508987\n",
      "train loss:2.302657946388885\n",
      "train loss:2.301178856624722\n",
      "train loss:2.307058247743778\n",
      "train loss:2.3019943670609653\n",
      "train loss:2.3039552904475338\n",
      "train loss:2.3028856992291646\n",
      "train loss:2.3075002915627554\n",
      "train loss:2.307284757627748\n",
      "train loss:2.29981181511153\n",
      "train loss:2.2964388614267697\n",
      "train loss:2.29704285592172\n",
      "train loss:2.3123352905259753\n",
      "train loss:2.2969940453992415\n",
      "=== epoch:95, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3040897634847872\n",
      "train loss:2.3057940361015303\n",
      "train loss:2.305925816167757\n",
      "train loss:2.3052771519466413\n",
      "train loss:2.3013563736117586\n",
      "train loss:2.2984607823868526\n",
      "train loss:2.301257546229294\n",
      "train loss:2.303542278304385\n",
      "train loss:2.3029788614328894\n",
      "train loss:2.2965246399372314\n",
      "train loss:2.30832188710087\n",
      "train loss:2.297756426423173\n",
      "train loss:2.2947160313266592\n",
      "train loss:2.3051440558235345\n",
      "train loss:2.3001645303327374\n",
      "train loss:2.298345348799575\n",
      "train loss:2.3079424546766245\n",
      "train loss:2.303522116263274\n",
      "train loss:2.302717076967873\n",
      "train loss:2.2954240722710173\n",
      "train loss:2.301137821125549\n",
      "train loss:2.3022477901642464\n",
      "train loss:2.2995116726233764\n",
      "train loss:2.3043073185987533\n",
      "train loss:2.301767404229999\n",
      "train loss:2.306558798005625\n",
      "train loss:2.299563554823744\n",
      "train loss:2.3012471958858796\n",
      "train loss:2.302372044021258\n",
      "train loss:2.2988792916342495\n",
      "train loss:2.303508145398501\n",
      "train loss:2.3072798532412415\n",
      "train loss:2.294652278265075\n",
      "train loss:2.30450432851602\n",
      "train loss:2.2983129297072136\n",
      "train loss:2.3067883818210175\n",
      "train loss:2.300186080846413\n",
      "train loss:2.2920360173645675\n",
      "train loss:2.303976603530636\n",
      "train loss:2.3062172173048356\n",
      "train loss:2.3037471250156956\n",
      "train loss:2.3065666255928994\n",
      "train loss:2.3093143291684846\n",
      "train loss:2.295467862904501\n",
      "train loss:2.297362835433878\n",
      "train loss:2.304236073850453\n",
      "train loss:2.301365785417169\n",
      "train loss:2.3065403511630285\n",
      "train loss:2.2933451991493166\n",
      "train loss:2.3061399397111844\n",
      "train loss:2.304521422366588\n",
      "train loss:2.298164001030851\n",
      "train loss:2.3044402164283313\n",
      "train loss:2.300978413830768\n",
      "train loss:2.2991157530077873\n",
      "train loss:2.298660195031101\n",
      "train loss:2.2971734748419603\n",
      "train loss:2.299180760438967\n",
      "train loss:2.3056069214734207\n",
      "train loss:2.30273499768172\n",
      "train loss:2.2978883908364973\n",
      "train loss:2.2942602616321377\n",
      "train loss:2.3006626872230997\n",
      "train loss:2.295956941984395\n",
      "train loss:2.301648476786359\n",
      "train loss:2.2987902626415457\n",
      "train loss:2.299399213652811\n",
      "train loss:2.2999688061579193\n",
      "train loss:2.300795554661083\n",
      "train loss:2.3098312722340792\n",
      "train loss:2.3005541163639367\n",
      "train loss:2.289500449171214\n",
      "train loss:2.3016667855314834\n",
      "train loss:2.2981029795583523\n",
      "train loss:2.3081002042595626\n",
      "train loss:2.3071235579035054\n",
      "train loss:2.2959947969373737\n",
      "train loss:2.3040438947004915\n",
      "train loss:2.3018403139941297\n",
      "train loss:2.302304928122328\n",
      "train loss:2.300462824599645\n",
      "train loss:2.303572197586348\n",
      "train loss:2.3101752683009367\n",
      "train loss:2.30294617348875\n",
      "train loss:2.310008396584723\n",
      "train loss:2.3044910973035635\n",
      "train loss:2.298238342044959\n",
      "train loss:2.3019034587733826\n",
      "train loss:2.3016350882766217\n",
      "train loss:2.293927618845438\n",
      "train loss:2.2951679704062453\n",
      "train loss:2.3096112600028005\n",
      "train loss:2.2989656997700023\n",
      "train loss:2.2921480219685755\n",
      "train loss:2.3051269677455783\n",
      "train loss:2.2980476011259374\n",
      "train loss:2.3039720746806283\n",
      "train loss:2.303998209764917\n",
      "train loss:2.301856786185306\n",
      "train loss:2.2980787194826164\n",
      "train loss:2.309847820406053\n",
      "train loss:2.3043382050725136\n",
      "train loss:2.3052689116438936\n",
      "train loss:2.2977474752122853\n",
      "train loss:2.3014600891372594\n",
      "train loss:2.294793650575294\n",
      "train loss:2.295936747505696\n",
      "train loss:2.3039189206503137\n",
      "train loss:2.297552216211608\n",
      "train loss:2.301409344969202\n",
      "train loss:2.285169798093652\n",
      "train loss:2.291913368043517\n",
      "train loss:2.2997731333245057\n",
      "train loss:2.299321669542259\n",
      "train loss:2.3036119705253886\n",
      "train loss:2.3004473014022953\n",
      "train loss:2.300537314589085\n",
      "train loss:2.3066809635667025\n",
      "train loss:2.2995611630687396\n",
      "train loss:2.297238989331552\n",
      "train loss:2.3088636515430383\n",
      "train loss:2.299040673036941\n",
      "train loss:2.3034074241489777\n",
      "train loss:2.297963242164627\n",
      "train loss:2.297485502454986\n",
      "train loss:2.3079042511362124\n",
      "train loss:2.299304561540549\n",
      "train loss:2.303390615009401\n",
      "train loss:2.3041068557702307\n",
      "train loss:2.292884006149523\n",
      "train loss:2.309394977067254\n",
      "train loss:2.2974904418436597\n",
      "train loss:2.298745223971531\n",
      "train loss:2.297414163227577\n",
      "train loss:2.296546534362907\n",
      "train loss:2.297936462150897\n",
      "train loss:2.3096887358874616\n",
      "train loss:2.3008599471319324\n",
      "train loss:2.2951532397249568\n",
      "train loss:2.30553072584696\n",
      "train loss:2.309223015523388\n",
      "train loss:2.298154069985909\n",
      "train loss:2.2998385529285\n",
      "train loss:2.2996078220838516\n",
      "train loss:2.305264894525434\n",
      "train loss:2.3104857765322553\n",
      "train loss:2.2997459812116925\n",
      "train loss:2.3075596114647614\n",
      "train loss:2.3000722232625446\n",
      "train loss:2.304023371957619\n",
      "train loss:2.307035916843724\n",
      "train loss:2.3046111666874975\n",
      "train loss:2.303339937216964\n",
      "train loss:2.311641033695856\n",
      "train loss:2.2976894839835476\n",
      "train loss:2.3003023597886316\n",
      "train loss:2.3031725163349472\n",
      "train loss:2.2987029653577644\n",
      "train loss:2.297571851533683\n",
      "train loss:2.2963406969525035\n",
      "train loss:2.3014139984933757\n",
      "train loss:2.3012884402395937\n",
      "train loss:2.30789759032278\n",
      "train loss:2.300756096473849\n",
      "train loss:2.2994628631421783\n",
      "train loss:2.3011291394353273\n",
      "train loss:2.298491130640684\n",
      "train loss:2.298909531557055\n",
      "train loss:2.300839534461239\n",
      "train loss:2.300999948436904\n",
      "train loss:2.298928523984423\n",
      "train loss:2.297235683407817\n",
      "train loss:2.3054509324089634\n",
      "train loss:2.2993217749745396\n",
      "train loss:2.302762802827503\n",
      "train loss:2.3008028234588678\n",
      "train loss:2.303325267502003\n",
      "train loss:2.301154801727962\n",
      "train loss:2.2952902385377043\n",
      "train loss:2.296692547711579\n",
      "train loss:2.301143933612731\n",
      "train loss:2.2983512596386015\n",
      "train loss:2.3049002610708063\n",
      "train loss:2.296063247247447\n",
      "train loss:2.3001710303537317\n",
      "train loss:2.3070497605731255\n",
      "train loss:2.2974891973960925\n",
      "train loss:2.3006786455974413\n",
      "train loss:2.292596057726337\n",
      "train loss:2.3043987427695436\n",
      "train loss:2.2976620333990416\n",
      "train loss:2.2963853327137937\n",
      "train loss:2.3089828595816893\n",
      "train loss:2.302385847808455\n",
      "train loss:2.3000423802924774\n",
      "train loss:2.2991058416098475\n",
      "train loss:2.3012498935689467\n",
      "train loss:2.3005100883679703\n",
      "train loss:2.308984381360441\n",
      "train loss:2.298806335503544\n",
      "=== epoch:96, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3064935782310645\n",
      "train loss:2.3058183448143006\n",
      "train loss:2.3015685623293223\n",
      "train loss:2.3005333163430595\n",
      "train loss:2.2995977211676917\n",
      "train loss:2.2914874909265732\n",
      "train loss:2.2876038750912886\n",
      "train loss:2.291341620899022\n",
      "train loss:2.295458617649224\n",
      "train loss:2.296193471428034\n",
      "train loss:2.301281753943337\n",
      "train loss:2.308606029794348\n",
      "train loss:2.3086358999879444\n",
      "train loss:2.3052294852786037\n",
      "train loss:2.3035012325423856\n",
      "train loss:2.301994687977477\n",
      "train loss:2.3020971019904906\n",
      "train loss:2.302403298401837\n",
      "train loss:2.3019898712468607\n",
      "train loss:2.2938995791684333\n",
      "train loss:2.291240856964554\n",
      "train loss:2.3086555499592256\n",
      "train loss:2.2981180193499164\n",
      "train loss:2.2936117152160023\n",
      "train loss:2.306603287705663\n",
      "train loss:2.2982252747886047\n",
      "train loss:2.3042551889034715\n",
      "train loss:2.2969575745894835\n",
      "train loss:2.2950143293145935\n",
      "train loss:2.2997678443508986\n",
      "train loss:2.301713677245212\n",
      "train loss:2.301165498908095\n",
      "train loss:2.3024092291658658\n",
      "train loss:2.3028550436283464\n",
      "train loss:2.304547809577323\n",
      "train loss:2.3031389157270685\n",
      "train loss:2.3024513959390953\n",
      "train loss:2.3002911989049277\n",
      "train loss:2.2946731755884704\n",
      "train loss:2.299851374739506\n",
      "train loss:2.286950424278677\n",
      "train loss:2.302180706229002\n",
      "train loss:2.297504893201425\n",
      "train loss:2.303006954080663\n",
      "train loss:2.2961562575517185\n",
      "train loss:2.3003859567163896\n",
      "train loss:2.285458800773149\n",
      "train loss:2.2994908477640235\n",
      "train loss:2.2997414487900754\n",
      "train loss:2.2959597598899673\n",
      "train loss:2.3062086032098708\n",
      "train loss:2.294014518273554\n",
      "train loss:2.303533742752229\n",
      "train loss:2.3082974587312752\n",
      "train loss:2.3050578016894128\n",
      "train loss:2.2915751115153706\n",
      "train loss:2.303849075750953\n",
      "train loss:2.3047886563449835\n",
      "train loss:2.3096632235021777\n",
      "train loss:2.3000450017151755\n",
      "train loss:2.298999532636938\n",
      "train loss:2.2968432139623243\n",
      "train loss:2.299186033053521\n",
      "train loss:2.2974891429389626\n",
      "train loss:2.3045132367171752\n",
      "train loss:2.310762793495711\n",
      "train loss:2.304986723404594\n",
      "train loss:2.29723516182654\n",
      "train loss:2.291027496657023\n",
      "train loss:2.3049001300949277\n",
      "train loss:2.304880775746707\n",
      "train loss:2.299535726573566\n",
      "train loss:2.3027560661637043\n",
      "train loss:2.2986541163405874\n",
      "train loss:2.3064114953633155\n",
      "train loss:2.3031953376413297\n",
      "train loss:2.3002490677828384\n",
      "train loss:2.3046071758570017\n",
      "train loss:2.2985210931222784\n",
      "train loss:2.296269507444881\n",
      "train loss:2.308412586399676\n",
      "train loss:2.3121692039068065\n",
      "train loss:2.307539877779626\n",
      "train loss:2.30889627105881\n",
      "train loss:2.2970807591596816\n",
      "train loss:2.2896428001862854\n",
      "train loss:2.299572496474429\n",
      "train loss:2.3002855822081893\n",
      "train loss:2.2998520988561966\n",
      "train loss:2.28884222050084\n",
      "train loss:2.302991467853333\n",
      "train loss:2.3105317174368047\n",
      "train loss:2.300618564984042\n",
      "train loss:2.2964390971837187\n",
      "train loss:2.2969130743709454\n",
      "train loss:2.292816561844562\n",
      "train loss:2.308655642446224\n",
      "train loss:2.3141378738150093\n",
      "train loss:2.30498138722835\n",
      "train loss:2.3047873156509815\n",
      "train loss:2.304339994427634\n",
      "train loss:2.2948202744028947\n",
      "train loss:2.304579202930618\n",
      "train loss:2.305551394324643\n",
      "train loss:2.291515626073652\n",
      "train loss:2.3029271684603936\n",
      "train loss:2.3005677297302745\n",
      "train loss:2.2997026632996516\n",
      "train loss:2.296593846906022\n",
      "train loss:2.29716015861087\n",
      "train loss:2.3082494208571123\n",
      "train loss:2.3056142484369535\n",
      "train loss:2.3055359926538035\n",
      "train loss:2.2985770252715456\n",
      "train loss:2.297460342222408\n",
      "train loss:2.299584033430662\n",
      "train loss:2.3024893776697875\n",
      "train loss:2.3058012133003416\n",
      "train loss:2.2985367735867404\n",
      "train loss:2.298307864471804\n",
      "train loss:2.30175581287201\n",
      "train loss:2.3114743197322283\n",
      "train loss:2.29881909533721\n",
      "train loss:2.302712869213221\n",
      "train loss:2.3024298571583444\n",
      "train loss:2.3044372428063595\n",
      "train loss:2.2931332441350305\n",
      "train loss:2.2972396662221897\n",
      "train loss:2.298096130764251\n",
      "train loss:2.3048556702403924\n",
      "train loss:2.3009014218613975\n",
      "train loss:2.3020980321420157\n",
      "train loss:2.2995507920902103\n",
      "train loss:2.3042495758843544\n",
      "train loss:2.295555674967462\n",
      "train loss:2.3030543723563826\n",
      "train loss:2.3012428568236536\n",
      "train loss:2.3039620379729024\n",
      "train loss:2.2976748900942954\n",
      "train loss:2.300345664754386\n",
      "train loss:2.297248859996801\n",
      "train loss:2.298199804473169\n",
      "train loss:2.2954202732092805\n",
      "train loss:2.300255078080188\n",
      "train loss:2.303166654632283\n",
      "train loss:2.2957495969914277\n",
      "train loss:2.307300279499373\n",
      "train loss:2.3105716772096248\n",
      "train loss:2.297749423180256\n",
      "train loss:2.3020488018979215\n",
      "train loss:2.3068932956269563\n",
      "train loss:2.2983090017929166\n",
      "train loss:2.297923102854663\n",
      "train loss:2.3002294828161025\n",
      "train loss:2.306057930745564\n",
      "train loss:2.302945278976307\n",
      "train loss:2.30259130691254\n",
      "train loss:2.299295861563215\n",
      "train loss:2.305549506425987\n",
      "train loss:2.3073643649347098\n",
      "train loss:2.304150796510972\n",
      "train loss:2.304353143371232\n",
      "train loss:2.300562927549674\n",
      "train loss:2.29842821171473\n",
      "train loss:2.3050630992490917\n",
      "train loss:2.300140389000875\n",
      "train loss:2.306256066593939\n",
      "train loss:2.3041598683910562\n",
      "train loss:2.299623176912856\n",
      "train loss:2.3050987530905624\n",
      "train loss:2.303327617639272\n",
      "train loss:2.3064693407498855\n",
      "train loss:2.3071787947767284\n",
      "train loss:2.305975475587383\n",
      "train loss:2.3150779134793154\n",
      "train loss:2.29091622926346\n",
      "train loss:2.3015567247930653\n",
      "train loss:2.3089505780311366\n",
      "train loss:2.2964801579187992\n",
      "train loss:2.3026480830747493\n",
      "train loss:2.302285249836187\n",
      "train loss:2.309891665369496\n",
      "train loss:2.3073965054720817\n",
      "train loss:2.3007593493072314\n",
      "train loss:2.2940523257427867\n",
      "train loss:2.296087860892022\n",
      "train loss:2.306241559351128\n",
      "train loss:2.298476936172628\n",
      "train loss:2.2969606110781426\n",
      "train loss:2.3015049090316206\n",
      "train loss:2.3082546381082314\n",
      "train loss:2.299767234450008\n",
      "train loss:2.313900806205224\n",
      "train loss:2.29469421164798\n",
      "train loss:2.2938637790573395\n",
      "train loss:2.3026422335898697\n",
      "train loss:2.3007544396475885\n",
      "train loss:2.3060859517800028\n",
      "train loss:2.301819756645421\n",
      "train loss:2.287421473976696\n",
      "=== epoch:97, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2963067534959927\n",
      "train loss:2.302407331531483\n",
      "train loss:2.2991140497918163\n",
      "train loss:2.3051907451185287\n",
      "train loss:2.3045407105342886\n",
      "train loss:2.3012833958541363\n",
      "train loss:2.306796602119991\n",
      "train loss:2.307154110299993\n",
      "train loss:2.2989511103257896\n",
      "train loss:2.3085211462596495\n",
      "train loss:2.3047514909536013\n",
      "train loss:2.3062145151953595\n",
      "train loss:2.296342654812961\n",
      "train loss:2.300535016131164\n",
      "train loss:2.3040805245449874\n",
      "train loss:2.306335395484664\n",
      "train loss:2.30273776337002\n",
      "train loss:2.30232391935707\n",
      "train loss:2.298542195980879\n",
      "train loss:2.292871026378025\n",
      "train loss:2.301322160197475\n",
      "train loss:2.303223999457915\n",
      "train loss:2.3104409343409698\n",
      "train loss:2.3086953050973\n",
      "train loss:2.297468652171878\n",
      "train loss:2.298428414510996\n",
      "train loss:2.301651227664249\n",
      "train loss:2.3010654701557387\n",
      "train loss:2.2986721772372576\n",
      "train loss:2.307372989529663\n",
      "train loss:2.2998347686948315\n",
      "train loss:2.306414232627421\n",
      "train loss:2.300841018953372\n",
      "train loss:2.303736691594018\n",
      "train loss:2.2982389167494577\n",
      "train loss:2.2993215440493144\n",
      "train loss:2.3018428404887774\n",
      "train loss:2.3117120889786054\n",
      "train loss:2.2980696669335376\n",
      "train loss:2.2979830758715365\n",
      "train loss:2.30073175056472\n",
      "train loss:2.298588045948975\n",
      "train loss:2.3032204067570636\n",
      "train loss:2.2939950493221786\n",
      "train loss:2.3010093565813716\n",
      "train loss:2.299660187498632\n",
      "train loss:2.295180228850026\n",
      "train loss:2.3103793519736837\n",
      "train loss:2.2979164365771867\n",
      "train loss:2.2979584195980425\n",
      "train loss:2.30048354405618\n",
      "train loss:2.301732251488941\n",
      "train loss:2.300451261300968\n",
      "train loss:2.301469700415591\n",
      "train loss:2.291041385903414\n",
      "train loss:2.304074079730983\n",
      "train loss:2.3005135103698047\n",
      "train loss:2.300373921211306\n",
      "train loss:2.296093712088833\n",
      "train loss:2.3072820524615634\n",
      "train loss:2.3071434787444915\n",
      "train loss:2.3072829761866664\n",
      "train loss:2.304390543579477\n",
      "train loss:2.293502006931057\n",
      "train loss:2.3005568389840385\n",
      "train loss:2.2979906458455153\n",
      "train loss:2.2923461813616286\n",
      "train loss:2.306705935869461\n",
      "train loss:2.301199223823813\n",
      "train loss:2.3040575372566843\n",
      "train loss:2.2877469707855553\n",
      "train loss:2.3017139971348777\n",
      "train loss:2.302886297983724\n",
      "train loss:2.2962685138470196\n",
      "train loss:2.3009064071649448\n",
      "train loss:2.300441000431305\n",
      "train loss:2.303566208804811\n",
      "train loss:2.312269759642763\n",
      "train loss:2.306627884834531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.305362895746546\n",
      "train loss:2.296827481366579\n",
      "train loss:2.308903622708453\n",
      "train loss:2.307218220068781\n",
      "train loss:2.306705355037568\n",
      "train loss:2.306159445922381\n",
      "train loss:2.299991843075973\n",
      "train loss:2.308639489388578\n",
      "train loss:2.298686364782082\n",
      "train loss:2.2925943557672492\n",
      "train loss:2.295630103299981\n",
      "train loss:2.2998614297881894\n",
      "train loss:2.2965339629652424\n",
      "train loss:2.2936943873226436\n",
      "train loss:2.2977341247787706\n",
      "train loss:2.3030042875752255\n",
      "train loss:2.296584532675293\n",
      "train loss:2.3009887124608697\n",
      "train loss:2.3046360782168316\n",
      "train loss:2.3021592200404886\n",
      "train loss:2.3031837440723906\n",
      "train loss:2.2982854666637387\n",
      "train loss:2.2987676095475646\n",
      "train loss:2.299047438311446\n",
      "train loss:2.3094523922087333\n",
      "train loss:2.3025254821450143\n",
      "train loss:2.3094965021474247\n",
      "train loss:2.2948444336415084\n",
      "train loss:2.3067824962127546\n",
      "train loss:2.2935857702484146\n",
      "train loss:2.305661464106575\n",
      "train loss:2.30247114508566\n",
      "train loss:2.3059717179074637\n",
      "train loss:2.3012071857684413\n",
      "train loss:2.3039903755124205\n",
      "train loss:2.308618475117218\n",
      "train loss:2.2982190124783664\n",
      "train loss:2.303653579571809\n",
      "train loss:2.2998101940192717\n",
      "train loss:2.2953073529300494\n",
      "train loss:2.30172489166098\n",
      "train loss:2.3067288175341876\n",
      "train loss:2.301857309877175\n",
      "train loss:2.308150822411624\n",
      "train loss:2.306521669622642\n",
      "train loss:2.3000257059752327\n",
      "train loss:2.294667725172167\n",
      "train loss:2.308593113452669\n",
      "train loss:2.295651230533997\n",
      "train loss:2.313830445414105\n",
      "train loss:2.3010451305012274\n",
      "train loss:2.307496784677022\n",
      "train loss:2.299375279083696\n",
      "train loss:2.303105280942703\n",
      "train loss:2.302190604406489\n",
      "train loss:2.3025809001878925\n",
      "train loss:2.299726209345251\n",
      "train loss:2.2958338617328984\n",
      "train loss:2.298197502408445\n",
      "train loss:2.3070274702077542\n",
      "train loss:2.304131227395796\n",
      "train loss:2.296975270018788\n",
      "train loss:2.299144676381611\n",
      "train loss:2.307942730721345\n",
      "train loss:2.3056470369476965\n",
      "train loss:2.302387234875716\n",
      "train loss:2.299776253735172\n",
      "train loss:2.2944901623223863\n",
      "train loss:2.30620796605723\n",
      "train loss:2.307298359175399\n",
      "train loss:2.303972808785555\n",
      "train loss:2.296562163282499\n",
      "train loss:2.294481617565123\n",
      "train loss:2.304599798721273\n",
      "train loss:2.3039933936094252\n",
      "train loss:2.297445471264964\n",
      "train loss:2.298767231326625\n",
      "train loss:2.301022065970453\n",
      "train loss:2.309185299643141\n",
      "train loss:2.308215919365607\n",
      "train loss:2.3056962746858836\n",
      "train loss:2.3047899361694424\n",
      "train loss:2.299435309663218\n",
      "train loss:2.2924473335764635\n",
      "train loss:2.2992082361904087\n",
      "train loss:2.299733194128709\n",
      "train loss:2.2946226501980025\n",
      "train loss:2.2992889984852782\n",
      "train loss:2.294907793526329\n",
      "train loss:2.3046141007827265\n",
      "train loss:2.301636639443304\n",
      "train loss:2.2985114480021904\n",
      "train loss:2.307274091882071\n",
      "train loss:2.298987194889819\n",
      "train loss:2.295932754806819\n",
      "train loss:2.300589519331711\n",
      "train loss:2.303100897210818\n",
      "train loss:2.3044820610285393\n",
      "train loss:2.301654951919857\n",
      "train loss:2.2998508397596766\n",
      "train loss:2.3033367586594085\n",
      "train loss:2.304027246318128\n",
      "train loss:2.3005085420559026\n",
      "train loss:2.2942272612353274\n",
      "train loss:2.302652414326231\n",
      "train loss:2.294044314906422\n",
      "train loss:2.3012296856396985\n",
      "train loss:2.2903801053385466\n",
      "train loss:2.303739295716582\n",
      "train loss:2.2973755174540185\n",
      "train loss:2.306172089177649\n",
      "train loss:2.2994721359882675\n",
      "train loss:2.304181630306618\n",
      "train loss:2.305536990054715\n",
      "train loss:2.3061967670115453\n",
      "train loss:2.3021585642286353\n",
      "train loss:2.2958360902328683\n",
      "train loss:2.298602210645305\n",
      "train loss:2.3015790303338175\n",
      "train loss:2.2948264285428115\n",
      "train loss:2.300524235112573\n",
      "=== epoch:98, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2925473840154553\n",
      "train loss:2.293987887036872\n",
      "train loss:2.30232175336451\n",
      "train loss:2.305237135889153\n",
      "train loss:2.300720425297467\n",
      "train loss:2.2991489618596423\n",
      "train loss:2.297321110199257\n",
      "train loss:2.298521295084667\n",
      "train loss:2.3052896044822586\n",
      "train loss:2.2965411704521976\n",
      "train loss:2.3048831386032798\n",
      "train loss:2.3131309945967335\n",
      "train loss:2.3021522525708025\n",
      "train loss:2.2938655141447697\n",
      "train loss:2.303601150576678\n",
      "train loss:2.3009505905140824\n",
      "train loss:2.3005636122146145\n",
      "train loss:2.303947218121639\n",
      "train loss:2.309401079799941\n",
      "train loss:2.3033507089367626\n",
      "train loss:2.3092797530557667\n",
      "train loss:2.3006437641467574\n",
      "train loss:2.298890745243465\n",
      "train loss:2.306838901895134\n",
      "train loss:2.289798820346375\n",
      "train loss:2.303476888351368\n",
      "train loss:2.302612960305854\n",
      "train loss:2.297598746564208\n",
      "train loss:2.3083152512873903\n",
      "train loss:2.3050276484283403\n",
      "train loss:2.30748639516221\n",
      "train loss:2.2993070002093865\n",
      "train loss:2.298309428238664\n",
      "train loss:2.30295098125748\n",
      "train loss:2.306030863019858\n",
      "train loss:2.295857201628097\n",
      "train loss:2.3001228483052323\n",
      "train loss:2.3049144807343844\n",
      "train loss:2.3045173377468937\n",
      "train loss:2.3045350469414663\n",
      "train loss:2.2920470962164545\n",
      "train loss:2.3010952153454265\n",
      "train loss:2.3050881491214836\n",
      "train loss:2.3062841717694775\n",
      "train loss:2.3010017564485334\n",
      "train loss:2.29889031935129\n",
      "train loss:2.3047537509967837\n",
      "train loss:2.298306351505119\n",
      "train loss:2.3036573298324328\n",
      "train loss:2.3039809655620784\n",
      "train loss:2.3047121205169634\n",
      "train loss:2.304064653830592\n",
      "train loss:2.3036129082608885\n",
      "train loss:2.3099730178400923\n",
      "train loss:2.2965303134564707\n",
      "train loss:2.2982342212852616\n",
      "train loss:2.290329565254512\n",
      "train loss:2.3060606520782696\n",
      "train loss:2.2940880355722255\n",
      "train loss:2.303768048232318\n",
      "train loss:2.3038920904288616\n",
      "train loss:2.293166678516208\n",
      "train loss:2.3039643028733368\n",
      "train loss:2.298085167283708\n",
      "train loss:2.3069802180475625\n",
      "train loss:2.3021507909347605\n",
      "train loss:2.300114235561937\n",
      "train loss:2.2895669380030994\n",
      "train loss:2.308595793718052\n",
      "train loss:2.301887067449206\n",
      "train loss:2.303658836881149\n",
      "train loss:2.305102322733614\n",
      "train loss:2.2972041824575706\n",
      "train loss:2.3073708347070325\n",
      "train loss:2.310115157143572\n",
      "train loss:2.299171075285797\n",
      "train loss:2.3004456131464353\n",
      "train loss:2.2996625676138707\n",
      "train loss:2.294671629959263\n",
      "train loss:2.2925017891429547\n",
      "train loss:2.299650933527273\n",
      "train loss:2.304005243000571\n",
      "train loss:2.295941666115289\n",
      "train loss:2.2939079271915777\n",
      "train loss:2.303354532370641\n",
      "train loss:2.299748092681045\n",
      "train loss:2.29420087182024\n",
      "train loss:2.294694498895839\n",
      "train loss:2.3022906356087307\n",
      "train loss:2.299843707079413\n",
      "train loss:2.306474301263558\n",
      "train loss:2.2969425967269883\n",
      "train loss:2.292509345288548\n",
      "train loss:2.305159359710328\n",
      "train loss:2.306032555443135\n",
      "train loss:2.302014020535005\n",
      "train loss:2.3081505021594277\n",
      "train loss:2.305572727618122\n",
      "train loss:2.299480278738967\n",
      "train loss:2.3061507426511763\n",
      "train loss:2.2998107193935504\n",
      "train loss:2.3005495706635886\n",
      "train loss:2.2971238237978486\n",
      "train loss:2.307279812496521\n",
      "train loss:2.304438059319636\n",
      "train loss:2.3091687647433803\n",
      "train loss:2.314096769794185\n",
      "train loss:2.3045644376699013\n",
      "train loss:2.3067997473834\n",
      "train loss:2.30326128855614\n",
      "train loss:2.3105073886746075\n",
      "train loss:2.3057908356337826\n",
      "train loss:2.301927773711674\n",
      "train loss:2.2982830964557115\n",
      "train loss:2.3009341202278755\n",
      "train loss:2.2978245985801182\n",
      "train loss:2.304167624495459\n",
      "train loss:2.2937510977787654\n",
      "train loss:2.3078031913316446\n",
      "train loss:2.306596572751469\n",
      "train loss:2.309822019830759\n",
      "train loss:2.29720372771024\n",
      "train loss:2.300245387998621\n",
      "train loss:2.303211558304979\n",
      "train loss:2.303666908059003\n",
      "train loss:2.300091695068595\n",
      "train loss:2.306308319375782\n",
      "train loss:2.3029957986342966\n",
      "train loss:2.3037908412974124\n",
      "train loss:2.2994344465329375\n",
      "train loss:2.3068344407974006\n",
      "train loss:2.3029970402070297\n",
      "train loss:2.298134405079272\n",
      "train loss:2.3016512898086448\n",
      "train loss:2.2971840450397214\n",
      "train loss:2.2953029842579977\n",
      "train loss:2.297708280955578\n",
      "train loss:2.305932531333482\n",
      "train loss:2.2988532068997847\n",
      "train loss:2.296744171611338\n",
      "train loss:2.3063704902751856\n",
      "train loss:2.2953174499548545\n",
      "train loss:2.295614262574198\n",
      "train loss:2.29533725402299\n",
      "train loss:2.308212158407397\n",
      "train loss:2.2934571149694634\n",
      "train loss:2.306505485575203\n",
      "train loss:2.297541650057644\n",
      "train loss:2.298587968893033\n",
      "train loss:2.305563394304538\n",
      "train loss:2.3026143685409215\n",
      "train loss:2.29581827448489\n",
      "train loss:2.2956912862632546\n",
      "train loss:2.2959709876625958\n",
      "train loss:2.305909526806541\n",
      "train loss:2.2989104987373206\n",
      "train loss:2.302867336230464\n",
      "train loss:2.296791561343848\n",
      "train loss:2.307674173469789\n",
      "train loss:2.3003729809530005\n",
      "train loss:2.309055244324935\n",
      "train loss:2.296411138148175\n",
      "train loss:2.302698312860449\n",
      "train loss:2.294620254669629\n",
      "train loss:2.3012729096084406\n",
      "train loss:2.2940722688507194\n",
      "train loss:2.3034215186183893\n",
      "train loss:2.297047270042063\n",
      "train loss:2.3040119828143237\n",
      "train loss:2.2956460792796975\n",
      "train loss:2.3033949031380887\n",
      "train loss:2.2984812013724962\n",
      "train loss:2.302228182389448\n",
      "train loss:2.2978268864897022\n",
      "train loss:2.3087999850938976\n",
      "train loss:2.2980820785531693\n",
      "train loss:2.3042279945738846\n",
      "train loss:2.302190175311465\n",
      "train loss:2.3027467669459667\n",
      "train loss:2.3038450494591762\n",
      "train loss:2.303920661968263\n",
      "train loss:2.308147737580332\n",
      "train loss:2.302453360320547\n",
      "train loss:2.2919947099672555\n",
      "train loss:2.2952724697027778\n",
      "train loss:2.298060036484165\n",
      "train loss:2.303512124205084\n",
      "train loss:2.3008866908627814\n",
      "train loss:2.2969221417120145\n",
      "train loss:2.304388893257286\n",
      "train loss:2.3056551546075514\n",
      "train loss:2.3108422108263915\n",
      "train loss:2.3091546689304625\n",
      "train loss:2.2979758118495934\n",
      "train loss:2.3088490804074473\n",
      "train loss:2.299598983851951\n",
      "train loss:2.29946629521588\n",
      "train loss:2.3046007585064463\n",
      "train loss:2.300553270377451\n",
      "train loss:2.303821552169785\n",
      "=== epoch:99, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.302990377633325\n",
      "train loss:2.3024625287374567\n",
      "train loss:2.2972240539262385\n",
      "train loss:2.301724290805439\n",
      "train loss:2.309490509612176\n",
      "train loss:2.302145279952544\n",
      "train loss:2.2987904058328392\n",
      "train loss:2.3042357339728445\n",
      "train loss:2.3005717899263822\n",
      "train loss:2.30169391649929\n",
      "train loss:2.2938229574250197\n",
      "train loss:2.305636406096669\n",
      "train loss:2.2943154306243625\n",
      "train loss:2.3064092334944672\n",
      "train loss:2.3009527394327534\n",
      "train loss:2.3011302301163785\n",
      "train loss:2.2986421410345566\n",
      "train loss:2.2999930783802487\n",
      "train loss:2.301115471510855\n",
      "train loss:2.3008973153387666\n",
      "train loss:2.306162160924511\n",
      "train loss:2.3094720880144246\n",
      "train loss:2.2989356819730005\n",
      "train loss:2.3018509235262967\n",
      "train loss:2.302485837545806\n",
      "train loss:2.2959501551898747\n",
      "train loss:2.2993232001540083\n",
      "train loss:2.305895238371539\n",
      "train loss:2.2930247740863723\n",
      "train loss:2.3050295805283962\n",
      "train loss:2.303004362736524\n",
      "train loss:2.3031111765623566\n",
      "train loss:2.3112447924885795\n",
      "train loss:2.293574554733091\n",
      "train loss:2.308618147379988\n",
      "train loss:2.310046790856614\n",
      "train loss:2.3032147088549917\n",
      "train loss:2.298102130488727\n",
      "train loss:2.294178533378011\n",
      "train loss:2.295048159009336\n",
      "train loss:2.2998789789494047\n",
      "train loss:2.3009438338315884\n",
      "train loss:2.297575853127859\n",
      "train loss:2.298280363461822\n",
      "train loss:2.3039744822002137\n",
      "train loss:2.299381723048196\n",
      "train loss:2.3049297684779244\n",
      "train loss:2.3008387131896626\n",
      "train loss:2.3035193015266455\n",
      "train loss:2.3042301691755225\n",
      "train loss:2.305189876232348\n",
      "train loss:2.301758317127488\n",
      "train loss:2.2977836944519816\n",
      "train loss:2.2979689880472645\n",
      "train loss:2.3081547876520467\n",
      "train loss:2.2998103044785023\n",
      "train loss:2.3047847909409604\n",
      "train loss:2.3079662660811353\n",
      "train loss:2.29750184386494\n",
      "train loss:2.294869366541558\n",
      "train loss:2.290849156906647\n",
      "train loss:2.3148082041889397\n",
      "train loss:2.294630114358745\n",
      "train loss:2.3022266008863093\n",
      "train loss:2.3119583043252114\n",
      "train loss:2.2897610246339886\n",
      "train loss:2.2992599896616652\n",
      "train loss:2.304082599848176\n",
      "train loss:2.29615227997305\n",
      "train loss:2.300552774504507\n",
      "train loss:2.3031336914126275\n",
      "train loss:2.3049513032298132\n",
      "train loss:2.295164944910122\n",
      "train loss:2.2976106717646934\n",
      "train loss:2.292907020796072\n",
      "train loss:2.294172719534964\n",
      "train loss:2.3002297567083283\n",
      "train loss:2.295215796844519\n",
      "train loss:2.301846656208033\n",
      "train loss:2.310669901987332\n",
      "train loss:2.293641492323337\n",
      "train loss:2.3059843925651324\n",
      "train loss:2.2947539633768588\n",
      "train loss:2.2991570038049063\n",
      "train loss:2.2995603716061153\n",
      "train loss:2.3018027157203718\n",
      "train loss:2.301779585958436\n",
      "train loss:2.3056445199023585\n",
      "train loss:2.295713827182741\n",
      "train loss:2.3012680933232454\n",
      "train loss:2.3040283725523873\n",
      "train loss:2.300926064760987\n",
      "train loss:2.3025910330712454\n",
      "train loss:2.3009803512343776\n",
      "train loss:2.3033581186268024\n",
      "train loss:2.2916439219478115\n",
      "train loss:2.296963161394847\n",
      "train loss:2.3047552313488335\n",
      "train loss:2.295288073112443\n",
      "train loss:2.3020008077702747\n",
      "train loss:2.2942259842804864\n",
      "train loss:2.3019023018058644\n",
      "train loss:2.304086183003847\n",
      "train loss:2.308060542950058\n",
      "train loss:2.29335670143383\n",
      "train loss:2.302771452031433\n",
      "train loss:2.3030411268561903\n",
      "train loss:2.300470915182044\n",
      "train loss:2.2950037195646513\n",
      "train loss:2.2950420545888064\n",
      "train loss:2.2970811445075516\n",
      "train loss:2.3088075352542985\n",
      "train loss:2.301985251591505\n",
      "train loss:2.2963133291623365\n",
      "train loss:2.305810059959708\n",
      "train loss:2.3022244506793883\n",
      "train loss:2.3040770613301085\n",
      "train loss:2.303442162185807\n",
      "train loss:2.3064269127045507\n",
      "train loss:2.2997767769326494\n",
      "train loss:2.2937052914469866\n",
      "train loss:2.295260544075914\n",
      "train loss:2.300581541541396\n",
      "train loss:2.3045490333805603\n",
      "train loss:2.2976825711332203\n",
      "train loss:2.300833683152248\n",
      "train loss:2.311746869888181\n",
      "train loss:2.303140805349212\n",
      "train loss:2.293362397467297\n",
      "train loss:2.300401300239619\n",
      "train loss:2.3006276643014507\n",
      "train loss:2.306691265795526\n",
      "train loss:2.303049627878748\n",
      "train loss:2.3055726218499486\n",
      "train loss:2.3043224381883105\n",
      "train loss:2.3027303365017997\n",
      "train loss:2.299044415270349\n",
      "train loss:2.294787827539058\n",
      "train loss:2.2979927253867753\n",
      "train loss:2.29977101239608\n",
      "train loss:2.3037669986093228\n",
      "train loss:2.2998591550942895\n",
      "train loss:2.303958703226393\n",
      "train loss:2.298033995576629\n",
      "train loss:2.3016500552406827\n",
      "train loss:2.299059103378584\n",
      "train loss:2.298268005663753\n",
      "train loss:2.296752099728607\n",
      "train loss:2.3008781295712817\n",
      "train loss:2.2899558628947565\n",
      "train loss:2.3053899319604585\n",
      "train loss:2.292761091905456\n",
      "train loss:2.297362360397178\n",
      "train loss:2.298255272328744\n",
      "train loss:2.3023391359372125\n",
      "train loss:2.3014950344794167\n",
      "train loss:2.3072908794228963\n",
      "train loss:2.2980928461205274\n",
      "train loss:2.3107533569219405\n",
      "train loss:2.305754799721243\n",
      "train loss:2.3104172010590815\n",
      "train loss:2.3054793215775162\n",
      "train loss:2.304198096399642\n",
      "train loss:2.3132667118784043\n",
      "train loss:2.3070763999117907\n",
      "train loss:2.2971271153641455\n",
      "train loss:2.2989671796364917\n",
      "train loss:2.3100378429716124\n",
      "train loss:2.304579352470211\n",
      "train loss:2.2994214138131994\n",
      "train loss:2.302918333985897\n",
      "train loss:2.3003937737369586\n",
      "train loss:2.299081985075085\n",
      "train loss:2.2996362848565206\n",
      "train loss:2.2999467699570584\n",
      "train loss:2.292931873868894\n",
      "train loss:2.304686535301564\n",
      "train loss:2.30532752302319\n",
      "train loss:2.2989732158966234\n",
      "train loss:2.2962193125623\n",
      "train loss:2.3070760628807454\n",
      "train loss:2.3009324143182805\n",
      "train loss:2.2957834296472015\n",
      "train loss:2.2967699320572974\n",
      "train loss:2.297149951117373\n",
      "train loss:2.3045356543364566\n",
      "train loss:2.2996862801524656\n",
      "train loss:2.301515750987751\n",
      "train loss:2.3017377925701266\n",
      "train loss:2.2955618253355183\n",
      "train loss:2.306828155124302\n",
      "train loss:2.298248244050236\n",
      "train loss:2.306356732793228\n",
      "train loss:2.299585822047553\n",
      "train loss:2.2976074236511406\n",
      "train loss:2.298683449278219\n",
      "train loss:2.309418044967291\n",
      "train loss:2.3086130984619126\n",
      "train loss:2.3057855617505587\n",
      "train loss:2.3062907651528763\n",
      "=== epoch:100, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.299709523494566\n",
      "train loss:2.295876644700172\n",
      "train loss:2.290325399660936\n",
      "train loss:2.298281187196047\n",
      "train loss:2.300864845876648\n",
      "train loss:2.306695152298247\n",
      "train loss:2.3039187994032466\n",
      "train loss:2.301621916887059\n",
      "train loss:2.3011132916888695\n",
      "train loss:2.297263164363368\n",
      "train loss:2.3019015467108233\n",
      "train loss:2.29877417627002\n",
      "train loss:2.294152101665402\n",
      "train loss:2.3006867940740774\n",
      "train loss:2.302991863019382\n",
      "train loss:2.298694874626424\n",
      "train loss:2.300930573614952\n",
      "train loss:2.300266868351273\n",
      "train loss:2.303270153867068\n",
      "train loss:2.295740400919627\n",
      "train loss:2.3002877344233736\n",
      "train loss:2.314280861125124\n",
      "train loss:2.299303419134103\n",
      "train loss:2.310162804891494\n",
      "train loss:2.304054386459572\n",
      "train loss:2.29117077436221\n",
      "train loss:2.3015082509758584\n",
      "train loss:2.307511029754157\n",
      "train loss:2.299473614253036\n",
      "train loss:2.293759754274975\n",
      "train loss:2.300631300093181\n",
      "train loss:2.301639489014329\n",
      "train loss:2.3010554877953577\n",
      "train loss:2.2976827341887995\n",
      "train loss:2.296331841805933\n",
      "train loss:2.3051774088285284\n",
      "train loss:2.307442458887469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3055142613750226\n",
      "train loss:2.3018978078098695\n",
      "train loss:2.294369140313785\n",
      "train loss:2.3030153706823264\n",
      "train loss:2.2962988527979227\n",
      "train loss:2.3085173466098947\n",
      "train loss:2.3073061497685727\n",
      "train loss:2.301351692548616\n",
      "train loss:2.3114229579448096\n",
      "train loss:2.297986712467595\n",
      "train loss:2.3017381252740075\n",
      "train loss:2.2988842107186445\n",
      "train loss:2.301361586866051\n",
      "train loss:2.306478055926421\n",
      "train loss:2.305601448848877\n",
      "train loss:2.300136517257488\n",
      "train loss:2.2977974953075457\n",
      "train loss:2.306882980660749\n",
      "train loss:2.303816621898614\n",
      "train loss:2.300624529787416\n",
      "train loss:2.2994530911427744\n",
      "train loss:2.3046943045339434\n",
      "train loss:2.298142102760406\n",
      "train loss:2.3004350319065994\n",
      "train loss:2.30896788837187\n",
      "train loss:2.29408607075175\n",
      "train loss:2.2981616832818874\n",
      "train loss:2.297769057713945\n",
      "train loss:2.293879668543766\n",
      "train loss:2.3058865592341435\n",
      "train loss:2.3015203822244312\n",
      "train loss:2.300233394306262\n",
      "train loss:2.300389654385777\n",
      "train loss:2.2924434330214885\n",
      "train loss:2.3003482860071065\n",
      "train loss:2.3031865462690972\n",
      "train loss:2.296776237150203\n",
      "train loss:2.2989454072422433\n",
      "train loss:2.2980073337516096\n",
      "train loss:2.3000877875883434\n",
      "train loss:2.3073572374094455\n",
      "train loss:2.2968389579448014\n",
      "train loss:2.2936773508253623\n",
      "train loss:2.3153075655620188\n",
      "train loss:2.3057273223982615\n",
      "train loss:2.3062785104245305\n",
      "train loss:2.3018890089672372\n",
      "train loss:2.301450151985964\n",
      "train loss:2.295330401882539\n",
      "train loss:2.309832943487431\n",
      "train loss:2.2978652716312724\n",
      "train loss:2.300814027385424\n",
      "train loss:2.2951510888778457\n",
      "train loss:2.29290856861784\n",
      "train loss:2.301841621935795\n",
      "train loss:2.298995273491096\n",
      "train loss:2.30174932342722\n",
      "train loss:2.297874735702771\n",
      "train loss:2.3113845466315808\n",
      "train loss:2.2981899954256524\n",
      "train loss:2.3063470092418537\n",
      "train loss:2.306482984010114\n",
      "train loss:2.297415640474172\n",
      "train loss:2.2991411050111457\n",
      "train loss:2.302388299339366\n",
      "train loss:2.295285623720585\n",
      "train loss:2.3008148086104785\n",
      "train loss:2.3064453591668004\n",
      "train loss:2.2935855940083183\n",
      "train loss:2.3108497327498063\n",
      "train loss:2.300489413796752\n",
      "train loss:2.3045793647642774\n",
      "train loss:2.2908138100559956\n",
      "train loss:2.2987145558540676\n",
      "train loss:2.3024382540276793\n",
      "train loss:2.298501902364651\n",
      "train loss:2.2995029815517714\n",
      "train loss:2.2977041746368974\n",
      "train loss:2.301190927396904\n",
      "train loss:2.3057930795121053\n",
      "train loss:2.3042297312326405\n",
      "train loss:2.2974694845904065\n",
      "train loss:2.3018870871883883\n",
      "train loss:2.298869274539994\n",
      "train loss:2.3006790669042725\n",
      "train loss:2.3069613403002727\n",
      "train loss:2.3059945915724076\n",
      "train loss:2.3001962141090804\n",
      "train loss:2.2994367219425587\n",
      "train loss:2.301663548789739\n",
      "train loss:2.2994393796957135\n",
      "train loss:2.299929557106067\n",
      "train loss:2.305435197934998\n",
      "train loss:2.3073213234757586\n",
      "train loss:2.2960235356093506\n",
      "train loss:2.2954658806847696\n",
      "train loss:2.3033532747643948\n",
      "train loss:2.3048293773320117\n",
      "train loss:2.2955038908383956\n",
      "train loss:2.302072078164216\n",
      "train loss:2.301650968959284\n",
      "train loss:2.295238559174393\n",
      "train loss:2.3030754025785383\n",
      "train loss:2.3017461909113006\n",
      "train loss:2.298949826159531\n",
      "train loss:2.2939568260231544\n",
      "train loss:2.3087391949130485\n",
      "train loss:2.304462180387163\n",
      "train loss:2.3060995095922174\n",
      "train loss:2.2900731039084343\n",
      "train loss:2.297391073317369\n",
      "train loss:2.310199803341377\n",
      "train loss:2.3044835560983494\n",
      "train loss:2.2981700567478662\n",
      "train loss:2.304542640391741\n",
      "train loss:2.2905227444529435\n",
      "train loss:2.296075915060729\n",
      "train loss:2.3001732730478843\n",
      "train loss:2.3003243266883517\n",
      "train loss:2.297574539328091\n",
      "train loss:2.3008985105517676\n",
      "train loss:2.2999923642657167\n",
      "train loss:2.298125393096868\n",
      "train loss:2.301285721303688\n",
      "train loss:2.3063715893050105\n",
      "train loss:2.305060407276595\n",
      "train loss:2.3035895499686956\n",
      "train loss:2.2964420243071904\n",
      "train loss:2.309788113175279\n",
      "train loss:2.2892865168399004\n",
      "train loss:2.300327651234357\n",
      "train loss:2.3055995182124267\n",
      "train loss:2.3003602358260857\n",
      "train loss:2.3031282504930375\n",
      "train loss:2.2931955483933164\n",
      "train loss:2.2918659163913433\n",
      "train loss:2.2948850585195304\n",
      "train loss:2.2954032279398198\n",
      "train loss:2.3009659296044043\n",
      "train loss:2.305555027516029\n",
      "train loss:2.297130571413998\n",
      "train loss:2.297274605330044\n",
      "train loss:2.302042559641441\n",
      "train loss:2.296691618622825\n",
      "train loss:2.315393077422788\n",
      "train loss:2.2973747536066296\n",
      "train loss:2.3063229532558838\n",
      "train loss:2.2980009252079725\n",
      "train loss:2.305026098824489\n",
      "train loss:2.2994329111535046\n",
      "train loss:2.2948870963396133\n",
      "train loss:2.2960961604968113\n",
      "train loss:2.296343798316091\n",
      "train loss:2.3050303706278408\n",
      "train loss:2.300716484440755\n",
      "train loss:2.2986571180405964\n",
      "train loss:2.3003078163630377\n",
      "train loss:2.2985053161155604\n",
      "train loss:2.3096861930457626\n",
      "train loss:2.306803716497125\n",
      "train loss:2.2982611684512393\n",
      "train loss:2.311510729818813\n",
      "train loss:2.3006224047575854\n",
      "=== epoch:101, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.303710899125051\n",
      "train loss:2.301584487687331\n",
      "train loss:2.3040069035357917\n",
      "train loss:2.3068971526914006\n",
      "train loss:2.3012485948870736\n",
      "train loss:2.301275565451891\n",
      "train loss:2.2904695986565717\n",
      "train loss:2.2928887448151594\n",
      "train loss:2.2969131428287906\n",
      "train loss:2.3049970262942083\n",
      "train loss:2.3082354472323323\n",
      "train loss:2.2998563919237225\n",
      "train loss:2.2969541179184216\n",
      "train loss:2.3043138756056742\n",
      "train loss:2.295147397319626\n",
      "train loss:2.298752543940469\n",
      "train loss:2.301535603651489\n",
      "train loss:2.299913384750463\n",
      "train loss:2.3033362054237316\n",
      "train loss:2.2963264592071586\n",
      "train loss:2.299951957186933\n",
      "train loss:2.3059435840860267\n",
      "train loss:2.3038348475969173\n",
      "train loss:2.2946815274949794\n",
      "train loss:2.2940128196684744\n",
      "train loss:2.2947725077102303\n",
      "train loss:2.298360025708791\n",
      "train loss:2.2973863447374954\n",
      "train loss:2.297331216041283\n",
      "train loss:2.2995839417793094\n",
      "train loss:2.2925240326327114\n",
      "train loss:2.3110495239240425\n",
      "train loss:2.2991672100976692\n",
      "train loss:2.2990152199477443\n",
      "train loss:2.297887954964788\n",
      "train loss:2.3058821409109043\n",
      "train loss:2.29411446949925\n",
      "train loss:2.3074401949575516\n",
      "train loss:2.297907826945322\n",
      "train loss:2.3060242011387744\n",
      "train loss:2.3009255620286093\n",
      "train loss:2.295298902393215\n",
      "train loss:2.307734689155308\n",
      "train loss:2.3004372056857525\n",
      "train loss:2.302005081080644\n",
      "train loss:2.297389360090305\n",
      "train loss:2.293300220091142\n",
      "train loss:2.290923145645498\n",
      "train loss:2.307559037223311\n",
      "train loss:2.297910812252105\n",
      "train loss:2.3143679898694876\n",
      "train loss:2.2993828110352723\n",
      "train loss:2.2910605743076013\n",
      "train loss:2.304972503955435\n",
      "train loss:2.3053470313504816\n",
      "train loss:2.3018741129877722\n",
      "train loss:2.304271904529777\n",
      "train loss:2.306796219229106\n",
      "train loss:2.3100562425485682\n",
      "train loss:2.3013821520488826\n",
      "train loss:2.3036735511495663\n",
      "train loss:2.3018287686505565\n",
      "train loss:2.3010402456616945\n",
      "train loss:2.3001362697246948\n",
      "train loss:2.2951659628705734\n",
      "train loss:2.2979865153788994\n",
      "train loss:2.30373228843741\n",
      "train loss:2.300119037469523\n",
      "train loss:2.3043908563157496\n",
      "train loss:2.2979105738086654\n",
      "train loss:2.3095251288335596\n",
      "train loss:2.2960536285707667\n",
      "train loss:2.3020971155674967\n",
      "train loss:2.308978535640754\n",
      "train loss:2.308870433036487\n",
      "train loss:2.297785822488624\n",
      "train loss:2.3061753055026326\n",
      "train loss:2.3011360192477914\n",
      "train loss:2.2989559963916504\n",
      "train loss:2.3058672500449684\n",
      "train loss:2.3036150463141123\n",
      "train loss:2.2990615955060028\n",
      "train loss:2.301727875868965\n",
      "train loss:2.2975898156900256\n",
      "train loss:2.2986658395751247\n",
      "train loss:2.3074697411130733\n",
      "train loss:2.3048652512414702\n",
      "train loss:2.293820518824429\n",
      "train loss:2.311495504308637\n",
      "train loss:2.3014030778426675\n",
      "train loss:2.3057864392425698\n",
      "train loss:2.298665124212103\n",
      "train loss:2.302470510018323\n",
      "train loss:2.308914203714355\n",
      "train loss:2.29370366972767\n",
      "train loss:2.3060621663397485\n",
      "train loss:2.3002040426912522\n",
      "train loss:2.303432435879711\n",
      "train loss:2.312567172661643\n",
      "train loss:2.3027423389710306\n",
      "train loss:2.2993687460755017\n",
      "train loss:2.2965416237991065\n",
      "train loss:2.302510422095185\n",
      "train loss:2.3023094020745147\n",
      "train loss:2.2976640382981217\n",
      "train loss:2.2981293769678683\n",
      "train loss:2.301344208616429\n",
      "train loss:2.2990827752388467\n",
      "train loss:2.2984284818052654\n",
      "train loss:2.2989228543877633\n",
      "train loss:2.304180484779558\n",
      "train loss:2.3075308167311626\n",
      "train loss:2.2988484765411337\n",
      "train loss:2.3026941190639056\n",
      "train loss:2.300412521924307\n",
      "train loss:2.3040093544569893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3028054021869013\n",
      "train loss:2.29870539852026\n",
      "train loss:2.2973957121691497\n",
      "train loss:2.2972480457412936\n",
      "train loss:2.298596055411906\n",
      "train loss:2.299686102684811\n",
      "train loss:2.3021033782288383\n",
      "train loss:2.2981428548966014\n",
      "train loss:2.293168528081937\n",
      "train loss:2.299483528638655\n",
      "train loss:2.301444187198779\n",
      "train loss:2.302897228199835\n",
      "train loss:2.304519434775812\n",
      "train loss:2.2968732262152605\n",
      "train loss:2.2920791364111732\n",
      "train loss:2.3018339147145213\n",
      "train loss:2.304212999074613\n",
      "train loss:2.2950504707527877\n",
      "train loss:2.2993139011934685\n",
      "train loss:2.297289122082323\n",
      "train loss:2.297157381375911\n",
      "train loss:2.3067627202439493\n",
      "train loss:2.30968267675168\n",
      "train loss:2.307409910888468\n",
      "train loss:2.2932095975896933\n",
      "train loss:2.3062180082298522\n",
      "train loss:2.3035964727958516\n",
      "train loss:2.2979793412089515\n",
      "train loss:2.3000862931393717\n",
      "train loss:2.2982432199346463\n",
      "train loss:2.296470699058032\n",
      "train loss:2.301509345809998\n",
      "train loss:2.309003642344063\n",
      "train loss:2.301275653170129\n",
      "train loss:2.2984503537931884\n",
      "train loss:2.300822608396178\n",
      "train loss:2.3020706337403407\n",
      "train loss:2.3097502310859186\n",
      "train loss:2.303733419018447\n",
      "train loss:2.3067395605721863\n",
      "train loss:2.3043362464638975\n",
      "train loss:2.2962794330934257\n",
      "train loss:2.2993070270374236\n",
      "train loss:2.302503706336596\n",
      "train loss:2.3013635727811868\n",
      "train loss:2.3026731777502696\n",
      "train loss:2.3071234620644523\n",
      "train loss:2.3023528803925917\n",
      "train loss:2.3053270623570454\n",
      "train loss:2.293706289961565\n",
      "train loss:2.3033366764807663\n",
      "train loss:2.295745923932717\n",
      "train loss:2.3013460373393824\n",
      "train loss:2.302290378191187\n",
      "train loss:2.305940230435847\n",
      "train loss:2.307411830952355\n",
      "train loss:2.3001708198557655\n",
      "train loss:2.3032872133696705\n",
      "train loss:2.2936337117057235\n",
      "train loss:2.2960495300219224\n",
      "train loss:2.3060786316785107\n",
      "train loss:2.2972364659367988\n",
      "train loss:2.303111869700279\n",
      "train loss:2.292674971924731\n",
      "train loss:2.2958405705673943\n",
      "train loss:2.3059712315901226\n",
      "train loss:2.301818198626938\n",
      "train loss:2.2995768309935856\n",
      "train loss:2.2986883698614706\n",
      "train loss:2.3063178147331027\n",
      "train loss:2.2984438077205778\n",
      "train loss:2.301026880163706\n",
      "train loss:2.3011689545843814\n",
      "train loss:2.301454542091438\n",
      "train loss:2.2996626494426264\n",
      "train loss:2.3029359679318007\n",
      "train loss:2.301159900168765\n",
      "train loss:2.2897365345392817\n",
      "train loss:2.303185018016259\n",
      "train loss:2.305029455954781\n",
      "train loss:2.302777696570045\n",
      "train loss:2.301263453778462\n",
      "train loss:2.288210577857286\n",
      "train loss:2.3025313914823786\n",
      "=== epoch:102, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.302750553697662\n",
      "train loss:2.301103978803045\n",
      "train loss:2.3015384416518705\n",
      "train loss:2.301021421178917\n",
      "train loss:2.298331210066584\n",
      "train loss:2.3015165732893235\n",
      "train loss:2.3108817756973954\n",
      "train loss:2.3079803717710576\n",
      "train loss:2.2980175373341\n",
      "train loss:2.3051671397876943\n",
      "train loss:2.30608766754342\n",
      "train loss:2.308277809771532\n",
      "train loss:2.302047247267668\n",
      "train loss:2.297877168221255\n",
      "train loss:2.297626688585739\n",
      "train loss:2.3096739167151528\n",
      "train loss:2.3034482699906684\n",
      "train loss:2.289570928029717\n",
      "train loss:2.300012376818904\n",
      "train loss:2.30386609391199\n",
      "train loss:2.309731749565301\n",
      "train loss:2.303526760545097\n",
      "train loss:2.3048116113306825\n",
      "train loss:2.3000498362486197\n",
      "train loss:2.2972186057032027\n",
      "train loss:2.304799458480202\n",
      "train loss:2.3120992091406505\n",
      "train loss:2.302528791938511\n",
      "train loss:2.299899000267466\n",
      "train loss:2.288546281476961\n",
      "train loss:2.301314641617748\n",
      "train loss:2.3011851840302096\n",
      "train loss:2.3116986765573375\n",
      "train loss:2.290385651969335\n",
      "train loss:2.303635172085437\n",
      "train loss:2.2954361355530284\n",
      "train loss:2.3079481073945516\n",
      "train loss:2.3043427753963592\n",
      "train loss:2.3016621834865605\n",
      "train loss:2.3009586606877304\n",
      "train loss:2.2956837365047673\n",
      "train loss:2.2960239878386046\n",
      "train loss:2.29914696660723\n",
      "train loss:2.298077681102393\n",
      "train loss:2.305198809501606\n",
      "train loss:2.295171535077772\n",
      "train loss:2.302410938913959\n",
      "train loss:2.304317402801956\n",
      "train loss:2.300762131423601\n",
      "train loss:2.305868528163992\n",
      "train loss:2.299634514129451\n",
      "train loss:2.3031796179680906\n",
      "train loss:2.303774574903668\n",
      "train loss:2.2960831693308044\n",
      "train loss:2.3011546404242353\n",
      "train loss:2.298632991114621\n",
      "train loss:2.3062442431858434\n",
      "train loss:2.3046574377021334\n",
      "train loss:2.304676696964938\n",
      "train loss:2.294647078770718\n",
      "train loss:2.3006301538849474\n",
      "train loss:2.297914713145204\n",
      "train loss:2.3029869494973103\n",
      "train loss:2.2992011572752062\n",
      "train loss:2.304147346880822\n",
      "train loss:2.3080388098201365\n",
      "train loss:2.299516884241894\n",
      "train loss:2.304835523282903\n",
      "train loss:2.300794732614924\n",
      "train loss:2.3021824089969214\n",
      "train loss:2.3000043926434968\n",
      "train loss:2.295218353767685\n",
      "train loss:2.300202145050811\n",
      "train loss:2.3029906525897266\n",
      "train loss:2.3014268771061195\n",
      "train loss:2.2917930901660286\n",
      "train loss:2.3054407076608077\n",
      "train loss:2.2979059576082737\n",
      "train loss:2.297082412104287\n",
      "train loss:2.3025070626818365\n",
      "train loss:2.301547340279664\n",
      "train loss:2.304677966188115\n",
      "train loss:2.2905921680566794\n",
      "train loss:2.2951619229169213\n",
      "train loss:2.296344773355939\n",
      "train loss:2.304068614970353\n",
      "train loss:2.300560725619243\n",
      "train loss:2.3037619477880593\n",
      "train loss:2.298513714736644\n",
      "train loss:2.3073586411967395\n",
      "train loss:2.300481427333865\n",
      "train loss:2.302269360499339\n",
      "train loss:2.300959075831605\n",
      "train loss:2.2896432459701157\n",
      "train loss:2.3051832614035335\n",
      "train loss:2.300690518778869\n",
      "train loss:2.3086581990130255\n",
      "train loss:2.301600823268458\n",
      "train loss:2.295570212025998\n",
      "train loss:2.2966780072273196\n",
      "train loss:2.303216692361109\n",
      "train loss:2.29609182122294\n",
      "train loss:2.3045957224565887\n",
      "train loss:2.3031237453593065\n",
      "train loss:2.312074470997105\n",
      "train loss:2.2978268037036376\n",
      "train loss:2.3038110731239487\n",
      "train loss:2.3037525671394996\n",
      "train loss:2.300491791764702\n",
      "train loss:2.3009964367967104\n",
      "train loss:2.298534531572595\n",
      "train loss:2.304169703512304\n",
      "train loss:2.299070632620005\n",
      "train loss:2.2907376230293357\n",
      "train loss:2.30842085443327\n",
      "train loss:2.303434513286728\n",
      "train loss:2.3033841117878833\n",
      "train loss:2.3009448600092304\n",
      "train loss:2.2985058814960038\n",
      "train loss:2.3062710537470523\n",
      "train loss:2.3063133160830738\n",
      "train loss:2.3059056920215317\n",
      "train loss:2.2942794009381315\n",
      "train loss:2.2969153638867668\n",
      "train loss:2.300759611171129\n",
      "train loss:2.307771320622121\n",
      "train loss:2.311282971595142\n",
      "train loss:2.2997035624224615\n",
      "train loss:2.298280155466187\n",
      "train loss:2.304919252777141\n",
      "train loss:2.304646233703945\n",
      "train loss:2.297016934374933\n",
      "train loss:2.302375370587585\n",
      "train loss:2.3049334574208538\n",
      "train loss:2.2935556745544625\n",
      "train loss:2.2992714105830694\n",
      "train loss:2.311459228595279\n",
      "train loss:2.2926208220997224\n",
      "train loss:2.305566596279413\n",
      "train loss:2.2989505765688034\n",
      "train loss:2.3065875466858428\n",
      "train loss:2.3043146144779936\n",
      "train loss:2.3057577248001224\n",
      "train loss:2.2988814489177214\n",
      "train loss:2.303726680216785\n",
      "train loss:2.3172914494395904\n",
      "train loss:2.3029443178927984\n",
      "train loss:2.301157949216433\n",
      "train loss:2.3066637235791467\n",
      "train loss:2.3094648660502117\n",
      "train loss:2.2998211012423435\n",
      "train loss:2.3072998076220754\n",
      "train loss:2.3027012592978364\n",
      "train loss:2.301603257199423\n",
      "train loss:2.3038852684414293\n",
      "train loss:2.3032648001314784\n",
      "train loss:2.304537492306656\n",
      "train loss:2.3086685294005918\n",
      "train loss:2.2986302925148867\n",
      "train loss:2.3111135207634264\n",
      "train loss:2.30854891242241\n",
      "train loss:2.306587741203835\n",
      "train loss:2.2998342419774604\n",
      "train loss:2.3002231150376207\n",
      "train loss:2.305618869113816\n",
      "train loss:2.299699257925905\n",
      "train loss:2.295494366358758\n",
      "train loss:2.293423464573236\n",
      "train loss:2.3101314362668965\n",
      "train loss:2.2976533643506207\n",
      "train loss:2.286289956414385\n",
      "train loss:2.296342947251027\n",
      "train loss:2.2957816209241733\n",
      "train loss:2.300101963685166\n",
      "train loss:2.2980725747095954\n",
      "train loss:2.298678147775213\n",
      "train loss:2.2996373115224693\n",
      "train loss:2.3068266857174202\n",
      "train loss:2.307866462439885\n",
      "train loss:2.3089217380860676\n",
      "train loss:2.3001500203452165\n",
      "train loss:2.296696870678064\n",
      "train loss:2.3000408010009585\n",
      "train loss:2.2992217094081955\n",
      "train loss:2.29928136984734\n",
      "train loss:2.295918406603336\n",
      "train loss:2.3111916014508327\n",
      "train loss:2.31185034820498\n",
      "train loss:2.304559769042964\n",
      "train loss:2.295361662746899\n",
      "train loss:2.3001625494256643\n",
      "train loss:2.3001455657320693\n",
      "train loss:2.3053398172175155\n",
      "train loss:2.302544832273544\n",
      "train loss:2.301188046891497\n",
      "train loss:2.295233031034388\n",
      "train loss:2.3004344135382397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.307089278361679\n",
      "train loss:2.30141420836312\n",
      "train loss:2.295146377601503\n",
      "=== epoch:103, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2934254489395167\n",
      "train loss:2.2956126121909666\n",
      "train loss:2.2985310007208466\n",
      "train loss:2.298059616511979\n",
      "train loss:2.3059719604754845\n",
      "train loss:2.300663353418201\n",
      "train loss:2.2995967675832336\n",
      "train loss:2.308246833248896\n",
      "train loss:2.2998583547064184\n",
      "train loss:2.2964248043836455\n",
      "train loss:2.298195257313366\n",
      "train loss:2.3011491418887675\n",
      "train loss:2.3011074752081497\n",
      "train loss:2.299647290362073\n",
      "train loss:2.2972234956903135\n",
      "train loss:2.308713336561967\n",
      "train loss:2.304190453295275\n",
      "train loss:2.303932981739449\n",
      "train loss:2.3001998626559983\n",
      "train loss:2.2973987257880015\n",
      "train loss:2.3004426149910526\n",
      "train loss:2.30094220679564\n",
      "train loss:2.299863590549942\n",
      "train loss:2.3049466691105245\n",
      "train loss:2.3071780371471586\n",
      "train loss:2.3028413375422896\n",
      "train loss:2.3045455751055415\n",
      "train loss:2.3011364901031146\n",
      "train loss:2.3126696808571143\n",
      "train loss:2.299172234383486\n",
      "train loss:2.296874311690327\n",
      "train loss:2.296425707737338\n",
      "train loss:2.3014508910436966\n",
      "train loss:2.298361306555322\n",
      "train loss:2.304003092650797\n",
      "train loss:2.29825597240926\n",
      "train loss:2.3038288952375665\n",
      "train loss:2.306037009117091\n",
      "train loss:2.3014355461007034\n",
      "train loss:2.2930302081179814\n",
      "train loss:2.3008583415525816\n",
      "train loss:2.2974711616979095\n",
      "train loss:2.302235546431051\n",
      "train loss:2.29839997653905\n",
      "train loss:2.3093287960840385\n",
      "train loss:2.298305739113662\n",
      "train loss:2.302495890415162\n",
      "train loss:2.3058967604444103\n",
      "train loss:2.303931470972368\n",
      "train loss:2.308845714487553\n",
      "train loss:2.305989894531477\n",
      "train loss:2.300111411831942\n",
      "train loss:2.310824932009545\n",
      "train loss:2.31063212332685\n",
      "train loss:2.297987426761199\n",
      "train loss:2.3067030510581303\n",
      "train loss:2.3124704078940272\n",
      "train loss:2.3090691749733154\n",
      "train loss:2.3011403259604375\n",
      "train loss:2.295314086541854\n",
      "train loss:2.2999515474517938\n",
      "train loss:2.2995204951261807\n",
      "train loss:2.3060445337265776\n",
      "train loss:2.299171654344765\n",
      "train loss:2.2990096508165085\n",
      "train loss:2.298759820299083\n",
      "train loss:2.2958869623047082\n",
      "train loss:2.301603073639474\n",
      "train loss:2.2913700551414475\n",
      "train loss:2.29318240849654\n",
      "train loss:2.3005774931521707\n",
      "train loss:2.2925813996897446\n",
      "train loss:2.298082143076039\n",
      "train loss:2.305149377967274\n",
      "train loss:2.303330533625527\n",
      "train loss:2.3024936895902313\n",
      "train loss:2.303977990883338\n",
      "train loss:2.2986505735349527\n",
      "train loss:2.3011815806090032\n",
      "train loss:2.3059775871220807\n",
      "train loss:2.2979283324697883\n",
      "train loss:2.3069529633769603\n",
      "train loss:2.3046398764979554\n",
      "train loss:2.305872082315084\n",
      "train loss:2.313629998080563\n",
      "train loss:2.3077697274715168\n",
      "train loss:2.2944913647391445\n",
      "train loss:2.298772483581386\n",
      "train loss:2.2924366161421204\n",
      "train loss:2.3067575333617536\n",
      "train loss:2.307835580119872\n",
      "train loss:2.302870608291146\n",
      "train loss:2.305545327507506\n",
      "train loss:2.3021514629107873\n",
      "train loss:2.308420364426863\n",
      "train loss:2.305143797257156\n",
      "train loss:2.3076299868887427\n",
      "train loss:2.2986864816444696\n",
      "train loss:2.3002086269049364\n",
      "train loss:2.313351271606301\n",
      "train loss:2.302858428344059\n",
      "train loss:2.3024504710764657\n",
      "train loss:2.30251558705218\n",
      "train loss:2.2907430253956886\n",
      "train loss:2.2950267428362023\n",
      "train loss:2.3030703597574593\n",
      "train loss:2.2922869379818835\n",
      "train loss:2.300479344707068\n",
      "train loss:2.305643238095675\n",
      "train loss:2.307935327927782\n",
      "train loss:2.3066158481079397\n",
      "train loss:2.2980936132162464\n",
      "train loss:2.306344356951157\n",
      "train loss:2.294744488267658\n",
      "train loss:2.2954730999609265\n",
      "train loss:2.293099560989237\n",
      "train loss:2.3094201045933143\n",
      "train loss:2.2961130784750616\n",
      "train loss:2.30089127845349\n",
      "train loss:2.3064938451298307\n",
      "train loss:2.295074491779703\n",
      "train loss:2.299615317533586\n",
      "train loss:2.301966894268411\n",
      "train loss:2.298357115645276\n",
      "train loss:2.3064162302139115\n",
      "train loss:2.2966377613300097\n",
      "train loss:2.2999550034941065\n",
      "train loss:2.299079863814663\n",
      "train loss:2.3018901734412354\n",
      "train loss:2.3002382338361924\n",
      "train loss:2.3020960354823408\n",
      "train loss:2.301081159118667\n",
      "train loss:2.3095062125634858\n",
      "train loss:2.3002353309386723\n",
      "train loss:2.3025252002675436\n",
      "train loss:2.305469798124217\n",
      "train loss:2.310373185546362\n",
      "train loss:2.3040080412977564\n",
      "train loss:2.2966794318236095\n",
      "train loss:2.3006292053996193\n",
      "train loss:2.3035627899020823\n",
      "train loss:2.2945454078770235\n",
      "train loss:2.3030919777917767\n",
      "train loss:2.3036578363038687\n",
      "train loss:2.295684272855075\n",
      "train loss:2.3020780710488364\n",
      "train loss:2.298816073767561\n",
      "train loss:2.306144507944406\n",
      "train loss:2.3020541100714893\n",
      "train loss:2.3040437099320967\n",
      "train loss:2.3046988835619113\n",
      "train loss:2.2985898205109265\n",
      "train loss:2.302612571615011\n",
      "train loss:2.304009442261783\n",
      "train loss:2.2915726110274397\n",
      "train loss:2.3033432958209548\n",
      "train loss:2.303706494876378\n",
      "train loss:2.299895647260172\n",
      "train loss:2.3056913603895675\n",
      "train loss:2.2949837381625113\n",
      "train loss:2.2925159767352365\n",
      "train loss:2.2979212755916505\n",
      "train loss:2.3008621339071014\n",
      "train loss:2.297773962655822\n",
      "train loss:2.308620326652604\n",
      "train loss:2.2950075692847713\n",
      "train loss:2.3017575921207567\n",
      "train loss:2.301766088832564\n",
      "train loss:2.300747620637058\n",
      "train loss:2.3035677394838157\n",
      "train loss:2.303680821380975\n",
      "train loss:2.292343457619246\n",
      "train loss:2.3044606109091954\n",
      "train loss:2.299622841856334\n",
      "train loss:2.296574265737981\n",
      "train loss:2.3076492870421426\n",
      "train loss:2.2968873118362616\n",
      "train loss:2.301552999304949\n",
      "train loss:2.3099790982066786\n",
      "train loss:2.2953537491633513\n",
      "train loss:2.2957259504520033\n",
      "train loss:2.2940943311776545\n",
      "train loss:2.3091022721245675\n",
      "train loss:2.298213484784078\n",
      "train loss:2.30058575447041\n",
      "train loss:2.2936467443689654\n",
      "train loss:2.301918476135735\n",
      "train loss:2.299565018406144\n",
      "train loss:2.2992156208636425\n",
      "train loss:2.304190991433236\n",
      "train loss:2.297608739484618\n",
      "train loss:2.301036723427031\n",
      "train loss:2.293038714274785\n",
      "train loss:2.3033400135141893\n",
      "train loss:2.2964864310765254\n",
      "train loss:2.3001598246104535\n",
      "train loss:2.296230003755516\n",
      "train loss:2.2925325299248507\n",
      "train loss:2.3030660219861834\n",
      "train loss:2.2971171956316914\n",
      "=== epoch:104, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2978208866241707\n",
      "train loss:2.3021799275953434\n",
      "train loss:2.3075184884489133\n",
      "train loss:2.297829258656113\n",
      "train loss:2.3015102624834523\n",
      "train loss:2.2886160029767746\n",
      "train loss:2.297346983449732\n",
      "train loss:2.30209800006863\n",
      "train loss:2.306268520363343\n",
      "train loss:2.297707232386077\n",
      "train loss:2.299055981946076\n",
      "train loss:2.2986364794006495\n",
      "train loss:2.299385131100883\n",
      "train loss:2.3017656289807102\n",
      "train loss:2.2957536534967664\n",
      "train loss:2.3067674098684625\n",
      "train loss:2.3033209778100825\n",
      "train loss:2.299460923495385\n",
      "train loss:2.304637433122463\n",
      "train loss:2.3001855185629476\n",
      "train loss:2.3090483862514817\n",
      "train loss:2.3015837915945476\n",
      "train loss:2.3006283050557346\n",
      "train loss:2.2980798424287747\n",
      "train loss:2.313414237542099\n",
      "train loss:2.299902166437578\n",
      "train loss:2.298556421997371\n",
      "train loss:2.309007481040204\n",
      "train loss:2.301709214598736\n",
      "train loss:2.3028640736773838\n",
      "train loss:2.2983938253763574\n",
      "train loss:2.301826323290326\n",
      "train loss:2.3059665079626384\n",
      "train loss:2.3057928817361906\n",
      "train loss:2.299260544401849\n",
      "train loss:2.3009562048140184\n",
      "train loss:2.299961675590539\n",
      "train loss:2.2991970624144833\n",
      "train loss:2.2932768705903244\n",
      "train loss:2.300768518513315\n",
      "train loss:2.3027261686365708\n",
      "train loss:2.300823558959666\n",
      "train loss:2.298329092549072\n",
      "train loss:2.295411132459102\n",
      "train loss:2.29841353288289\n",
      "train loss:2.3065769172719124\n",
      "train loss:2.304723896738394\n",
      "train loss:2.3077407700196284\n",
      "train loss:2.307214752058921\n",
      "train loss:2.3073867720968777\n",
      "train loss:2.2950678612524347\n",
      "train loss:2.298861911961922\n",
      "train loss:2.2975988891657364\n",
      "train loss:2.3016948957534877\n",
      "train loss:2.306451471699709\n",
      "train loss:2.2981482180721047\n",
      "train loss:2.309808312883964\n",
      "train loss:2.3042766119581644\n",
      "train loss:2.2972507801125683\n",
      "train loss:2.3050729888911405\n",
      "train loss:2.309259645877585\n",
      "train loss:2.306211793024989\n",
      "train loss:2.3082682075502774\n",
      "train loss:2.3032433269533374\n",
      "train loss:2.2985676518795515\n",
      "train loss:2.307714308003786\n",
      "train loss:2.2953775126989218\n",
      "train loss:2.3031749932129717\n",
      "train loss:2.302364176053359\n",
      "train loss:2.3043128272457087\n",
      "train loss:2.2953008476953447\n",
      "train loss:2.293641324012372\n",
      "train loss:2.302021039309353\n",
      "train loss:2.2989873381054573\n",
      "train loss:2.309677981759609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3036668527935085\n",
      "train loss:2.2969939884744246\n",
      "train loss:2.2968757301981366\n",
      "train loss:2.2947129686477235\n",
      "train loss:2.3059521492228656\n",
      "train loss:2.310853528630702\n",
      "train loss:2.301891263211437\n",
      "train loss:2.306514791812458\n",
      "train loss:2.29886555734769\n",
      "train loss:2.3060829177542543\n",
      "train loss:2.300035831939913\n",
      "train loss:2.305232211508417\n",
      "train loss:2.306041156304377\n",
      "train loss:2.3040823609640144\n",
      "train loss:2.300177974223282\n",
      "train loss:2.308768675129419\n",
      "train loss:2.3004389549165185\n",
      "train loss:2.3083596103533113\n",
      "train loss:2.2999091485299545\n",
      "train loss:2.2926304767262615\n",
      "train loss:2.292371857127782\n",
      "train loss:2.2970389563074156\n",
      "train loss:2.299663907189478\n",
      "train loss:2.308747414072617\n",
      "train loss:2.300863990209393\n",
      "train loss:2.299114656105909\n",
      "train loss:2.305645280839885\n",
      "train loss:2.3033079955118554\n",
      "train loss:2.3009240896346808\n",
      "train loss:2.301714887876942\n",
      "train loss:2.2958130460818107\n",
      "train loss:2.291343997113856\n",
      "train loss:2.299019434511845\n",
      "train loss:2.2900766243898785\n",
      "train loss:2.303518276557869\n",
      "train loss:2.3084997546097297\n",
      "train loss:2.304737909903791\n",
      "train loss:2.2908619880154877\n",
      "train loss:2.297980589842097\n",
      "train loss:2.3034838883138793\n",
      "train loss:2.3015366884024493\n",
      "train loss:2.2967618857066485\n",
      "train loss:2.2964928099147635\n",
      "train loss:2.3020135631197554\n",
      "train loss:2.3090184824504867\n",
      "train loss:2.302093703415394\n",
      "train loss:2.2904694632432947\n",
      "train loss:2.3066036516841066\n",
      "train loss:2.3001889825281228\n",
      "train loss:2.298087864714158\n",
      "train loss:2.303639538617362\n",
      "train loss:2.3090033319514567\n",
      "train loss:2.293385042939313\n",
      "train loss:2.302483163771571\n",
      "train loss:2.296577530022689\n",
      "train loss:2.2943242121366167\n",
      "train loss:2.300074200414654\n",
      "train loss:2.299043505513952\n",
      "train loss:2.3016422090664963\n",
      "train loss:2.2974219284831694\n",
      "train loss:2.299478294035918\n",
      "train loss:2.3072396039623957\n",
      "train loss:2.30899924745288\n",
      "train loss:2.2988194313435915\n",
      "train loss:2.301515745282498\n",
      "train loss:2.298609816019784\n",
      "train loss:2.3007362473836053\n",
      "train loss:2.304843265621505\n",
      "train loss:2.2989763719597645\n",
      "train loss:2.2994936368231063\n",
      "train loss:2.306258554195597\n",
      "train loss:2.303792198865472\n",
      "train loss:2.3036981633347517\n",
      "train loss:2.3002738141622863\n",
      "train loss:2.3034810262775913\n",
      "train loss:2.298041978284356\n",
      "train loss:2.2965367848626306\n",
      "train loss:2.302099636127916\n",
      "train loss:2.2973476873818264\n",
      "train loss:2.2991503899396495\n",
      "train loss:2.302030934199541\n",
      "train loss:2.2972140246183654\n",
      "train loss:2.2971594175927676\n",
      "train loss:2.301418255363553\n",
      "train loss:2.2979214677557795\n",
      "train loss:2.299442924403769\n",
      "train loss:2.2983804620484234\n",
      "train loss:2.29329329246491\n",
      "train loss:2.307649590477242\n",
      "train loss:2.301107062637719\n",
      "train loss:2.295848071677846\n",
      "train loss:2.2996989529544574\n",
      "train loss:2.3104340295290298\n",
      "train loss:2.29942677405041\n",
      "train loss:2.296641082695948\n",
      "train loss:2.30349330548374\n",
      "train loss:2.298818320744661\n",
      "train loss:2.297042234392317\n",
      "train loss:2.3010767772187224\n",
      "train loss:2.3016088380124917\n",
      "train loss:2.2960872777783057\n",
      "train loss:2.306465148004548\n",
      "train loss:2.3059473499962047\n",
      "train loss:2.307166152935135\n",
      "train loss:2.3115801801398597\n",
      "train loss:2.2866732678261936\n",
      "train loss:2.3044566365529753\n",
      "train loss:2.302850060051297\n",
      "train loss:2.2957245853932013\n",
      "train loss:2.3019433268970806\n",
      "train loss:2.306635305675913\n",
      "train loss:2.310894691946982\n",
      "train loss:2.3095422416497953\n",
      "train loss:2.2993982223782927\n",
      "train loss:2.296112156805402\n",
      "train loss:2.298970843634471\n",
      "train loss:2.309916098623858\n",
      "train loss:2.298420830839419\n",
      "train loss:2.3024635828299815\n",
      "train loss:2.301820540894678\n",
      "train loss:2.3040860231676894\n",
      "train loss:2.2943866265541715\n",
      "train loss:2.3137877624219776\n",
      "train loss:2.3026640358622585\n",
      "train loss:2.3083648155310317\n",
      "=== epoch:105, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.298802765317206\n",
      "train loss:2.299309049647778\n",
      "train loss:2.295677725142457\n",
      "train loss:2.3033330880172964\n",
      "train loss:2.304757198495376\n",
      "train loss:2.2930838722564713\n",
      "train loss:2.2967272882743752\n",
      "train loss:2.297221641775298\n",
      "train loss:2.296696764196488\n",
      "train loss:2.3014405806696123\n",
      "train loss:2.3006340329336656\n",
      "train loss:2.2973690683052532\n",
      "train loss:2.2907760891758833\n",
      "train loss:2.2983884647060715\n",
      "train loss:2.3009693429080733\n",
      "train loss:2.306862835908955\n",
      "train loss:2.2953115940557125\n",
      "train loss:2.2991034750350474\n",
      "train loss:2.303978305568601\n",
      "train loss:2.2975214306452636\n",
      "train loss:2.3069597805868662\n",
      "train loss:2.30075185364267\n",
      "train loss:2.2995064589468495\n",
      "train loss:2.2987991724979615\n",
      "train loss:2.3078267018600473\n",
      "train loss:2.299349496352522\n",
      "train loss:2.2931151153951292\n",
      "train loss:2.302146716788674\n",
      "train loss:2.3033573690662177\n",
      "train loss:2.2996921388952916\n",
      "train loss:2.3049432082812893\n",
      "train loss:2.302877111493963\n",
      "train loss:2.298198644413827\n",
      "train loss:2.305949228652966\n",
      "train loss:2.302576188166749\n",
      "train loss:2.307680317023941\n",
      "train loss:2.294570487135062\n",
      "train loss:2.293889518638435\n",
      "train loss:2.3151138434268086\n",
      "train loss:2.2963142254360696\n",
      "train loss:2.2969940215110194\n",
      "train loss:2.3001913355488814\n",
      "train loss:2.3048118463421847\n",
      "train loss:2.297069089595068\n",
      "train loss:2.2975110321307306\n",
      "train loss:2.294933102271819\n",
      "train loss:2.305857625969238\n",
      "train loss:2.2997283995448248\n",
      "train loss:2.303074741377687\n",
      "train loss:2.300329574160354\n",
      "train loss:2.3107157958293087\n",
      "train loss:2.301020003955573\n",
      "train loss:2.3026696742264257\n",
      "train loss:2.2943140373211155\n",
      "train loss:2.304104100211196\n",
      "train loss:2.311515954857345\n",
      "train loss:2.3024115043071034\n",
      "train loss:2.299413943926376\n",
      "train loss:2.306385960473756\n",
      "train loss:2.3104233254750013\n",
      "train loss:2.3009376922911207\n",
      "train loss:2.3123696472517214\n",
      "train loss:2.2976939416582316\n",
      "train loss:2.3080227222341883\n",
      "train loss:2.2952379850784217\n",
      "train loss:2.3056680205191453\n",
      "train loss:2.312079513642614\n",
      "train loss:2.2999316625200206\n",
      "train loss:2.3061893187615836\n",
      "train loss:2.3029034882268014\n",
      "train loss:2.2960014496148\n",
      "train loss:2.307612002868084\n",
      "train loss:2.3044523490247286\n",
      "train loss:2.3063815130775076\n",
      "train loss:2.301249353647682\n",
      "train loss:2.2996834180192858\n",
      "train loss:2.292626836079867\n",
      "train loss:2.3017703463598935\n",
      "train loss:2.301303394267365\n",
      "train loss:2.304622262260334\n",
      "train loss:2.2985868638471216\n",
      "train loss:2.3023077471715214\n",
      "train loss:2.306454121645761\n",
      "train loss:2.2983849884831837\n",
      "train loss:2.290943243051133\n",
      "train loss:2.297105065598236\n",
      "train loss:2.3000503757499855\n",
      "train loss:2.2982148600688856\n",
      "train loss:2.302843282699328\n",
      "train loss:2.302465903057929\n",
      "train loss:2.294531099813226\n",
      "train loss:2.3021511225482985\n",
      "train loss:2.307261934657994\n",
      "train loss:2.297660673835431\n",
      "train loss:2.3077900254727615\n",
      "train loss:2.298892035278022\n",
      "train loss:2.3054377386558516\n",
      "train loss:2.2928546279438335\n",
      "train loss:2.304137581927709\n",
      "train loss:2.300721981504633\n",
      "train loss:2.306882105864985\n",
      "train loss:2.2989091011543747\n",
      "train loss:2.2965742283627577\n",
      "train loss:2.303437924690361\n",
      "train loss:2.3075103170551765\n",
      "train loss:2.3083622705442313\n",
      "train loss:2.3041132564310227\n",
      "train loss:2.300769325115083\n",
      "train loss:2.303191917263891\n",
      "train loss:2.3042425557450583\n",
      "train loss:2.305376830001776\n",
      "train loss:2.3042368058464335\n",
      "train loss:2.3002222571737727\n",
      "train loss:2.2979943355750363\n",
      "train loss:2.3119336249354214\n",
      "train loss:2.3003018332901437\n",
      "train loss:2.300631047324045\n",
      "train loss:2.299297573274607\n",
      "train loss:2.3031857094538783\n",
      "train loss:2.2990317942267766\n",
      "train loss:2.3004478123052285\n",
      "train loss:2.29734704249464\n",
      "train loss:2.2957667293263953\n",
      "train loss:2.3000888239030974\n",
      "train loss:2.299799290059818\n",
      "train loss:2.298624437257811\n",
      "train loss:2.2923568727332513\n",
      "train loss:2.295296418353784\n",
      "train loss:2.3003164412421238\n",
      "train loss:2.301835546172578\n",
      "train loss:2.30506436021866\n",
      "train loss:2.294051388675388\n",
      "train loss:2.3067743705389123\n",
      "train loss:2.303625549041546\n",
      "train loss:2.3024077105783407\n",
      "train loss:2.2958990431203237\n",
      "train loss:2.297976193374405\n",
      "train loss:2.305955225301008\n",
      "train loss:2.294817898409547\n",
      "train loss:2.3049121851999717\n",
      "train loss:2.301792771814037\n",
      "train loss:2.3024311819139505\n",
      "train loss:2.2995565649402088\n",
      "train loss:2.2941019629310446\n",
      "train loss:2.306491325181964\n",
      "train loss:2.302196644684321\n",
      "train loss:2.3019467727193126\n",
      "train loss:2.2982400907887968\n",
      "train loss:2.2963126562074194\n",
      "train loss:2.2994669415262425\n",
      "train loss:2.307521584045886\n",
      "train loss:2.2949434207789734\n",
      "train loss:2.302103565509498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2882773705504404\n",
      "train loss:2.3108302741032487\n",
      "train loss:2.293675494216601\n",
      "train loss:2.295906419827002\n",
      "train loss:2.29423271355255\n",
      "train loss:2.3045010681978795\n",
      "train loss:2.3106666849074204\n",
      "train loss:2.302954140771305\n",
      "train loss:2.303066377043095\n",
      "train loss:2.3110350625129006\n",
      "train loss:2.297632353480358\n",
      "train loss:2.3074640815323657\n",
      "train loss:2.3026839821620912\n",
      "train loss:2.3062522221890176\n",
      "train loss:2.296501640630094\n",
      "train loss:2.3081804456501827\n",
      "train loss:2.3040009280590423\n",
      "train loss:2.2976594736249276\n",
      "train loss:2.2965313867206274\n",
      "train loss:2.3022506142704686\n",
      "train loss:2.3065015351598013\n",
      "train loss:2.298204757690599\n",
      "train loss:2.3001565894439318\n",
      "train loss:2.306094587985692\n",
      "train loss:2.304777581669495\n",
      "train loss:2.3002642525496597\n",
      "train loss:2.3076942889634986\n",
      "train loss:2.3099256754898665\n",
      "train loss:2.3054129785609763\n",
      "train loss:2.3089920922180758\n",
      "train loss:2.299385617205024\n",
      "train loss:2.2970591388280415\n",
      "train loss:2.305436833275758\n",
      "train loss:2.297006151905117\n",
      "train loss:2.2992450726631426\n",
      "train loss:2.3021790560053303\n",
      "train loss:2.301583976168926\n",
      "train loss:2.3005022790185294\n",
      "train loss:2.309394945827715\n",
      "train loss:2.3048359272313985\n",
      "train loss:2.295144175123032\n",
      "train loss:2.2994411614801904\n",
      "train loss:2.295406371707529\n",
      "train loss:2.3014072816759925\n",
      "train loss:2.303541688748813\n",
      "train loss:2.3136267823837287\n",
      "train loss:2.301761952011653\n",
      "=== epoch:106, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301594467136472\n",
      "train loss:2.305751537299604\n",
      "train loss:2.3020004028501106\n",
      "train loss:2.297547884917838\n",
      "train loss:2.3015150009370053\n",
      "train loss:2.3058558480811406\n",
      "train loss:2.30104943327039\n",
      "train loss:2.296444523444167\n",
      "train loss:2.3028711288719794\n",
      "train loss:2.300488764907932\n",
      "train loss:2.3006018944289077\n",
      "train loss:2.2977232997159382\n",
      "train loss:2.3012454960778883\n",
      "train loss:2.305869115610851\n",
      "train loss:2.300524776060878\n",
      "train loss:2.3028096474623982\n",
      "train loss:2.3007658025387365\n",
      "train loss:2.31214829221821\n",
      "train loss:2.2993813888490755\n",
      "train loss:2.3122236737189312\n",
      "train loss:2.306358947087551\n",
      "train loss:2.29879471741612\n",
      "train loss:2.299274220190301\n",
      "train loss:2.305611021617483\n",
      "train loss:2.2998139733057483\n",
      "train loss:2.2988238111578316\n",
      "train loss:2.2928836144206657\n",
      "train loss:2.3098213302334245\n",
      "train loss:2.303039262039452\n",
      "train loss:2.304118511442345\n",
      "train loss:2.300788917111652\n",
      "train loss:2.3050769667870874\n",
      "train loss:2.302688403149941\n",
      "train loss:2.300039887532926\n",
      "train loss:2.3068186568597406\n",
      "train loss:2.3050921119606715\n",
      "train loss:2.298242877689074\n",
      "train loss:2.3048063010810553\n",
      "train loss:2.302250209771479\n",
      "train loss:2.301787161208656\n",
      "train loss:2.30493185714856\n",
      "train loss:2.3004938060647886\n",
      "train loss:2.301844841056655\n",
      "train loss:2.301399922791708\n",
      "train loss:2.3025033148238743\n",
      "train loss:2.297003066094709\n",
      "train loss:2.30363045158264\n",
      "train loss:2.301886601244411\n",
      "train loss:2.29934568160719\n",
      "train loss:2.295592695282124\n",
      "train loss:2.3012863295132653\n",
      "train loss:2.2945069991427087\n",
      "train loss:2.303919324644992\n",
      "train loss:2.299309585253283\n",
      "train loss:2.300426656165565\n",
      "train loss:2.305002678162722\n",
      "train loss:2.297632951076651\n",
      "train loss:2.302025696023702\n",
      "train loss:2.293744255415646\n",
      "train loss:2.3019651841166358\n",
      "train loss:2.304788397184762\n",
      "train loss:2.3061188500060665\n",
      "train loss:2.303766703314136\n",
      "train loss:2.3090371382248702\n",
      "train loss:2.2935023931459324\n",
      "train loss:2.300048658774672\n",
      "train loss:2.3076225607026744\n",
      "train loss:2.2955929474130436\n",
      "train loss:2.3005317921429382\n",
      "train loss:2.3027183237183415\n",
      "train loss:2.3041143799600663\n",
      "train loss:2.3042332105232073\n",
      "train loss:2.2981880283896228\n",
      "train loss:2.30070274745752\n",
      "train loss:2.306944083178446\n",
      "train loss:2.312262128057152\n",
      "train loss:2.305601054553874\n",
      "train loss:2.291072629237762\n",
      "train loss:2.3088725371258265\n",
      "train loss:2.2984523582752967\n",
      "train loss:2.3022350197476213\n",
      "train loss:2.3057456074556164\n",
      "train loss:2.3025643968031795\n",
      "train loss:2.292204515747321\n",
      "train loss:2.2977493872433934\n",
      "train loss:2.299006570757225\n",
      "train loss:2.298820995784458\n",
      "train loss:2.300748479773522\n",
      "train loss:2.305079630122948\n",
      "train loss:2.2980258290812827\n",
      "train loss:2.3039534212632904\n",
      "train loss:2.3014516883690423\n",
      "train loss:2.3005281075444723\n",
      "train loss:2.3023736645199926\n",
      "train loss:2.29466413689813\n",
      "train loss:2.3043797559610364\n",
      "train loss:2.3020047086036675\n",
      "train loss:2.2970212069991547\n",
      "train loss:2.313121774943864\n",
      "train loss:2.302683938657407\n",
      "train loss:2.3132766666142475\n",
      "train loss:2.2974997749429713\n",
      "train loss:2.300157884022577\n",
      "train loss:2.2972836206914558\n",
      "train loss:2.3028786722058623\n",
      "train loss:2.2989019280584246\n",
      "train loss:2.3076575172014997\n",
      "train loss:2.305620643776307\n",
      "train loss:2.299041573040978\n",
      "train loss:2.2991148492922315\n",
      "train loss:2.2945868349631042\n",
      "train loss:2.3035241386717114\n",
      "train loss:2.3015941462315053\n",
      "train loss:2.303698852468809\n",
      "train loss:2.3030855944194624\n",
      "train loss:2.303451382078924\n",
      "train loss:2.3158146581742454\n",
      "train loss:2.3041361754557306\n",
      "train loss:2.3078013409859564\n",
      "train loss:2.3001071730926355\n",
      "train loss:2.2981815348744923\n",
      "train loss:2.2948804554695177\n",
      "train loss:2.300949972095341\n",
      "train loss:2.2948762335962716\n",
      "train loss:2.2970808770668323\n",
      "train loss:2.298405382292016\n",
      "train loss:2.300547455341681\n",
      "train loss:2.304839065927806\n",
      "train loss:2.300002260593425\n",
      "train loss:2.3060880308969196\n",
      "train loss:2.305263815013336\n",
      "train loss:2.298777046155353\n",
      "train loss:2.2956950239446385\n",
      "train loss:2.308703432308114\n",
      "train loss:2.3030556292231696\n",
      "train loss:2.304347711147688\n",
      "train loss:2.3055425670168583\n",
      "train loss:2.3141504511876847\n",
      "train loss:2.2944430511652483\n",
      "train loss:2.301624120459136\n",
      "train loss:2.298601371728406\n",
      "train loss:2.2979201328038683\n",
      "train loss:2.299123940399525\n",
      "train loss:2.303200772574306\n",
      "train loss:2.2994815557974455\n",
      "train loss:2.2993253825468996\n",
      "train loss:2.3001399866654006\n",
      "train loss:2.299772892517656\n",
      "train loss:2.299038894673798\n",
      "train loss:2.2999181952072045\n",
      "train loss:2.2964106514575606\n",
      "train loss:2.3027907868048487\n",
      "train loss:2.3010187988854276\n",
      "train loss:2.296776116397483\n",
      "train loss:2.298019591609843\n",
      "train loss:2.2975802587660494\n",
      "train loss:2.297344225576613\n",
      "train loss:2.3014340336633845\n",
      "train loss:2.3097285667612666\n",
      "train loss:2.299594359098471\n",
      "train loss:2.3030862653299544\n",
      "train loss:2.303769598643876\n",
      "train loss:2.303564681721886\n",
      "train loss:2.3020015587210017\n",
      "train loss:2.297486778955058\n",
      "train loss:2.303654144148564\n",
      "train loss:2.3078604671184726\n",
      "train loss:2.294468728887012\n",
      "train loss:2.305233629531148\n",
      "train loss:2.2981440935672715\n",
      "train loss:2.3035495557429235\n",
      "train loss:2.29777407715244\n",
      "train loss:2.2931021721784997\n",
      "train loss:2.3000542050090997\n",
      "train loss:2.2983052340056114\n",
      "train loss:2.309299305470649\n",
      "train loss:2.302694039877472\n",
      "train loss:2.2959081581285328\n",
      "train loss:2.2999821395228075\n",
      "train loss:2.3037064946761503\n",
      "train loss:2.300379668331569\n",
      "train loss:2.3026948928963757\n",
      "train loss:2.30311287967379\n",
      "train loss:2.3009603058880614\n",
      "train loss:2.303128976103816\n",
      "train loss:2.3011209706023315\n",
      "train loss:2.302066035056724\n",
      "train loss:2.306079539688416\n",
      "train loss:2.3100056619812213\n",
      "train loss:2.298311141400064\n",
      "train loss:2.3004839033749467\n",
      "train loss:2.296399330628855\n",
      "train loss:2.3018452768848703\n",
      "train loss:2.2931016138815346\n",
      "train loss:2.3049180710718002\n",
      "train loss:2.294589613481647\n",
      "train loss:2.3057077558836645\n",
      "train loss:2.307176603135788\n",
      "train loss:2.3058019266860192\n",
      "train loss:2.3001547169392835\n",
      "=== epoch:107, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3022015747270044\n",
      "train loss:2.2984030677298906\n",
      "train loss:2.2964090645790427\n",
      "train loss:2.30522116122141\n",
      "train loss:2.30499229388267\n",
      "train loss:2.294420041599383\n",
      "train loss:2.3073309762752094\n",
      "train loss:2.302133117423761\n",
      "train loss:2.3029333508492966\n",
      "train loss:2.3054947035507802\n",
      "train loss:2.2933229855164567\n",
      "train loss:2.2995429839124397\n",
      "train loss:2.292396194175448\n",
      "train loss:2.3084721860978528\n",
      "train loss:2.3048422099695176\n",
      "train loss:2.2963801204953374\n",
      "train loss:2.299890746947917\n",
      "train loss:2.2954612694472094\n",
      "train loss:2.287879194393715\n",
      "train loss:2.303259692128861\n",
      "train loss:2.30013337511091\n",
      "train loss:2.295042424631557\n",
      "train loss:2.298229836495455\n",
      "train loss:2.3117999896420414\n",
      "train loss:2.3029432857471765\n",
      "train loss:2.3059913741992237\n",
      "train loss:2.306813882030293\n",
      "train loss:2.3079809643994706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.305047605028588\n",
      "train loss:2.3052420309168604\n",
      "train loss:2.3040824415527386\n",
      "train loss:2.3021596772370607\n",
      "train loss:2.2960396964663987\n",
      "train loss:2.3012860316096106\n",
      "train loss:2.3048866109970945\n",
      "train loss:2.3003881956375176\n",
      "train loss:2.3019636840149826\n",
      "train loss:2.301337380320995\n",
      "train loss:2.3048554504537986\n",
      "train loss:2.2989316753681575\n",
      "train loss:2.300956235795429\n",
      "train loss:2.2998296428218237\n",
      "train loss:2.3010198228770653\n",
      "train loss:2.3004418420447776\n",
      "train loss:2.3013991405088783\n",
      "train loss:2.3012074330505636\n",
      "train loss:2.305795984220629\n",
      "train loss:2.300005345705746\n",
      "train loss:2.2999509718321423\n",
      "train loss:2.3011308326379374\n",
      "train loss:2.298626972060138\n",
      "train loss:2.3065047402894763\n",
      "train loss:2.2936732142252794\n",
      "train loss:2.2858134103917283\n",
      "train loss:2.2987122097364905\n",
      "train loss:2.301898777002519\n",
      "train loss:2.2983961055043913\n",
      "train loss:2.2910200471492157\n",
      "train loss:2.2956272962452955\n",
      "train loss:2.309058925591339\n",
      "train loss:2.307203441709111\n",
      "train loss:2.303797828271793\n",
      "train loss:2.300573598663722\n",
      "train loss:2.303376326063017\n",
      "train loss:2.29605600921016\n",
      "train loss:2.309111708730416\n",
      "train loss:2.3042675888933446\n",
      "train loss:2.297170644132819\n",
      "train loss:2.305395737736334\n",
      "train loss:2.302026649411994\n",
      "train loss:2.316735064778328\n",
      "train loss:2.299587647569793\n",
      "train loss:2.3047206573411683\n",
      "train loss:2.29698362503218\n",
      "train loss:2.305556079075613\n",
      "train loss:2.3064541080915757\n",
      "train loss:2.2908840020420618\n",
      "train loss:2.3024863534183457\n",
      "train loss:2.303840810757532\n",
      "train loss:2.3023436157620187\n",
      "train loss:2.305588254977256\n",
      "train loss:2.300494197494199\n",
      "train loss:2.295625945298133\n",
      "train loss:2.3030487002647133\n",
      "train loss:2.305905642077484\n",
      "train loss:2.3009077511177134\n",
      "train loss:2.3073141079534127\n",
      "train loss:2.2936639639588488\n",
      "train loss:2.305411025643076\n",
      "train loss:2.300514376795414\n",
      "train loss:2.3057419737282174\n",
      "train loss:2.3142444028294062\n",
      "train loss:2.294714427938441\n",
      "train loss:2.295934415844838\n",
      "train loss:2.298072190667848\n",
      "train loss:2.3028860617379996\n",
      "train loss:2.305413023045587\n",
      "train loss:2.302801452887735\n",
      "train loss:2.292660126484204\n",
      "train loss:2.3021334289341437\n",
      "train loss:2.30446124720523\n",
      "train loss:2.3041993682169086\n",
      "train loss:2.308795984620033\n",
      "train loss:2.303198399607159\n",
      "train loss:2.302190070303747\n",
      "train loss:2.3047014230643152\n",
      "train loss:2.3021536944050434\n",
      "train loss:2.300274790264912\n",
      "train loss:2.2996560855857306\n",
      "train loss:2.300857862852553\n",
      "train loss:2.3070192311588507\n",
      "train loss:2.302538661374394\n",
      "train loss:2.298929157125253\n",
      "train loss:2.302873300548109\n",
      "train loss:2.2995575577850915\n",
      "train loss:2.304250461842194\n",
      "train loss:2.3067764142242457\n",
      "train loss:2.3009559957373082\n",
      "train loss:2.2994990288598096\n",
      "train loss:2.2986534667492293\n",
      "train loss:2.3036032667211783\n",
      "train loss:2.2977000699751673\n",
      "train loss:2.294140658852154\n",
      "train loss:2.3044321845007016\n",
      "train loss:2.3083297775743064\n",
      "train loss:2.3010439308216806\n",
      "train loss:2.3070312724213657\n",
      "train loss:2.304929804885432\n",
      "train loss:2.3025020506360416\n",
      "train loss:2.3006837501632385\n",
      "train loss:2.2978141928916083\n",
      "train loss:2.300931939963989\n",
      "train loss:2.3048373439190004\n",
      "train loss:2.299047095848973\n",
      "train loss:2.3004826234650535\n",
      "train loss:2.3023052017425996\n",
      "train loss:2.3047950161712976\n",
      "train loss:2.301831120256113\n",
      "train loss:2.300503747026241\n",
      "train loss:2.30181550877721\n",
      "train loss:2.3001888865291606\n",
      "train loss:2.3006401222251687\n",
      "train loss:2.306234144102726\n",
      "train loss:2.3046458316474068\n",
      "train loss:2.29670530254218\n",
      "train loss:2.292660931105905\n",
      "train loss:2.301728092289187\n",
      "train loss:2.3034417721134934\n",
      "train loss:2.3081444989567412\n",
      "train loss:2.3073016374676047\n",
      "train loss:2.3011986673785607\n",
      "train loss:2.308373896212802\n",
      "train loss:2.29653568167063\n",
      "train loss:2.2943884238807937\n",
      "train loss:2.3025776485804608\n",
      "train loss:2.2991506210721635\n",
      "train loss:2.302627909337316\n",
      "train loss:2.304219955507467\n",
      "train loss:2.296225220907293\n",
      "train loss:2.2979684249355588\n",
      "train loss:2.3017859902264832\n",
      "train loss:2.3058844731761505\n",
      "train loss:2.3077002603217043\n",
      "train loss:2.3014885564322918\n",
      "train loss:2.3029770653701926\n",
      "train loss:2.299861330939629\n",
      "train loss:2.297538634653173\n",
      "train loss:2.307623185422027\n",
      "train loss:2.299978171795565\n",
      "train loss:2.301083007625454\n",
      "train loss:2.3085908121769543\n",
      "train loss:2.3007575298717837\n",
      "train loss:2.2967822247791854\n",
      "train loss:2.29825446811301\n",
      "train loss:2.29803053096534\n",
      "train loss:2.3007592849999696\n",
      "train loss:2.2962122598968073\n",
      "train loss:2.303100267660633\n",
      "train loss:2.307968557899972\n",
      "train loss:2.2965075514652478\n",
      "train loss:2.3011859749030004\n",
      "train loss:2.301377864845993\n",
      "train loss:2.303037950953203\n",
      "train loss:2.300170562244512\n",
      "train loss:2.2997288183974156\n",
      "train loss:2.3075167914567105\n",
      "train loss:2.2988836701152855\n",
      "train loss:2.2963629679154045\n",
      "train loss:2.295197054625803\n",
      "train loss:2.302508685820035\n",
      "train loss:2.3043588162472606\n",
      "train loss:2.3024530500287654\n",
      "train loss:2.299334680344829\n",
      "train loss:2.297995773356367\n",
      "train loss:2.300574682232123\n",
      "train loss:2.3005563123984856\n",
      "train loss:2.3054663108387183\n",
      "train loss:2.302939341303762\n",
      "train loss:2.2995184336550034\n",
      "train loss:2.3047317247812327\n",
      "=== epoch:108, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2986905890807168\n",
      "train loss:2.2938495210986605\n",
      "train loss:2.3039291687400496\n",
      "train loss:2.3006256478559224\n",
      "train loss:2.3018374144044036\n",
      "train loss:2.303059741694399\n",
      "train loss:2.295804684261061\n",
      "train loss:2.3070243961888717\n",
      "train loss:2.3045143745460295\n",
      "train loss:2.30711456732932\n",
      "train loss:2.3014394825563036\n",
      "train loss:2.3074655372096124\n",
      "train loss:2.31310782915661\n",
      "train loss:2.305357433732145\n",
      "train loss:2.3008541800751154\n",
      "train loss:2.3067628912375393\n",
      "train loss:2.301974980643416\n",
      "train loss:2.3039224002121665\n",
      "train loss:2.2991923586106426\n",
      "train loss:2.310048864868681\n",
      "train loss:2.2992914817513728\n",
      "train loss:2.3028480405767335\n",
      "train loss:2.3071007383934043\n",
      "train loss:2.30921334130275\n",
      "train loss:2.3052717622238026\n",
      "train loss:2.294728299216864\n",
      "train loss:2.300129523148542\n",
      "train loss:2.307694694747547\n",
      "train loss:2.3026357488492075\n",
      "train loss:2.3032256977260337\n",
      "train loss:2.2970872189048315\n",
      "train loss:2.30303691232327\n",
      "train loss:2.2978095568677417\n",
      "train loss:2.2978158083994225\n",
      "train loss:2.2968443445305513\n",
      "train loss:2.3051317842877403\n",
      "train loss:2.3050123286271473\n",
      "train loss:2.3073883155441792\n",
      "train loss:2.298947576814024\n",
      "train loss:2.2897325790743137\n",
      "train loss:2.2980906605832274\n",
      "train loss:2.3020496485572814\n",
      "train loss:2.3017539782201126\n",
      "train loss:2.302209082053089\n",
      "train loss:2.3020057288242803\n",
      "train loss:2.299488341055135\n",
      "train loss:2.298209430090875\n",
      "train loss:2.3008984939760992\n",
      "train loss:2.3026199263272495\n",
      "train loss:2.2963001723670895\n",
      "train loss:2.3016242479495967\n",
      "train loss:2.3009720511714575\n",
      "train loss:2.3038491447181517\n",
      "train loss:2.3086400261620743\n",
      "train loss:2.3020350799684675\n",
      "train loss:2.300168942128516\n",
      "train loss:2.298158488744051\n",
      "train loss:2.2970793331265327\n",
      "train loss:2.308086299099548\n",
      "train loss:2.3068647186187015\n",
      "train loss:2.305734711514822\n",
      "train loss:2.3031360417075324\n",
      "train loss:2.301359678499492\n",
      "train loss:2.3000309438910214\n",
      "train loss:2.301896761372801\n",
      "train loss:2.2958779419154514\n",
      "train loss:2.307208772557789\n",
      "train loss:2.3010504674059016\n",
      "train loss:2.2966391662912717\n",
      "train loss:2.3030694604475968\n",
      "train loss:2.296911080388352\n",
      "train loss:2.30110205913745\n",
      "train loss:2.3001866086897356\n",
      "train loss:2.2981188808382402\n",
      "train loss:2.3023293974325227\n",
      "train loss:2.3017458978810774\n",
      "train loss:2.298499968289846\n",
      "train loss:2.296717491836641\n",
      "train loss:2.2942689362593347\n",
      "train loss:2.3077499880155408\n",
      "train loss:2.2998744083253775\n",
      "train loss:2.305282776714888\n",
      "train loss:2.298688254683741\n",
      "train loss:2.303351663643476\n",
      "train loss:2.304846483680797\n",
      "train loss:2.2995073050894534\n",
      "train loss:2.2916170880770275\n",
      "train loss:2.2960300427745084\n",
      "train loss:2.300432151390152\n",
      "train loss:2.2969108641210445\n",
      "train loss:2.296707617900682\n",
      "train loss:2.309960631185312\n",
      "train loss:2.303665167011574\n",
      "train loss:2.2937463455966216\n",
      "train loss:2.293159039727902\n",
      "train loss:2.2995301548781497\n",
      "train loss:2.297209280313024\n",
      "train loss:2.3002869000813675\n",
      "train loss:2.3000159331515824\n",
      "train loss:2.3000976887915847\n",
      "train loss:2.2990830108749214\n",
      "train loss:2.3008640123826116\n",
      "train loss:2.3092047045450643\n",
      "train loss:2.2974329579491837\n",
      "train loss:2.304587428949506\n",
      "train loss:2.308264748347288\n",
      "train loss:2.2917436369754483\n",
      "train loss:2.3016296346452765\n",
      "train loss:2.306889271336592\n",
      "train loss:2.304404100115361\n",
      "train loss:2.30492528494409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302960535929121\n",
      "train loss:2.3018102101415834\n",
      "train loss:2.302782979893412\n",
      "train loss:2.293312999421215\n",
      "train loss:2.2992520645118657\n",
      "train loss:2.3024188035143043\n",
      "train loss:2.303317306147496\n",
      "train loss:2.3014016344063966\n",
      "train loss:2.303242761999567\n",
      "train loss:2.2980190885893372\n",
      "train loss:2.3006563352256415\n",
      "train loss:2.2985648474247005\n",
      "train loss:2.306199034427412\n",
      "train loss:2.3105762967681125\n",
      "train loss:2.30250382652481\n",
      "train loss:2.30579812670023\n",
      "train loss:2.308638601945997\n",
      "train loss:2.2945235948576848\n",
      "train loss:2.3092354127396875\n",
      "train loss:2.3061271892939126\n",
      "train loss:2.296362602447379\n",
      "train loss:2.2995281562099037\n",
      "train loss:2.3041906839640114\n",
      "train loss:2.300656439659916\n",
      "train loss:2.3020006526727537\n",
      "train loss:2.2914171336620046\n",
      "train loss:2.2959264716290004\n",
      "train loss:2.304319778714678\n",
      "train loss:2.3076786334727712\n",
      "train loss:2.29703785767665\n",
      "train loss:2.3001536429387697\n",
      "train loss:2.3013600895760504\n",
      "train loss:2.304828414032916\n",
      "train loss:2.303604820088784\n",
      "train loss:2.2955508714560486\n",
      "train loss:2.305062753208796\n",
      "train loss:2.3072672626796598\n",
      "train loss:2.2992858550328616\n",
      "train loss:2.301715615745017\n",
      "train loss:2.3029869087398476\n",
      "train loss:2.2928702441926734\n",
      "train loss:2.302055955399538\n",
      "train loss:2.3026870294072266\n",
      "train loss:2.302487157967382\n",
      "train loss:2.2926408574429122\n",
      "train loss:2.3067063104265433\n",
      "train loss:2.3050961390787514\n",
      "train loss:2.2993548664361456\n",
      "train loss:2.2966714979062557\n",
      "train loss:2.309994291003484\n",
      "train loss:2.290093883230979\n",
      "train loss:2.2966200988475345\n",
      "train loss:2.3072665004264192\n",
      "train loss:2.3039134827436007\n",
      "train loss:2.3054592436529213\n",
      "train loss:2.2966304822511603\n",
      "train loss:2.2999466795244228\n",
      "train loss:2.303955048504927\n",
      "train loss:2.3042110123816317\n",
      "train loss:2.297658441130138\n",
      "train loss:2.30210805488009\n",
      "train loss:2.3019650483879452\n",
      "train loss:2.3036428761015015\n",
      "train loss:2.299083087105908\n",
      "train loss:2.2967192674291517\n",
      "train loss:2.292035502855162\n",
      "train loss:2.2997288438370522\n",
      "train loss:2.3037531916197573\n",
      "train loss:2.3039729750774463\n",
      "train loss:2.299853419198135\n",
      "train loss:2.2993725967327214\n",
      "train loss:2.311955007219462\n",
      "train loss:2.3085228569869414\n",
      "train loss:2.3073049433009265\n",
      "train loss:2.3017373285666225\n",
      "train loss:2.3102286894943638\n",
      "train loss:2.3046176174288\n",
      "train loss:2.3067422057979137\n",
      "train loss:2.290923269832781\n",
      "train loss:2.3048101074474605\n",
      "train loss:2.304462906212112\n",
      "train loss:2.3016147582007673\n",
      "train loss:2.3073695984312885\n",
      "train loss:2.2983383549012233\n",
      "train loss:2.29932237807732\n",
      "train loss:2.30809740248078\n",
      "train loss:2.3036674934538643\n",
      "train loss:2.3083652997194113\n",
      "train loss:2.3031331988039088\n",
      "=== epoch:109, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.300218740767803\n",
      "train loss:2.3077381053230233\n",
      "train loss:2.3000866252425407\n",
      "train loss:2.307513593733479\n",
      "train loss:2.303298340775864\n",
      "train loss:2.303643788822901\n",
      "train loss:2.2997315212768172\n",
      "train loss:2.3039127572550644\n",
      "train loss:2.3036216720548426\n",
      "train loss:2.3077705742626087\n",
      "train loss:2.3018914772457695\n",
      "train loss:2.3022143174246352\n",
      "train loss:2.3112980401939462\n",
      "train loss:2.304587280802149\n",
      "train loss:2.2926293941488547\n",
      "train loss:2.294218214780669\n",
      "train loss:2.2996688890834647\n",
      "train loss:2.30607017168425\n",
      "train loss:2.301570210922815\n",
      "train loss:2.301899685640181\n",
      "train loss:2.3007879693791256\n",
      "train loss:2.309936387337641\n",
      "train loss:2.3060601767396847\n",
      "train loss:2.2981963069770814\n",
      "train loss:2.2974390611748663\n",
      "train loss:2.294122250923597\n",
      "train loss:2.3070635315960324\n",
      "train loss:2.2990341372879888\n",
      "train loss:2.2986875189025278\n",
      "train loss:2.3041461963044565\n",
      "train loss:2.2978580138108673\n",
      "train loss:2.295207052617894\n",
      "train loss:2.302203346316932\n",
      "train loss:2.299045270274451\n",
      "train loss:2.3017514980268907\n",
      "train loss:2.29959197574755\n",
      "train loss:2.29680130814377\n",
      "train loss:2.299840401618731\n",
      "train loss:2.3078433345509324\n",
      "train loss:2.311016737294055\n",
      "train loss:2.3030951996816595\n",
      "train loss:2.3015611176470188\n",
      "train loss:2.3041546521924374\n",
      "train loss:2.299103279501751\n",
      "train loss:2.3001700096516875\n",
      "train loss:2.3055835735490984\n",
      "train loss:2.3072669274826025\n",
      "train loss:2.304782629905395\n",
      "train loss:2.3083732040384697\n",
      "train loss:2.2988733817793747\n",
      "train loss:2.298492058413904\n",
      "train loss:2.2975543743486138\n",
      "train loss:2.298188122830943\n",
      "train loss:2.29584469729121\n",
      "train loss:2.3027086410484525\n",
      "train loss:2.3007566733592353\n",
      "train loss:2.2973372284766818\n",
      "train loss:2.2977464540719725\n",
      "train loss:2.2988989703913996\n",
      "train loss:2.302385129383033\n",
      "train loss:2.30038488328912\n",
      "train loss:2.2981346227346733\n",
      "train loss:2.2987812041196767\n",
      "train loss:2.3043422246215006\n",
      "train loss:2.2995533140123423\n",
      "train loss:2.304755258073188\n",
      "train loss:2.29417508154015\n",
      "train loss:2.298667886058649\n",
      "train loss:2.2926038101442874\n",
      "train loss:2.300214156691055\n",
      "train loss:2.3013469607672974\n",
      "train loss:2.300389820100172\n",
      "train loss:2.3013219217280394\n",
      "train loss:2.2992872487353972\n",
      "train loss:2.2967632181810855\n",
      "train loss:2.299070815771912\n",
      "train loss:2.292861820583414\n",
      "train loss:2.299546434625125\n",
      "train loss:2.3132962104588812\n",
      "train loss:2.298828064187093\n",
      "train loss:2.3047545100633333\n",
      "train loss:2.3066292807593345\n",
      "train loss:2.2917194325489305\n",
      "train loss:2.299064901504451\n",
      "train loss:2.2985369395370583\n",
      "train loss:2.307923894670936\n",
      "train loss:2.3046967765994295\n",
      "train loss:2.293427830447182\n",
      "train loss:2.293654181768633\n",
      "train loss:2.3051844405282464\n",
      "train loss:2.3006803594971834\n",
      "train loss:2.296474697313342\n",
      "train loss:2.3056371838922645\n",
      "train loss:2.3060738742908296\n",
      "train loss:2.299783091537487\n",
      "train loss:2.2952287698678546\n",
      "train loss:2.303266279485054\n",
      "train loss:2.301299627966466\n",
      "train loss:2.2958530269332087\n",
      "train loss:2.296309486495766\n",
      "train loss:2.3077285212299308\n",
      "train loss:2.2976923099044932\n",
      "train loss:2.301541241072913\n",
      "train loss:2.3103065573092447\n",
      "train loss:2.3081471500854125\n",
      "train loss:2.301832441347857\n",
      "train loss:2.314797290762179\n",
      "train loss:2.3057988728243695\n",
      "train loss:2.2961347287442955\n",
      "train loss:2.303767634774636\n",
      "train loss:2.299243125502747\n",
      "train loss:2.3033824748190854\n",
      "train loss:2.289110716025752\n",
      "train loss:2.2987797380031956\n",
      "train loss:2.301308674953116\n",
      "train loss:2.300820244694444\n",
      "train loss:2.2983009753053003\n",
      "train loss:2.3031812495200565\n",
      "train loss:2.2969109897853164\n",
      "train loss:2.3048402506725147\n",
      "train loss:2.3076162532887268\n",
      "train loss:2.2994732565313054\n",
      "train loss:2.3027114594263893\n",
      "train loss:2.3031835127568736\n",
      "train loss:2.3043506304821575\n",
      "train loss:2.2955737718989564\n",
      "train loss:2.305569856198847\n",
      "train loss:2.295141206442172\n",
      "train loss:2.301566557830653\n",
      "train loss:2.307945152373987\n",
      "train loss:2.304724663250435\n",
      "train loss:2.302564520648017\n",
      "train loss:2.2999124783685723\n",
      "train loss:2.30418730605138\n",
      "train loss:2.297100804648822\n",
      "train loss:2.311016927348615\n",
      "train loss:2.3081727069912916\n",
      "train loss:2.3014312707972486\n",
      "train loss:2.3024186713246495\n",
      "train loss:2.3031683740366495\n",
      "train loss:2.305676901181893\n",
      "train loss:2.3027737428648862\n",
      "train loss:2.2961863877832123\n",
      "train loss:2.2928721199576483\n",
      "train loss:2.296749369386973\n",
      "train loss:2.304104231655182\n",
      "train loss:2.3019218684292024\n",
      "train loss:2.3043517573963594\n",
      "train loss:2.296691535776715\n",
      "train loss:2.3091378465589103\n",
      "train loss:2.305251108218951\n",
      "train loss:2.3067670196078485\n",
      "train loss:2.305843743100978\n",
      "train loss:2.2995522986125856\n",
      "train loss:2.2982935243060947\n",
      "train loss:2.298086367584499\n",
      "train loss:2.294106997434256\n",
      "train loss:2.3105448121756242\n",
      "train loss:2.3039945907548094\n",
      "train loss:2.3035296548499193\n",
      "train loss:2.300589970002773\n",
      "train loss:2.2937683675051934\n",
      "train loss:2.305486838593572\n",
      "train loss:2.304123991839059\n",
      "train loss:2.3072881970306462\n",
      "train loss:2.304898379086046\n",
      "train loss:2.2975603179143422\n",
      "train loss:2.293989007964831\n",
      "train loss:2.303150804370179\n",
      "train loss:2.3011951982350536\n",
      "train loss:2.300218254000041\n",
      "train loss:2.2996258129316205\n",
      "train loss:2.304749066915657\n",
      "train loss:2.2986750634079427\n",
      "train loss:2.2993835081289875\n",
      "train loss:2.297858553602777\n",
      "train loss:2.2991438550907666\n",
      "train loss:2.3061175122719746\n",
      "train loss:2.29793338312356\n",
      "train loss:2.3018549381917572\n",
      "train loss:2.2991155819893714\n",
      "train loss:2.2970027940305315\n",
      "train loss:2.301834215008893\n",
      "train loss:2.3070234392199267\n",
      "train loss:2.298211725512323\n",
      "train loss:2.30372800566467\n",
      "train loss:2.3052889847166385\n",
      "train loss:2.2980989097361193\n",
      "train loss:2.2988957768114604\n",
      "train loss:2.3000130791653235\n",
      "train loss:2.308901466637781\n",
      "train loss:2.29908473544901\n",
      "train loss:2.3096162618704668\n",
      "train loss:2.30282892017204\n",
      "train loss:2.309353439584807\n",
      "train loss:2.3007045408447007\n",
      "train loss:2.292659592609967\n",
      "train loss:2.3033271396739106\n",
      "train loss:2.299227686252235\n",
      "train loss:2.297292240121419\n",
      "=== epoch:110, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3024305684746063\n",
      "train loss:2.309946610552092\n",
      "train loss:2.299970633254068\n",
      "train loss:2.2929507987572495\n",
      "train loss:2.3115223531984963\n",
      "train loss:2.304154957098536\n",
      "train loss:2.292498091962456\n",
      "train loss:2.30255934861872\n",
      "train loss:2.30759390913258\n",
      "train loss:2.295789672418088\n",
      "train loss:2.299864101777244\n",
      "train loss:2.3022903250058344\n",
      "train loss:2.296815245770596\n",
      "train loss:2.2975425460017256\n",
      "train loss:2.301458549294526\n",
      "train loss:2.2999576569091085\n",
      "train loss:2.299462954965422\n",
      "train loss:2.30120816462232\n",
      "train loss:2.3080384600447523\n",
      "train loss:2.3053261240567147\n",
      "train loss:2.3116247471107982\n",
      "train loss:2.298314864363994\n",
      "train loss:2.296141312774693\n",
      "train loss:2.3104242728902946\n",
      "train loss:2.300516281541402\n",
      "train loss:2.299963695502026\n",
      "train loss:2.304551160115097\n",
      "train loss:2.305417335268777\n",
      "train loss:2.304468890862646\n",
      "train loss:2.3067432444589184\n",
      "train loss:2.3033686893168603\n",
      "train loss:2.306035570199899\n",
      "train loss:2.2938520946533143\n",
      "train loss:2.3020001566218595\n",
      "train loss:2.3051707944946607\n",
      "train loss:2.301882148366707\n",
      "train loss:2.297211370752588\n",
      "train loss:2.3085966336923653\n",
      "train loss:2.2915676234983495\n",
      "train loss:2.3062738777375063\n",
      "train loss:2.3059771177565116\n",
      "train loss:2.305008983871673\n",
      "train loss:2.3032882515420376\n",
      "train loss:2.305764040158204\n",
      "train loss:2.293769926348439\n",
      "train loss:2.2961804051511163\n",
      "train loss:2.2969133906663015\n",
      "train loss:2.3155161530915924\n",
      "train loss:2.304040503866948\n",
      "train loss:2.292274900160966\n",
      "train loss:2.309817948057583\n",
      "train loss:2.3134039407670817\n",
      "train loss:2.294136114003933\n",
      "train loss:2.301026486252847\n",
      "train loss:2.2984514667449805\n",
      "train loss:2.301009620605939\n",
      "train loss:2.3022741677541583\n",
      "train loss:2.301725078291499\n",
      "train loss:2.2903525176188064\n",
      "train loss:2.309832780554222\n",
      "train loss:2.304467495122873\n",
      "train loss:2.3032733553973403\n",
      "train loss:2.2996848393035125\n",
      "train loss:2.298620794357015\n",
      "train loss:2.306657584910399\n",
      "train loss:2.3011346330792257\n",
      "train loss:2.3090572315317592\n",
      "train loss:2.2978214246462962\n",
      "train loss:2.294647559731913\n",
      "train loss:2.2948223240925882\n",
      "train loss:2.3063215264000445\n",
      "train loss:2.3017544631095888\n",
      "train loss:2.30718077525098\n",
      "train loss:2.3019611490135974\n",
      "train loss:2.298989054329688\n",
      "train loss:2.3002903488444235\n",
      "train loss:2.3155343293117983\n",
      "train loss:2.300500044956265\n",
      "train loss:2.308955269360102\n",
      "train loss:2.3038396405711965\n",
      "train loss:2.2962875607184996\n",
      "train loss:2.3044390927901093\n",
      "train loss:2.3052541253537617\n",
      "train loss:2.302991028421887\n",
      "train loss:2.3039205254039237\n",
      "train loss:2.300746169600275\n",
      "train loss:2.307661571694533\n",
      "train loss:2.2983004175573054\n",
      "train loss:2.2984865637872502\n",
      "train loss:2.301903930054421\n",
      "train loss:2.299098192614645\n",
      "train loss:2.296439480517681\n",
      "train loss:2.3002898706960697\n",
      "train loss:2.309425902608799\n",
      "train loss:2.2974847884961043\n",
      "train loss:2.3071333785669603\n",
      "train loss:2.306521881029224\n",
      "train loss:2.293188144310112\n",
      "train loss:2.304336319926793\n",
      "train loss:2.2935866871295496\n",
      "train loss:2.3038810397181697\n",
      "train loss:2.306518203598836\n",
      "train loss:2.3080842992574344\n",
      "train loss:2.300410993893785\n",
      "train loss:2.30507492515251\n",
      "train loss:2.301385887777576\n",
      "train loss:2.301393699628442\n",
      "train loss:2.3114724592482334\n",
      "train loss:2.2953502657959657\n",
      "train loss:2.2973029332123045\n",
      "train loss:2.30507485044838\n",
      "train loss:2.298469808151522\n",
      "train loss:2.2981568783533732\n",
      "train loss:2.306792498890007\n",
      "train loss:2.3021848928036053\n",
      "train loss:2.306432164985331\n",
      "train loss:2.30359851756926\n",
      "train loss:2.2977508201905703\n",
      "train loss:2.297571781562119\n",
      "train loss:2.2994297926947147\n",
      "train loss:2.2998816265147646\n",
      "train loss:2.298705791612455\n",
      "train loss:2.2991532223702222\n",
      "train loss:2.2945220944233116\n",
      "train loss:2.2966004277387593\n",
      "train loss:2.2903949573640303\n",
      "train loss:2.301888257387974\n",
      "train loss:2.3012888118654207\n",
      "train loss:2.30026805563251\n",
      "train loss:2.306376209637962\n",
      "train loss:2.302392558510431\n",
      "train loss:2.3030634308985913\n",
      "train loss:2.300997038833671\n",
      "train loss:2.294349134953306\n",
      "train loss:2.2978790964074607\n",
      "train loss:2.2989957905412584\n",
      "train loss:2.299332867176326\n",
      "train loss:2.3022362618202514\n",
      "train loss:2.3069725505323926\n",
      "train loss:2.3036641971267438\n",
      "train loss:2.3012010213210656\n",
      "train loss:2.3041654833592906\n",
      "train loss:2.305464671425948\n",
      "train loss:2.3070644968734553\n",
      "train loss:2.3026201847329784\n",
      "train loss:2.2992466201364916\n",
      "train loss:2.305852206387252\n",
      "train loss:2.2990675900814797\n",
      "train loss:2.3021605335038564\n",
      "train loss:2.3031078009545927\n",
      "train loss:2.304565412009207\n",
      "train loss:2.301290273482526\n",
      "train loss:2.3040250282516395\n",
      "train loss:2.302442674006193\n",
      "train loss:2.300311283641844\n",
      "train loss:2.302885831552633\n",
      "train loss:2.3020237159467754\n",
      "train loss:2.300421213789277\n",
      "train loss:2.3016377945857003\n",
      "train loss:2.3002489330158866\n",
      "train loss:2.3004215459991584\n",
      "train loss:2.3025045072559243\n",
      "train loss:2.298603004230356\n",
      "train loss:2.2972385797439023\n",
      "train loss:2.310298613905537\n",
      "train loss:2.2977114241594547\n",
      "train loss:2.290180777157053\n",
      "train loss:2.3006481393213765\n",
      "train loss:2.2947418567212874\n",
      "train loss:2.3032653541884947\n",
      "train loss:2.3052410932149416\n",
      "train loss:2.302657761282321\n",
      "train loss:2.303450996131662\n",
      "train loss:2.299465178145707\n",
      "train loss:2.3043020975749724\n",
      "train loss:2.29734707012955\n",
      "train loss:2.302429429996373\n",
      "train loss:2.3063969383471434\n",
      "train loss:2.3080755212754385\n",
      "train loss:2.2970352371671163\n",
      "train loss:2.2977398773732833\n",
      "train loss:2.2964708926809125\n",
      "train loss:2.3036543444823914\n",
      "train loss:2.3036960569878535\n",
      "train loss:2.2974068942598898\n",
      "train loss:2.3032897425543357\n",
      "train loss:2.3081365252498114\n",
      "train loss:2.30208082159561\n",
      "train loss:2.2997700518070223\n",
      "train loss:2.2988532960648715\n",
      "train loss:2.293394478960942\n",
      "train loss:2.3013759352258556\n",
      "train loss:2.297935359514468\n",
      "train loss:2.3006687293202877\n",
      "train loss:2.300283153604427\n",
      "train loss:2.299577364139605\n",
      "train loss:2.3017279429232\n",
      "train loss:2.300401197267172\n",
      "train loss:2.297083356073237\n",
      "train loss:2.3029897831962733\n",
      "=== epoch:111, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.296788381888193\n",
      "train loss:2.2977379099695607\n",
      "train loss:2.2960683822988206\n",
      "train loss:2.3079604157435005\n",
      "train loss:2.3039201788813584\n",
      "train loss:2.301295042892951\n",
      "train loss:2.30269351651357\n",
      "train loss:2.3019716412954367\n",
      "train loss:2.3105262867968004\n",
      "train loss:2.3003543200114014\n",
      "train loss:2.3037244226333957\n",
      "train loss:2.3112521383291305\n",
      "train loss:2.3015598585966512\n",
      "train loss:2.305610026506744\n",
      "train loss:2.300693365821838\n",
      "train loss:2.299648579280747\n",
      "train loss:2.2991179932451025\n",
      "train loss:2.3010656454554295\n",
      "train loss:2.3080508337004133\n",
      "train loss:2.297855379965203\n",
      "train loss:2.302227839619875\n",
      "train loss:2.296937278299924\n",
      "train loss:2.296954802594907\n",
      "train loss:2.3013180081521165\n",
      "train loss:2.309236039626684\n",
      "train loss:2.304270698822545\n",
      "train loss:2.303390794834641\n",
      "train loss:2.2903537098340006\n",
      "train loss:2.3026104888814873\n",
      "train loss:2.297968578072367\n",
      "train loss:2.299009727517335\n",
      "train loss:2.2961617778743726\n",
      "train loss:2.3010756264449563\n",
      "train loss:2.303935286806094\n",
      "train loss:2.300723079484547\n",
      "train loss:2.303163229985685\n",
      "train loss:2.3036883560316883\n",
      "train loss:2.3008092203110695\n",
      "train loss:2.3008297910213282\n",
      "train loss:2.308977391359195\n",
      "train loss:2.302676798998308\n",
      "train loss:2.294359251525405\n",
      "train loss:2.298160009116908\n",
      "train loss:2.304313411834456\n",
      "train loss:2.299368059116732\n",
      "train loss:2.3086157643900527\n",
      "train loss:2.298146291856012\n",
      "train loss:2.295340696695971\n",
      "train loss:2.3001077156754643\n",
      "train loss:2.300353368441012\n",
      "train loss:2.2961494752661427\n",
      "train loss:2.3043169835510073\n",
      "train loss:2.2994276101179327\n",
      "train loss:2.2987872236024187\n",
      "train loss:2.2988434917319713\n",
      "train loss:2.3001435285883076\n",
      "train loss:2.2943643076208375\n",
      "train loss:2.305675564111161\n",
      "train loss:2.29984179998394\n",
      "train loss:2.296796034207803\n",
      "train loss:2.307167923757704\n",
      "train loss:2.2982720541638577\n",
      "train loss:2.3006998099776688\n",
      "train loss:2.302899242841757\n",
      "train loss:2.310438002787653\n",
      "train loss:2.3064735955812483\n",
      "train loss:2.2900917163134022\n",
      "train loss:2.294910467448442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.293708517689798\n",
      "train loss:2.3047559716312644\n",
      "train loss:2.2979661191487293\n",
      "train loss:2.3029785970308114\n",
      "train loss:2.305647407278087\n",
      "train loss:2.3050604680271274\n",
      "train loss:2.2980370102279215\n",
      "train loss:2.2985275439008204\n",
      "train loss:2.303638298762465\n",
      "train loss:2.2991983845901487\n",
      "train loss:2.2943062651968638\n",
      "train loss:2.2999952157912844\n",
      "train loss:2.3046516510339\n",
      "train loss:2.2922094906364845\n",
      "train loss:2.3130514184914683\n",
      "train loss:2.2951897329688107\n",
      "train loss:2.29938275912449\n",
      "train loss:2.302109258913969\n",
      "train loss:2.303490173181678\n",
      "train loss:2.2893025566335843\n",
      "train loss:2.3013863094508333\n",
      "train loss:2.30317977254181\n",
      "train loss:2.2969283528314652\n",
      "train loss:2.2964413754160367\n",
      "train loss:2.30866420896905\n",
      "train loss:2.30051148204409\n",
      "train loss:2.3097609629107843\n",
      "train loss:2.301597370950304\n",
      "train loss:2.304493349315228\n",
      "train loss:2.3037245388552448\n",
      "train loss:2.309515352225919\n",
      "train loss:2.3074619842548367\n",
      "train loss:2.3026435257344384\n",
      "train loss:2.300444951874927\n",
      "train loss:2.301928588038605\n",
      "train loss:2.2911330861605594\n",
      "train loss:2.3090625329344956\n",
      "train loss:2.3019689373995758\n",
      "train loss:2.299761002894591\n",
      "train loss:2.303155470119879\n",
      "train loss:2.294645430801003\n",
      "train loss:2.3073173195300654\n",
      "train loss:2.300821469199878\n",
      "train loss:2.2974130193894196\n",
      "train loss:2.3088472909196547\n",
      "train loss:2.300806686502833\n",
      "train loss:2.2961804579215053\n",
      "train loss:2.304352169801199\n",
      "train loss:2.309453379989061\n",
      "train loss:2.3004259242759435\n",
      "train loss:2.2997260463535394\n",
      "train loss:2.3064055604035594\n",
      "train loss:2.2973821151737472\n",
      "train loss:2.305915946891345\n",
      "train loss:2.3047767268108954\n",
      "train loss:2.2922592207040884\n",
      "train loss:2.29716282597124\n",
      "train loss:2.3073013765242374\n",
      "train loss:2.3052822318084996\n",
      "train loss:2.3005825739800585\n",
      "train loss:2.300750300505098\n",
      "train loss:2.307953547090307\n",
      "train loss:2.297640275721071\n",
      "train loss:2.3030331659045564\n",
      "train loss:2.3085140493267104\n",
      "train loss:2.3006782343583674\n",
      "train loss:2.302362354347714\n",
      "train loss:2.3037247494312654\n",
      "train loss:2.308282890585649\n",
      "train loss:2.308053104418927\n",
      "train loss:2.304245954806219\n",
      "train loss:2.297534333759105\n",
      "train loss:2.303485144954396\n",
      "train loss:2.307351521366227\n",
      "train loss:2.30560660603249\n",
      "train loss:2.297395706205983\n",
      "train loss:2.2921987446583785\n",
      "train loss:2.298645991279291\n",
      "train loss:2.3075284218233687\n",
      "train loss:2.299866560916541\n",
      "train loss:2.2993847499420794\n",
      "train loss:2.3012207861028067\n",
      "train loss:2.3022303135646363\n",
      "train loss:2.307519862614283\n",
      "train loss:2.2983012035576227\n",
      "train loss:2.2948703769807652\n",
      "train loss:2.2997488741793\n",
      "train loss:2.305545770009016\n",
      "train loss:2.299829625103412\n",
      "train loss:2.299311490371336\n",
      "train loss:2.2983154905820076\n",
      "train loss:2.2962703576072667\n",
      "train loss:2.292918272082831\n",
      "train loss:2.2976508801399746\n",
      "train loss:2.307420401150108\n",
      "train loss:2.2987897828300894\n",
      "train loss:2.2987396063145638\n",
      "train loss:2.3057213312681264\n",
      "train loss:2.2980669321631164\n",
      "train loss:2.306238563486205\n",
      "train loss:2.3041623395081556\n",
      "train loss:2.3037515639501125\n",
      "train loss:2.3026495305228005\n",
      "train loss:2.295404718796594\n",
      "train loss:2.297026920880998\n",
      "train loss:2.2961278258182203\n",
      "train loss:2.3054639084916726\n",
      "train loss:2.2925134624029093\n",
      "train loss:2.303242310752375\n",
      "train loss:2.306042213566749\n",
      "train loss:2.307450778294407\n",
      "train loss:2.295099091258165\n",
      "train loss:2.29848212242995\n",
      "train loss:2.2969263547337206\n",
      "train loss:2.3059893066339967\n",
      "train loss:2.2987464848593797\n",
      "train loss:2.3036381015562553\n",
      "train loss:2.305838911552092\n",
      "train loss:2.2925539740169634\n",
      "train loss:2.3004745923665975\n",
      "train loss:2.2945984090890437\n",
      "train loss:2.3079861145428957\n",
      "train loss:2.3028852031619924\n",
      "train loss:2.3058343347047976\n",
      "train loss:2.2971151602743554\n",
      "train loss:2.297290908227049\n",
      "train loss:2.3058123748424895\n",
      "train loss:2.3030890839338527\n",
      "train loss:2.304132331784201\n",
      "train loss:2.3007859746244366\n",
      "train loss:2.3019933211965338\n",
      "train loss:2.2979241412018054\n",
      "=== epoch:112, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.307150059338158\n",
      "train loss:2.3022557601755897\n",
      "train loss:2.2950213681405995\n",
      "train loss:2.308632786636464\n",
      "train loss:2.2961587089232327\n",
      "train loss:2.300249540285038\n",
      "train loss:2.309513421850347\n",
      "train loss:2.2976412949286495\n",
      "train loss:2.2981703943911067\n",
      "train loss:2.3011649408018697\n",
      "train loss:2.302866145422323\n",
      "train loss:2.29508669528895\n",
      "train loss:2.29387814740822\n",
      "train loss:2.304598659955263\n",
      "train loss:2.2960271911920223\n",
      "train loss:2.296545153615704\n",
      "train loss:2.308983060703489\n",
      "train loss:2.3001094196853376\n",
      "train loss:2.3002568649683077\n",
      "train loss:2.2962062176916374\n",
      "train loss:2.299824309891689\n",
      "train loss:2.2960491308913227\n",
      "train loss:2.3011745599262037\n",
      "train loss:2.3050322218198303\n",
      "train loss:2.3071994976997217\n",
      "train loss:2.30491065638725\n",
      "train loss:2.2994679613790305\n",
      "train loss:2.300894581801297\n",
      "train loss:2.2980854863028313\n",
      "train loss:2.305036311912974\n",
      "train loss:2.303160461177959\n",
      "train loss:2.299392240262815\n",
      "train loss:2.3041195499432066\n",
      "train loss:2.305675049955258\n",
      "train loss:2.303425528411941\n",
      "train loss:2.301988576303273\n",
      "train loss:2.301983113931885\n",
      "train loss:2.294495096290962\n",
      "train loss:2.2987669176579693\n",
      "train loss:2.305470511016314\n",
      "train loss:2.3046961347420103\n",
      "train loss:2.302961264175134\n",
      "train loss:2.300455405395609\n",
      "train loss:2.299412681936641\n",
      "train loss:2.3015711107956367\n",
      "train loss:2.2954842276181595\n",
      "train loss:2.3063784698495238\n",
      "train loss:2.2923480137188137\n",
      "train loss:2.2999135746427806\n",
      "train loss:2.308252687704634\n",
      "train loss:2.299629337456928\n",
      "train loss:2.302026811028861\n",
      "train loss:2.304037495286281\n",
      "train loss:2.2983683662981083\n",
      "train loss:2.2989428081619834\n",
      "train loss:2.3148434941407157\n",
      "train loss:2.298117497870763\n",
      "train loss:2.300457946757299\n",
      "train loss:2.307643402993292\n",
      "train loss:2.3034086884829827\n",
      "train loss:2.2940137608265165\n",
      "train loss:2.3015388224349898\n",
      "train loss:2.3040249450310633\n",
      "train loss:2.3018233419438405\n",
      "train loss:2.299654676256214\n",
      "train loss:2.300879379719187\n",
      "train loss:2.305698076655889\n",
      "train loss:2.301377317567474\n",
      "train loss:2.2870968334262995\n",
      "train loss:2.3007642224638323\n",
      "train loss:2.3033677668739942\n",
      "train loss:2.3007113599097533\n",
      "train loss:2.302497150680326\n",
      "train loss:2.3001458992208725\n",
      "train loss:2.298006648448633\n",
      "train loss:2.295863539204595\n",
      "train loss:2.307271767766453\n",
      "train loss:2.3014513569810333\n",
      "train loss:2.301350126300597\n",
      "train loss:2.297139178597354\n",
      "train loss:2.304069758509136\n",
      "train loss:2.308873319892125\n",
      "train loss:2.2971671708097303\n",
      "train loss:2.3028200229546023\n",
      "train loss:2.3068867050190236\n",
      "train loss:2.3046287157546175\n",
      "train loss:2.2992622963092524\n",
      "train loss:2.300580389046303\n",
      "train loss:2.303883338300424\n",
      "train loss:2.300521949713317\n",
      "train loss:2.2995238859584735\n",
      "train loss:2.2995704723509593\n",
      "train loss:2.302016319224922\n",
      "train loss:2.298797341919449\n",
      "train loss:2.296919904988796\n",
      "train loss:2.3031242349361536\n",
      "train loss:2.3072364595078474\n",
      "train loss:2.298795416369253\n",
      "train loss:2.307347569484866\n",
      "train loss:2.300661880417175\n",
      "train loss:2.2995252872329837\n",
      "train loss:2.2929263494549565\n",
      "train loss:2.3019872264782055\n",
      "train loss:2.297161267564078\n",
      "train loss:2.30024353254706\n",
      "train loss:2.2985199703662915\n",
      "train loss:2.3080064593517426\n",
      "train loss:2.299267775988512\n",
      "train loss:2.294012535203102\n",
      "train loss:2.2965458883202152\n",
      "train loss:2.301339545810238\n",
      "train loss:2.3014510237552375\n",
      "train loss:2.2990477872246324\n",
      "train loss:2.3051805182554266\n",
      "train loss:2.3075427035471066\n",
      "train loss:2.30063565748793\n",
      "train loss:2.302490666330641\n",
      "train loss:2.296599567971427\n",
      "train loss:2.3021267342819134\n",
      "train loss:2.304730193873957\n",
      "train loss:2.298852761427962\n",
      "train loss:2.2957697557097276\n",
      "train loss:2.3013971656792496\n",
      "train loss:2.298448160253549\n",
      "train loss:2.295034621150061\n",
      "train loss:2.291281324246033\n",
      "train loss:2.299707184806215\n",
      "train loss:2.303883521074766\n",
      "train loss:2.3056029141091856\n",
      "train loss:2.293507956417428\n",
      "train loss:2.3027046961100046\n",
      "train loss:2.296785073892928\n",
      "train loss:2.302393346973123\n",
      "train loss:2.303207163490974\n",
      "train loss:2.29830728448887\n",
      "train loss:2.30448045627116\n",
      "train loss:2.3071413129454923\n",
      "train loss:2.302266346141948\n",
      "train loss:2.30417548230215\n",
      "train loss:2.295275530615764\n",
      "train loss:2.29492256256787\n",
      "train loss:2.30364905753223\n",
      "train loss:2.301550197808505\n",
      "train loss:2.305756921327392\n",
      "train loss:2.299353250524968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3013920249829467\n",
      "train loss:2.298383430820719\n",
      "train loss:2.2900964903431786\n",
      "train loss:2.3024123075426997\n",
      "train loss:2.298783146124012\n",
      "train loss:2.2978922377070905\n",
      "train loss:2.3010397381692123\n",
      "train loss:2.301169815645223\n",
      "train loss:2.302732913076546\n",
      "train loss:2.29842386885004\n",
      "train loss:2.29321764138858\n",
      "train loss:2.3118264639070483\n",
      "train loss:2.302903705500476\n",
      "train loss:2.3103539399794\n",
      "train loss:2.3059277165456114\n",
      "train loss:2.3017635966910728\n",
      "train loss:2.300385647222268\n",
      "train loss:2.295456887544424\n",
      "train loss:2.3032932575172502\n",
      "train loss:2.299902226002596\n",
      "train loss:2.3065506213041385\n",
      "train loss:2.2991878992423773\n",
      "train loss:2.299879901080578\n",
      "train loss:2.3007147794460883\n",
      "train loss:2.3040118250072905\n",
      "train loss:2.2975172691682206\n",
      "train loss:2.2997373307947173\n",
      "train loss:2.301690687902773\n",
      "train loss:2.30964715648917\n",
      "train loss:2.3005137544057908\n",
      "train loss:2.294408948116723\n",
      "train loss:2.301098315903586\n",
      "train loss:2.299156115130305\n",
      "train loss:2.3039781997860103\n",
      "train loss:2.30870054792072\n",
      "train loss:2.3028017501100595\n",
      "train loss:2.3036847794033464\n",
      "train loss:2.2893684626234183\n",
      "train loss:2.2925907533649945\n",
      "train loss:2.301872865939084\n",
      "train loss:2.2979007084564738\n",
      "train loss:2.2973577014018254\n",
      "train loss:2.2996946554701734\n",
      "train loss:2.297847843723892\n",
      "train loss:2.300580133635378\n",
      "train loss:2.304526688180198\n",
      "train loss:2.3017461362948337\n",
      "train loss:2.288955012361319\n",
      "train loss:2.293085521564419\n",
      "train loss:2.305853259773086\n",
      "train loss:2.3002544103858753\n",
      "train loss:2.297592225610226\n",
      "train loss:2.304387825811508\n",
      "train loss:2.3055137002978165\n",
      "train loss:2.303519267234136\n",
      "=== epoch:113, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3076345257491457\n",
      "train loss:2.2952732707977535\n",
      "train loss:2.308421201166393\n",
      "train loss:2.3034196278714774\n",
      "train loss:2.306315323769562\n",
      "train loss:2.295201376367167\n",
      "train loss:2.3112070211392948\n",
      "train loss:2.3100615378240956\n",
      "train loss:2.289832190376112\n",
      "train loss:2.3096132958654465\n",
      "train loss:2.301154317103814\n",
      "train loss:2.3037934478394426\n",
      "train loss:2.304367213181658\n",
      "train loss:2.2927418732216505\n",
      "train loss:2.298913265214251\n",
      "train loss:2.2914539031741734\n",
      "train loss:2.304230238494909\n",
      "train loss:2.2984839239977326\n",
      "train loss:2.3009644772576014\n",
      "train loss:2.3029525927256316\n",
      "train loss:2.301678217189317\n",
      "train loss:2.3086887305051134\n",
      "train loss:2.29247898436057\n",
      "train loss:2.293523808197369\n",
      "train loss:2.2971831655459645\n",
      "train loss:2.308168608320172\n",
      "train loss:2.2908906455700406\n",
      "train loss:2.3011359558072457\n",
      "train loss:2.304463885449004\n",
      "train loss:2.2986656196590642\n",
      "train loss:2.3080870289093083\n",
      "train loss:2.2971217087374978\n",
      "train loss:2.295876729647099\n",
      "train loss:2.2997851697830827\n",
      "train loss:2.3014303638365985\n",
      "train loss:2.30412633651361\n",
      "train loss:2.2997461142846825\n",
      "train loss:2.293813281466312\n",
      "train loss:2.301803857744921\n",
      "train loss:2.29520217515492\n",
      "train loss:2.3060078852371673\n",
      "train loss:2.296703025916599\n",
      "train loss:2.295244288704249\n",
      "train loss:2.294350049223464\n",
      "train loss:2.2916357690396905\n",
      "train loss:2.3017953316588704\n",
      "train loss:2.3127852744397996\n",
      "train loss:2.301839546715009\n",
      "train loss:2.298029304175575\n",
      "train loss:2.305252376372025\n",
      "train loss:2.303471764645566\n",
      "train loss:2.3069451373901595\n",
      "train loss:2.3096168364045857\n",
      "train loss:2.3088478091240088\n",
      "train loss:2.3013873079779827\n",
      "train loss:2.296999238669826\n",
      "train loss:2.299998459697576\n",
      "train loss:2.2943285113602925\n",
      "train loss:2.30281294402529\n",
      "train loss:2.2979286722473744\n",
      "train loss:2.2966269215625266\n",
      "train loss:2.293613820402669\n",
      "train loss:2.307457034728678\n",
      "train loss:2.299109292740126\n",
      "train loss:2.303594395081815\n",
      "train loss:2.2974007829586953\n",
      "train loss:2.2998228785980124\n",
      "train loss:2.311584820394156\n",
      "train loss:2.2964775492567644\n",
      "train loss:2.302140693619587\n",
      "train loss:2.301654906736192\n",
      "train loss:2.30721886397864\n",
      "train loss:2.2934553949306156\n",
      "train loss:2.2963030745520916\n",
      "train loss:2.304693321277183\n",
      "train loss:2.30538147374965\n",
      "train loss:2.3073276315803204\n",
      "train loss:2.301648098808284\n",
      "train loss:2.3057795562246803\n",
      "train loss:2.2950859344597343\n",
      "train loss:2.301543376520389\n",
      "train loss:2.300882130522076\n",
      "train loss:2.3041464125902316\n",
      "train loss:2.3079699968310585\n",
      "train loss:2.3053110384603603\n",
      "train loss:2.2909365684611536\n",
      "train loss:2.299571667141881\n",
      "train loss:2.3019677322554046\n",
      "train loss:2.3037351971243694\n",
      "train loss:2.2975087316638465\n",
      "train loss:2.301177959014584\n",
      "train loss:2.2958183574309925\n",
      "train loss:2.308545698173956\n",
      "train loss:2.303085023169032\n",
      "train loss:2.3055430159722956\n",
      "train loss:2.294560146843733\n",
      "train loss:2.2972534897194974\n",
      "train loss:2.2981843622097426\n",
      "train loss:2.2922324933831577\n",
      "train loss:2.3038983939731663\n",
      "train loss:2.298990796525058\n",
      "train loss:2.298277701346119\n",
      "train loss:2.29348183883451\n",
      "train loss:2.3076119700627626\n",
      "train loss:2.301262164774536\n",
      "train loss:2.3024273273532576\n",
      "train loss:2.3042962840468375\n",
      "train loss:2.304655815069083\n",
      "train loss:2.299866347258642\n",
      "train loss:2.299059233173532\n",
      "train loss:2.304133416973401\n",
      "train loss:2.3005915471092724\n",
      "train loss:2.304210978232954\n",
      "train loss:2.3014543741654676\n",
      "train loss:2.304593506232215\n",
      "train loss:2.301242356108768\n",
      "train loss:2.302932118948263\n",
      "train loss:2.297011850028936\n",
      "train loss:2.2984807290288103\n",
      "train loss:2.2915869516339504\n",
      "train loss:2.304696304419121\n",
      "train loss:2.295357366991255\n",
      "train loss:2.2955464341573633\n",
      "train loss:2.307789274189308\n",
      "train loss:2.308074280865104\n",
      "train loss:2.306843571437901\n",
      "train loss:2.298345144374085\n",
      "train loss:2.2956451065366523\n",
      "train loss:2.29951162935553\n",
      "train loss:2.2999666576803515\n",
      "train loss:2.303289565845414\n",
      "train loss:2.304831094159606\n",
      "train loss:2.303772674823234\n",
      "train loss:2.3056801908845768\n",
      "train loss:2.299243569192938\n",
      "train loss:2.3059480931404353\n",
      "train loss:2.304329752454195\n",
      "train loss:2.300237657695862\n",
      "train loss:2.303880619790481\n",
      "train loss:2.2953217439996423\n",
      "train loss:2.2886052791320477\n",
      "train loss:2.3061297614765044\n",
      "train loss:2.295531282925627\n",
      "train loss:2.3008323026457633\n",
      "train loss:2.298733965475896\n",
      "train loss:2.305474985021657\n",
      "train loss:2.303126763068008\n",
      "train loss:2.3011641761662545\n",
      "train loss:2.296961543915929\n",
      "train loss:2.3043938701576896\n",
      "train loss:2.3124880661837444\n",
      "train loss:2.298444596165443\n",
      "train loss:2.2982818127731592\n",
      "train loss:2.296698783159728\n",
      "train loss:2.3058254810271888\n",
      "train loss:2.299941626096859\n",
      "train loss:2.309603108772007\n",
      "train loss:2.2988541852920874\n",
      "train loss:2.2978241746601173\n",
      "train loss:2.301992499028819\n",
      "train loss:2.2939221827912304\n",
      "train loss:2.307677528903791\n",
      "train loss:2.307634384821023\n",
      "train loss:2.3015081922703584\n",
      "train loss:2.304621513096009\n",
      "train loss:2.300327560842186\n",
      "train loss:2.3085950263847264\n",
      "train loss:2.3000318595601215\n",
      "train loss:2.2947915902057248\n",
      "train loss:2.292667808642655\n",
      "train loss:2.3063557268828996\n",
      "train loss:2.3002817045797728\n",
      "train loss:2.291058962729929\n",
      "train loss:2.301328091657908\n",
      "train loss:2.3013372944530284\n",
      "train loss:2.3046522656593074\n",
      "train loss:2.3019821636807434\n",
      "train loss:2.3004596465933864\n",
      "train loss:2.305031370073467\n",
      "train loss:2.302014968922652\n",
      "train loss:2.304122460948723\n",
      "train loss:2.296482877532648\n",
      "train loss:2.3017119037913916\n",
      "train loss:2.285956976224549\n",
      "train loss:2.306480883554237\n",
      "train loss:2.2924276813146656\n",
      "train loss:2.3064496641563736\n",
      "train loss:2.3026899740047937\n",
      "train loss:2.3050182094939435\n",
      "train loss:2.303891552174957\n",
      "train loss:2.3098420940969207\n",
      "train loss:2.3021990665353953\n",
      "train loss:2.298775551207965\n",
      "train loss:2.3030422522920286\n",
      "train loss:2.3111786395630203\n",
      "train loss:2.2978171720503164\n",
      "train loss:2.2956955784303403\n",
      "train loss:2.2994397931416524\n",
      "train loss:2.297136205669021\n",
      "train loss:2.300956078739135\n",
      "=== epoch:114, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.30563157450059\n",
      "train loss:2.3015662569744446\n",
      "train loss:2.2912897775154084\n",
      "train loss:2.3019342263440064\n",
      "train loss:2.30456242795878\n",
      "train loss:2.3003558752778064\n",
      "train loss:2.304059049636496\n",
      "train loss:2.305516257996316\n",
      "train loss:2.294433528620836\n",
      "train loss:2.298232448354562\n",
      "train loss:2.305963256358938\n",
      "train loss:2.2982755321172417\n",
      "train loss:2.297503487227587\n",
      "train loss:2.3048960987998717\n",
      "train loss:2.3084546569056283\n",
      "train loss:2.3046102913643973\n",
      "train loss:2.297336327432693\n",
      "train loss:2.30674968827754\n",
      "train loss:2.297277486857799\n",
      "train loss:2.300757405406716\n",
      "train loss:2.2985155275761535\n",
      "train loss:2.304157992170874\n",
      "train loss:2.303849067636346\n",
      "train loss:2.290923742688428\n",
      "train loss:2.3051972641283944\n",
      "train loss:2.3021343704248975\n",
      "train loss:2.2986705377504615\n",
      "train loss:2.299554031909628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3022367964858494\n",
      "train loss:2.3036943229394633\n",
      "train loss:2.3063553911727923\n",
      "train loss:2.2911523018109516\n",
      "train loss:2.3007900689824243\n",
      "train loss:2.3030716700439604\n",
      "train loss:2.30249508246595\n",
      "train loss:2.298898964164799\n",
      "train loss:2.3002481457111292\n",
      "train loss:2.3012995544020933\n",
      "train loss:2.293407394001583\n",
      "train loss:2.292497554135485\n",
      "train loss:2.3024404963385217\n",
      "train loss:2.2990992951277422\n",
      "train loss:2.305150728708206\n",
      "train loss:2.301448760429898\n",
      "train loss:2.3010193824872918\n",
      "train loss:2.3005716134812406\n",
      "train loss:2.2952388596094155\n",
      "train loss:2.311588639039658\n",
      "train loss:2.302540936838996\n",
      "train loss:2.3017715815409066\n",
      "train loss:2.2985211196291475\n",
      "train loss:2.3048662579388752\n",
      "train loss:2.3050905901679735\n",
      "train loss:2.296451640241289\n",
      "train loss:2.303590239570065\n",
      "train loss:2.3036697898322904\n",
      "train loss:2.31050455409019\n",
      "train loss:2.3218664009631156\n",
      "train loss:2.2977422430796097\n",
      "train loss:2.2988565139759376\n",
      "train loss:2.3052050420872536\n",
      "train loss:2.30527231417158\n",
      "train loss:2.3116814712316036\n",
      "train loss:2.2993143816302224\n",
      "train loss:2.302237919542289\n",
      "train loss:2.3052514096165915\n",
      "train loss:2.2933219392337962\n",
      "train loss:2.2857639341300797\n",
      "train loss:2.3037597483630887\n",
      "train loss:2.2977552265350263\n",
      "train loss:2.3053924367379715\n",
      "train loss:2.2966046743334116\n",
      "train loss:2.29370956615717\n",
      "train loss:2.2994892253624633\n",
      "train loss:2.300989119157803\n",
      "train loss:2.2844285378905624\n",
      "train loss:2.2983079655162557\n",
      "train loss:2.2945474455754136\n",
      "train loss:2.2989173337447024\n",
      "train loss:2.2994082862947125\n",
      "train loss:2.2976406133997314\n",
      "train loss:2.2927955174309127\n",
      "train loss:2.2951624968078033\n",
      "train loss:2.295329539693406\n",
      "train loss:2.296463611923576\n",
      "train loss:2.310302186235974\n",
      "train loss:2.3057951330726922\n",
      "train loss:2.3038492286077754\n",
      "train loss:2.303979382777439\n",
      "train loss:2.3076896618668705\n",
      "train loss:2.3023054335670254\n",
      "train loss:2.2974292287882263\n",
      "train loss:2.310413198722651\n",
      "train loss:2.300447461941949\n",
      "train loss:2.3070633480414093\n",
      "train loss:2.299572805642346\n",
      "train loss:2.310089509917103\n",
      "train loss:2.306841944327916\n",
      "train loss:2.3078492781251594\n",
      "train loss:2.294699534494771\n",
      "train loss:2.3081071669341098\n",
      "train loss:2.306403664955075\n",
      "train loss:2.30016757233283\n",
      "train loss:2.3071806241304538\n",
      "train loss:2.3080957909438164\n",
      "train loss:2.297892599773064\n",
      "train loss:2.296269667390705\n",
      "train loss:2.2936153107571613\n",
      "train loss:2.307319331062712\n",
      "train loss:2.3038219860020575\n",
      "train loss:2.3015405504956696\n",
      "train loss:2.3013510698975814\n",
      "train loss:2.3095882912275565\n",
      "train loss:2.309611191611043\n",
      "train loss:2.2957806297960612\n",
      "train loss:2.303354046616132\n",
      "train loss:2.3069920725961075\n",
      "train loss:2.296712251533534\n",
      "train loss:2.2981941434055027\n",
      "train loss:2.307635025461591\n",
      "train loss:2.3021240369225295\n",
      "train loss:2.3088262758857727\n",
      "train loss:2.306952168345948\n",
      "train loss:2.299151156034852\n",
      "train loss:2.30949592322872\n",
      "train loss:2.298946635389657\n",
      "train loss:2.3022234724259483\n",
      "train loss:2.311782469485737\n",
      "train loss:2.301016691676644\n",
      "train loss:2.3007466305492708\n",
      "train loss:2.3108092135612806\n",
      "train loss:2.303738104449972\n",
      "train loss:2.3016843505144577\n",
      "train loss:2.299713869502249\n",
      "train loss:2.3092264793473896\n",
      "train loss:2.3018832402059832\n",
      "train loss:2.2996189489373013\n",
      "train loss:2.3099733319977216\n",
      "train loss:2.2979174914624982\n",
      "train loss:2.301196200004573\n",
      "train loss:2.309957695428502\n",
      "train loss:2.306525151421892\n",
      "train loss:2.3065531883926234\n",
      "train loss:2.295212133253188\n",
      "train loss:2.296647481163649\n",
      "train loss:2.3110711282129244\n",
      "train loss:2.3084622898703606\n",
      "train loss:2.3043286003861194\n",
      "train loss:2.3060475703742136\n",
      "train loss:2.306438713117889\n",
      "train loss:2.3102195018936804\n",
      "train loss:2.300257105686443\n",
      "train loss:2.302073634210023\n",
      "train loss:2.3062044068054246\n",
      "train loss:2.3035508972928262\n",
      "train loss:2.299938197527868\n",
      "train loss:2.3000632449971645\n",
      "train loss:2.3084114162806717\n",
      "train loss:2.3038282015487033\n",
      "train loss:2.3020377496468085\n",
      "train loss:2.3018623600889736\n",
      "train loss:2.308259736719306\n",
      "train loss:2.2993167539671187\n",
      "train loss:2.293541171862202\n",
      "train loss:2.3046983249244133\n",
      "train loss:2.3092188347180165\n",
      "train loss:2.31008587769335\n",
      "train loss:2.3018984658202255\n",
      "train loss:2.306010837385582\n",
      "train loss:2.3037823844753613\n",
      "train loss:2.2957489470019103\n",
      "train loss:2.3068923586390864\n",
      "train loss:2.3017673615963763\n",
      "train loss:2.2991436234134994\n",
      "train loss:2.304090769448026\n",
      "train loss:2.3070833465930236\n",
      "train loss:2.3033271645375653\n",
      "train loss:2.2965564043961777\n",
      "train loss:2.3045153075660623\n",
      "train loss:2.303948243959554\n",
      "train loss:2.2992615167192136\n",
      "train loss:2.2906828771875936\n",
      "train loss:2.3005662430292824\n",
      "train loss:2.3049052292617165\n",
      "train loss:2.3086299181259555\n",
      "train loss:2.2989561940315246\n",
      "train loss:2.296755678344499\n",
      "train loss:2.306026539993525\n",
      "train loss:2.3040740249000597\n",
      "train loss:2.2956853697445876\n",
      "train loss:2.290119059463033\n",
      "train loss:2.30202371375653\n",
      "train loss:2.2982830028907997\n",
      "train loss:2.2967127985128952\n",
      "train loss:2.308592162541703\n",
      "train loss:2.3062875265842218\n",
      "train loss:2.299568914977084\n",
      "train loss:2.301439785815555\n",
      "train loss:2.3020105216811504\n",
      "train loss:2.3016580949206475\n",
      "=== epoch:115, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.303317068308274\n",
      "train loss:2.3026830293557925\n",
      "train loss:2.305118281936371\n",
      "train loss:2.2950593195258877\n",
      "train loss:2.302203532709319\n",
      "train loss:2.306237575533273\n",
      "train loss:2.2918464720307625\n",
      "train loss:2.301259326699767\n",
      "train loss:2.304361776016766\n",
      "train loss:2.310130869144192\n",
      "train loss:2.3052335268108815\n",
      "train loss:2.3094228291098102\n",
      "train loss:2.303526895692483\n",
      "train loss:2.3058344707392804\n",
      "train loss:2.299413569126444\n",
      "train loss:2.3023175731530627\n",
      "train loss:2.2950604482970376\n",
      "train loss:2.2981760461978378\n",
      "train loss:2.30441160366905\n",
      "train loss:2.2935677450027887\n",
      "train loss:2.298311106125763\n",
      "train loss:2.3084786872181304\n",
      "train loss:2.297226124485617\n",
      "train loss:2.3010505669000696\n",
      "train loss:2.3021181552387096\n",
      "train loss:2.303926771884806\n",
      "train loss:2.303495424017638\n",
      "train loss:2.30177307485626\n",
      "train loss:2.2977109747540707\n",
      "train loss:2.298341074564457\n",
      "train loss:2.3085051639011462\n",
      "train loss:2.3087233664828135\n",
      "train loss:2.3143006338046637\n",
      "train loss:2.3025252092179516\n",
      "train loss:2.3095921700897852\n",
      "train loss:2.3037883708965188\n",
      "train loss:2.301597398287743\n",
      "train loss:2.2918968502781834\n",
      "train loss:2.3040061971980226\n",
      "train loss:2.3027268580255167\n",
      "train loss:2.3038842815290903\n",
      "train loss:2.293540454738664\n",
      "train loss:2.2918726251269135\n",
      "train loss:2.2932120762623436\n",
      "train loss:2.294899480876357\n",
      "train loss:2.3021601565311336\n",
      "train loss:2.302836493498346\n",
      "train loss:2.3010248750216005\n",
      "train loss:2.298446665339606\n",
      "train loss:2.3034117277412984\n",
      "train loss:2.297731765194964\n",
      "train loss:2.301709469247582\n",
      "train loss:2.300380331432956\n",
      "train loss:2.300314543187421\n",
      "train loss:2.301234992486253\n",
      "train loss:2.300390096462836\n",
      "train loss:2.2914694164925873\n",
      "train loss:2.2987750023821003\n",
      "train loss:2.2972713724641536\n",
      "train loss:2.3005497793855034\n",
      "train loss:2.3001467131550735\n",
      "train loss:2.3073374596118303\n",
      "train loss:2.298168914036904\n",
      "train loss:2.296198204693162\n",
      "train loss:2.2946231518985143\n",
      "train loss:2.298731734820489\n",
      "train loss:2.295016022014506\n",
      "train loss:2.2960227864308074\n",
      "train loss:2.3005052894651126\n",
      "train loss:2.3021922149948453\n",
      "train loss:2.3086478645758004\n",
      "train loss:2.309373031924595\n",
      "train loss:2.2945625751817724\n",
      "train loss:2.3105064261266843\n",
      "train loss:2.3003171862295986\n",
      "train loss:2.302260884933322\n",
      "train loss:2.297599310256822\n",
      "train loss:2.30287765993415\n",
      "train loss:2.312594325513156\n",
      "train loss:2.3018753644687036\n",
      "train loss:2.3012569616484035\n",
      "train loss:2.3069738438268987\n",
      "train loss:2.3025403350179636\n",
      "train loss:2.2986505777538633\n",
      "train loss:2.2984053827671285\n",
      "train loss:2.29867870679837\n",
      "train loss:2.2971593116207973\n",
      "train loss:2.301755461012618\n",
      "train loss:2.299136093361467\n",
      "train loss:2.2990407920518567\n",
      "train loss:2.301904304332769\n",
      "train loss:2.2987884585225435\n",
      "train loss:2.304835441577312\n",
      "train loss:2.2893038145483864\n",
      "train loss:2.3085151599476936\n",
      "train loss:2.3001377865923773\n",
      "train loss:2.3013649147225737\n",
      "train loss:2.308820680716463\n",
      "train loss:2.3064872415747035\n",
      "train loss:2.3105217302262546\n",
      "train loss:2.3044747247976276\n",
      "train loss:2.2982002128672954\n",
      "train loss:2.3017116991884468\n",
      "train loss:2.3002749342681987\n",
      "train loss:2.3008367420590186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.296226285943263\n",
      "train loss:2.301141924288233\n",
      "train loss:2.298242906527526\n",
      "train loss:2.301750987380159\n",
      "train loss:2.3035163288996183\n",
      "train loss:2.3015613705710782\n",
      "train loss:2.2959877548562924\n",
      "train loss:2.3103842312879443\n",
      "train loss:2.2964552299359138\n",
      "train loss:2.3037906320449806\n",
      "train loss:2.3039544122795084\n",
      "train loss:2.3019576806972863\n",
      "train loss:2.2987328610910605\n",
      "train loss:2.308497973814585\n",
      "train loss:2.298266466004428\n",
      "train loss:2.3029451729515285\n",
      "train loss:2.2971689929683508\n",
      "train loss:2.302849630971475\n",
      "train loss:2.2946536098362023\n",
      "train loss:2.297849141531015\n",
      "train loss:2.304636655777667\n",
      "train loss:2.3011196003049816\n",
      "train loss:2.295761302134268\n",
      "train loss:2.3024007356455134\n",
      "train loss:2.302333633565581\n",
      "train loss:2.301539444439318\n",
      "train loss:2.2941996429531164\n",
      "train loss:2.3093190135955117\n",
      "train loss:2.3043896680326954\n",
      "train loss:2.2995069703221525\n",
      "train loss:2.3040025630446594\n",
      "train loss:2.2964365946109697\n",
      "train loss:2.3017571620424255\n",
      "train loss:2.2971561487417302\n",
      "train loss:2.3075461696506534\n",
      "train loss:2.2981502746264386\n",
      "train loss:2.304287597619471\n",
      "train loss:2.3059992082913436\n",
      "train loss:2.296121330440156\n",
      "train loss:2.307904641089914\n",
      "train loss:2.2920675967935566\n",
      "train loss:2.2977754407147986\n",
      "train loss:2.3051970516028817\n",
      "train loss:2.299718958535404\n",
      "train loss:2.302669071839767\n",
      "train loss:2.300085848199423\n",
      "train loss:2.2969848431371673\n",
      "train loss:2.2990101263656846\n",
      "train loss:2.3069104127886253\n",
      "train loss:2.304671421912318\n",
      "train loss:2.2952084570820683\n",
      "train loss:2.3017795878288987\n",
      "train loss:2.2984812998400064\n",
      "train loss:2.2922658014960873\n",
      "train loss:2.3058624838167123\n",
      "train loss:2.299067270516309\n",
      "train loss:2.29785223362427\n",
      "train loss:2.294450737725975\n",
      "train loss:2.300546493333419\n",
      "train loss:2.304551007169847\n",
      "train loss:2.307604811129586\n",
      "train loss:2.2996196738453607\n",
      "train loss:2.308534875613006\n",
      "train loss:2.3010267262576862\n",
      "train loss:2.297460897936333\n",
      "train loss:2.298365927407307\n",
      "train loss:2.303808096141713\n",
      "train loss:2.293290036251687\n",
      "train loss:2.302934106272959\n",
      "train loss:2.300207226552638\n",
      "train loss:2.3052429455668393\n",
      "train loss:2.304484423085382\n",
      "train loss:2.2974969424491714\n",
      "train loss:2.304338725402746\n",
      "train loss:2.3061337994321507\n",
      "train loss:2.298178367168165\n",
      "train loss:2.2886740331756696\n",
      "train loss:2.3067965639327346\n",
      "train loss:2.3030817180981082\n",
      "train loss:2.2969255112085776\n",
      "train loss:2.295945382788116\n",
      "train loss:2.3086597922426844\n",
      "train loss:2.3028206001029066\n",
      "train loss:2.3014452316420555\n",
      "train loss:2.3070185658385527\n",
      "train loss:2.3012916593081845\n",
      "train loss:2.30431425582596\n",
      "train loss:2.3060932569463204\n",
      "train loss:2.3015096966232655\n",
      "train loss:2.3120287408210847\n",
      "train loss:2.30534853048896\n",
      "train loss:2.3027146679755037\n",
      "train loss:2.2985657206199774\n",
      "train loss:2.305406885420297\n",
      "train loss:2.3026129527339085\n",
      "=== epoch:116, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3030922187011127\n",
      "train loss:2.298966721942702\n",
      "train loss:2.301373392520632\n",
      "train loss:2.306030184314486\n",
      "train loss:2.3091789606234214\n",
      "train loss:2.298690351899822\n",
      "train loss:2.2984107340575144\n",
      "train loss:2.2986119439535293\n",
      "train loss:2.299236193470309\n",
      "train loss:2.310702834870125\n",
      "train loss:2.3062092441465993\n",
      "train loss:2.292961462990534\n",
      "train loss:2.302224223140351\n",
      "train loss:2.300407849527238\n",
      "train loss:2.290226774464095\n",
      "train loss:2.3065526824305915\n",
      "train loss:2.3027145495592567\n",
      "train loss:2.2949444953342266\n",
      "train loss:2.302301339163188\n",
      "train loss:2.298926549442356\n",
      "train loss:2.295364634704754\n",
      "train loss:2.2984058034604553\n",
      "train loss:2.300916812461255\n",
      "train loss:2.3052545198210144\n",
      "train loss:2.2927543804659374\n",
      "train loss:2.302493510089339\n",
      "train loss:2.2963007443308654\n",
      "train loss:2.3013384397623264\n",
      "train loss:2.2967873052306524\n",
      "train loss:2.299259596493238\n",
      "train loss:2.3041042446225655\n",
      "train loss:2.304333125917022\n",
      "train loss:2.3052370553089814\n",
      "train loss:2.3122024761463185\n",
      "train loss:2.300203772980868\n",
      "train loss:2.301715970161119\n",
      "train loss:2.3072614985460276\n",
      "train loss:2.302055529065\n",
      "train loss:2.2995344301296052\n",
      "train loss:2.304805910304383\n",
      "train loss:2.30622623047023\n",
      "train loss:2.302303312167604\n",
      "train loss:2.292564798153505\n",
      "train loss:2.2994292785052006\n",
      "train loss:2.301237597246717\n",
      "train loss:2.2994488290918054\n",
      "train loss:2.297565616437277\n",
      "train loss:2.3106718374862765\n",
      "train loss:2.302331445110626\n",
      "train loss:2.303903581196288\n",
      "train loss:2.3022365227065973\n",
      "train loss:2.300169940988518\n",
      "train loss:2.297470526431653\n",
      "train loss:2.3039213563837793\n",
      "train loss:2.3064708551190796\n",
      "train loss:2.2959308894100823\n",
      "train loss:2.3041749955836974\n",
      "train loss:2.308119645703121\n",
      "train loss:2.2938294823500383\n",
      "train loss:2.303409098759508\n",
      "train loss:2.3018095206918128\n",
      "train loss:2.302534004382116\n",
      "train loss:2.3061795889222476\n",
      "train loss:2.306443091343795\n",
      "train loss:2.3054789495971955\n",
      "train loss:2.300837350450183\n",
      "train loss:2.298568730905534\n",
      "train loss:2.3029256580122506\n",
      "train loss:2.3039697057131328\n",
      "train loss:2.3046111609030286\n",
      "train loss:2.3011842881327462\n",
      "train loss:2.3050390789952733\n",
      "train loss:2.301695583628098\n",
      "train loss:2.2959110211786173\n",
      "train loss:2.29608984048181\n",
      "train loss:2.302523539105889\n",
      "train loss:2.3042880858078894\n",
      "train loss:2.302965775140244\n",
      "train loss:2.3016227219406558\n",
      "train loss:2.30879549274864\n",
      "train loss:2.300410068265372\n",
      "train loss:2.3059980741632575\n",
      "train loss:2.300987591281489\n",
      "train loss:2.30363689236107\n",
      "train loss:2.295708813657877\n",
      "train loss:2.2984732507290886\n",
      "train loss:2.3006138226760386\n",
      "train loss:2.2992468277814995\n",
      "train loss:2.2952754912782867\n",
      "train loss:2.305784495184952\n",
      "train loss:2.3094191742846255\n",
      "train loss:2.3036729972698584\n",
      "train loss:2.306638771292299\n",
      "train loss:2.300170330582352\n",
      "train loss:2.3071799047875254\n",
      "train loss:2.3018563753265355\n",
      "train loss:2.2920623417767616\n",
      "train loss:2.2964231610496517\n",
      "train loss:2.300589449969174\n",
      "train loss:2.302259495139234\n",
      "train loss:2.3061646076135665\n",
      "train loss:2.3055789960602335\n",
      "train loss:2.294973123658241\n",
      "train loss:2.3004606129649248\n",
      "train loss:2.306781212348933\n",
      "train loss:2.300246410632781\n",
      "train loss:2.300306613853889\n",
      "train loss:2.303803128206441\n",
      "train loss:2.2982713878294536\n",
      "train loss:2.29821995262143\n",
      "train loss:2.2964407273511984\n",
      "train loss:2.2937145432599544\n",
      "train loss:2.2934132251832207\n",
      "train loss:2.300021098123397\n",
      "train loss:2.3046100960383358\n",
      "train loss:2.299255956476868\n",
      "train loss:2.3062909254332746\n",
      "train loss:2.3028377950473007\n",
      "train loss:2.2979373468779154\n",
      "train loss:2.3002270169339316\n",
      "train loss:2.3035478467417154\n",
      "train loss:2.309135279030369\n",
      "train loss:2.2982135488634414\n",
      "train loss:2.302276502299238\n",
      "train loss:2.299426870955853\n",
      "train loss:2.3079289882101914\n",
      "train loss:2.2978083307135977\n",
      "train loss:2.3031474324405488\n",
      "train loss:2.3018212080475933\n",
      "train loss:2.3002549221022766\n",
      "train loss:2.302017066141847\n",
      "train loss:2.289180884249542\n",
      "train loss:2.3034167391942177\n",
      "train loss:2.298933108403117\n",
      "train loss:2.2986059149326388\n",
      "train loss:2.3014812331299623\n",
      "train loss:2.308583554732251\n",
      "train loss:2.301241619796341\n",
      "train loss:2.2969139546718926\n",
      "train loss:2.2957530837456064\n",
      "train loss:2.29770455939232\n",
      "train loss:2.3063132352666265\n",
      "train loss:2.3021434170099644\n",
      "train loss:2.3108275117497477\n",
      "train loss:2.307650855065018\n",
      "train loss:2.306258817408004\n",
      "train loss:2.2954142348753663\n",
      "train loss:2.297917779298123\n",
      "train loss:2.297128377782018\n",
      "train loss:2.292966416254928\n",
      "train loss:2.30728789188093\n",
      "train loss:2.3061977211587594\n",
      "train loss:2.300290538191182\n",
      "train loss:2.311339837813144\n",
      "train loss:2.296174177022306\n",
      "train loss:2.3081681061348736\n",
      "train loss:2.2934587898625582\n",
      "train loss:2.3049519479400744\n",
      "train loss:2.300022685894829\n",
      "train loss:2.3046616864104936\n",
      "train loss:2.304274550531698\n",
      "train loss:2.3059777775523616\n",
      "train loss:2.29885226043121\n",
      "train loss:2.308119533272897\n",
      "train loss:2.302218099350919\n",
      "train loss:2.298161667232479\n",
      "train loss:2.303983729729247\n",
      "train loss:2.3022903810978996\n",
      "train loss:2.307250253311497\n",
      "train loss:2.297264984509646\n",
      "train loss:2.3002661398681266\n",
      "train loss:2.3023044047965513\n",
      "train loss:2.296133781181863\n",
      "train loss:2.3024891581484814\n",
      "train loss:2.2921845243683063\n",
      "train loss:2.294593578846358\n",
      "train loss:2.293491579748141\n",
      "train loss:2.304601983307152\n",
      "train loss:2.29380711578466\n",
      "train loss:2.2954874983230082\n",
      "train loss:2.3029075336999245\n",
      "train loss:2.304015843407041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3048839181634153\n",
      "train loss:2.297997535907538\n",
      "train loss:2.30322106165738\n",
      "train loss:2.2942694623498845\n",
      "train loss:2.303623166416887\n",
      "train loss:2.2987307303720783\n",
      "train loss:2.307386747607908\n",
      "train loss:2.300426345395857\n",
      "train loss:2.2980255703132584\n",
      "train loss:2.3101425442909047\n",
      "train loss:2.2971032779640015\n",
      "train loss:2.3135361565968364\n",
      "train loss:2.3003429322785194\n",
      "train loss:2.299136413122368\n",
      "train loss:2.3013842534066247\n",
      "train loss:2.300090795869637\n",
      "train loss:2.303040825935174\n",
      "train loss:2.3040260342567374\n",
      "=== epoch:117, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.303103185072347\n",
      "train loss:2.303946876616587\n",
      "train loss:2.2953514032452773\n",
      "train loss:2.297994657386782\n",
      "train loss:2.3090777263165627\n",
      "train loss:2.2945718318113717\n",
      "train loss:2.300955679733067\n",
      "train loss:2.3000221451161664\n",
      "train loss:2.3006985629954944\n",
      "train loss:2.3059115658786613\n",
      "train loss:2.3000726942432714\n",
      "train loss:2.294822488151699\n",
      "train loss:2.3078730330019526\n",
      "train loss:2.304586842605798\n",
      "train loss:2.298972637740881\n",
      "train loss:2.2994087618531927\n",
      "train loss:2.2994947642752233\n",
      "train loss:2.297799341745909\n",
      "train loss:2.296329947144354\n",
      "train loss:2.2907131582081215\n",
      "train loss:2.300641852899476\n",
      "train loss:2.302645765723631\n",
      "train loss:2.300355966495504\n",
      "train loss:2.2978835223804914\n",
      "train loss:2.2927857729282817\n",
      "train loss:2.301948687741191\n",
      "train loss:2.3059939642093945\n",
      "train loss:2.299859857496508\n",
      "train loss:2.3038614373226554\n",
      "train loss:2.30174414380859\n",
      "train loss:2.3164659755914454\n",
      "train loss:2.3040241210064107\n",
      "train loss:2.3036042461401682\n",
      "train loss:2.3077320433189974\n",
      "train loss:2.2969937907886773\n",
      "train loss:2.3098283416757135\n",
      "train loss:2.30105212459504\n",
      "train loss:2.3082700855264187\n",
      "train loss:2.2969590360019767\n",
      "train loss:2.3009773513223686\n",
      "train loss:2.298939693225351\n",
      "train loss:2.306789398837055\n",
      "train loss:2.3016068584506217\n",
      "train loss:2.296874062822318\n",
      "train loss:2.3099233997420114\n",
      "train loss:2.3032655471433374\n",
      "train loss:2.2987437649675537\n",
      "train loss:2.2985036556772904\n",
      "train loss:2.3019876334855307\n",
      "train loss:2.2998535058298915\n",
      "train loss:2.3041418084112917\n",
      "train loss:2.3080922837407987\n",
      "train loss:2.30190647322524\n",
      "train loss:2.3098801578913704\n",
      "train loss:2.2950594637654635\n",
      "train loss:2.292939600440847\n",
      "train loss:2.3017048330782077\n",
      "train loss:2.301451972645907\n",
      "train loss:2.3007524036187363\n",
      "train loss:2.304507921839396\n",
      "train loss:2.3004567412769594\n",
      "train loss:2.301713997943447\n",
      "train loss:2.2995534249161897\n",
      "train loss:2.3045593182186694\n",
      "train loss:2.294988262846602\n",
      "train loss:2.2895194058161406\n",
      "train loss:2.2976609534769397\n",
      "train loss:2.3019015194466044\n",
      "train loss:2.3000404119455915\n",
      "train loss:2.302515966279542\n",
      "train loss:2.303814530418208\n",
      "train loss:2.297526206795842\n",
      "train loss:2.313333541058719\n",
      "train loss:2.296256189551891\n",
      "train loss:2.2973074938526983\n",
      "train loss:2.301091550686965\n",
      "train loss:2.3002391350310716\n",
      "train loss:2.2922143830725976\n",
      "train loss:2.297083172143241\n",
      "train loss:2.2956001180838017\n",
      "train loss:2.3026086579629945\n",
      "train loss:2.30878914305793\n",
      "train loss:2.303100529244136\n",
      "train loss:2.3031727721179545\n",
      "train loss:2.298023919523504\n",
      "train loss:2.2931300023104435\n",
      "train loss:2.3070566092198073\n",
      "train loss:2.3015522821391214\n",
      "train loss:2.3025330890867766\n",
      "train loss:2.2936966942418175\n",
      "train loss:2.301154392794467\n",
      "train loss:2.3014175200284384\n",
      "train loss:2.2972161834626372\n",
      "train loss:2.3099258641542635\n",
      "train loss:2.301783837566426\n",
      "train loss:2.303474643900931\n",
      "train loss:2.3022437422866417\n",
      "train loss:2.3026627935656996\n",
      "train loss:2.2912180434841702\n",
      "train loss:2.303795764937163\n",
      "train loss:2.3024628761377497\n",
      "train loss:2.304560642391733\n",
      "train loss:2.2967092097189834\n",
      "train loss:2.2940503578633424\n",
      "train loss:2.3013842040530217\n",
      "train loss:2.301008360668622\n",
      "train loss:2.307105775837667\n",
      "train loss:2.3060747134547817\n",
      "train loss:2.2985997865952\n",
      "train loss:2.295886023507088\n",
      "train loss:2.302315816881028\n",
      "train loss:2.306427301554241\n",
      "train loss:2.303955734244302\n",
      "train loss:2.2889563406869318\n",
      "train loss:2.3058242402555225\n",
      "train loss:2.299772636554154\n",
      "train loss:2.29815929059565\n",
      "train loss:2.299471202973464\n",
      "train loss:2.3050777700088\n",
      "train loss:2.3006426289182142\n",
      "train loss:2.2994479858482717\n",
      "train loss:2.3053844204022105\n",
      "train loss:2.310413543521877\n",
      "train loss:2.3007567085687692\n",
      "train loss:2.301545956723469\n",
      "train loss:2.304395969714537\n",
      "train loss:2.303149940926827\n",
      "train loss:2.2987270645394764\n",
      "train loss:2.301361489691071\n",
      "train loss:2.305783731979053\n",
      "train loss:2.301322223754217\n",
      "train loss:2.297221263175336\n",
      "train loss:2.30689065780883\n",
      "train loss:2.3094219465086123\n",
      "train loss:2.298050357161668\n",
      "train loss:2.2979852355856987\n",
      "train loss:2.307016883854628\n",
      "train loss:2.2859751581720746\n",
      "train loss:2.2958353055149683\n",
      "train loss:2.3088500277422335\n",
      "train loss:2.298885857875015\n",
      "train loss:2.3005325770272544\n",
      "train loss:2.3007073377464313\n",
      "train loss:2.3013953021774514\n",
      "train loss:2.2983795686698567\n",
      "train loss:2.2999750362179783\n",
      "train loss:2.2997215385392202\n",
      "train loss:2.30018354854776\n",
      "train loss:2.2964786583343884\n",
      "train loss:2.30814737472777\n",
      "train loss:2.3091123552437365\n",
      "train loss:2.300731990246399\n",
      "train loss:2.3032223883679914\n",
      "train loss:2.2986805137919766\n",
      "train loss:2.2993394858409575\n",
      "train loss:2.307036687121563\n",
      "train loss:2.3016138515647815\n",
      "train loss:2.3000691634452006\n",
      "train loss:2.306759644140571\n",
      "train loss:2.306690353050326\n",
      "train loss:2.3108327762844776\n",
      "train loss:2.2946633559806737\n",
      "train loss:2.3001452697527967\n",
      "train loss:2.29946346375582\n",
      "train loss:2.299188121286953\n",
      "train loss:2.3153750435782245\n",
      "train loss:2.3008146856667446\n",
      "train loss:2.293124177418542\n",
      "train loss:2.3000977602039367\n",
      "train loss:2.2990895047864797\n",
      "train loss:2.3066797698271837\n",
      "train loss:2.3008135596967065\n",
      "train loss:2.3077900641983744\n",
      "train loss:2.309499259903166\n",
      "train loss:2.2995588547475\n",
      "train loss:2.2985612455344717\n",
      "train loss:2.2998523780594233\n",
      "train loss:2.2978016265293264\n",
      "train loss:2.2963851985662247\n",
      "train loss:2.3028168622568326\n",
      "train loss:2.302932493997947\n",
      "train loss:2.307331954117488\n",
      "train loss:2.3033414435005044\n",
      "train loss:2.305920094928048\n",
      "train loss:2.293419367440492\n",
      "train loss:2.3010729591195336\n",
      "train loss:2.2986871335470855\n",
      "train loss:2.301901492776441\n",
      "train loss:2.2978424310637235\n",
      "train loss:2.301306869252993\n",
      "train loss:2.3017253268925764\n",
      "train loss:2.3053976310796007\n",
      "train loss:2.3063779692872397\n",
      "train loss:2.3005541974154533\n",
      "train loss:2.3024163288504984\n",
      "train loss:2.305362335820803\n",
      "train loss:2.298726793689119\n",
      "train loss:2.301942817273813\n",
      "train loss:2.3084145257611053\n",
      "train loss:2.300103269257213\n",
      "=== epoch:118, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3046080401602893\n",
      "train loss:2.3006453303191874\n",
      "train loss:2.3118677904984275\n",
      "train loss:2.3049627573101827\n",
      "train loss:2.3011724108217497\n",
      "train loss:2.3009046298664657\n",
      "train loss:2.29944403867975\n",
      "train loss:2.2946502239117725\n",
      "train loss:2.300247987465695\n",
      "train loss:2.299220242925182\n",
      "train loss:2.3021225243871544\n",
      "train loss:2.3040061616406957\n",
      "train loss:2.296895482489256\n",
      "train loss:2.306818694108473\n",
      "train loss:2.3032530700474925\n",
      "train loss:2.293934661972386\n",
      "train loss:2.2991042569266074\n",
      "train loss:2.2918180502209893\n",
      "train loss:2.303834575280717\n",
      "train loss:2.2979747001286794\n",
      "train loss:2.2960036428120847\n",
      "train loss:2.299648855689854\n",
      "train loss:2.3067567994455405\n",
      "train loss:2.3057871566354753\n",
      "train loss:2.2993106426732237\n",
      "train loss:2.297285049660973\n",
      "train loss:2.3030377806749383\n",
      "train loss:2.299759479151627\n",
      "train loss:2.2947368180136185\n",
      "train loss:2.3042345398122364\n",
      "train loss:2.296461944584911\n",
      "train loss:2.3001834973365876\n",
      "train loss:2.301711084235114\n",
      "train loss:2.299408411289278\n",
      "train loss:2.3020537832183505\n",
      "train loss:2.297113384363336\n",
      "train loss:2.3023788829564276\n",
      "train loss:2.2959091780048784\n",
      "train loss:2.3006857098456854\n",
      "train loss:2.292924117382052\n",
      "train loss:2.299431291120957\n",
      "train loss:2.306686915323052\n",
      "train loss:2.2972848802617167\n",
      "train loss:2.295409271968137\n",
      "train loss:2.2990272148733197\n",
      "train loss:2.3047031605202917\n",
      "train loss:2.298836036224489\n",
      "train loss:2.3054408031864306\n",
      "train loss:2.2973661205143974\n",
      "train loss:2.302773312959798\n",
      "train loss:2.2925081003168786\n",
      "train loss:2.2997059456138684\n",
      "train loss:2.302597089477764\n",
      "train loss:2.3087488874514497\n",
      "train loss:2.2980668045186174\n",
      "train loss:2.300677993344466\n",
      "train loss:2.297185757023639\n",
      "train loss:2.2973308907440924\n",
      "train loss:2.3048520278548676\n",
      "train loss:2.2998042406594843\n",
      "train loss:2.300631768148021\n",
      "train loss:2.2958936989877703\n",
      "train loss:2.3008389517220977\n",
      "train loss:2.294918564086633\n",
      "train loss:2.2976240937391523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302416726750228\n",
      "train loss:2.3037917715717953\n",
      "train loss:2.3028431666170524\n",
      "train loss:2.303797213234365\n",
      "train loss:2.2944157607300335\n",
      "train loss:2.3028031731901457\n",
      "train loss:2.3063250269247373\n",
      "train loss:2.300720552265021\n",
      "train loss:2.293553619394298\n",
      "train loss:2.3033762776670046\n",
      "train loss:2.2914385396758603\n",
      "train loss:2.3031673885639212\n",
      "train loss:2.303244626313491\n",
      "train loss:2.298483901322873\n",
      "train loss:2.305903684542651\n",
      "train loss:2.3041360770953876\n",
      "train loss:2.3066555118387213\n",
      "train loss:2.3039767119972767\n",
      "train loss:2.3035882227500872\n",
      "train loss:2.2975900106626277\n",
      "train loss:2.300166747441808\n",
      "train loss:2.2991907729267256\n",
      "train loss:2.2998230004819087\n",
      "train loss:2.3025811776509877\n",
      "train loss:2.3138120322581552\n",
      "train loss:2.30535585548896\n",
      "train loss:2.2987661154209236\n",
      "train loss:2.3010442278506558\n",
      "train loss:2.301575018958286\n",
      "train loss:2.294284044605496\n",
      "train loss:2.294659912936522\n",
      "train loss:2.3023154148759275\n",
      "train loss:2.2956949972018323\n",
      "train loss:2.306993240974294\n",
      "train loss:2.307800365506495\n",
      "train loss:2.3045400890942993\n",
      "train loss:2.301929611471711\n",
      "train loss:2.2927207618594445\n",
      "train loss:2.3029812308536366\n",
      "train loss:2.2948114752374646\n",
      "train loss:2.3098445823910048\n",
      "train loss:2.2907886775164754\n",
      "train loss:2.300931930400934\n",
      "train loss:2.302227427370606\n",
      "train loss:2.301296504459313\n",
      "train loss:2.2852823020463036\n",
      "train loss:2.3060155513503657\n",
      "train loss:2.290753211718768\n",
      "train loss:2.3015355563007174\n",
      "train loss:2.3005189834527275\n",
      "train loss:2.295094765584257\n",
      "train loss:2.3039269195463286\n",
      "train loss:2.302669940653763\n",
      "train loss:2.296652934111178\n",
      "train loss:2.299499018897263\n",
      "train loss:2.2961799106265004\n",
      "train loss:2.315128651580348\n",
      "train loss:2.304413999464424\n",
      "train loss:2.3041557402394264\n",
      "train loss:2.304390324590367\n",
      "train loss:2.3046717484139028\n",
      "train loss:2.300473238224323\n",
      "train loss:2.304646389656335\n",
      "train loss:2.2994129098135825\n",
      "train loss:2.3007851542679068\n",
      "train loss:2.2937107910950814\n",
      "train loss:2.292923170712976\n",
      "train loss:2.2942139359779623\n",
      "train loss:2.304910587776133\n",
      "train loss:2.2892011384999966\n",
      "train loss:2.304725628473539\n",
      "train loss:2.2975288304556645\n",
      "train loss:2.3027125391925494\n",
      "train loss:2.2926409908365777\n",
      "train loss:2.2980827288108934\n",
      "train loss:2.289630041649365\n",
      "train loss:2.3070599995745167\n",
      "train loss:2.2982899010867617\n",
      "train loss:2.2879957942244133\n",
      "train loss:2.3010880558656543\n",
      "train loss:2.3061331494163113\n",
      "train loss:2.306278972882494\n",
      "train loss:2.29931986835964\n",
      "train loss:2.3020444684679475\n",
      "train loss:2.2962029872897904\n",
      "train loss:2.2958874042331368\n",
      "train loss:2.303076171770546\n",
      "train loss:2.307367658337181\n",
      "train loss:2.301085597343687\n",
      "train loss:2.303532620302086\n",
      "train loss:2.297710718468982\n",
      "train loss:2.2978197619023644\n",
      "train loss:2.2881724390966967\n",
      "train loss:2.3064055447592566\n",
      "train loss:2.3043277835342195\n",
      "train loss:2.29928072889807\n",
      "train loss:2.3006794884189943\n",
      "train loss:2.2985592461424007\n",
      "train loss:2.2949470408741677\n",
      "train loss:2.307110459641262\n",
      "train loss:2.294028552394327\n",
      "train loss:2.3106212349979516\n",
      "train loss:2.2974290069171985\n",
      "train loss:2.2978350459053463\n",
      "train loss:2.307818002522307\n",
      "train loss:2.3063122047893714\n",
      "train loss:2.2979705443894147\n",
      "train loss:2.3094475537506964\n",
      "train loss:2.299697169588286\n",
      "train loss:2.2996204073330007\n",
      "train loss:2.297919202392599\n",
      "train loss:2.3039832420505677\n",
      "train loss:2.30309292665737\n",
      "train loss:2.30451786589105\n",
      "train loss:2.2946263166910374\n",
      "train loss:2.3036909831598495\n",
      "train loss:2.3002427772460505\n",
      "train loss:2.3049891588075617\n",
      "train loss:2.3093055549212838\n",
      "train loss:2.3023031909864584\n",
      "train loss:2.3094877908329416\n",
      "train loss:2.306053394437522\n",
      "train loss:2.3024635205176316\n",
      "train loss:2.304145531677998\n",
      "train loss:2.2987539514125896\n",
      "train loss:2.2989611930825395\n",
      "train loss:2.3062325103414034\n",
      "train loss:2.3026822710685475\n",
      "train loss:2.301066482414066\n",
      "train loss:2.311560542968804\n",
      "train loss:2.295213351144032\n",
      "train loss:2.2982595566672495\n",
      "train loss:2.29781137995949\n",
      "train loss:2.3097236823963154\n",
      "train loss:2.302363706824006\n",
      "=== epoch:119, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.306862495553007\n",
      "train loss:2.3019683168305356\n",
      "train loss:2.3011843687795404\n",
      "train loss:2.294286478785164\n",
      "train loss:2.2915145459889144\n",
      "train loss:2.2974031765414127\n",
      "train loss:2.2986436459647868\n",
      "train loss:2.3004070981682974\n",
      "train loss:2.298015532456859\n",
      "train loss:2.2978488629938174\n",
      "train loss:2.275014827646592\n",
      "train loss:2.2987328897431416\n",
      "train loss:2.3032842662637503\n",
      "train loss:2.296672675739365\n",
      "train loss:2.305674463824737\n",
      "train loss:2.295736745319957\n",
      "train loss:2.306928214209088\n",
      "train loss:2.296105103160746\n",
      "train loss:2.3024330641161677\n",
      "train loss:2.2924039420120264\n",
      "train loss:2.305928537611286\n",
      "train loss:2.3022851716848445\n",
      "train loss:2.2965795933100517\n",
      "train loss:2.295066240040858\n",
      "train loss:2.3037330096081643\n",
      "train loss:2.299047744619066\n",
      "train loss:2.302227207226085\n",
      "train loss:2.300967601587458\n",
      "train loss:2.302541431714224\n",
      "train loss:2.303688656889136\n",
      "train loss:2.288604500379155\n",
      "train loss:2.3022620669391833\n",
      "train loss:2.2964290264248555\n",
      "train loss:2.2975472947246316\n",
      "train loss:2.300581163703757\n",
      "train loss:2.2996573521903265\n",
      "train loss:2.2849959603000616\n",
      "train loss:2.2961012346475203\n",
      "train loss:2.303235822785305\n",
      "train loss:2.296370485730637\n",
      "train loss:2.3010440733575\n",
      "train loss:2.2953434277129885\n",
      "train loss:2.290676362302109\n",
      "train loss:2.307264679222177\n",
      "train loss:2.306801326856312\n",
      "train loss:2.306285900047689\n",
      "train loss:2.2999237633740597\n",
      "train loss:2.302357189941719\n",
      "train loss:2.3013352095457145\n",
      "train loss:2.3076608945143477\n",
      "train loss:2.309932609345397\n",
      "train loss:2.2995383879901348\n",
      "train loss:2.303957096482194\n",
      "train loss:2.299248887720484\n",
      "train loss:2.2989046692904647\n",
      "train loss:2.3024869535629366\n",
      "train loss:2.3103111611318643\n",
      "train loss:2.3008738985459694\n",
      "train loss:2.3003838357744697\n",
      "train loss:2.3085628200404376\n",
      "train loss:2.3039360425159723\n",
      "train loss:2.3047591633348983\n",
      "train loss:2.2938200382513294\n",
      "train loss:2.3063258652137906\n",
      "train loss:2.30421069091351\n",
      "train loss:2.2971039878237907\n",
      "train loss:2.295897211577619\n",
      "train loss:2.3020594225923925\n",
      "train loss:2.3000550925897483\n",
      "train loss:2.298401550246791\n",
      "train loss:2.306742472590406\n",
      "train loss:2.297084693394023\n",
      "train loss:2.2990304674193007\n",
      "train loss:2.3031623788718565\n",
      "train loss:2.299104410076984\n",
      "train loss:2.3102371150247016\n",
      "train loss:2.3058383968325975\n",
      "train loss:2.306450429284715\n",
      "train loss:2.306464631929309\n",
      "train loss:2.3044801174369622\n",
      "train loss:2.3054746838455036\n",
      "train loss:2.2959165640085732\n",
      "train loss:2.3050990964914244\n",
      "train loss:2.309252870448347\n",
      "train loss:2.2932623636537146\n",
      "train loss:2.2885194267746094\n",
      "train loss:2.293481225154213\n",
      "train loss:2.307664852170017\n",
      "train loss:2.3075171538200396\n",
      "train loss:2.300200996431766\n",
      "train loss:2.293298449989416\n",
      "train loss:2.308487213742702\n",
      "train loss:2.305985929009972\n",
      "train loss:2.303161633485429\n",
      "train loss:2.2993415496695877\n",
      "train loss:2.301799941974616\n",
      "train loss:2.294862338930848\n",
      "train loss:2.2940218344737353\n",
      "train loss:2.29981998228295\n",
      "train loss:2.305318920805329\n",
      "train loss:2.2931695745301575\n",
      "train loss:2.297482983898818\n",
      "train loss:2.2994099068978144\n",
      "train loss:2.303160020153891\n",
      "train loss:2.291465745340007\n",
      "train loss:2.3090541260961404\n",
      "train loss:2.303044830105223\n",
      "train loss:2.3055441777604724\n",
      "train loss:2.305045752033627\n",
      "train loss:2.2962923507464357\n",
      "train loss:2.3096317175017895\n",
      "train loss:2.2979519423008474\n",
      "train loss:2.2985854606586336\n",
      "train loss:2.2943857387351834\n",
      "train loss:2.3029746746850064\n",
      "train loss:2.3058232369532017\n",
      "train loss:2.2924798925278287\n",
      "train loss:2.302821915785267\n",
      "train loss:2.302856567503975\n",
      "train loss:2.3062945264666457\n",
      "train loss:2.308027000570346\n",
      "train loss:2.2989462968140955\n",
      "train loss:2.300904983863643\n",
      "train loss:2.2943306597543667\n",
      "train loss:2.315558480902847\n",
      "train loss:2.2969678698600453\n",
      "train loss:2.2986131996944366\n",
      "train loss:2.3090171497944465\n",
      "train loss:2.2901593940686498\n",
      "train loss:2.296088495034908\n",
      "train loss:2.3059797925953895\n",
      "train loss:2.296981877531899\n",
      "train loss:2.2958366725386825\n",
      "train loss:2.3045121103719097\n",
      "train loss:2.3093938738058797\n",
      "train loss:2.3020863524775548\n",
      "train loss:2.3030967069087986\n",
      "train loss:2.295414274382897\n",
      "train loss:2.292348446524308\n",
      "train loss:2.2946420654247035\n",
      "train loss:2.3058734247151467\n",
      "train loss:2.299740060575887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29741668799625\n",
      "train loss:2.3056318944544305\n",
      "train loss:2.309030738599691\n",
      "train loss:2.296838619232961\n",
      "train loss:2.2959224537190015\n",
      "train loss:2.3108946478412418\n",
      "train loss:2.3099772340797733\n",
      "train loss:2.2964847968653004\n",
      "train loss:2.3016694755571496\n",
      "train loss:2.299818941465567\n",
      "train loss:2.311919615384601\n",
      "train loss:2.2938808867288643\n",
      "train loss:2.299326626057366\n",
      "train loss:2.304464417001902\n",
      "train loss:2.307655038313868\n",
      "train loss:2.2960255000591987\n",
      "train loss:2.294292401167364\n",
      "train loss:2.2948429383508526\n",
      "train loss:2.2959194280193547\n",
      "train loss:2.3005937738867055\n",
      "train loss:2.2909177257907425\n",
      "train loss:2.3061204151061956\n",
      "train loss:2.2965887377406844\n",
      "train loss:2.2932195932148454\n",
      "train loss:2.302002930915626\n",
      "train loss:2.300470682214921\n",
      "train loss:2.295347551220975\n",
      "train loss:2.299698641987451\n",
      "train loss:2.300473496484099\n",
      "train loss:2.3011107915425946\n",
      "train loss:2.300500079071271\n",
      "train loss:2.3022519308732665\n",
      "train loss:2.3019928043155877\n",
      "train loss:2.295941030044808\n",
      "train loss:2.294924419395614\n",
      "train loss:2.307076864580435\n",
      "train loss:2.302509987786928\n",
      "train loss:2.305334380432417\n",
      "train loss:2.300818775414706\n",
      "train loss:2.305383305264862\n",
      "train loss:2.2916470885585003\n",
      "train loss:2.298634956736282\n",
      "train loss:2.3041371288702743\n",
      "train loss:2.296017489853712\n",
      "train loss:2.302718411489248\n",
      "train loss:2.2924945926262064\n",
      "train loss:2.3044597028394254\n",
      "train loss:2.3096647361207565\n",
      "train loss:2.3015875572836997\n",
      "train loss:2.3062110146320123\n",
      "train loss:2.2989616497705474\n",
      "train loss:2.2943615333910836\n",
      "train loss:2.3058544270898937\n",
      "train loss:2.2977161433599442\n",
      "train loss:2.2894766307519983\n",
      "train loss:2.294012957747948\n",
      "train loss:2.2964575243373218\n",
      "train loss:2.301205746662885\n",
      "=== epoch:120, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.298485201417275\n",
      "train loss:2.2937339338205573\n",
      "train loss:2.287208517114271\n",
      "train loss:2.298524899617253\n",
      "train loss:2.302479226521797\n",
      "train loss:2.2936781080924082\n",
      "train loss:2.2990353888462884\n",
      "train loss:2.3009279009912142\n",
      "train loss:2.303756612787784\n",
      "train loss:2.306588377733532\n",
      "train loss:2.3030971275421304\n",
      "train loss:2.3051128861042125\n",
      "train loss:2.309481641780112\n",
      "train loss:2.3048411499131007\n",
      "train loss:2.3038524831465494\n",
      "train loss:2.2967076936725137\n",
      "train loss:2.2967001467570833\n",
      "train loss:2.2952805903906706\n",
      "train loss:2.298722203591746\n",
      "train loss:2.306717342974203\n",
      "train loss:2.298151915078876\n",
      "train loss:2.3017953369804895\n",
      "train loss:2.298673933475772\n",
      "train loss:2.3101481944877293\n",
      "train loss:2.298005427997152\n",
      "train loss:2.3072163407985107\n",
      "train loss:2.3009387973827784\n",
      "train loss:2.3039023625530355\n",
      "train loss:2.3108402701369073\n",
      "train loss:2.3037651939645487\n",
      "train loss:2.309579690634661\n",
      "train loss:2.3002702136130866\n",
      "train loss:2.302312800388212\n",
      "train loss:2.300655755125003\n",
      "train loss:2.295623333573134\n",
      "train loss:2.303812829260617\n",
      "train loss:2.3036004671179873\n",
      "train loss:2.302770041132083\n",
      "train loss:2.3015936459568245\n",
      "train loss:2.2995973751634087\n",
      "train loss:2.3036524257669706\n",
      "train loss:2.297524813048379\n",
      "train loss:2.2943837798845506\n",
      "train loss:2.2939002435234777\n",
      "train loss:2.302142687879728\n",
      "train loss:2.2984775053334077\n",
      "train loss:2.2988390627523083\n",
      "train loss:2.2928716840046715\n",
      "train loss:2.2988696488660376\n",
      "train loss:2.3018808880145394\n",
      "train loss:2.2975075666052054\n",
      "train loss:2.298926309074661\n",
      "train loss:2.3088981678856575\n",
      "train loss:2.291107597733176\n",
      "train loss:2.2962185671142077\n",
      "train loss:2.3004993236639977\n",
      "train loss:2.302406128594723\n",
      "train loss:2.3008571083176106\n",
      "train loss:2.300499200025884\n",
      "train loss:2.3063388423228943\n",
      "train loss:2.2953375111481673\n",
      "train loss:2.290503885033217\n",
      "train loss:2.296164707676717\n",
      "train loss:2.299893443882924\n",
      "train loss:2.303874977935152\n",
      "train loss:2.303232369699887\n",
      "train loss:2.3124830905767086\n",
      "train loss:2.296181653761374\n",
      "train loss:2.308224451173992\n",
      "train loss:2.2978618675898934\n",
      "train loss:2.300792097241685\n",
      "train loss:2.3106030176068604\n",
      "train loss:2.3056829280074416\n",
      "train loss:2.30399689903816\n",
      "train loss:2.300613453828888\n",
      "train loss:2.2944189182345838\n",
      "train loss:2.3023561412064555\n",
      "train loss:2.309360853136707\n",
      "train loss:2.292628046504726\n",
      "train loss:2.3060788267411834\n",
      "train loss:2.3006690302886272\n",
      "train loss:2.3066594317629474\n",
      "train loss:2.307282565231388\n",
      "train loss:2.300253943658699\n",
      "train loss:2.306021950803799\n",
      "train loss:2.3087757104034172\n",
      "train loss:2.301527045901335\n",
      "train loss:2.293348048837756\n",
      "train loss:2.3033832282252273\n",
      "train loss:2.3007199541632146\n",
      "train loss:2.298424842461506\n",
      "train loss:2.2987348526251963\n",
      "train loss:2.303651064360136\n",
      "train loss:2.3024182417876164\n",
      "train loss:2.3038291150291146\n",
      "train loss:2.299638134089672\n",
      "train loss:2.3037726775189333\n",
      "train loss:2.3041690657080585\n",
      "train loss:2.2850878415652756\n",
      "train loss:2.305460523314986\n",
      "train loss:2.2964527570240083\n",
      "train loss:2.295065191786321\n",
      "train loss:2.309365710506554\n",
      "train loss:2.3061181910718966\n",
      "train loss:2.298533548649749\n",
      "train loss:2.296777223294644\n",
      "train loss:2.305708715244449\n",
      "train loss:2.3006166701262143\n",
      "train loss:2.2975619031304952\n",
      "train loss:2.2901513199434986\n",
      "train loss:2.3039373189404793\n",
      "train loss:2.2976682967263145\n",
      "train loss:2.305320227367566\n",
      "train loss:2.2952598468204832\n",
      "train loss:2.306988596984058\n",
      "train loss:2.3057136447363074\n",
      "train loss:2.3073111236574304\n",
      "train loss:2.30121937281479\n",
      "train loss:2.2959479118175627\n",
      "train loss:2.3002477243423303\n",
      "train loss:2.3009478522690134\n",
      "train loss:2.2996485003048903\n",
      "train loss:2.2987588071311817\n",
      "train loss:2.3021207442926768\n",
      "train loss:2.3008847603855185\n",
      "train loss:2.3026957889876867\n",
      "train loss:2.298997978697144\n",
      "train loss:2.3062126250517005\n",
      "train loss:2.30557558782357\n",
      "train loss:2.3039494809399423\n",
      "train loss:2.2997639482589576\n",
      "train loss:2.298829344608235\n",
      "train loss:2.305931434433641\n",
      "train loss:2.302012194235716\n",
      "train loss:2.296978777221574\n",
      "train loss:2.3091228342268133\n",
      "train loss:2.2990479414912444\n",
      "train loss:2.2993281341138716\n",
      "train loss:2.2987883488138183\n",
      "train loss:2.296174962264327\n",
      "train loss:2.3061415681263324\n",
      "train loss:2.300048950407129\n",
      "train loss:2.303083427464746\n",
      "train loss:2.2972090310086033\n",
      "train loss:2.3034773534005804\n",
      "train loss:2.3057431437032543\n",
      "train loss:2.2935904728867613\n",
      "train loss:2.297713968711045\n",
      "train loss:2.296576651124437\n",
      "train loss:2.302785887295601\n",
      "train loss:2.302988852712524\n",
      "train loss:2.306282678622936\n",
      "train loss:2.2938298760565345\n",
      "train loss:2.2925957137852633\n",
      "train loss:2.3075410871730972\n",
      "train loss:2.3038054797123944\n",
      "train loss:2.301766251922253\n",
      "train loss:2.306547766486309\n",
      "train loss:2.2991970325087467\n",
      "train loss:2.3049514938826725\n",
      "train loss:2.317069711559232\n",
      "train loss:2.298403857139325\n",
      "train loss:2.294824312574665\n",
      "train loss:2.3054781687895307\n",
      "train loss:2.2981691150615804\n",
      "train loss:2.2985548507321916\n",
      "train loss:2.305171594242044\n",
      "train loss:2.3017070466092706\n",
      "train loss:2.304908569704527\n",
      "train loss:2.303463069627884\n",
      "train loss:2.305252891728524\n",
      "train loss:2.2938323974823205\n",
      "train loss:2.303791147061322\n",
      "train loss:2.2977810482134564\n",
      "train loss:2.3003636329906447\n",
      "train loss:2.3071247714648413\n",
      "train loss:2.2946353455624515\n",
      "train loss:2.2958299173125636\n",
      "train loss:2.3028913395359005\n",
      "train loss:2.3036631106517\n",
      "train loss:2.2956025331203906\n",
      "train loss:2.290433089819001\n",
      "train loss:2.295087902146543\n",
      "train loss:2.2999972250652245\n",
      "train loss:2.306455989854972\n",
      "train loss:2.2995937577863854\n",
      "train loss:2.301307966714559\n",
      "train loss:2.301945902938938\n",
      "train loss:2.302633291324944\n",
      "train loss:2.3109638419616294\n",
      "train loss:2.3022309363081757\n",
      "train loss:2.299427419529786\n",
      "train loss:2.313640181278642\n",
      "train loss:2.299022165744907\n",
      "train loss:2.301739914298403\n",
      "train loss:2.305307971763602\n",
      "train loss:2.296070407961117\n",
      "train loss:2.302482029089752\n",
      "train loss:2.305009413836122\n",
      "train loss:2.299410067616872\n",
      "=== epoch:121, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2941435706777376\n",
      "train loss:2.300118814733752\n",
      "train loss:2.302817166498859\n",
      "train loss:2.3002550657263714\n",
      "train loss:2.3072367348720393\n",
      "train loss:2.3017736797784174\n",
      "train loss:2.3000332398696304\n",
      "train loss:2.303580941942948\n",
      "train loss:2.2989629255888033\n",
      "train loss:2.3071887730389875\n",
      "train loss:2.295370537695144\n",
      "train loss:2.3081385519785753\n",
      "train loss:2.298419031871247\n",
      "train loss:2.2980871647492913\n",
      "train loss:2.3062765760321833\n",
      "train loss:2.3018632567826525\n",
      "train loss:2.285771904003714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3061838805393395\n",
      "train loss:2.3092649851545484\n",
      "train loss:2.3003343515383317\n",
      "train loss:2.2984906195850754\n",
      "train loss:2.2938202573710442\n",
      "train loss:2.307641804176507\n",
      "train loss:2.309844528469409\n",
      "train loss:2.3053692045891334\n",
      "train loss:2.300963939005539\n",
      "train loss:2.299010001407451\n",
      "train loss:2.2967778386960487\n",
      "train loss:2.312208667053363\n",
      "train loss:2.3025722711461354\n",
      "train loss:2.298420049487886\n",
      "train loss:2.2994365477638317\n",
      "train loss:2.306092643145948\n",
      "train loss:2.2994358380289968\n",
      "train loss:2.2939172211613914\n",
      "train loss:2.2943353194396487\n",
      "train loss:2.301435838105395\n",
      "train loss:2.2971758105738123\n",
      "train loss:2.305080848339786\n",
      "train loss:2.293104439051205\n",
      "train loss:2.3056387917981054\n",
      "train loss:2.29439279518719\n",
      "train loss:2.302342776852561\n",
      "train loss:2.2988607121809426\n",
      "train loss:2.3095540700586747\n",
      "train loss:2.299083359668123\n",
      "train loss:2.2960009717468526\n",
      "train loss:2.2973794484704957\n",
      "train loss:2.306774104282809\n",
      "train loss:2.301048298286711\n",
      "train loss:2.297424277601075\n",
      "train loss:2.3138485859582496\n",
      "train loss:2.3022745372902493\n",
      "train loss:2.2975878889571044\n",
      "train loss:2.3129366867654784\n",
      "train loss:2.3025200074291083\n",
      "train loss:2.2971043683807095\n",
      "train loss:2.3045578400337523\n",
      "train loss:2.2838281729761296\n",
      "train loss:2.3005390259219443\n",
      "train loss:2.293440311738186\n",
      "train loss:2.30616442859533\n",
      "train loss:2.304163336519461\n",
      "train loss:2.3030452476359806\n",
      "train loss:2.3086302617735908\n",
      "train loss:2.29696079356076\n",
      "train loss:2.300863507206441\n",
      "train loss:2.308134549665408\n",
      "train loss:2.291820908157117\n",
      "train loss:2.302876590827113\n",
      "train loss:2.3070739790140835\n",
      "train loss:2.294907517058878\n",
      "train loss:2.294944916112105\n",
      "train loss:2.297218193509632\n",
      "train loss:2.2933567314756123\n",
      "train loss:2.2979137991138545\n",
      "train loss:2.2952728721550426\n",
      "train loss:2.305561572182026\n",
      "train loss:2.3051155122197704\n",
      "train loss:2.304754255185268\n",
      "train loss:2.3060102359594468\n",
      "train loss:2.301137587389611\n",
      "train loss:2.2986478296004678\n",
      "train loss:2.303251012601139\n",
      "train loss:2.3037752589679545\n",
      "train loss:2.290247719217408\n",
      "train loss:2.287165962680842\n",
      "train loss:2.2991243923908264\n",
      "train loss:2.2973687444234674\n",
      "train loss:2.3049753168392275\n",
      "train loss:2.297617080400244\n",
      "train loss:2.2893896180993525\n",
      "train loss:2.292311798929645\n",
      "train loss:2.3027438764356503\n",
      "train loss:2.310305319441157\n",
      "train loss:2.310657504616752\n",
      "train loss:2.2979674723090504\n",
      "train loss:2.296585952150004\n",
      "train loss:2.302199637848481\n",
      "train loss:2.302917076716242\n",
      "train loss:2.3045076472995816\n",
      "train loss:2.298186248580262\n",
      "train loss:2.3045580674437396\n",
      "train loss:2.3078766216140445\n",
      "train loss:2.2955765109371544\n",
      "train loss:2.306724036540023\n",
      "train loss:2.295170482951692\n",
      "train loss:2.304620810297933\n",
      "train loss:2.3024326979953487\n",
      "train loss:2.298658186793354\n",
      "train loss:2.3026911061969346\n",
      "train loss:2.309349477169165\n",
      "train loss:2.309038392611345\n",
      "train loss:2.293615948864755\n",
      "train loss:2.29920917547318\n",
      "train loss:2.2994744742063107\n",
      "train loss:2.296892642845118\n",
      "train loss:2.303908336918655\n",
      "train loss:2.2808520883361205\n",
      "train loss:2.304909597591383\n",
      "train loss:2.3097262543890014\n",
      "train loss:2.2960218353402557\n",
      "train loss:2.2997270906452036\n",
      "train loss:2.305658419795445\n",
      "train loss:2.3054669991646444\n",
      "train loss:2.2980382063369635\n",
      "train loss:2.2897873740842014\n",
      "train loss:2.298332435086081\n",
      "train loss:2.2925187945318832\n",
      "train loss:2.3002976321777138\n",
      "train loss:2.3003454072891403\n",
      "train loss:2.3010526394561728\n",
      "train loss:2.3030215345311613\n",
      "train loss:2.3026373834887788\n",
      "train loss:2.2979992630829056\n",
      "train loss:2.301010394726366\n",
      "train loss:2.2990037567167687\n",
      "train loss:2.292405079974961\n",
      "train loss:2.2930596654870623\n",
      "train loss:2.303721830895851\n",
      "train loss:2.3078070388497793\n",
      "train loss:2.2976016149244547\n",
      "train loss:2.3047351602106114\n",
      "train loss:2.2918441584693956\n",
      "train loss:2.305479869587365\n",
      "train loss:2.2989623816419513\n",
      "train loss:2.308033656595037\n",
      "train loss:2.3113630375026064\n",
      "train loss:2.2987018494202642\n",
      "train loss:2.3029015649432103\n",
      "train loss:2.2975659767493197\n",
      "train loss:2.292642813366925\n",
      "train loss:2.3018240328420543\n",
      "train loss:2.305630778970609\n",
      "train loss:2.2867092723491464\n",
      "train loss:2.3013862048929097\n",
      "train loss:2.3007283840078374\n",
      "train loss:2.307135746291177\n",
      "train loss:2.305442026820702\n",
      "train loss:2.311497015785743\n",
      "train loss:2.2988184144028883\n",
      "train loss:2.3089525947155054\n",
      "train loss:2.3088043262886493\n",
      "train loss:2.293652045047029\n",
      "train loss:2.309076110218052\n",
      "train loss:2.3011970905043024\n",
      "train loss:2.3019855145475625\n",
      "train loss:2.297924256304149\n",
      "train loss:2.2995854395365964\n",
      "train loss:2.3001995122875827\n",
      "train loss:2.307012129055668\n",
      "train loss:2.2934058523618877\n",
      "train loss:2.302057776122523\n",
      "train loss:2.30264550127488\n",
      "train loss:2.298508600506988\n",
      "train loss:2.291538578324002\n",
      "train loss:2.301037437431234\n",
      "train loss:2.2963300270113316\n",
      "train loss:2.303404298790177\n",
      "train loss:2.300357780971193\n",
      "train loss:2.303167898509769\n",
      "train loss:2.299469133955124\n",
      "train loss:2.2961663632459604\n",
      "train loss:2.29903603802918\n",
      "train loss:2.2967023230505546\n",
      "train loss:2.300179717047884\n",
      "train loss:2.302667762555624\n",
      "train loss:2.3011138484401488\n",
      "train loss:2.3036015998176973\n",
      "train loss:2.3079478692384665\n",
      "train loss:2.3146732080721084\n",
      "train loss:2.2982218017527027\n",
      "train loss:2.307119838128656\n",
      "train loss:2.297822397057779\n",
      "train loss:2.299012103316489\n",
      "train loss:2.2985669598059313\n",
      "train loss:2.3057135885241675\n",
      "train loss:2.296169126985947\n",
      "train loss:2.296473894479243\n",
      "train loss:2.2998190018093436\n",
      "=== epoch:122, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3043513380476526\n",
      "train loss:2.3102504484873947\n",
      "train loss:2.2881369089339954\n",
      "train loss:2.304790043063874\n",
      "train loss:2.2999353800874243\n",
      "train loss:2.295977605378065\n",
      "train loss:2.2986748012891933\n",
      "train loss:2.303144949076899\n",
      "train loss:2.2987897996828286\n",
      "train loss:2.3024551696309965\n",
      "train loss:2.3054413784285392\n",
      "train loss:2.300860087789048\n",
      "train loss:2.297212992601925\n",
      "train loss:2.2986902252630914\n",
      "train loss:2.303403419638741\n",
      "train loss:2.302636291734589\n",
      "train loss:2.3079850410067557\n",
      "train loss:2.3033428348646705\n",
      "train loss:2.301790643570822\n",
      "train loss:2.2995080526883496\n",
      "train loss:2.3014520243090786\n",
      "train loss:2.2971216284902725\n",
      "train loss:2.297748579012566\n",
      "train loss:2.299058656966435\n",
      "train loss:2.3028374341577464\n",
      "train loss:2.3035956633300287\n",
      "train loss:2.3047307645645914\n",
      "train loss:2.2970205971952202\n",
      "train loss:2.304431680578745\n",
      "train loss:2.30640177506397\n",
      "train loss:2.3005374479828014\n",
      "train loss:2.3066017233151013\n",
      "train loss:2.300220459052824\n",
      "train loss:2.2893717930812802\n",
      "train loss:2.2986148596626768\n",
      "train loss:2.2994172592592896\n",
      "train loss:2.293550668466179\n",
      "train loss:2.30112791798831\n",
      "train loss:2.3165109941314186\n",
      "train loss:2.3026024540537082\n",
      "train loss:2.292408852507279\n",
      "train loss:2.304235675743324\n",
      "train loss:2.2943162503079124\n",
      "train loss:2.305245718913513\n",
      "train loss:2.289760627830978\n",
      "train loss:2.2995851745831133\n",
      "train loss:2.307886599892035\n",
      "train loss:2.2982586540834404\n",
      "train loss:2.3061742916921126\n",
      "train loss:2.3060463044966824\n",
      "train loss:2.3024029942110924\n",
      "train loss:2.29239067231706\n",
      "train loss:2.307453648024665\n",
      "train loss:2.304938719465002\n",
      "train loss:2.3046057643711806\n",
      "train loss:2.3042891199296873\n",
      "train loss:2.3022352779617306\n",
      "train loss:2.2971035485013744\n",
      "train loss:2.3063012992863032\n",
      "train loss:2.298055666012643\n",
      "train loss:2.294285493265755\n",
      "train loss:2.3022666858441463\n",
      "train loss:2.2943783870891648\n",
      "train loss:2.2944126683902915\n",
      "train loss:2.298992118049553\n",
      "train loss:2.310174759214051\n",
      "train loss:2.307168107110082\n",
      "train loss:2.3066553364379234\n",
      "train loss:2.303167193167754\n",
      "train loss:2.2999362378846846\n",
      "train loss:2.3051535407964527\n",
      "train loss:2.2973375062706256\n",
      "train loss:2.299878487453989\n",
      "train loss:2.300562419501003\n",
      "train loss:2.2951025171069435\n",
      "train loss:2.2961063328100324\n",
      "train loss:2.3014560035072256\n",
      "train loss:2.303495105920955\n",
      "train loss:2.2988057166656017\n",
      "train loss:2.2932854969881973\n",
      "train loss:2.290514999425449\n",
      "train loss:2.2934338331138346\n",
      "train loss:2.3078052970150806\n",
      "train loss:2.305287092460837\n",
      "train loss:2.3071690971731984\n",
      "train loss:2.2918871907939833\n",
      "train loss:2.3027051026051266\n",
      "train loss:2.3043093454573667\n",
      "train loss:2.299595447525886\n",
      "train loss:2.2984910548702127\n",
      "train loss:2.3009803792183883\n",
      "train loss:2.302335706792875\n",
      "train loss:2.2958078702314437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2922055554799057\n",
      "train loss:2.294838485243954\n",
      "train loss:2.291360142364713\n",
      "train loss:2.2915150965336006\n",
      "train loss:2.2965498640492674\n",
      "train loss:2.306660257596867\n",
      "train loss:2.305397172007105\n",
      "train loss:2.3049489575726096\n",
      "train loss:2.298046686760845\n",
      "train loss:2.2971629469887196\n",
      "train loss:2.307874680019188\n",
      "train loss:2.3093241953502512\n",
      "train loss:2.3030142640116242\n",
      "train loss:2.2984519495736353\n",
      "train loss:2.3078987045867643\n",
      "train loss:2.300186383892655\n",
      "train loss:2.3005773575632547\n",
      "train loss:2.3037809610728006\n",
      "train loss:2.300184010094178\n",
      "train loss:2.3044127583971434\n",
      "train loss:2.2933839795529956\n",
      "train loss:2.293943714225955\n",
      "train loss:2.2935875527076712\n",
      "train loss:2.2965189569871454\n",
      "train loss:2.296496307673044\n",
      "train loss:2.301921222953424\n",
      "train loss:2.3136973082124355\n",
      "train loss:2.2972453401557145\n",
      "train loss:2.302287571472344\n",
      "train loss:2.289692487261392\n",
      "train loss:2.2906638422246024\n",
      "train loss:2.296495753067776\n",
      "train loss:2.307133312023787\n",
      "train loss:2.295971406446405\n",
      "train loss:2.3113279257671198\n",
      "train loss:2.3036985939406143\n",
      "train loss:2.294489655994988\n",
      "train loss:2.3044006928829135\n",
      "train loss:2.314914267334945\n",
      "train loss:2.3024648972051205\n",
      "train loss:2.297895354350017\n",
      "train loss:2.3021886975627877\n",
      "train loss:2.302517711524966\n",
      "train loss:2.309811330588389\n",
      "train loss:2.299448916395999\n",
      "train loss:2.310461418876952\n",
      "train loss:2.30185845151167\n",
      "train loss:2.290852543482365\n",
      "train loss:2.299397662232658\n",
      "train loss:2.2975733054683714\n",
      "train loss:2.3048835664837277\n",
      "train loss:2.300394387191524\n",
      "train loss:2.2972064893882638\n",
      "train loss:2.303557101576423\n",
      "train loss:2.3084604894859186\n",
      "train loss:2.2944495143571517\n",
      "train loss:2.3015706640890694\n",
      "train loss:2.304063952660088\n",
      "train loss:2.30691716225669\n",
      "train loss:2.3019190674297327\n",
      "train loss:2.3078201473422597\n",
      "train loss:2.2872836531601934\n",
      "train loss:2.294767442236816\n",
      "train loss:2.3063768128779656\n",
      "train loss:2.3024542190401656\n",
      "train loss:2.293077153879042\n",
      "train loss:2.305459036899154\n",
      "train loss:2.292114522151034\n",
      "train loss:2.285639138173149\n",
      "train loss:2.305107058274342\n",
      "train loss:2.305832805228469\n",
      "train loss:2.2991121965756762\n",
      "train loss:2.3013500172624144\n",
      "train loss:2.3021515823173178\n",
      "train loss:2.2989527946819033\n",
      "train loss:2.3057808374021986\n",
      "train loss:2.294173120580596\n",
      "train loss:2.310611019230829\n",
      "train loss:2.3044535510109685\n",
      "train loss:2.29809077460474\n",
      "train loss:2.310766740659218\n",
      "train loss:2.3019842372033996\n",
      "train loss:2.2996548481905092\n",
      "train loss:2.298171038277409\n",
      "train loss:2.304065936212993\n",
      "train loss:2.303110286602262\n",
      "train loss:2.3027923489022606\n",
      "train loss:2.2936601645282035\n",
      "train loss:2.30024167128517\n",
      "train loss:2.308422002998948\n",
      "train loss:2.3018176721556816\n",
      "train loss:2.295908264641351\n",
      "train loss:2.299803930725667\n",
      "train loss:2.310064529630001\n",
      "train loss:2.2955680560160627\n",
      "train loss:2.300343117644079\n",
      "train loss:2.304046435505107\n",
      "train loss:2.3083339435926526\n",
      "train loss:2.3047444062208546\n",
      "train loss:2.2950015877381498\n",
      "train loss:2.2987024195569754\n",
      "train loss:2.304244934353934\n",
      "train loss:2.3048759036790285\n",
      "train loss:2.3015363504340036\n",
      "train loss:2.3031901280157263\n",
      "train loss:2.2993693890475497\n",
      "train loss:2.296001799609376\n",
      "=== epoch:123, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.304677371753224\n",
      "train loss:2.2922616817051926\n",
      "train loss:2.293746439986924\n",
      "train loss:2.2975721439430252\n",
      "train loss:2.298459945395635\n",
      "train loss:2.3014875197609053\n",
      "train loss:2.2933377208876915\n",
      "train loss:2.2948866277971205\n",
      "train loss:2.3055084125029475\n",
      "train loss:2.290144062704321\n",
      "train loss:2.3035671522166856\n",
      "train loss:2.302616288182966\n",
      "train loss:2.306943806501665\n",
      "train loss:2.3014974689435337\n",
      "train loss:2.3064965411223173\n",
      "train loss:2.3043074630122784\n",
      "train loss:2.304660545811037\n",
      "train loss:2.298594227711599\n",
      "train loss:2.300525351500531\n",
      "train loss:2.294919231323855\n",
      "train loss:2.3022874609995947\n",
      "train loss:2.294080959994109\n",
      "train loss:2.3072938822421225\n",
      "train loss:2.300457785311629\n",
      "train loss:2.310243201457547\n",
      "train loss:2.3010481124043776\n",
      "train loss:2.299541826184409\n",
      "train loss:2.299628302064183\n",
      "train loss:2.2960607532662225\n",
      "train loss:2.2973318963009244\n",
      "train loss:2.3053057046326857\n",
      "train loss:2.302979971336286\n",
      "train loss:2.2950473621864154\n",
      "train loss:2.302515491944845\n",
      "train loss:2.29965082255986\n",
      "train loss:2.2999274182724903\n",
      "train loss:2.3123935198103123\n",
      "train loss:2.3082842022963286\n",
      "train loss:2.300182236571337\n",
      "train loss:2.298885731518743\n",
      "train loss:2.3072642957816303\n",
      "train loss:2.2966485776758736\n",
      "train loss:2.308963699440557\n",
      "train loss:2.3036227590755574\n",
      "train loss:2.2953925929669943\n",
      "train loss:2.2973817977798867\n",
      "train loss:2.290792146723008\n",
      "train loss:2.302246354720366\n",
      "train loss:2.3013819664974973\n",
      "train loss:2.3088853633315862\n",
      "train loss:2.2993341159470457\n",
      "train loss:2.302947334178611\n",
      "train loss:2.3147192307224085\n",
      "train loss:2.30873227111177\n",
      "train loss:2.3041567262915437\n",
      "train loss:2.3023163712667283\n",
      "train loss:2.291121845376621\n",
      "train loss:2.302425152295115\n",
      "train loss:2.2952121502803586\n",
      "train loss:2.3025972009580067\n",
      "train loss:2.3013092006619424\n",
      "train loss:2.3076419997980384\n",
      "train loss:2.291208245657596\n",
      "train loss:2.2961808361241074\n",
      "train loss:2.3047864027077756\n",
      "train loss:2.304772394780969\n",
      "train loss:2.2873489122384085\n",
      "train loss:2.299050291762177\n",
      "train loss:2.2950004584696346\n",
      "train loss:2.30562046284262\n",
      "train loss:2.3026393445927016\n",
      "train loss:2.300372413580912\n",
      "train loss:2.2979515299931004\n",
      "train loss:2.2962468897258734\n",
      "train loss:2.2956478325910146\n",
      "train loss:2.3006947493807948\n",
      "train loss:2.3052440706652626\n",
      "train loss:2.3021862261186334\n",
      "train loss:2.300180193308883\n",
      "train loss:2.305143224285418\n",
      "train loss:2.3046739735728017\n",
      "train loss:2.2938565705459273\n",
      "train loss:2.309922075293589\n",
      "train loss:2.292569480893005\n",
      "train loss:2.301400480851772\n",
      "train loss:2.3068030248495117\n",
      "train loss:2.298217433352848\n",
      "train loss:2.292324388444038\n",
      "train loss:2.307907663733798\n",
      "train loss:2.296930880294617\n",
      "train loss:2.3043523549840974\n",
      "train loss:2.3088669550366347\n",
      "train loss:2.305978803713466\n",
      "train loss:2.295724944588917\n",
      "train loss:2.305530590490783\n",
      "train loss:2.2862130976374835\n",
      "train loss:2.3015801133169793\n",
      "train loss:2.310111536332276\n",
      "train loss:2.3010601939859803\n",
      "train loss:2.308541662186773\n",
      "train loss:2.297852211586109\n",
      "train loss:2.3030023862817717\n",
      "train loss:2.3026810170065337\n",
      "train loss:2.301472952095873\n",
      "train loss:2.3020370723705814\n",
      "train loss:2.2964411844382284\n",
      "train loss:2.3097595871608294\n",
      "train loss:2.2928140558399974\n",
      "train loss:2.3022601912262823\n",
      "train loss:2.2992077412451284\n",
      "train loss:2.3041982983587213\n",
      "train loss:2.3006877988460803\n",
      "train loss:2.3018427960888386\n",
      "train loss:2.296437632469425\n",
      "train loss:2.2981089719787375\n",
      "train loss:2.3013948818288528\n",
      "train loss:2.3086962844394368\n",
      "train loss:2.297230749804386\n",
      "train loss:2.298300122391205\n",
      "train loss:2.2944596990465644\n",
      "train loss:2.3022475045471187\n",
      "train loss:2.29975553834345\n",
      "train loss:2.2943722431838647\n",
      "train loss:2.300093573730241\n",
      "train loss:2.3004804437401236\n",
      "train loss:2.292552209421649\n",
      "train loss:2.3033948931758785\n",
      "train loss:2.316352463527686\n",
      "train loss:2.301103253668062\n",
      "train loss:2.30011474059309\n",
      "train loss:2.305992945492082\n",
      "train loss:2.3032326264098386\n",
      "train loss:2.299236707655463\n",
      "train loss:2.3063557778175245\n",
      "train loss:2.3016733032931156\n",
      "train loss:2.307550547304178\n",
      "train loss:2.2922050186491987\n",
      "train loss:2.301169172369139\n",
      "train loss:2.2988805875105847\n",
      "train loss:2.3017648743338697\n",
      "train loss:2.309058734441583\n",
      "train loss:2.2842635575754766\n",
      "train loss:2.2982649047462083\n",
      "train loss:2.312906490588108\n",
      "train loss:2.3023798920347667\n",
      "train loss:2.3051086710976363\n",
      "train loss:2.3039659251647544\n",
      "train loss:2.301751917265293\n",
      "train loss:2.3030103613678756\n",
      "train loss:2.3052590100138697\n",
      "train loss:2.304357630333057\n",
      "train loss:2.307639036415374\n",
      "train loss:2.3023111886447056\n",
      "train loss:2.2889367048337546\n",
      "train loss:2.30325722434806\n",
      "train loss:2.2983436377590034\n",
      "train loss:2.299686391171757\n",
      "train loss:2.291710582738814\n",
      "train loss:2.2986138300164294\n",
      "train loss:2.2973226284202948\n",
      "train loss:2.2911146442457677\n",
      "train loss:2.3066362423540823\n",
      "train loss:2.29572351703235\n",
      "train loss:2.301757878559695\n",
      "train loss:2.3033971041136443\n",
      "train loss:2.2963839017067262\n",
      "train loss:2.3003937204648506\n",
      "train loss:2.306873211001424\n",
      "train loss:2.305968791482846\n",
      "train loss:2.3096507339726817\n",
      "train loss:2.298537930034636\n",
      "train loss:2.3091498109215545\n",
      "train loss:2.2923517784660166\n",
      "train loss:2.299259670381019\n",
      "train loss:2.292307421226626\n",
      "train loss:2.296743302405364\n",
      "train loss:2.313919900744648\n",
      "train loss:2.3042975044655365\n",
      "train loss:2.2913129632770586\n",
      "train loss:2.3088204466608397\n",
      "train loss:2.3092807380803007\n",
      "train loss:2.2996401407233527\n",
      "train loss:2.2979781931210472\n",
      "train loss:2.2990776114246345\n",
      "train loss:2.2987159669721215\n",
      "train loss:2.2949432862573724\n",
      "train loss:2.3026794235091486\n",
      "train loss:2.3007432489210493\n",
      "train loss:2.307060609404061\n",
      "train loss:2.287545582263228\n",
      "train loss:2.3007737086276414\n",
      "train loss:2.3025704305160692\n",
      "train loss:2.2987189205036493\n",
      "train loss:2.294767416812292\n",
      "train loss:2.3043924429764107\n",
      "train loss:2.3033111500204737\n",
      "train loss:2.3033262214270462\n",
      "train loss:2.296464811611877\n",
      "train loss:2.295842985798808\n",
      "train loss:2.297639940113959\n",
      "=== epoch:124, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2908337356880972\n",
      "train loss:2.299016093219638\n",
      "train loss:2.301846136182918\n",
      "train loss:2.3017978998764397\n",
      "train loss:2.3014723044612957\n",
      "train loss:2.301311468687297\n",
      "train loss:2.2962588748559085\n",
      "train loss:2.302777313197555\n",
      "train loss:2.313661072742176\n",
      "train loss:2.300009975886794\n",
      "train loss:2.3009578282690724\n",
      "train loss:2.306854257602877\n",
      "train loss:2.3042624364231967\n",
      "train loss:2.2990473423369795\n",
      "train loss:2.292177862037839\n",
      "train loss:2.302250454643769\n",
      "train loss:2.2978217560795042\n",
      "train loss:2.2976179235705603\n",
      "train loss:2.30878379989264\n",
      "train loss:2.3029695895157554\n",
      "train loss:2.3084945632144573\n",
      "train loss:2.2995599413361267\n",
      "train loss:2.314934624774289\n",
      "train loss:2.300813639180607\n",
      "train loss:2.3032364606878972\n",
      "train loss:2.3020136572058414\n",
      "train loss:2.295139966152374\n",
      "train loss:2.3056428788973333\n",
      "train loss:2.2929306486092007\n",
      "train loss:2.2938042712713713\n",
      "train loss:2.3062889587976088\n",
      "train loss:2.298624375766216\n",
      "train loss:2.304111028352265\n",
      "train loss:2.301167328103369\n",
      "train loss:2.2939051076175274\n",
      "train loss:2.2896678575649028\n",
      "train loss:2.2906018063165052\n",
      "train loss:2.303234359552914\n",
      "train loss:2.306840119096356\n",
      "train loss:2.302502315402896\n",
      "train loss:2.3086465550744815\n",
      "train loss:2.297652046446355\n",
      "train loss:2.295863990349968\n",
      "train loss:2.301271948924849\n",
      "train loss:2.306660152492604\n",
      "train loss:2.297547647474392\n",
      "train loss:2.2990994492631716\n",
      "train loss:2.2948496359020365\n",
      "train loss:2.2964680114502065\n",
      "train loss:2.3048656063355777\n",
      "train loss:2.2965998685773314\n",
      "train loss:2.2976770671158637\n",
      "train loss:2.2977651276824274\n",
      "train loss:2.3079807491393076\n",
      "train loss:2.298211748880656\n",
      "train loss:2.292214672315797\n",
      "train loss:2.2985322511487074\n",
      "train loss:2.2947140020132264\n",
      "train loss:2.301751915703891\n",
      "train loss:2.298993745471988\n",
      "train loss:2.2978676964258864\n",
      "train loss:2.3063799612679476\n",
      "train loss:2.3027525308841708\n",
      "train loss:2.2828862222801476\n",
      "train loss:2.3057859847524567\n",
      "train loss:2.3061195493017337\n",
      "train loss:2.301625554493269\n",
      "train loss:2.30430499751271\n",
      "train loss:2.2997861485886957\n",
      "train loss:2.296231335346212\n",
      "train loss:2.2884388159470874\n",
      "train loss:2.2855066098443437\n",
      "train loss:2.3122575298561814\n",
      "train loss:2.2859949582388324\n",
      "train loss:2.302780353770781\n",
      "train loss:2.2980231708055356\n",
      "train loss:2.3063539886314093\n",
      "train loss:2.2984058385167616\n",
      "train loss:2.2976039390413567\n",
      "train loss:2.3000451633154233\n",
      "train loss:2.2939691995891174\n",
      "train loss:2.2946602804344884\n",
      "train loss:2.307650443880623\n",
      "train loss:2.303606963114256\n",
      "train loss:2.2987382309088122\n",
      "train loss:2.3021956515989244\n",
      "train loss:2.3064288684298795\n",
      "train loss:2.2981794131035946\n",
      "train loss:2.2983738803805145\n",
      "train loss:2.30284895089113\n",
      "train loss:2.2947961764302884\n",
      "train loss:2.300131558812041\n",
      "train loss:2.3069101268018133\n",
      "train loss:2.3119153105299715\n",
      "train loss:2.3053915215698715\n",
      "train loss:2.299377898991871\n",
      "train loss:2.2999325939185407\n",
      "train loss:2.30199035033516\n",
      "train loss:2.3072805592541465\n",
      "train loss:2.2952839772296274\n",
      "train loss:2.3121993917069283\n",
      "train loss:2.296346026466073\n",
      "train loss:2.3014560472743497\n",
      "train loss:2.297584644236723\n",
      "train loss:2.3046536715355956\n",
      "train loss:2.3070698516566464\n",
      "train loss:2.306043328184206\n",
      "train loss:2.303016458665066\n",
      "train loss:2.3038693753448447\n",
      "train loss:2.3057721464216883\n",
      "train loss:2.3053867684029936\n",
      "train loss:2.3035420127824118\n",
      "train loss:2.2997909732072204\n",
      "train loss:2.303887510610446\n",
      "train loss:2.296615333618819\n",
      "train loss:2.3004578376971496\n",
      "train loss:2.302201608293595\n",
      "train loss:2.299037776230761\n",
      "train loss:2.3007455200764126\n",
      "train loss:2.296704474988647\n",
      "train loss:2.30222502658309\n",
      "train loss:2.305639655921858\n",
      "train loss:2.2983761619464493\n",
      "train loss:2.305343508313737\n",
      "train loss:2.3029347068905475\n",
      "train loss:2.294701404306215\n",
      "train loss:2.2980957312813057\n",
      "train loss:2.301325811319166\n",
      "train loss:2.301198851218823\n",
      "train loss:2.3017221656140685\n",
      "train loss:2.3031029892133157\n",
      "train loss:2.302638277528434\n",
      "train loss:2.298762551308164\n",
      "train loss:2.3040821805363674\n",
      "train loss:2.3069703940419903\n",
      "train loss:2.3117737292591176\n",
      "train loss:2.294334015125827\n",
      "train loss:2.3066512587758616\n",
      "train loss:2.306132713502107\n",
      "train loss:2.290695945298816\n",
      "train loss:2.294507410163364\n",
      "train loss:2.3004588193684063\n",
      "train loss:2.3095011218115697\n",
      "train loss:2.30075303659117\n",
      "train loss:2.3000917576920545\n",
      "train loss:2.300443273645592\n",
      "train loss:2.2969101531340166\n",
      "train loss:2.2930614136829703\n",
      "train loss:2.296033436649076\n",
      "train loss:2.3015517555286142\n",
      "train loss:2.2975709773885185\n",
      "train loss:2.293459696719905\n",
      "train loss:2.2910553412492645\n",
      "train loss:2.30287869782533\n",
      "train loss:2.3017195941262774\n",
      "train loss:2.2964840222090013\n",
      "train loss:2.291859594296338\n",
      "train loss:2.29905290817496\n",
      "train loss:2.289732258787982\n",
      "train loss:2.3024704004456726\n",
      "train loss:2.295835112761574\n",
      "train loss:2.303291184736402\n",
      "train loss:2.2928934700379813\n",
      "train loss:2.297811171537509\n",
      "train loss:2.3030754955287858\n",
      "train loss:2.3030987968851546\n",
      "train loss:2.295168955085769\n",
      "train loss:2.302803657395796\n",
      "train loss:2.3030257538579177\n",
      "train loss:2.3021079996206457\n",
      "train loss:2.302755325256238\n",
      "train loss:2.2962308412911647\n",
      "train loss:2.300910119958533\n",
      "train loss:2.307588951603666\n",
      "train loss:2.302568748776586\n",
      "train loss:2.298964607161513\n",
      "train loss:2.3016127327096143\n",
      "train loss:2.2990534119136523\n",
      "train loss:2.3010910408408742\n",
      "train loss:2.3058307452803075\n",
      "train loss:2.2967397858116034\n",
      "train loss:2.303235586245688\n",
      "train loss:2.305947511060603\n",
      "train loss:2.303020273299822\n",
      "train loss:2.3068845670911196\n",
      "train loss:2.306159278750414\n",
      "train loss:2.3028481568260357\n",
      "train loss:2.306730818461821\n",
      "train loss:2.3014497073112232\n",
      "train loss:2.297288254953013\n",
      "train loss:2.30090996248312\n",
      "train loss:2.302161528896837\n",
      "train loss:2.293712360739354\n",
      "train loss:2.3054194932318666\n",
      "train loss:2.3003887235184806\n",
      "train loss:2.28715440653208\n",
      "train loss:2.3062607266799575\n",
      "train loss:2.299557753130352\n",
      "train loss:2.3045605140072154\n",
      "train loss:2.3008319249756686\n",
      "=== epoch:125, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.301663162834487\n",
      "train loss:2.297206824879242\n",
      "train loss:2.301041935096139\n",
      "train loss:2.2981644107300907\n",
      "train loss:2.2983579495443127\n",
      "train loss:2.3022138744517298\n",
      "train loss:2.3062015789370784\n",
      "train loss:2.2949760514680437\n",
      "train loss:2.3034164167075377\n",
      "train loss:2.3128263166175267\n",
      "train loss:2.3008968700378882\n",
      "train loss:2.3069297764543415\n",
      "train loss:2.290385515917426\n",
      "train loss:2.3023205853343787\n",
      "train loss:2.306634242892488\n",
      "train loss:2.30541557006548\n",
      "train loss:2.3025234322251027\n",
      "train loss:2.3015830993440387\n",
      "train loss:2.3177993018941336\n",
      "train loss:2.297367101039369\n",
      "train loss:2.306124810310457\n",
      "train loss:2.2973759529102873\n",
      "train loss:2.3106714564220128\n",
      "train loss:2.3041649484208966\n",
      "train loss:2.3093190584554066\n",
      "train loss:2.2965698129973595\n",
      "train loss:2.3015526780967845\n",
      "train loss:2.3049326709213402\n",
      "train loss:2.2982509062064134\n",
      "train loss:2.3023021049462056\n",
      "train loss:2.296376272007504\n",
      "train loss:2.3122338137326137\n",
      "train loss:2.3077581201665294\n",
      "train loss:2.3030722445540803\n",
      "train loss:2.3050354635166967\n",
      "train loss:2.3050465518355634\n",
      "train loss:2.292959842297242\n",
      "train loss:2.305611336870173\n",
      "train loss:2.303372755354421\n",
      "train loss:2.299675441628327\n",
      "train loss:2.305281588390549\n",
      "train loss:2.3033038496390517\n",
      "train loss:2.3052824012472324\n",
      "train loss:2.2998247481148395\n",
      "train loss:2.301172226163523\n",
      "train loss:2.2989132878413088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.294047847325237\n",
      "train loss:2.2971117924898627\n",
      "train loss:2.299817349537094\n",
      "train loss:2.3075147793801807\n",
      "train loss:2.291591961168883\n",
      "train loss:2.3027699264841637\n",
      "train loss:2.300818176524295\n",
      "train loss:2.300333314066569\n",
      "train loss:2.2983466237903913\n",
      "train loss:2.2982832691849255\n",
      "train loss:2.299664832509383\n",
      "train loss:2.296427639842704\n",
      "train loss:2.2863663286313374\n",
      "train loss:2.294415571164382\n",
      "train loss:2.315552168768922\n",
      "train loss:2.3038254302630006\n",
      "train loss:2.302058538971901\n",
      "train loss:2.3006440822499856\n",
      "train loss:2.30632469964828\n",
      "train loss:2.3090818073848136\n",
      "train loss:2.3011021529479057\n",
      "train loss:2.3018798688687343\n",
      "train loss:2.308137644918698\n",
      "train loss:2.2954939750546393\n",
      "train loss:2.300267741318215\n",
      "train loss:2.303200062238888\n",
      "train loss:2.3027947297021734\n",
      "train loss:2.3038093388015985\n",
      "train loss:2.303601018719118\n",
      "train loss:2.3049070268538054\n",
      "train loss:2.307296000945094\n",
      "train loss:2.301966858372017\n",
      "train loss:2.292060567353783\n",
      "train loss:2.3037434804444463\n",
      "train loss:2.295658508441023\n",
      "train loss:2.3004963436703005\n",
      "train loss:2.2927891242474256\n",
      "train loss:2.307137589697352\n",
      "train loss:2.2972843338456905\n",
      "train loss:2.3031718595121236\n",
      "train loss:2.303470888334874\n",
      "train loss:2.2974787820029525\n",
      "train loss:2.3020805337346246\n",
      "train loss:2.30414692783633\n",
      "train loss:2.2933952307203405\n",
      "train loss:2.2941595237926227\n",
      "train loss:2.3108842420674556\n",
      "train loss:2.3058317606980037\n",
      "train loss:2.3008963360873644\n",
      "train loss:2.3115636804277617\n",
      "train loss:2.303015767760852\n",
      "train loss:2.3053128488913517\n",
      "train loss:2.300478381481644\n",
      "train loss:2.303117651257942\n",
      "train loss:2.298794305996796\n",
      "train loss:2.300099567206543\n",
      "train loss:2.299939263910635\n",
      "train loss:2.296572517963987\n",
      "train loss:2.2981702526052397\n",
      "train loss:2.301093628529626\n",
      "train loss:2.3011419273807445\n",
      "train loss:2.3049200374486682\n",
      "train loss:2.300449509849518\n",
      "train loss:2.308714012614587\n",
      "train loss:2.3010329825450446\n",
      "train loss:2.2917900562486277\n",
      "train loss:2.2991270746245367\n",
      "train loss:2.308946806626028\n",
      "train loss:2.2996375709771337\n",
      "train loss:2.31064160370831\n",
      "train loss:2.3025667227560893\n",
      "train loss:2.292270444086695\n",
      "train loss:2.3025031189067877\n",
      "train loss:2.3006380562636335\n",
      "train loss:2.2966597134744737\n",
      "train loss:2.303219513457732\n",
      "train loss:2.295757029484768\n",
      "train loss:2.2990115651627487\n",
      "train loss:2.299513455926675\n",
      "train loss:2.29875311962583\n",
      "train loss:2.2994287555216557\n",
      "train loss:2.2933125080983743\n",
      "train loss:2.307297133006481\n",
      "train loss:2.3090667811794843\n",
      "train loss:2.300785424326074\n",
      "train loss:2.2929186371367445\n",
      "train loss:2.2961086477763604\n",
      "train loss:2.2965965664959844\n",
      "train loss:2.3020326552106\n",
      "train loss:2.2968334673651984\n",
      "train loss:2.2989608791724434\n",
      "train loss:2.3009203767036586\n",
      "train loss:2.307465952520588\n",
      "train loss:2.2963473700044905\n",
      "train loss:2.3093718792809534\n",
      "train loss:2.3073489700048553\n",
      "train loss:2.2973156971173614\n",
      "train loss:2.295575925778246\n",
      "train loss:2.2943832073340777\n",
      "train loss:2.2931164675383338\n",
      "train loss:2.288534851831534\n",
      "train loss:2.3097346911302434\n",
      "train loss:2.300249323350204\n",
      "train loss:2.3063265491423444\n",
      "train loss:2.299757431006272\n",
      "train loss:2.313821938292022\n",
      "train loss:2.30475647430083\n",
      "train loss:2.2955678910911113\n",
      "train loss:2.3089952521539523\n",
      "train loss:2.2975309068266987\n",
      "train loss:2.3054748850756397\n",
      "train loss:2.295315592770805\n",
      "train loss:2.296828520780567\n",
      "train loss:2.2927568519450676\n",
      "train loss:2.3016162132463105\n",
      "train loss:2.3065504678448283\n",
      "train loss:2.3002418635316437\n",
      "train loss:2.3035475840362825\n",
      "train loss:2.2978006832684668\n",
      "train loss:2.306881958328027\n",
      "train loss:2.3006697095103346\n",
      "train loss:2.2992122893098106\n",
      "train loss:2.3012910071069217\n",
      "train loss:2.2986119528251376\n",
      "train loss:2.299876724554184\n",
      "train loss:2.3018032077537125\n",
      "train loss:2.2920866387905803\n",
      "train loss:2.2972985543627087\n",
      "train loss:2.2971590983723136\n",
      "train loss:2.2966670470496995\n",
      "train loss:2.303470668662084\n",
      "train loss:2.2916824201742787\n",
      "train loss:2.30287420163528\n",
      "train loss:2.305045767259401\n",
      "train loss:2.299283153828846\n",
      "train loss:2.3017722222974992\n",
      "train loss:2.3020203878729273\n",
      "train loss:2.3101656293762836\n",
      "train loss:2.306317248974541\n",
      "train loss:2.2947611639608074\n",
      "train loss:2.303922763214131\n",
      "train loss:2.3008815942396694\n",
      "train loss:2.3061111731685084\n",
      "train loss:2.2977338853939\n",
      "train loss:2.2967719438747496\n",
      "train loss:2.2865356990580206\n",
      "train loss:2.3026862327664386\n",
      "train loss:2.2897345800006073\n",
      "train loss:2.3103617113190884\n",
      "train loss:2.304903595105177\n",
      "train loss:2.296399215973629\n",
      "train loss:2.2931592752723247\n",
      "train loss:2.3114549766334753\n",
      "train loss:2.2949512553988467\n",
      "=== epoch:126, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3112365416090657\n",
      "train loss:2.3098564706991698\n",
      "train loss:2.3046310104296945\n",
      "train loss:2.2903818830651628\n",
      "train loss:2.299463334885983\n",
      "train loss:2.3070925074435573\n",
      "train loss:2.3022528088017773\n",
      "train loss:2.304018707914483\n",
      "train loss:2.307012565568193\n",
      "train loss:2.3056834584755665\n",
      "train loss:2.304426971655893\n",
      "train loss:2.298101983335974\n",
      "train loss:2.307281423760982\n",
      "train loss:2.3065321024351357\n",
      "train loss:2.306045701792867\n",
      "train loss:2.302863975085605\n",
      "train loss:2.293939660314381\n",
      "train loss:2.2946533564470406\n",
      "train loss:2.3042694636633434\n",
      "train loss:2.3025270242397937\n",
      "train loss:2.2961957287606474\n",
      "train loss:2.310984545135515\n",
      "train loss:2.302531221753425\n",
      "train loss:2.3127189054428596\n",
      "train loss:2.304996850390553\n",
      "train loss:2.3050378039231227\n",
      "train loss:2.2972295361878\n",
      "train loss:2.3032548408222633\n",
      "train loss:2.3040649918946556\n",
      "train loss:2.290820014634377\n",
      "train loss:2.301442970839987\n",
      "train loss:2.3008085206657087\n",
      "train loss:2.3076165216581863\n",
      "train loss:2.301253083700577\n",
      "train loss:2.302294235579534\n",
      "train loss:2.2913322972395576\n",
      "train loss:2.2924204368820935\n",
      "train loss:2.3051660115019263\n",
      "train loss:2.2998015999132417\n",
      "train loss:2.312704457221727\n",
      "train loss:2.28949555133443\n",
      "train loss:2.3052875646053637\n",
      "train loss:2.2949486469959117\n",
      "train loss:2.292117101586055\n",
      "train loss:2.3022395189896923\n",
      "train loss:2.2983245518283146\n",
      "train loss:2.300291162959531\n",
      "train loss:2.292685233929248\n",
      "train loss:2.302777453843976\n",
      "train loss:2.305260695206215\n",
      "train loss:2.302303535750929\n",
      "train loss:2.303716354358531\n",
      "train loss:2.3064109921602483\n",
      "train loss:2.3025455947596156\n",
      "train loss:2.300961820353114\n",
      "train loss:2.297441610902365\n",
      "train loss:2.3075516491349677\n",
      "train loss:2.2948895970705205\n",
      "train loss:2.3058933489397817\n",
      "train loss:2.3036421515544183\n",
      "train loss:2.3026283531508818\n",
      "train loss:2.306852594140197\n",
      "train loss:2.2912775319759344\n",
      "train loss:2.304465222679525\n",
      "train loss:2.304463534498703\n",
      "train loss:2.2965688518041985\n",
      "train loss:2.2918032129703017\n",
      "train loss:2.289004348996108\n",
      "train loss:2.3155099359503346\n",
      "train loss:2.3038042586742553\n",
      "train loss:2.3024006487774793\n",
      "train loss:2.29974122650853\n",
      "train loss:2.305726735741675\n",
      "train loss:2.30354512807245\n",
      "train loss:2.29845897086418\n",
      "train loss:2.307211773602111\n",
      "train loss:2.295903883636946\n",
      "train loss:2.294696832553598\n",
      "train loss:2.2999991115346328\n",
      "train loss:2.2966567261286475\n",
      "train loss:2.298682621214511\n",
      "train loss:2.2983119796979365\n",
      "train loss:2.289673092133553\n",
      "train loss:2.3026740218143398\n",
      "train loss:2.3037217667803525\n",
      "train loss:2.2989832742553893\n",
      "train loss:2.3025354035235934\n",
      "train loss:2.305108119672826\n",
      "train loss:2.302050612090629\n",
      "train loss:2.2927383439384332\n",
      "train loss:2.3069436294032037\n",
      "train loss:2.3016051700871802\n",
      "train loss:2.301344008125405\n",
      "train loss:2.3091216590231225\n",
      "train loss:2.2976131849699835\n",
      "train loss:2.2880188686413776\n",
      "train loss:2.291025080081855\n",
      "train loss:2.299135490605398\n",
      "train loss:2.305327845221603\n",
      "train loss:2.296757258088788\n",
      "train loss:2.2930555028080177\n",
      "train loss:2.2975546013501225\n",
      "train loss:2.300283958346624\n",
      "train loss:2.290948031183175\n",
      "train loss:2.3018896725699345\n",
      "train loss:2.2939333314095482\n",
      "train loss:2.3090508054116365\n",
      "train loss:2.2940823160054014\n",
      "train loss:2.291261791052681\n",
      "train loss:2.293679101956546\n",
      "train loss:2.3004754724490235\n",
      "train loss:2.2828712040153283\n",
      "train loss:2.308695873556324\n",
      "train loss:2.3041114577551283\n",
      "train loss:2.2953136706160997\n",
      "train loss:2.295081024114606\n",
      "train loss:2.308003181345495\n",
      "train loss:2.303794333557142\n",
      "train loss:2.3017360978521184\n",
      "train loss:2.293759586930151\n",
      "train loss:2.3073900212818934\n",
      "train loss:2.2988081811431895\n",
      "train loss:2.3065484711597697\n",
      "train loss:2.299606177701233\n",
      "train loss:2.3000569930678565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3018201798555404\n",
      "train loss:2.301540928776316\n",
      "train loss:2.300683640358422\n",
      "train loss:2.3035935931715854\n",
      "train loss:2.3029530705077845\n",
      "train loss:2.3022575339217277\n",
      "train loss:2.306043822535334\n",
      "train loss:2.2956633673424016\n",
      "train loss:2.299823500174259\n",
      "train loss:2.3022127887413624\n",
      "train loss:2.2983541153734106\n",
      "train loss:2.29818355301033\n",
      "train loss:2.298486845358583\n",
      "train loss:2.3101061936327554\n",
      "train loss:2.295986998101791\n",
      "train loss:2.3065285103961903\n",
      "train loss:2.3025931423262542\n",
      "train loss:2.291616509196404\n",
      "train loss:2.305511776128287\n",
      "train loss:2.304687319775407\n",
      "train loss:2.3012647199736644\n",
      "train loss:2.316190808078141\n",
      "train loss:2.2962214504436878\n",
      "train loss:2.3102448980497474\n",
      "train loss:2.310144655582605\n",
      "train loss:2.3034278502053285\n",
      "train loss:2.30140452561859\n",
      "train loss:2.296648375126368\n",
      "train loss:2.3070514691633526\n",
      "train loss:2.3047156561905195\n",
      "train loss:2.308644333718522\n",
      "train loss:2.303790969134405\n",
      "train loss:2.301908929866188\n",
      "train loss:2.298574539690303\n",
      "train loss:2.3041899862909543\n",
      "train loss:2.301929642955689\n",
      "train loss:2.3029812268736327\n",
      "train loss:2.297344452751564\n",
      "train loss:2.3009537136856313\n",
      "train loss:2.306923706472518\n",
      "train loss:2.2951868565730114\n",
      "train loss:2.3047938744410486\n",
      "train loss:2.3062796528179184\n",
      "train loss:2.312166482312296\n",
      "train loss:2.3062990023597423\n",
      "train loss:2.305667373729051\n",
      "train loss:2.2992870652279374\n",
      "train loss:2.3050975981266237\n",
      "train loss:2.2965155821746706\n",
      "train loss:2.298941392747357\n",
      "train loss:2.2976450637230665\n",
      "train loss:2.304112223164951\n",
      "train loss:2.290268106661867\n",
      "train loss:2.294701569951854\n",
      "train loss:2.2897127107307806\n",
      "train loss:2.292655105937043\n",
      "train loss:2.31424349350816\n",
      "train loss:2.3002220778818723\n",
      "train loss:2.310314514223492\n",
      "train loss:2.3091419438690552\n",
      "train loss:2.305574721785635\n",
      "train loss:2.2920043372855603\n",
      "train loss:2.3049891544662926\n",
      "train loss:2.2989337173059536\n",
      "train loss:2.2948115753525444\n",
      "train loss:2.302207454150467\n",
      "train loss:2.2984577802500312\n",
      "train loss:2.3014635097910814\n",
      "train loss:2.2947841391592103\n",
      "train loss:2.308829438577594\n",
      "train loss:2.2995401966680733\n",
      "train loss:2.309395932850339\n",
      "train loss:2.307949100879813\n",
      "train loss:2.2970200654509094\n",
      "train loss:2.298614977863476\n",
      "=== epoch:127, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2890333959097195\n",
      "train loss:2.304081339519605\n",
      "train loss:2.301409879151152\n",
      "train loss:2.295102446845387\n",
      "train loss:2.289995516239138\n",
      "train loss:2.3049533781166733\n",
      "train loss:2.299673558445933\n",
      "train loss:2.292715854458872\n",
      "train loss:2.29753619639427\n",
      "train loss:2.308396394503374\n",
      "train loss:2.2929643127631616\n",
      "train loss:2.3020565245925027\n",
      "train loss:2.2891796143847793\n",
      "train loss:2.299277581218055\n",
      "train loss:2.298709904790057\n",
      "train loss:2.2962625689160094\n",
      "train loss:2.305377179608087\n",
      "train loss:2.302180694072228\n",
      "train loss:2.298498769684527\n",
      "train loss:2.2934576316194093\n",
      "train loss:2.3111324441285364\n",
      "train loss:2.3100515616905954\n",
      "train loss:2.3005451998202773\n",
      "train loss:2.303719594027817\n",
      "train loss:2.292219020588503\n",
      "train loss:2.3121943172535566\n",
      "train loss:2.2947820452585983\n",
      "train loss:2.299279986749292\n",
      "train loss:2.2943411040536286\n",
      "train loss:2.304513234809923\n",
      "train loss:2.298963512598444\n",
      "train loss:2.2997085816790417\n",
      "train loss:2.2959803504314986\n",
      "train loss:2.30569543170133\n",
      "train loss:2.2954420541012195\n",
      "train loss:2.29852095551488\n",
      "train loss:2.310598637198919\n",
      "train loss:2.3005408722259744\n",
      "train loss:2.3001015222513956\n",
      "train loss:2.300878837713078\n",
      "train loss:2.3018934857190447\n",
      "train loss:2.305936440494877\n",
      "train loss:2.299072705385288\n",
      "train loss:2.3047602580839754\n",
      "train loss:2.2948430547766248\n",
      "train loss:2.3084633614842676\n",
      "train loss:2.311905040344068\n",
      "train loss:2.3037625690959684\n",
      "train loss:2.308613818792861\n",
      "train loss:2.300810259042459\n",
      "train loss:2.297999325078485\n",
      "train loss:2.2946167544587537\n",
      "train loss:2.299690168856516\n",
      "train loss:2.291082049075312\n",
      "train loss:2.302888534750523\n",
      "train loss:2.3102751204667267\n",
      "train loss:2.293071397680666\n",
      "train loss:2.3171016693516875\n",
      "train loss:2.2955336379128592\n",
      "train loss:2.296485150266915\n",
      "train loss:2.305277290653963\n",
      "train loss:2.2999686724779362\n",
      "train loss:2.3050432018631533\n",
      "train loss:2.2971326423442666\n",
      "train loss:2.2989649180946583\n",
      "train loss:2.288157701745916\n",
      "train loss:2.29778444116831\n",
      "train loss:2.303993006422127\n",
      "train loss:2.298092116635717\n",
      "train loss:2.307537454321911\n",
      "train loss:2.2856995355747185\n",
      "train loss:2.3121029330997187\n",
      "train loss:2.3020199848369383\n",
      "train loss:2.2938432442817493\n",
      "train loss:2.3041028146739926\n",
      "train loss:2.294505066977401\n",
      "train loss:2.297943882670874\n",
      "train loss:2.3050203874082245\n",
      "train loss:2.3036231998337486\n",
      "train loss:2.3040559045128877\n",
      "train loss:2.310803904959665\n",
      "train loss:2.3085023815919175\n",
      "train loss:2.3044108976963007\n",
      "train loss:2.295208038541259\n",
      "train loss:2.3004357634265578\n",
      "train loss:2.31065770894583\n",
      "train loss:2.307203185618256\n",
      "train loss:2.3075298753249047\n",
      "train loss:2.2920083402667113\n",
      "train loss:2.2985613742565865\n",
      "train loss:2.303847388487249\n",
      "train loss:2.305668028082816\n",
      "train loss:2.301140629747288\n",
      "train loss:2.3064213180923634\n",
      "train loss:2.3022081582800706\n",
      "train loss:2.3006367098149987\n",
      "train loss:2.301109071342595\n",
      "train loss:2.302297197099568\n",
      "train loss:2.3024790635701997\n",
      "train loss:2.297530058686205\n",
      "train loss:2.2948473002186303\n",
      "train loss:2.305311625637273\n",
      "train loss:2.3032357082875285\n",
      "train loss:2.2943872563522634\n",
      "train loss:2.3054782655508266\n",
      "train loss:2.300677240895602\n",
      "train loss:2.3114140412143906\n",
      "train loss:2.2973186482956347\n",
      "train loss:2.303787878550786\n",
      "train loss:2.301678364392335\n",
      "train loss:2.306563776843169\n",
      "train loss:2.3044129442341084\n",
      "train loss:2.304576219832822\n",
      "train loss:2.3079525468959354\n",
      "train loss:2.2974687381388343\n",
      "train loss:2.299842110765929\n",
      "train loss:2.3016077763661484\n",
      "train loss:2.29705438050384\n",
      "train loss:2.295792397148538\n",
      "train loss:2.2967449597505816\n",
      "train loss:2.292642564651558\n",
      "train loss:2.2980278708236623\n",
      "train loss:2.3064804871540265\n",
      "train loss:2.296707847802518\n",
      "train loss:2.3036564997944926\n",
      "train loss:2.3051689757883134\n",
      "train loss:2.29833854130797\n",
      "train loss:2.3068079166676427\n",
      "train loss:2.29403165379527\n",
      "train loss:2.303533346687458\n",
      "train loss:2.2916027157492853\n",
      "train loss:2.3002594636394047\n",
      "train loss:2.3047763864721658\n",
      "train loss:2.2957497162519602\n",
      "train loss:2.302349632312376\n",
      "train loss:2.3087049042996615\n",
      "train loss:2.3040227246084313\n",
      "train loss:2.2923487246065988\n",
      "train loss:2.301416845979543\n",
      "train loss:2.3119713658075574\n",
      "train loss:2.291116842059474\n",
      "train loss:2.307542651042976\n",
      "train loss:2.298433387074953\n",
      "train loss:2.3058613759196946\n",
      "train loss:2.3117048583427247\n",
      "train loss:2.2984170652108653\n",
      "train loss:2.3007802627630087\n",
      "train loss:2.288712289446336\n",
      "train loss:2.2973416200738854\n",
      "train loss:2.3029318350872785\n",
      "train loss:2.3017326071520854\n",
      "train loss:2.3058413126184463\n",
      "train loss:2.3005376003694504\n",
      "train loss:2.3088623093060225\n",
      "train loss:2.3063657916286338\n",
      "train loss:2.3022579494304134\n",
      "train loss:2.3089877563268635\n",
      "train loss:2.3059527523478436\n",
      "train loss:2.3060446640869783\n",
      "train loss:2.3023283082223114\n",
      "train loss:2.3020212063456498\n",
      "train loss:2.297516882004562\n",
      "train loss:2.3152839028346253\n",
      "train loss:2.3021874058593186\n",
      "train loss:2.294866313413848\n",
      "train loss:2.2985456238199107\n",
      "train loss:2.3002472782963723\n",
      "train loss:2.290184090174629\n",
      "train loss:2.2996884990664626\n",
      "train loss:2.305538485045819\n",
      "train loss:2.303439415353099\n",
      "train loss:2.301032745114588\n",
      "train loss:2.3099229128721945\n",
      "train loss:2.3025775070020593\n",
      "train loss:2.3087982206654343\n",
      "train loss:2.2998680058080456\n",
      "train loss:2.304297916208725\n",
      "train loss:2.302978960097445\n",
      "train loss:2.3030122391560885\n",
      "train loss:2.2972196401825493\n",
      "train loss:2.303853749528059\n",
      "train loss:2.291179146137736\n",
      "train loss:2.3095411715310044\n",
      "train loss:2.3040060911983264\n",
      "train loss:2.298859157573463\n",
      "train loss:2.3049177228857025\n",
      "train loss:2.301805114635317\n",
      "train loss:2.304449277472616\n",
      "train loss:2.3061386488088926\n",
      "train loss:2.2943898082541194\n",
      "train loss:2.301383703191835\n",
      "train loss:2.3001416791567975\n",
      "train loss:2.2997573088207037\n",
      "train loss:2.297646549829473\n",
      "train loss:2.3009918768346385\n",
      "train loss:2.3045080607083452\n",
      "train loss:2.309048856045974\n",
      "train loss:2.3051813687795497\n",
      "train loss:2.3043000161382503\n",
      "train loss:2.3017270343491476\n",
      "=== epoch:128, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.2964160585869906\n",
      "train loss:2.292012693484906\n",
      "train loss:2.314611385313603\n",
      "train loss:2.292749405606483\n",
      "train loss:2.300172448482105\n",
      "train loss:2.3025302030758756\n",
      "train loss:2.295482551206859\n",
      "train loss:2.3043788717225095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2984385328462684\n",
      "train loss:2.312633498548154\n",
      "train loss:2.291783532476499\n",
      "train loss:2.3020878238333515\n",
      "train loss:2.3067172912215006\n",
      "train loss:2.311921312603016\n",
      "train loss:2.3054247540452493\n",
      "train loss:2.3171365562408623\n",
      "train loss:2.303590702175468\n",
      "train loss:2.305377297543078\n",
      "train loss:2.2953075178803672\n",
      "train loss:2.2997496909538877\n",
      "train loss:2.2952179547549996\n",
      "train loss:2.3048343544224217\n",
      "train loss:2.300954885528102\n",
      "train loss:2.295120385682561\n",
      "train loss:2.3048054924213264\n",
      "train loss:2.307655719246176\n",
      "train loss:2.296295821776873\n",
      "train loss:2.29248897542901\n",
      "train loss:2.3058874677295433\n",
      "train loss:2.2948788843044188\n",
      "train loss:2.300638406036991\n",
      "train loss:2.3019662819999507\n",
      "train loss:2.2956796251745804\n",
      "train loss:2.307274239990841\n",
      "train loss:2.303541438251066\n",
      "train loss:2.3060555928260045\n",
      "train loss:2.2959791402660947\n",
      "train loss:2.3111287567012058\n",
      "train loss:2.3090352897151054\n",
      "train loss:2.2967936150128065\n",
      "train loss:2.290404000985747\n",
      "train loss:2.304292673155848\n",
      "train loss:2.2902449771463798\n",
      "train loss:2.307220917481372\n",
      "train loss:2.2998767432381912\n",
      "train loss:2.290113402285709\n",
      "train loss:2.30541795158172\n",
      "train loss:2.3129298349450726\n",
      "train loss:2.2970920380627353\n",
      "train loss:2.2995146182615014\n",
      "train loss:2.3065118447583552\n",
      "train loss:2.307899739820095\n",
      "train loss:2.299586524530475\n",
      "train loss:2.2958717128667594\n",
      "train loss:2.3049525290220787\n",
      "train loss:2.305006499593463\n",
      "train loss:2.3016648074538444\n",
      "train loss:2.3001379043099934\n",
      "train loss:2.3119253441134338\n",
      "train loss:2.300734614196926\n",
      "train loss:2.3051829417057705\n",
      "train loss:2.3001317026997605\n",
      "train loss:2.292336047840198\n",
      "train loss:2.29574069552314\n",
      "train loss:2.30860168428394\n",
      "train loss:2.3018642179764703\n",
      "train loss:2.296462719546829\n",
      "train loss:2.2965473938252305\n",
      "train loss:2.293178526431074\n",
      "train loss:2.288671625832119\n",
      "train loss:2.2952615278248243\n",
      "train loss:2.308524589901463\n",
      "train loss:2.2963919041859553\n",
      "train loss:2.300685158552043\n",
      "train loss:2.298108655639322\n",
      "train loss:2.3015269872643827\n",
      "train loss:2.3013361288242633\n",
      "train loss:2.301761571416939\n",
      "train loss:2.2993052385111525\n",
      "train loss:2.3003238500948164\n",
      "train loss:2.2995409232949644\n",
      "train loss:2.298242050960754\n",
      "train loss:2.30743427662971\n",
      "train loss:2.2944020356834027\n",
      "train loss:2.2973193843712734\n",
      "train loss:2.3048180910998513\n",
      "train loss:2.295141765648269\n",
      "train loss:2.3010729494374402\n",
      "train loss:2.3043004816751598\n",
      "train loss:2.2912543027549526\n",
      "train loss:2.3024584510476127\n",
      "train loss:2.310629119808289\n",
      "train loss:2.2942145878972404\n",
      "train loss:2.312406584558904\n",
      "train loss:2.2936847791614863\n",
      "train loss:2.2940844149514796\n",
      "train loss:2.3044352171387015\n",
      "train loss:2.296807551019341\n",
      "train loss:2.297275126859341\n",
      "train loss:2.296099692926036\n",
      "train loss:2.2949473128810363\n",
      "train loss:2.3069184440695674\n",
      "train loss:2.3051002568512198\n",
      "train loss:2.304334505454066\n",
      "train loss:2.2958495923473916\n",
      "train loss:2.3048142036489865\n",
      "train loss:2.2969476391435024\n",
      "train loss:2.303390027713454\n",
      "train loss:2.2965698685181657\n",
      "train loss:2.298562845600355\n",
      "train loss:2.306436616089398\n",
      "train loss:2.294853821033556\n",
      "train loss:2.2936843756075183\n",
      "train loss:2.3017565427136883\n",
      "train loss:2.299878557956355\n",
      "train loss:2.302245612793917\n",
      "train loss:2.3006292457837065\n",
      "train loss:2.3086657952421334\n",
      "train loss:2.310922593497974\n",
      "train loss:2.2918191072424854\n",
      "train loss:2.309558778641143\n",
      "train loss:2.285860158388723\n",
      "train loss:2.302175902991372\n",
      "train loss:2.3037810740438904\n",
      "train loss:2.2985212163697883\n",
      "train loss:2.3044447639069596\n",
      "train loss:2.300633151804435\n",
      "train loss:2.309192825104903\n",
      "train loss:2.296144355325622\n",
      "train loss:2.3020728113512563\n",
      "train loss:2.2976225985632595\n",
      "train loss:2.2990731319186377\n",
      "train loss:2.315717932078778\n",
      "train loss:2.3034304857395185\n",
      "train loss:2.3017431906132857\n",
      "train loss:2.294608387229843\n",
      "train loss:2.3087855339674013\n",
      "train loss:2.3026287991351286\n",
      "train loss:2.2975437014628914\n",
      "train loss:2.30239496597171\n",
      "train loss:2.301464642564828\n",
      "train loss:2.29527085729009\n",
      "train loss:2.2945030211861908\n",
      "train loss:2.302809967270276\n",
      "train loss:2.296569774732509\n",
      "train loss:2.300075971610337\n",
      "train loss:2.2949051669382254\n",
      "train loss:2.2988986982415813\n",
      "train loss:2.2864606261768987\n",
      "train loss:2.301466640722215\n",
      "train loss:2.2875657211875264\n",
      "train loss:2.302240044362552\n",
      "train loss:2.301439492833082\n",
      "train loss:2.298948593096925\n",
      "train loss:2.3096140811346832\n",
      "train loss:2.2958614826264614\n",
      "train loss:2.300417096597658\n",
      "train loss:2.3005899952629223\n",
      "train loss:2.297879223025972\n",
      "train loss:2.30433683373241\n",
      "train loss:2.29385383412384\n",
      "train loss:2.311517462041994\n",
      "train loss:2.304312308738992\n",
      "train loss:2.3072160526438585\n",
      "train loss:2.3077994887660416\n",
      "train loss:2.305006868215239\n",
      "train loss:2.2919900907158937\n",
      "train loss:2.2970598548129164\n",
      "train loss:2.2995174860118124\n",
      "train loss:2.308245900848801\n",
      "train loss:2.3110458126907503\n",
      "train loss:2.299316366722504\n",
      "train loss:2.3058655391665557\n",
      "train loss:2.303641779243321\n",
      "train loss:2.300207395701521\n",
      "train loss:2.300726268427001\n",
      "train loss:2.3028863747259942\n",
      "train loss:2.3004054159253777\n",
      "train loss:2.2869064009950644\n",
      "train loss:2.299568738459044\n",
      "train loss:2.312237102417571\n",
      "train loss:2.2984200069160776\n",
      "train loss:2.311670847778633\n",
      "train loss:2.2951872067799735\n",
      "train loss:2.301146806597635\n",
      "train loss:2.30582173737618\n",
      "train loss:2.298520988980904\n",
      "train loss:2.300032357066583\n",
      "train loss:2.308039810111952\n",
      "train loss:2.3042349973925003\n",
      "train loss:2.307064980873805\n",
      "train loss:2.298749894191362\n",
      "train loss:2.307665035250808\n",
      "train loss:2.308738249653155\n",
      "train loss:2.298459792476787\n",
      "train loss:2.297967931765071\n",
      "train loss:2.2962131419949228\n",
      "train loss:2.294727114436492\n",
      "train loss:2.3024435487504014\n",
      "train loss:2.314483876275283\n",
      "=== epoch:129, train acc:0.11185, test acc:0.1135 ===\n",
      "train loss:2.3026793611925775\n",
      "train loss:2.3013838247045637\n",
      "train loss:2.3013041014468185\n",
      "train loss:2.2991553577257906\n",
      "train loss:2.302977822013819\n",
      "train loss:2.284779316027235\n",
      "train loss:2.3000065600279602\n",
      "train loss:2.3065662201321726\n",
      "train loss:2.2914977299652435\n",
      "train loss:2.299411773528931\n",
      "train loss:2.298569191918267\n",
      "train loss:2.3097698593766647\n",
      "train loss:2.295574212300154\n",
      "train loss:2.3000440290662025\n",
      "train loss:2.302500313306001\n",
      "train loss:2.309194573697633\n",
      "train loss:2.302305656853393\n",
      "train loss:2.303942966672641\n",
      "train loss:2.309496844993635\n",
      "train loss:2.303612409166715\n",
      "train loss:2.296142877122724\n",
      "train loss:2.306781161498228\n",
      "train loss:2.2965270594103315\n",
      "train loss:2.3113012899700145\n",
      "train loss:2.297299392841866\n",
      "train loss:2.300685436137369\n",
      "train loss:2.3015155335273048\n",
      "train loss:2.3083133856106657\n",
      "train loss:2.3061320978209143\n",
      "train loss:2.297761505691157\n",
      "train loss:2.3010236842032827\n",
      "train loss:2.3021551855166154\n",
      "train loss:2.2926047095761506\n",
      "train loss:2.305465260156219\n",
      "train loss:2.3054169947420835\n",
      "train loss:2.300475067087017\n",
      "train loss:2.3092159407223978\n",
      "train loss:2.3070520290307974\n",
      "train loss:2.3034108034644674\n",
      "train loss:2.301198899604543\n",
      "train loss:2.3033072030174258\n",
      "train loss:2.2990528677737285\n",
      "train loss:2.2937998494507736\n",
      "train loss:2.3017115865148683\n",
      "train loss:2.290049441005716\n",
      "train loss:2.2987393299113967\n",
      "train loss:2.3023501967636153\n",
      "train loss:2.296382832909486\n",
      "train loss:2.2987188935226217\n",
      "train loss:2.3066688888684777\n",
      "train loss:2.3002469587105483\n",
      "train loss:2.2956522762650335\n",
      "train loss:2.302423145846064\n",
      "train loss:2.3044004755395506\n",
      "train loss:2.30183091032132\n",
      "train loss:2.305680712605921\n",
      "train loss:2.3048558738307134\n",
      "train loss:2.300138198468605\n",
      "train loss:2.298008075175161\n",
      "train loss:2.2973769195141474\n",
      "train loss:2.2973359045910913\n",
      "train loss:2.304986003717702\n",
      "train loss:2.301029261402924\n",
      "train loss:2.2957382410546217\n",
      "train loss:2.3013412153945216\n",
      "train loss:2.296622804050673\n",
      "train loss:2.302072984310243\n",
      "train loss:2.3036800027971616\n",
      "train loss:2.311224005907153\n",
      "train loss:2.2968280997568615\n",
      "train loss:2.2903418416182193\n",
      "train loss:2.3066278708433945\n",
      "train loss:2.2978585062534433\n",
      "train loss:2.3076450561383104\n",
      "train loss:2.299264037802796\n",
      "train loss:2.2960021902213055\n",
      "train loss:2.29952350359971\n",
      "train loss:2.2978854347727973\n",
      "train loss:2.3048749394755066\n",
      "train loss:2.295824280042301\n",
      "train loss:2.29801265403966\n",
      "train loss:2.3091069594183455\n",
      "train loss:2.2957817667291724\n",
      "train loss:2.2926913581686077\n",
      "train loss:2.29352587941428\n",
      "train loss:2.2998072949940283\n",
      "train loss:2.3036911190672695\n",
      "train loss:2.298575033367271\n",
      "train loss:2.305484105876636\n",
      "train loss:2.298759661529601\n",
      "train loss:2.303125990933815\n",
      "train loss:2.2983428343895373\n",
      "train loss:2.3046072387518417\n",
      "train loss:2.3040870618969684\n",
      "train loss:2.30032746294519\n",
      "train loss:2.305407253077778\n",
      "train loss:2.30118554562422\n",
      "train loss:2.3056573313990762\n",
      "train loss:2.3048670197804584\n",
      "train loss:2.3017829774589154\n",
      "train loss:2.306960499627682\n",
      "train loss:2.291635715759642\n",
      "train loss:2.2941738346054046\n",
      "train loss:2.2979812409449476\n",
      "train loss:2.299310241896389\n",
      "train loss:2.3045298276517743\n",
      "train loss:2.3014607984911652\n",
      "train loss:2.2988821127092836\n",
      "train loss:2.2994608735016002\n",
      "train loss:2.2939279487409223\n",
      "train loss:2.290219539603107\n",
      "train loss:2.3035502532595946\n",
      "train loss:2.301710706660657\n",
      "train loss:2.2914196115692578\n",
      "train loss:2.3070986822060338\n",
      "train loss:2.3086033707925044\n",
      "train loss:2.29832543485252\n",
      "train loss:2.3014806569197006\n",
      "train loss:2.3045365149657613\n",
      "train loss:2.3038238113380842\n",
      "train loss:2.304486027279936\n",
      "train loss:2.305173515245462\n",
      "train loss:2.287202187800906\n",
      "train loss:2.3104695781982603\n",
      "train loss:2.297358870524245\n",
      "train loss:2.3029611084775263\n",
      "train loss:2.3020907161293853\n",
      "train loss:2.3048387655203024\n",
      "train loss:2.3086862017758514\n",
      "train loss:2.2937584537902747\n",
      "train loss:2.292908662336005\n",
      "train loss:2.2976113495123283\n",
      "train loss:2.2980740797182206\n",
      "train loss:2.3090499045429227\n",
      "train loss:2.2988704237618607\n",
      "train loss:2.3104445735006482\n",
      "train loss:2.297382957426311\n",
      "train loss:2.2943705819752163\n",
      "train loss:2.3014142074026807\n",
      "train loss:2.293595422767185\n",
      "train loss:2.298453302561864\n",
      "train loss:2.305467830173949\n",
      "train loss:2.3120866278107135\n",
      "train loss:2.3065847839375593\n",
      "train loss:2.307424784323708\n",
      "train loss:2.2980730798083293\n",
      "train loss:2.303324177609569\n",
      "train loss:2.2980108110867628\n",
      "train loss:2.2958715239664524\n",
      "train loss:2.30109354496342\n",
      "train loss:2.299318863589924\n",
      "train loss:2.3013145207204686\n",
      "train loss:2.305395010500146\n",
      "train loss:2.2942174496561023\n",
      "train loss:2.2995088824632357\n",
      "train loss:2.3036971150882444\n",
      "train loss:2.3097362384434583\n",
      "train loss:2.30521879743534\n",
      "train loss:2.2997779970596834\n",
      "train loss:2.305254449667605\n",
      "train loss:2.291764523108821\n",
      "train loss:2.301528660324346\n",
      "train loss:2.3119177993072832\n",
      "train loss:2.300016916318876\n",
      "train loss:2.3093872846575736\n",
      "train loss:2.2911621995474776\n",
      "train loss:2.297654023605075\n",
      "train loss:2.2913064203581417\n",
      "train loss:2.2967611344151577\n",
      "train loss:2.311260284441161\n",
      "train loss:2.302838075082318\n",
      "train loss:2.298338638548607\n",
      "train loss:2.290930888995704\n",
      "train loss:2.3023241011533035\n",
      "train loss:2.291412649939939\n",
      "train loss:2.299767702018371\n",
      "train loss:2.306447629249809\n",
      "train loss:2.3007037912003434\n",
      "train loss:2.2985582297728135\n",
      "train loss:2.3066264842335915\n",
      "train loss:2.3128668971906787\n",
      "train loss:2.304018731401153\n",
      "train loss:2.291788994895106\n",
      "train loss:2.3080897586410476\n",
      "train loss:2.302571243563857\n",
      "train loss:2.300843389095189\n",
      "train loss:2.3043853502635523\n",
      "train loss:2.30203603772482\n",
      "train loss:2.301430880646628\n",
      "train loss:2.2991201476307643\n",
      "train loss:2.3027772774764093\n",
      "train loss:2.3011911414011945\n",
      "train loss:2.3020589415930646\n",
      "train loss:2.302589897365225\n",
      "train loss:2.305257778124145\n",
      "train loss:2.2964513125392636\n",
      "train loss:2.306054040585808\n",
      "train loss:2.3028051090368806\n",
      "train loss:2.305917909019977\n",
      "train loss:2.3033587589511852\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.2 # 드롭아웃 비율\n",
    "weight_decay_lambda = 0.1 # 가중치 감쇠\n",
    "\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio,\n",
    "                             weight_decay_lambda = weight_decay_lambda)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf5b40",
   "metadata": {},
   "source": [
    "데이터 개수가 적어서인지 가중치 감쇠와 드롭 아웃을 동시에 적용하면 효과적인 성능 수치가 나오지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4326d",
   "metadata": {},
   "source": [
    "### 검증셋 분리와 오버피팅 방지방법 결합해보기\n",
    "- 학습률 0.01\n",
    "- 드롭아웃 사용\n",
    "- 가중치 감쇠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b16737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe94ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    if x.ndim == 2:\n",
    "        x = x[permutation,:]\n",
    "    else:\n",
    "        x = x[permutation,:,:,:]\n",
    "\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d066f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 훈련 데이터 뒤섞기\n",
    "\n",
    "x_train,t_train = shuffle_dataset(x_train,t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e71d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# 검증셋 분리하기\n",
    "valid_rate = 0.2\n",
    "\n",
    "valid_num = int(x_train.shape[0] * valid_rate)\n",
    "\n",
    "print(valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4b8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:valid_num]\n",
    "\n",
    "x_train = x_train[valid_num:]\n",
    "\n",
    "t_val = t_train[:valid_num]\n",
    "\n",
    "t_train = t_train[valid_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1e680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋 데이터 개수  48000\n",
      "검증셋 데이터 개수  48000\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련셋 데이터 개수 \",len(x_train))\n",
    "print(\"검증셋 데이터 개수 \",len(t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 사용 유무와 비율 설정\n",
    "use_dropout = True  # 드롭아웃 사용여부\n",
    "dropout_ratio = 0.1 # 드롭아웃 비율\n",
    "weight_decay_lambda = 0.1 # 가중치 감쇠\n",
    "\n",
    "\n",
    "# 학습진행\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio,\n",
    "                             weight_decay_lambda = weight_decay_lambda)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
