{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f06fa8e",
   "metadata": {},
   "source": [
    "# 학습 알고리즘 구현하기\n",
    "- 미니배치\n",
    "- 기울기 산출\n",
    "- 매개변수 갱신\n",
    "- 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15675b62",
   "metadata": {},
   "source": [
    "확률적 경사하강법(SGD : Stochastic Gradient Descent)의 구현 : 확률적이라고 함은 미니배치를 의미하는데 확률적으로 무작위로 골라낸 데이터에 대해서 수행하는 경사하강법이라는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad5143",
   "metadata": {},
   "source": [
    "넘파이의 축(axis) 개념 이해하기 : https://pybasall.tistory.com/129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c544eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c9dea",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48934af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
    "        self.params[\"b1\"] = weight_init_std * np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b2\"] = weight_init_std * np.zeros(output_size)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1, W2 = self.params[\"W1\"],self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"],self.params[\"b2\"]\n",
    "        \n",
    "        a1 = np.dot(x,W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.armgax(t,axis=1)\n",
    "        # y와 t의 레이블이 일치하면 정답인 것\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        def loss_W(W):\n",
    "            return self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W,self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W,self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W,self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W,self.params[\"b2\"])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ced4771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784,hidden_size=100,output_size=10)\n",
    "print(net.params[\"W1\"].shape)\n",
    "print(net.params[\"b1\"].shape)\n",
    "print(net.params[\"W2\"].shape)\n",
    "print(net.params[\"b2\"].shape)\n",
    "# params 딕셔너리에 저장된 매개변수를 토대로 예측이 처리된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a7bef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100,784)\n",
    "y = net.predict(x) # 100장, 각 784개의 데이터 원소가 담긴 x에 대해서 예측을 진행할 수 있다.\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c8c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "print(grads[\"W1\"].shape)\n",
    "print(grads[\"b1\"].shape)\n",
    "print(grads[\"W2\"].shape)\n",
    "print(grads[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa54e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010674752415340549"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[\"W1\"][0][0]\n",
    "# 입력층과 은닉층 사이의 매개변수 w11은 0.0001 이기에 손실함수 값을 증가시키기에 음의 방향으로 갱신시켜야 감소시킬 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ad181",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현하기\n",
    "- 훈련 데이터 중 일부 데이터를 무작위로 꺼내는 '미니배치'\n",
    "- 미니배치에 대해서 경사하강법으로 손실함수를 줄이는 방향으로 매개변수 개선해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb36f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_lst = []\n",
    "# 하이퍼파라미터 : 사람이 설정해야 하는 파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1 # 학습률\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num): # iter_nums 만큼 미니배치 학습을 진행\n",
    "    batch_idx = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_idx] ; t_batch = t_train[batch_idx]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch,t_batch)\n",
    "    \n",
    "    # W1 W2 b1 b2에 대해서 각각 경사하강법 적용\n",
    "    for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        # 매개변수 딕셔너리인 params를 갱신해줘야 한다.\n",
    "        # 그래야 loss 값이 갱신된다.\n",
    "        \n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_lst.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559db81f",
   "metadata": {},
   "source": [
    "epoch(에포크)마다 시험데이터로 평가하기\n",
    "- 위 코드와 거의 동일한데 시험데이터로 평가하는 것만 추가\n",
    "- epoch란? 1epoch는 학습에서 훈련데이터를 모두 소진한 횟수.\n",
    "- 1 batch_size 100으로 6만개의 데이터를 모두 소진하려면 600번 학습해야함\n",
    "- 이 경우에는 600번 학습하면 그것이 1epoch인 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f09d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_lst = []\n",
    "train_acc_lst = [] # 학습정확도\n",
    "test_acc_lst = [] # 시험정확도\n",
    "\n",
    "# 하이퍼파라미터 : 사람이 설정해야 하는 파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1 # 학습률\n",
    "\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num): # iter_nums 만큼 미니배치 학습을 진행\n",
    "    batch_idx = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_idx] ; t_batch = t_train[batch_idx]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch,t_batch)\n",
    "    \n",
    "    # W1 W2 b1 b2에 대해서 각각 경사하강법 적용\n",
    "    for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        # 매개변수 딕셔너리인 params를 갱신해줘야 한다.\n",
    "        # 그래야 loss 값이 갱신된다.\n",
    "        \n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_lst.append(loss)\n",
    "    \n",
    "    iter_per_epoch = max(train_size/batch_size,1) # 6만 / 100 -> 1epoch 즉, 학습데이터를 모두 소진시키려면 600번 반복\n",
    "    \n",
    "    # 1 epoch 당 정확도 계산 -> 1epoch 600번\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_batch,t_batch)\n",
    "        test_acc = network.accuracy(x_test,t_test) # 갱신된 매개변수에 시험데이터 적용\n",
    "        train_acc_lst.append(train_acc)\n",
    "        test_acc_lst.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc254af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[[1,2],[3,4]],[[3,4],[6,7]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6673f0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4],\n",
       "       [6, 7]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(a,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30aad1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  4],\n",
       "       [20, 16],\n",
       "       [20, 19]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a3 = np.array([[[7,1],[9,4],[2,3]],\n",
    "               [[4,1],[0,4],[8,0]],\n",
    "               [[1,1],[3,4],[6,9]],\n",
    "               [[5,1],[8,4],[4,7]]])\n",
    "np.sum(a3,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa493fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18,  8],\n",
       "       [12,  5],\n",
       "       [10, 14],\n",
       "       [17, 12]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a3,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecbbb460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8, 13,  5],\n",
       "       [ 5,  4,  8],\n",
       "       [ 2,  7, 15],\n",
       "       [ 6, 12, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a3,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750589e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
